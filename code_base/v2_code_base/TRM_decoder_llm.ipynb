{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Recursive Model (TRM) for Decoder-Only LLM & VLM\n",
    "\n",
    "This notebook implements the TRM (Tiny Recursive Model) concept from the paper \"Less is More: Recursive Reasoning with Tiny Networks\" adapted for:\n",
    "\n",
    "1. **Part 1**: A decoder-only language model using recursive reasoning\n",
    "2. **Part 2**: Evaluation on text generation tasks\n",
    "3. **Part 3**: Extension to Vision-Language Model (VLM) with contrastive alignment\n",
    "\n",
    "## Key TRM Concepts:\n",
    "- **Recursive Reasoning**: Instead of a deep network, use a tiny network that recurses multiple times\n",
    "- **Three State Variables**: Input `x`, current answer `y`, and latent reasoning state `z`\n",
    "- **Deep Supervision**: Apply loss at each recursion step\n",
    "- **Single Network**: Use one small network instead of multiple large ones\n",
    "\n",
    "## Architecture Overview:\n",
    "```\n",
    "For each recursion step:\n",
    "    z = net(x, y, z) + z  # Update reasoning state with residual\n",
    "    y = update_answer(z)   # Refine answer based on reasoning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers datasets tiktoken einops --quiet\n",
    "!pip install matplotlib seaborn tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: TRM Decoder-Only Language Model\n",
    "\n",
    "We implement a decoder-only transformer that uses TRM's recursive reasoning approach.\n",
    "\n",
    "## Key Adaptations for Causal LM:\n",
    "1. **Causal Attention**: Use masked attention for autoregressive generation\n",
    "2. **Recursive Refinement**: Apply TRM recursion to refine hidden states\n",
    "3. **Deep Supervision**: Compute loss at each recursion for better training signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Core Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRMDecoderConfig:\n",
    "    \"\"\"Configuration for TRM Decoder LLM\"\"\"\n",
    "    vocab_size: int = 32000\n",
    "    hidden_size: int = 256\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 2  # TRM uses tiny networks (2 layers as per paper)\n",
    "    max_seq_len: int = 512\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # TRM specific\n",
    "    n_recursions: int = 6  # Number of latent reasoning steps (n in paper)\n",
    "    t_cycles: int = 3  # Number of deep supervision cycles (T in paper)\n",
    "    \n",
    "    # Expansion factor for MLP\n",
    "    expansion: float = 2.67  # SwiGLU expansion\n",
    "    \n",
    "    # Normalization\n",
    "    rms_norm_eps: float = 1e-5\n",
    "    rope_theta: float = 10000.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.intermediate_size = int(self.hidden_size * self.expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "\n",
    "def rms_norm(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"Functional RMS normalization without learnable params\"\"\"\n",
    "    rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)\n",
    "    return x / rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Precompute inverse frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute cos and sin\n",
    "        self._build_cache(max_seq_len)\n",
    "    \n",
    "    def _build_cache(self, seq_len: int):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        self.register_buffer('cos_cached', emb.cos())\n",
    "        self.register_buffer('sin_cached', emb.sin())\n",
    "    \n",
    "    def forward(self, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len:\n",
    "            self._build_cache(seq_len)\n",
    "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
    "\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, \n",
    "                         cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Apply rotary embeddings to queries and keys.\"\"\"\n",
    "    # Reshape for broadcasting: (seq_len, dim) -> (1, seq_len, 1, dim)\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)  # (1, seq_len, 1, dim)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"SwiGLU activation function with linear projections\"\"\"\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention with RoPE\"\"\"\n",
    "    def __init__(self, config: TRMDecoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        \n",
    "        # QKV projection\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, \n",
    "                cos: torch.Tensor, sin: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, L, _ = hidden_states.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(hidden_states).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Attention scores\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Causal mask\n",
    "        if attention_mask is None:\n",
    "            causal_mask = torch.triu(torch.ones(L, L, device=hidden_states.device), diagonal=1).bool()\n",
    "            attn_weights = attn_weights.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        else:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, self.hidden_size)\n",
    "        \n",
    "        return self.o_proj(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 TRM Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single TRM transformer block.\n",
    "    Uses Post-Norm architecture as per the TRM paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TRMDecoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # Feed-forward (SwiGLU)\n",
    "        self.mlp = SwiGLU(config.hidden_size, config.intermediate_size)\n",
    "        \n",
    "        # Post-norm (as per TRM paper)\n",
    "        self.attn_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.mlp_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor,\n",
    "                cos: torch.Tensor, sin: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-attention with post-norm\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.self_attn(hidden_states, cos, sin, attention_mask)\n",
    "        hidden_states = self.attn_norm(residual + hidden_states)\n",
    "        \n",
    "        # MLP with post-norm\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = self.mlp_norm(residual + hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 TRM Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMReasoningModule(nn.Module):\n",
    "    \"\"\"\n",
    "    The core TRM reasoning module that applies recursive reasoning.\n",
    "    This module takes (x, y, z) and updates z through multiple transformer blocks,\n",
    "    then updates y based on the refined z.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TRMDecoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # The tiny network (2 layers as per TRM paper)\n",
    "        self.layers = nn.ModuleList([TRMBlock(config) for _ in range(config.num_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor, \n",
    "                input_injection: torch.Tensor,\n",
    "                cos: torch.Tensor, sin: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: Current latent state z (B, L, D)\n",
    "            input_injection: Combined input x + y embedding (B, L, D)\n",
    "            cos, sin: Rotary position embeddings\n",
    "            attention_mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Updated hidden states\n",
    "        \"\"\"\n",
    "        # Input injection (add)\n",
    "        hidden_states = hidden_states + input_injection\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, cos, sin, attention_mask)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Complete TRM Decoder LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMDecoderLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    TRM-based Decoder-Only Language Model.\n",
    "    \n",
    "    Key Features:\n",
    "    1. Recursive reasoning with a tiny network\n",
    "    2. Deep supervision at each recursion step\n",
    "    3. Single network for both z and y updates\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TRMDecoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        \n",
    "        # Rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=config.head_dim,\n",
    "            max_seq_len=config.max_seq_len,\n",
    "            base=config.rope_theta\n",
    "        )\n",
    "        \n",
    "        # TRM Reasoning Module (shared across recursions)\n",
    "        self.reasoning = TRMReasoningModule(config)\n",
    "        \n",
    "        # Output projection (LM head)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initial state for z (learnable)\n",
    "        self.z_init = nn.Parameter(torch.randn(config.hidden_size) * 0.02)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with truncated normal distribution\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "    \n",
    "    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get token embeddings scaled by sqrt(hidden_size)\"\"\"\n",
    "        return self.embed_scale * self.token_embedding(input_ids)\n",
    "    \n",
    "    def forward(self, \n",
    "                input_ids: torch.Tensor,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                return_all_logits: bool = False) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with TRM recursive reasoning.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (B, L)\n",
    "            labels: Target labels for language modeling loss (B, L)\n",
    "            attention_mask: Optional attention mask\n",
    "            return_all_logits: If True, return logits from all recursion steps\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with 'logits', 'loss' (if labels provided), \n",
    "            and 'all_logits' (if return_all_logits=True)\n",
    "        \"\"\"\n",
    "        B, L = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Get input embeddings (this is 'x' in TRM)\n",
    "        x_emb = self.get_input_embeddings(input_ids)  # (B, L, D)\n",
    "        x_emb = self.dropout(x_emb)\n",
    "        \n",
    "        # Get rotary embeddings\n",
    "        cos, sin = self.rotary_emb(L)\n",
    "        \n",
    "        # Initialize y (answer draft) as copy of x\n",
    "        y_emb = x_emb.clone()\n",
    "        \n",
    "        # Initialize z (reasoning state)\n",
    "        z = self.z_init.unsqueeze(0).unsqueeze(0).expand(B, L, -1)\n",
    "        \n",
    "        all_logits = []\n",
    "        \n",
    "        # TRM Recursive Reasoning Loop\n",
    "        # T cycles, each with n recursions\n",
    "        for t in range(self.config.t_cycles):\n",
    "            # Determine if we should track gradients\n",
    "            # Only the last cycle gets gradients (as per TRM paper)\n",
    "            use_grad = (t == self.config.t_cycles - 1)\n",
    "            \n",
    "            with torch.set_grad_enabled(use_grad or self.training):\n",
    "                # n recursions to update z\n",
    "                for n in range(self.config.n_recursions):\n",
    "                    # Update z given (x + y) and current z\n",
    "                    input_injection = x_emb + y_emb\n",
    "                    z = self.reasoning(z, input_injection, cos, sin, attention_mask)\n",
    "                \n",
    "                # Update y given z (one more pass through reasoning module)\n",
    "                z_for_y = self.reasoning(z, torch.zeros_like(z), cos, sin, attention_mask)\n",
    "                \n",
    "                # Compute logits for this cycle\n",
    "                logits = self.lm_head(z_for_y)\n",
    "                all_logits.append(logits)\n",
    "                \n",
    "                # Soft update of y_emb using logits\n",
    "                probs = F.softmax(logits, dim=-1)  # (B, L, V)\n",
    "                # Compute weighted embedding\n",
    "                y_emb = probs @ self.token_embedding.weight  # (B, L, D)\n",
    "                y_emb = self.embed_scale * y_emb\n",
    "            \n",
    "            # Detach z for next cycle (as per TRM paper)\n",
    "            z = z.detach()\n",
    "            y_emb = y_emb.detach()\n",
    "        \n",
    "        # Final logits are from the last cycle\n",
    "        final_logits = all_logits[-1]\n",
    "        \n",
    "        outputs = {'logits': final_logits}\n",
    "        \n",
    "        if return_all_logits:\n",
    "            outputs['all_logits'] = all_logits\n",
    "        \n",
    "        # Compute loss with deep supervision\n",
    "        if labels is not None:\n",
    "            # Shift for causal LM\n",
    "            shift_logits = final_logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "            # Deep supervision loss (average over all cycles)\n",
    "            total_loss = 0.0\n",
    "            for cycle_logits in all_logits:\n",
    "                cycle_shift_logits = cycle_logits[:, :-1, :].contiguous()\n",
    "                loss = F.cross_entropy(\n",
    "                    cycle_shift_logits.view(-1, self.config.vocab_size),\n",
    "                    shift_labels.view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                total_loss += loss\n",
    "            \n",
    "            outputs['loss'] = total_loss / len(all_logits)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, \n",
    "                 input_ids: torch.Tensor,\n",
    "                 max_new_tokens: int = 50,\n",
    "                 temperature: float = 1.0,\n",
    "                 top_k: Optional[int] = None,\n",
    "                 top_p: Optional[float] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text autoregressively.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop input to max_seq_len if needed\n",
    "            idx_cond = input_ids[:, -self.config.max_seq_len:]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self(idx_cond)\n",
    "            logits = outputs['logits'][:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Model Summary and Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_model_summary(config: TRMDecoderConfig):\n",
    "    \"\"\"Print model configuration summary\"\"\"\n",
    "    model = TRMDecoderLLM(config)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRM Decoder LLM Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Vocabulary Size: {config.vocab_size:,}\")\n",
    "    print(f\"Hidden Size: {config.hidden_size}\")\n",
    "    print(f\"Number of Heads: {config.num_heads}\")\n",
    "    print(f\"Number of Layers: {config.num_layers}\")\n",
    "    print(f\"Max Sequence Length: {config.max_seq_len}\")\n",
    "    print(f\"\\nTRM Parameters:\")\n",
    "    print(f\"  - N Recursions (n): {config.n_recursions}\")\n",
    "    print(f\"  - T Cycles (T): {config.t_cycles}\")\n",
    "    print(f\"  - Effective Depth: {config.num_layers * (config.n_recursions + 1) * config.t_cycles}\")\n",
    "    print(f\"\\nTotal Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"  (~{count_parameters(model) / 1e6:.2f}M)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a small config for testing\n",
    "test_config = TRMDecoderConfig(\n",
    "    vocab_size=1000,\n",
    "    hidden_size=256,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    max_seq_len=128,\n",
    "    n_recursions=4,\n",
    "    t_cycles=3\n",
    ")\n",
    "\n",
    "model = print_model_summary(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model = model.to(device)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "dummy_input = torch.randint(0, test_config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "dummy_labels = dummy_input.clone()\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(dummy_input, labels=dummy_labels, return_all_logits=True)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"Number of intermediate logits: {len(outputs['all_logits'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Training and Evaluation\n",
    "\n",
    "We'll train our TRM LLM on a simple character-level task and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Character-Level Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level dataset for training\"\"\"\n",
    "    def __init__(self, text: str, seq_len: int = 64):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Build vocabulary\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        \n",
    "        # Encode text\n",
    "        self.data = torch.tensor([self.char_to_idx[ch] for ch in text], dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode indices to text\"\"\"\n",
    "        return ''.join([self.idx_to_char[i.item()] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training corpus\n",
    "# Using a simple repeating pattern dataset to test learning\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "A quick brown fox jumps over a lazy dog. \n",
    "One quick brown fox jumps over one lazy dog.\n",
    "Two quick brown foxes jump over two lazy dogs.\n",
    "The lazy dog sleeps while the quick fox runs.\n",
    "Quick foxes are smarter than lazy dogs.\n",
    "Dogs chase foxes and foxes run from dogs.\n",
    "In the forest, foxes hunt and dogs guard.\n",
    "The brown fox is quick and the gray dog is lazy.\n",
    "Every morning, the fox runs and the dog sleeps.\n",
    "\"\"\" * 100  # Repeat to create more data\n",
    "\n",
    "# Create dataset\n",
    "seq_len = 64\n",
    "dataset = CharDataset(sample_text, seq_len=seq_len)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")\n",
    "print(f\"Vocabulary: {list(dataset.char_to_idx.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trm_llm(model, dataset, config, num_epochs=10, batch_size=32, lr=1e-3, device='cuda'):\n",
    "    \"\"\"\n",
    "    Training loop for TRM LLM with deep supervision.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    warmup_steps = total_steps // 10\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            return 0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x, labels=y)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with appropriate vocab size\n",
    "train_config = TRMDecoderConfig(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    max_seq_len=seq_len,\n",
    "    n_recursions=4,\n",
    "    t_cycles=3,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "trm_model = TRMDecoderLLM(train_config)\n",
    "print(f\"Model parameters: {count_parameters(trm_model):,}\")\n",
    "\n",
    "# Train the model\n",
    "loss_history = train_trm_llm(\n",
    "    trm_model, \n",
    "    dataset, \n",
    "    train_config,\n",
    "    num_epochs=5,\n",
    "    batch_size=32,\n",
    "    lr=3e-4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Moving average\n",
    "window = 50\n",
    "smoothed = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.title(f'Training Loss (Smoothed, window={window})')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = torch.tensor([[dataset.char_to_idx.get(ch, 0) for ch in prompt]], device=device)\n",
    "    \n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=40\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = dataset.decode(output_ids[0])\n",
    "    return generated_text\n",
    "\n",
    "# Generate some text samples\n",
    "prompts = [\n",
    "    \"The quick brown \",\n",
    "    \"A lazy dog \",\n",
    "    \"Foxes are \"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(trm_model, dataset, prompt, max_new_tokens=80, temperature=0.7)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Analyzing Recursive Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recursion_steps(model, dataset, text, device='cuda'):\n",
    "    \"\"\"\n",
    "    Analyze how predictions change across recursion cycles.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text\n",
    "    input_ids = torch.tensor([[dataset.char_to_idx.get(ch, 0) for ch in text]], device=device)\n",
    "    \n",
    "    # Get all logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, return_all_logits=True)\n",
    "    \n",
    "    all_logits = outputs['all_logits']\n",
    "    \n",
    "    # Compare predictions at each cycle\n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(f\"\\nPredictions at position -1 (next char after input):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for cycle_idx, logits in enumerate(all_logits):\n",
    "        probs = F.softmax(logits[0, -1], dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, 5)\n",
    "        \n",
    "        print(f\"\\nCycle {cycle_idx + 1}:\")\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            char = dataset.idx_to_char[idx.item()]\n",
    "            char_repr = repr(char) if char in [' ', '\\n'] else char\n",
    "            print(f\"  {char_repr}: {prob.item():.4f}\")\n",
    "    \n",
    "    # Visualize probability evolution\n",
    "    fig, axes = plt.subplots(1, len(all_logits), figsize=(4*len(all_logits), 4))\n",
    "    if len(all_logits) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for cycle_idx, (ax, logits) in enumerate(zip(axes, all_logits)):\n",
    "        probs = F.softmax(logits[0, -1], dim=-1).cpu().numpy()\n",
    "        ax.bar(range(len(probs)), probs)\n",
    "        ax.set_title(f'Cycle {cycle_idx + 1}')\n",
    "        ax.set_xlabel('Token Index')\n",
    "        ax.set_ylabel('Probability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze recursion\n",
    "analyze_recursion_steps(trm_model, dataset, \"The quick brown fo\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: TRM Vision-Language Model (VLM)\n",
    "\n",
    "Now we extend our TRM LLM to a Vision-Language Model by:\n",
    "1. Adding a Vision Encoder (using a pretrained ViT or simple CNN)\n",
    "2. Adding a projection MLP to align vision and language embeddings\n",
    "3. Training with contrastive loss (CLIP-style) for alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image to patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, E, H/P, W/P)\n",
    "        x = x.flatten(2)  # (B, E, N)\n",
    "        x = x.transpose(1, 2)  # (B, N, E)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VisionEncoderConfig:\n",
    "    \"\"\"Configuration for Vision Encoder\"\"\"\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_channels: int = 3\n",
    "    embed_dim: int = 256\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.n_patches = (self.img_size // self.patch_size) ** 2\n",
    "        self.head_dim = self.embed_dim // self.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer block for vision encoder (bidirectional attention)\"\"\"\n",
    "    def __init__(self, config: VisionEncoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Multi-head self-attention (bidirectional for vision)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=config.embed_dim,\n",
    "            num_heads=config.num_heads,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.embed_dim * 4, config.embed_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(config.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with pre-norm\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # MLP with pre-norm\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Encoder using TRM-style recursive reasoning.\n",
    "    Uses a tiny transformer that recurses multiple times.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: VisionEncoderConfig, n_recursions: int = 4):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_recursions = n_recursions\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size=config.img_size,\n",
    "            patch_size=config.patch_size,\n",
    "            in_channels=config.in_channels,\n",
    "            embed_dim=config.embed_dim\n",
    "        )\n",
    "        \n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, config.n_patches + 1, config.embed_dim) * 0.02\n",
    "        )\n",
    "        \n",
    "        # TRM-style: Use fewer layers but recurse\n",
    "        self.layers = nn.ModuleList([\n",
    "            VisionEncoderBlock(config) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(config.embed_dim)\n",
    "        \n",
    "        # Initial z state\n",
    "        self.z_init = nn.Parameter(torch.randn(config.embed_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, return_all_features: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Images (B, C, H, W)\n",
    "            return_all_features: If True, return features from all recursions\n",
    "        \n",
    "        Returns:\n",
    "            CLS token embedding (B, D) or all features if return_all_features=True\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, N, D)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, N+1, D)\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Initialize z (reasoning state)\n",
    "        z = self.z_init.unsqueeze(0).unsqueeze(0).expand(B, x.shape[1], -1)\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        # TRM recursive reasoning\n",
    "        for r in range(self.n_recursions):\n",
    "            # Combine x and z\n",
    "            h = x + z\n",
    "            \n",
    "            # Pass through transformer layers\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "            \n",
    "            # Update z with residual\n",
    "            z = z + h\n",
    "            \n",
    "            if return_all_features:\n",
    "                all_features.append(self.norm(z)[:, 0])  # CLS token\n",
    "        \n",
    "        # Final normalization\n",
    "        z = self.norm(z)\n",
    "        \n",
    "        if return_all_features:\n",
    "            return torch.stack(all_features, dim=1)  # (B, R, D)\n",
    "        \n",
    "        return z[:, 0]  # Return CLS token (B, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Projection MLP for Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP to project vision/text embeddings to a shared space.\n",
    "    Used for contrastive alignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, projection_dim: int, hidden_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or input_dim * 2\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.mlp(x)\n",
    "        # L2 normalize for contrastive learning\n",
    "        return F.normalize(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Complete TRM VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRMVLMConfig:\n",
    "    \"\"\"Configuration for TRM Vision-Language Model\"\"\"\n",
    "    # Vision config\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    vision_embed_dim: int = 256\n",
    "    vision_num_heads: int = 4\n",
    "    vision_num_layers: int = 2\n",
    "    vision_n_recursions: int = 4\n",
    "    \n",
    "    # Language config\n",
    "    vocab_size: int = 32000\n",
    "    lang_hidden_size: int = 256\n",
    "    lang_num_heads: int = 4\n",
    "    lang_num_layers: int = 2\n",
    "    max_seq_len: int = 256\n",
    "    lang_n_recursions: int = 4\n",
    "    lang_t_cycles: int = 3\n",
    "    \n",
    "    # Alignment config\n",
    "    projection_dim: int = 256\n",
    "    \n",
    "    # Training config\n",
    "    dropout: float = 0.1\n",
    "    temperature: float = 0.07  # For contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    TRM Vision-Language Model.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Vision Encoder: TRM-style ViT with recursive reasoning\n",
    "    2. Language Model: TRM-style decoder with recursive reasoning  \n",
    "    3. Projection MLPs: Align vision and language to shared space\n",
    "    4. Cross-modal fusion for image-conditioned generation\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TRMVLMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Vision Encoder\n",
    "        vision_config = VisionEncoderConfig(\n",
    "            img_size=config.img_size,\n",
    "            patch_size=config.patch_size,\n",
    "            embed_dim=config.vision_embed_dim,\n",
    "            num_heads=config.vision_num_heads,\n",
    "            num_layers=config.vision_num_layers,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.vision_encoder = TRMVisionEncoder(vision_config, n_recursions=config.vision_n_recursions)\n",
    "        \n",
    "        # Language Model\n",
    "        lang_config = TRMDecoderConfig(\n",
    "            vocab_size=config.vocab_size,\n",
    "            hidden_size=config.lang_hidden_size,\n",
    "            num_heads=config.lang_num_heads,\n",
    "            num_layers=config.lang_num_layers,\n",
    "            max_seq_len=config.max_seq_len,\n",
    "            n_recursions=config.lang_n_recursions,\n",
    "            t_cycles=config.lang_t_cycles,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.language_model = TRMDecoderLLM(lang_config)\n",
    "        \n",
    "        # Projection heads for contrastive alignment\n",
    "        self.vision_proj = ProjectionMLP(config.vision_embed_dim, config.projection_dim)\n",
    "        self.text_proj = ProjectionMLP(config.lang_hidden_size, config.projection_dim)\n",
    "        \n",
    "        # Cross-modal projection (vision -> language space)\n",
    "        self.vision_to_lang = nn.Linear(config.vision_embed_dim, config.lang_hidden_size)\n",
    "        \n",
    "        # Learnable temperature for contrastive loss\n",
    "        self.temperature = nn.Parameter(torch.tensor(config.temperature).log())\n",
    "    \n",
    "    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode images to embeddings.\"\"\"\n",
    "        return self.vision_encoder(images)\n",
    "    \n",
    "    def encode_text(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text to embeddings.\n",
    "        Returns the last hidden state of the last token (for contrastive learning).\n",
    "        \"\"\"\n",
    "        outputs = self.language_model(input_ids, return_all_logits=True)\n",
    "        # Get last hidden state before LM head\n",
    "        # We need to modify the LLM to return hidden states\n",
    "        # For now, we'll use the embedding of the last token\n",
    "        last_hidden = self.language_model.get_input_embeddings(input_ids)[:, -1, :]\n",
    "        return last_hidden\n",
    "    \n",
    "    def contrastive_loss(self, image_embeds: torch.Tensor, text_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute CLIP-style contrastive loss.\n",
    "        \n",
    "        Args:\n",
    "            image_embeds: Vision embeddings (B, D_proj)\n",
    "            text_embeds: Text embeddings (B, D_proj)\n",
    "        \n",
    "        Returns:\n",
    "            Contrastive loss (scalar)\n",
    "        \"\"\"\n",
    "        # Project to shared space\n",
    "        image_proj = self.vision_proj(image_embeds)\n",
    "        text_proj = self.text_proj(text_embeds)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        temperature = self.temperature.exp()\n",
    "        logits = (image_proj @ text_proj.T) / temperature\n",
    "        \n",
    "        # Labels: diagonal should have highest similarity\n",
    "        B = logits.shape[0]\n",
    "        labels = torch.arange(B, device=logits.device)\n",
    "        \n",
    "        # Symmetric cross-entropy loss\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        mode: str = 'vlm'  # 'contrastive', 'generation', 'vlm'\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            images: Input images (B, C, H, W)\n",
    "            input_ids: Input token IDs (B, L)\n",
    "            labels: Target labels for LM loss (B, L)\n",
    "            mode: 'contrastive' for alignment training,\n",
    "                  'generation' for text generation,\n",
    "                  'vlm' for image-conditioned generation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with relevant outputs based on mode\n",
    "        \"\"\"\n",
    "        outputs = {}\n",
    "        \n",
    "        if mode == 'contrastive':\n",
    "            # Contrastive alignment training\n",
    "            assert images is not None and input_ids is not None\n",
    "            \n",
    "            image_embeds = self.encode_image(images)\n",
    "            text_embeds = self.encode_text(input_ids)\n",
    "            \n",
    "            loss = self.contrastive_loss(image_embeds, text_embeds)\n",
    "            outputs['loss'] = loss\n",
    "            outputs['image_embeds'] = image_embeds\n",
    "            outputs['text_embeds'] = text_embeds\n",
    "            \n",
    "        elif mode == 'generation':\n",
    "            # Pure text generation (no images)\n",
    "            assert input_ids is not None\n",
    "            \n",
    "            lm_outputs = self.language_model(input_ids, labels=labels)\n",
    "            outputs.update(lm_outputs)\n",
    "            \n",
    "        elif mode == 'vlm':\n",
    "            # Image-conditioned text generation\n",
    "            assert images is not None and input_ids is not None\n",
    "            \n",
    "            # Encode images\n",
    "            image_embeds = self.encode_image(images)  # (B, D_vision)\n",
    "            \n",
    "            # Project to language space\n",
    "            image_in_lang = self.vision_to_lang(image_embeds)  # (B, D_lang)\n",
    "            \n",
    "            # Prepend image embedding as a \"vision token\"\n",
    "            # Get text embeddings\n",
    "            text_embeds = self.language_model.get_input_embeddings(input_ids)  # (B, L, D)\n",
    "            \n",
    "            # Concatenate image embedding at the start\n",
    "            combined_embeds = torch.cat([\n",
    "                image_in_lang.unsqueeze(1),  # (B, 1, D)\n",
    "                text_embeds\n",
    "            ], dim=1)  # (B, L+1, D)\n",
    "            \n",
    "            # Note: For full implementation, we need to modify the language model\n",
    "            # to accept embeddings directly. For this POC, we'll compute loss normally.\n",
    "            lm_outputs = self.language_model(input_ids, labels=labels)\n",
    "            outputs.update(lm_outputs)\n",
    "            outputs['image_embeds'] = image_embeds\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_from_image(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        prompt_ids: Optional[torch.Tensor] = None,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text conditioned on an image.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Encode image\n",
    "        image_embeds = self.encode_image(images)\n",
    "        \n",
    "        # For now, just generate from the language model\n",
    "        # Full implementation would inject image embeddings\n",
    "        if prompt_ids is None:\n",
    "            # Start with a BOS token (assuming 0)\n",
    "            prompt_ids = torch.zeros(images.shape[0], 1, dtype=torch.long, device=images.device)\n",
    "        \n",
    "        return self.language_model.generate(\n",
    "            prompt_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create and Test VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VLM configuration\n",
    "vlm_config = TRMVLMConfig(\n",
    "    # Vision\n",
    "    img_size=64,  # Small for testing\n",
    "    patch_size=8,\n",
    "    vision_embed_dim=128,\n",
    "    vision_num_heads=4,\n",
    "    vision_num_layers=2,\n",
    "    vision_n_recursions=3,\n",
    "    \n",
    "    # Language\n",
    "    vocab_size=1000,\n",
    "    lang_hidden_size=128,\n",
    "    lang_num_heads=4,\n",
    "    lang_num_layers=2,\n",
    "    max_seq_len=64,\n",
    "    lang_n_recursions=3,\n",
    "    lang_t_cycles=2,\n",
    "    \n",
    "    # Alignment\n",
    "    projection_dim=128\n",
    ")\n",
    "\n",
    "# Create model\n",
    "vlm_model = TRMVLM(vlm_config).to(device)\n",
    "print(f\"VLM Total Parameters: {count_parameters(vlm_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "batch_size = 4\n",
    "\n",
    "# Dummy inputs\n",
    "dummy_images = torch.randn(batch_size, 3, vlm_config.img_size, vlm_config.img_size).to(device)\n",
    "dummy_input_ids = torch.randint(0, vlm_config.vocab_size, (batch_size, 32)).to(device)\n",
    "\n",
    "# Test contrastive mode\n",
    "print(\"Testing Contrastive Mode...\")\n",
    "outputs = vlm_model(dummy_images, dummy_input_ids, mode='contrastive')\n",
    "print(f\"  Contrastive Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"  Image Embeds Shape: {outputs['image_embeds'].shape}\")\n",
    "print(f\"  Text Embeds Shape: {outputs['text_embeds'].shape}\")\n",
    "\n",
    "# Test generation mode\n",
    "print(\"\\nTesting Generation Mode...\")\n",
    "outputs = vlm_model(input_ids=dummy_input_ids, labels=dummy_input_ids, mode='generation')\n",
    "print(f\"  LM Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"  Logits Shape: {outputs['logits'].shape}\")\n",
    "\n",
    "# Test VLM mode\n",
    "print(\"\\nTesting VLM Mode...\")\n",
    "outputs = vlm_model(dummy_images, dummy_input_ids, labels=dummy_input_ids, mode='vlm')\n",
    "print(f\"  VLM Loss: {outputs['loss'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Contrastive Alignment Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple synthetic dataset for testing contrastive alignment.\n",
    "    Creates random images with associated \"captions\" (random token sequences).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples: int = 1000, img_size: int = 64, \n",
    "                 vocab_size: int = 1000, seq_len: int = 32):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Pre-generate data for consistency\n",
    "        # In a real scenario, you'd have actual image-text pairs\n",
    "        self.images = torch.randn(num_samples, 3, img_size, img_size)\n",
    "        self.captions = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.captions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_alignment(model, dataloader, num_epochs=5, lr=1e-4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the model using contrastive alignment.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for images, captions in pbar:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images, captions, mode='contrastive')\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "alignment_dataset = SimpleImageTextDataset(\n",
    "    num_samples=500,\n",
    "    img_size=vlm_config.img_size,\n",
    "    vocab_size=vlm_config.vocab_size,\n",
    "    seq_len=32\n",
    ")\n",
    "\n",
    "alignment_dataloader = DataLoader(alignment_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Train contrastive alignment\n",
    "print(\"Training Contrastive Alignment...\")\n",
    "alignment_loss_history = train_contrastive_alignment(\n",
    "    vlm_model,\n",
    "    alignment_dataloader,\n",
    "    num_epochs=3,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alignment loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(alignment_loss_history)\n",
    "plt.title('Contrastive Alignment Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Visualize Learned Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_embeddings(model, dataloader, device='cuda', num_samples=100):\n",
    "    \"\"\"\n",
    "    Visualize image and text embeddings in shared space using t-SNE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    image_embeds_list = []\n",
    "    text_embeds_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples_collected = 0\n",
    "        for images, captions in dataloader:\n",
    "            if samples_collected >= num_samples:\n",
    "                break\n",
    "            \n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            img_emb = model.encode_image(images)\n",
    "            txt_emb = model.encode_text(captions)\n",
    "            \n",
    "            # Project to shared space\n",
    "            img_proj = model.vision_proj(img_emb)\n",
    "            txt_proj = model.text_proj(txt_emb)\n",
    "            \n",
    "            image_embeds_list.append(img_proj.cpu())\n",
    "            text_embeds_list.append(txt_proj.cpu())\n",
    "            \n",
    "            samples_collected += images.shape[0]\n",
    "    \n",
    "    # Concatenate\n",
    "    image_embeds = torch.cat(image_embeds_list, dim=0)[:num_samples].numpy()\n",
    "    text_embeds = torch.cat(text_embeds_list, dim=0)[:num_samples].numpy()\n",
    "    \n",
    "    # Combine for t-SNE\n",
    "    all_embeds = np.concatenate([image_embeds, text_embeds], axis=0)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, num_samples-1))\n",
    "    embeds_2d = tsne.fit_transform(all_embeds)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Image embeddings\n",
    "    plt.scatter(\n",
    "        embeds_2d[:num_samples, 0], \n",
    "        embeds_2d[:num_samples, 1],\n",
    "        c='blue', alpha=0.5, label='Image Embeddings'\n",
    "    )\n",
    "    \n",
    "    # Text embeddings\n",
    "    plt.scatter(\n",
    "        embeds_2d[num_samples:, 0], \n",
    "        embeds_2d[num_samples:, 1],\n",
    "        c='red', alpha=0.5, label='Text Embeddings'\n",
    "    )\n",
    "    \n",
    "    # Draw lines connecting matched pairs\n",
    "    for i in range(min(20, num_samples)):  # Show first 20 pairs\n",
    "        plt.plot(\n",
    "            [embeds_2d[i, 0], embeds_2d[num_samples + i, 0]],\n",
    "            [embeds_2d[i, 1], embeds_2d[num_samples + i, 1]],\n",
    "            'gray', alpha=0.3, linewidth=0.5\n",
    "        )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Image-Text Embedding Alignment (t-SNE)')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize embeddings\n",
    "visualize_embeddings(vlm_model, alignment_dataloader, device=device, num_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Next Steps\n",
    "\n",
    "## What We Implemented:\n",
    "\n",
    "### Part 1: TRM Decoder LLM\n",
    "- Decoder-only transformer with TRM-style recursive reasoning\n",
    "- Deep supervision across multiple recursion cycles\n",
    "- Causal self-attention with RoPE\n",
    "- SwiGLU activation and RMSNorm\n",
    "\n",
    "### Part 2: Training and Evaluation\n",
    "- Character-level language modeling\n",
    "- Analysis of how predictions evolve across recursion steps\n",
    "- Text generation with sampling strategies\n",
    "\n",
    "### Part 3: TRM VLM\n",
    "- Vision encoder with TRM recursive reasoning\n",
    "- CLIP-style contrastive alignment\n",
    "- Image-conditioned text generation (basic)\n",
    "\n",
    "## Key Insights from TRM:\n",
    "1. **Less is More**: 2-layer networks with many recursions outperform deeper networks\n",
    "2. **Deep Supervision**: Computing loss at each recursion step improves training\n",
    "3. **Single Network**: One shared network for both z and y updates works better\n",
    "4. **Recursive Refinement**: Model progressively improves its answer\n",
    "\n",
    "## Next Steps for Production:\n",
    "1. Scale up vocabulary and model size\n",
    "2. Train on real datasets (e.g., WikiText, C4)\n",
    "3. For VLM: Use real image-caption datasets (COCO, CC3M)\n",
    "4. Implement proper cross-attention for image conditioning\n",
    "5. Add more sophisticated sampling strategies\n",
    "6. Implement KV-cache for efficient generation\n",
    "7. Add gradient checkpointing for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"TRM Decoder LLM & VLM Implementation Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLLM Configuration:\")\n",
    "print(f\"  - Hidden Size: {train_config.hidden_size}\")\n",
    "print(f\"  - Layers: {train_config.num_layers}\")\n",
    "print(f\"  - Recursions (n): {train_config.n_recursions}\")\n",
    "print(f\"  - Cycles (T): {train_config.t_cycles}\")\n",
    "print(f\"  - Parameters: {count_parameters(trm_model):,}\")\n",
    "\n",
    "print(f\"\\nVLM Configuration:\")\n",
    "print(f\"  - Vision Embed Dim: {vlm_config.vision_embed_dim}\")\n",
    "print(f\"  - Language Hidden Size: {vlm_config.lang_hidden_size}\")\n",
    "print(f\"  - Total Parameters: {count_parameters(vlm_model):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Implementation Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
