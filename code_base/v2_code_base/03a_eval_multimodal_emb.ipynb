{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80987a95",
   "metadata": {},
   "source": [
    "# 03a â€“ Multimodal Alignment Retrieval Evaluation\n",
    "\n",
    "This notebook evaluates **Phase-1 multimodal alignment** checkpoints trained in:\n",
    "\n",
    "- `02_alig_multi_mlp.ipynb`\n",
    "- `02_alig_multi_perciever.ipynb`\n",
    "\n",
    "It computes retrieval metrics for:\n",
    "\n",
    "- **Vision â†” Text** (PixMo-Cap)\n",
    "- **Audio â†” Text** (Clotho)\n",
    "\n",
    "for both model variants:\n",
    "\n",
    "- **MLP + MRL** (no Perceiver bottleneck)\n",
    "- **Perceiver + MLP + MRL**\n",
    "\n",
    "and supports:\n",
    "\n",
    "- Matryoshka truncation curves over `mm_cfg.mrl_dims`\n",
    "- Rich metrics (R@K, mean/median rank, mAP@K, NDCG@K)\n",
    "- Optional logging to **Weights & Biases**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f657287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Imports & basic setup\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Transformers encoders\n",
    "from transformers import (\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperModel,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "# Local project imports\n",
    "from imports.core import (\n",
    "    set_seed,\n",
    "    l2_normalize,\n",
    ")\n",
    "from imports.multimodal_alignment_perceiver import (\n",
    "    MultimodalAlignmentConfig,\n",
    "    MultimodalAlignmentModel,\n",
    ")\n",
    "from imports.in_memory_datasets import (\n",
    "    InMemoryImageTextDataset,\n",
    "    collate_in_memory_images,\n",
    ")\n",
    "\n",
    "from imports.core import compute_retrieval_metrics\n",
    "from zipfile import Path as ZipPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Experiment config (mirrors 02_alig_multi_mlp.ipynb)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # General\n",
    "    seed: int = 42\n",
    "    num_epochs_mlp: int = 3\n",
    "    num_epochs_perceiver: int = 5\n",
    "    log_every: int = 20\n",
    "\n",
    "    # Data: if Parquet paths are None, we fall back to HF datasets\n",
    "    pixmo_parquet_dir: Path = Path.cwd() / \"data\" / \"alignment_offline\"\n",
    "    clotho_parquet_dir: Path = Path.cwd() / \"data\" / \"alignment_offline\"\n",
    "    pixmo_parquet_glob: str = \"pixmocap_offline_20000*.parquet\"\n",
    "    clotho_parquet_glob: str = \"clotho_development.parquet\"\n",
    "\n",
    "    image_hf_dataset: str = \"allenai/pixmo-cap\"\n",
    "    audio_hf_dataset: str = \"clotho_v2\"\n",
    "\n",
    "    max_image_samples: Optional[int] = 100   # None for all\n",
    "    max_audio_samples: Optional[int] = 1000    # None for all\n",
    "    image_size: Tuple[int, int] = (224, 224)\n",
    "    sample_val_fraction: float = 0.05  # fraction of samples for validation\n",
    "\n",
    "    # Training\n",
    "    base_batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # WandB\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"edgeglass_phase1_alignment\"\n",
    "    wandb_entity: Optional[str] = None  # set if you use a team\n",
    "    wandb_mode: str = \"online\"  # \"offline\" or \"disabled\" etc.\n",
    "    wandb_run_name: Optional[str] = \"phase1_multimodal_eval\"\n",
    "\n",
    "    # Checkpoints (Phase 1)\n",
    "    root_dir: Path = Path(\".\").resolve()\n",
    "    ckpt_root: str = \"checkpoints/phase1_multimodal\"  # relative to root_dir\n",
    "\n",
    "    # Alignment / model (weights are mirrored into mm_cfg later)\n",
    "    mrl_weight: float = 1.0\n",
    "    clip_weight: float = 0.5\n",
    "\n",
    "\n",
    "cfg = ExperimentConfig()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ec1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Checkpoint root: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal\n",
      "MLP run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/mlp_mrl\n",
      "Perceiver run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl\n",
      "MultimodalAlignmentConfig(vision_model_name='openai/clip-vit-base-patch32', audio_model_name='openai/whisper-base', text_model_name='sentence-transformers/all-MiniLM-L6-v2', llm_model_name='Qwen/Qwen2.5-1.5B-Instruct', d_vision=768, d_audio=512, d_text=384, perceiver_dim=512, num_latents=64, num_perceiver_layers=4, num_attn_heads=8, perceiver_mlp_ratio=4.0, perceiver_dropout=0.1, d_align=512, mrl_dims=(64, 128, 256, 512), llm_hidden_size=1536, num_prefix_tokens=64, batch_size=32, learning_rate=0.0001, weight_decay=0.01, num_epochs=10, warmup_ratio=0.1, max_grad_norm=1.0, temperature=0.07, mrl_weight=1.0, clip_weight=0.5, seed=42, device='cuda', dtype='float32')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT_DIR = cfg.root_dir\n",
    "CKPT_ROOT = ROOT_DIR / cfg.ckpt_root\n",
    "MLP_DIR = CKPT_ROOT / \"mlp_mrl\"\n",
    "PERCEIVER_DIR = CKPT_ROOT / \"perceiver_mrl\"\n",
    "\n",
    "for d in [CKPT_ROOT, MLP_DIR, PERCEIVER_DIR, cfg.pixmo_parquet_dir, cfg.clotho_parquet_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"Checkpoint root:\", CKPT_ROOT)\n",
    "print(\"MLP run dir:\", MLP_DIR)\n",
    "print(\"Perceiver run dir:\", PERCEIVER_DIR)\n",
    "\n",
    "# Matryoshka / alignment config used by the model\n",
    "mm_cfg = MultimodalAlignmentConfig()\n",
    "mm_cfg.device = str(device)\n",
    "mm_cfg.mrl_weight = cfg.mrl_weight\n",
    "mm_cfg.clip_weight = cfg.clip_weight\n",
    "print(mm_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf9a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Image-Text Datasets (PixMo-Cap) for EVAL ===\n",
      "Loading PixMo-Cap from Parquet: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_offline/pixmocap_offline_20000.parquet\n",
      "\n",
      "ðŸ“¥ Pre-loading 100 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 32 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   1%|          | 1/100 [00:00<00:30,  3.24it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 25.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 100 images into memory\n",
      "Image val size: 5 | batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Dataset helpers â€“ PixMo (vision-text) and Clotho (audio-text)\n",
    "# ============================================================\n",
    "\n",
    "import glob\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def get_pixmo_dataset():\n",
    "    \"\"\"Load PixMo-Cap 'train' split from Parquet if available, otherwise from HF.\"\"\"\n",
    "    parquet_pattern = cfg.pixmo_parquet_dir / cfg.pixmo_parquet_glob\n",
    "    matches = sorted(glob.glob(str(parquet_pattern)))\n",
    "    if matches:\n",
    "        print(f\"Loading PixMo-Cap from Parquet: {matches[-1]}\")\n",
    "        ds_dict = load_dataset(\"parquet\", data_files={\"train\": matches[-1]})\n",
    "        ds = ds_dict[\"train\"]\n",
    "    else:\n",
    "        print(f\"No PixMo Parquet found at pattern {parquet_pattern}, loading from HF: {cfg.image_hf_dataset}\")\n",
    "        ds = load_dataset(cfg.image_hf_dataset, split=\"train\")\n",
    "    return ds\n",
    "\n",
    "def build_image_val_loader() -> DataLoader:\n",
    "    \"\"\"Create *validation* DataLoader for image-text data using PixMo-Cap.\"\"\"\n",
    "    print(\"\\n=== Building Image-Text Datasets (PixMo-Cap) for EVAL ===\")\n",
    "    hf_ds = get_pixmo_dataset()\n",
    "\n",
    "    if cfg.max_image_samples is not None:\n",
    "        hf_ds = hf_ds.select(range(min(cfg.max_image_samples, len(hf_ds))))\n",
    "\n",
    "    # Expect PixMo-Cap columns: image_url + caption\n",
    "    colnames = hf_ds.column_names\n",
    "    if \"image_url\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'image_url' column in PixMo dataset, found: {colnames}\")\n",
    "    if \"caption\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'caption' column in PixMo dataset, found: {colnames}\")\n",
    "\n",
    "    ds = InMemoryImageTextDataset(\n",
    "        hf_dataset=hf_ds,\n",
    "        img_col=\"image_url\",\n",
    "        txt_col=\"caption\",\n",
    "        max_samples=None,  # already limited above\n",
    "        image_size=cfg.image_size,\n",
    "    )\n",
    "\n",
    "    # Train/val split via indices (mirror training)\n",
    "    n = len(ds)\n",
    "    idx = np.random.permutation(n)\n",
    "    split = int(n * (1.0 - cfg.sample_val_fraction))\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "    val_ds = Subset(ds, val_idx)\n",
    "\n",
    "    num_gpus = max(1, torch.cuda.device_count())\n",
    "    batch_size = cfg.base_batch_size * num_gpus\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Image val size: {len(val_ds)} | batch size: {batch_size}\")\n",
    "    return val_loader\n",
    "\n",
    "\n",
    "image_val_loader = build_image_val_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25405f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Loading Clotho records from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_offline/clotho_development.parquet\n",
      "[Eval] Clotho total records: 3839\n",
      "[Eval] Using first 1000 Clotho samples for eval\n",
      "[Eval] Clotho eval batches: 16 | batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# === Audioâ€“Text Eval Loader (Clotho records, same as training) ===\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Path to your torch-saved Clotho records file\n",
    "CLOTHO_RECORDS_PATH = ROOT_DIR / \"data\" / \"alignment_offline\" / \"clotho_development.parquet\"\n",
    "\n",
    "# Cache resamplers per original sr\n",
    "_resamplers: Dict[int, torchaudio.transforms.Resample] = {}\n",
    "\n",
    "\n",
    "def collate_clotho_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate function for ClothoAudioCaptionDataset (eval).\n",
    "\n",
    "    - Takes variable-length waveforms at original sr (typically 44100)\n",
    "    - Resamples each to 16 kHz for Whisper\n",
    "    - Pads to max length in the batch\n",
    "    \"\"\"\n",
    "    audios_16k = []\n",
    "    lengths_16k = []\n",
    "    captions = []\n",
    "    file_names = []\n",
    "\n",
    "    for b in batch:\n",
    "        wav = b[\"audio\"]        # Tensor [T_orig]\n",
    "        sr_orig = int(b[\"sr\"])  # e.g. 44100\n",
    "\n",
    "        # --- resample to 16 kHz ---\n",
    "        if sr_orig != 16000:\n",
    "            if sr_orig not in _resamplers:\n",
    "                _resamplers[sr_orig] = torchaudio.transforms.Resample(\n",
    "                    orig_freq=sr_orig,\n",
    "                    new_freq=16000,\n",
    "                )\n",
    "            wav = _resamplers[sr_orig](wav)  # [T_16k]\n",
    "\n",
    "        audios_16k.append(wav)\n",
    "        lengths_16k.append(wav.shape[0])\n",
    "        captions.append(b[\"caption\"])\n",
    "        file_names.append(b[\"file_name\"])\n",
    "\n",
    "    # Pad to max length at 16 kHz\n",
    "    max_len = max(lengths_16k)\n",
    "    B = len(audios_16k)\n",
    "\n",
    "    padded = audios_16k[0].new_zeros(B, max_len)  # (B, T_max_16k)\n",
    "    for i, a in enumerate(audios_16k):\n",
    "        padded[i, : a.shape[0]] = a\n",
    "\n",
    "    return {\n",
    "        \"audio\": padded,                                    # (B, T_max_16k)\n",
    "        \"audio_lengths\": torch.tensor(lengths_16k, dtype=torch.long),\n",
    "        \"sr\": 16000,                                        # now fixed to 16k\n",
    "        \"captions\": captions,                               # list[str]\n",
    "        \"file_names\": file_names,                           # list[str]\n",
    "    }\n",
    "\n",
    "\n",
    "class ClothoAudioCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset reading the torch-saved Clotho records used in training.\n",
    "    Each item returns a single (audio, caption) pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        records_path: str,\n",
    "        pick_random_caption: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.records: List[Dict[str, Any]] = torch.load(records_path)\n",
    "        self.pick_random_caption = pick_random_caption\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.records[idx]\n",
    "        waveform = item[\"waveform\"]       # torch.FloatTensor [T]\n",
    "        sr = item[\"sr\"]\n",
    "        file_name = item[\"file_name\"]\n",
    "        captions = item[\"captions\"]       # list[str]\n",
    "\n",
    "        if self.pick_random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]\n",
    "\n",
    "        return {\n",
    "            \"audio\": waveform,\n",
    "            \"sr\": sr,\n",
    "            \"caption\": caption,\n",
    "            \"file_name\": file_name,\n",
    "        }\n",
    "\n",
    "\n",
    "def build_audio_eval_dataloader() -> Optional[DataLoader]:\n",
    "    \"\"\"\n",
    "    Build evaluation DataLoader for audioâ€“text retrieval using the same\n",
    "    Clotho records format + collate function as used in training.\n",
    "    \"\"\"\n",
    "    if not CLOTHO_RECORDS_PATH.exists():\n",
    "        print(f\"[Eval] Clotho records file not found: {CLOTHO_RECORDS_PATH}\")\n",
    "        print(\"       Skipping audio alignment evaluation.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"[Eval] Loading Clotho records from: {CLOTHO_RECORDS_PATH}\")\n",
    "    ds = ClothoAudioCaptionDataset(\n",
    "        records_path=str(CLOTHO_RECORDS_PATH),\n",
    "        pick_random_caption=False,  # deterministic captions for eval\n",
    "    )\n",
    "\n",
    "    n = len(ds)\n",
    "    print(f\"[Eval] Clotho total records: {n}\")\n",
    "\n",
    "    # Optional subsampling for speed\n",
    "    if MAX_EVAL_SAMPLES is not None and n > MAX_EVAL_SAMPLES:\n",
    "        idx = torch.arange(n)[:MAX_EVAL_SAMPLES]\n",
    "        ds = Subset(ds, idx)\n",
    "        print(f\"[Eval] Using first {MAX_EVAL_SAMPLES} Clotho samples for eval\")\n",
    "\n",
    "    num_gpus = max(1, torch.cuda.device_count())\n",
    "    batch_size = BATCH_SIZE * num_gpus  # match training scaling if desired\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,                      # eval -> no shuffle\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_clotho_batch,    # same collate as training\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(f\"[Eval] Clotho eval batches: {len(loader)} | batch size: {batch_size}\")\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Build the eval loader (or None if file missing)\n",
    "audio_val_loader = build_audio_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2295a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIP vision encoder: openai/clip-vit-base-patch32\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading audio encoder: openai/whisper-base\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Frozen encoders: CLIP (vision), Whisper (audio), MiniLM (text)\n",
    "# ============================================================\n",
    "\n",
    "# Vision encoder (CLIP)\n",
    "vision_model_name = mm_cfg.vision_model_name\n",
    "print(\"\\nLoading CLIP vision encoder:\", vision_model_name)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n",
    "clip_vision = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "clip_vision.to(device)\n",
    "clip_vision.eval()\n",
    "for p in clip_vision.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Text encoder (MiniLM)\n",
    "text_model_name = mm_cfg.text_model_name\n",
    "print(\"Loading text encoder:\", text_model_name)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Audio encoder (Whisper)\n",
    "audio_model_name = mm_cfg.audio_model_name\n",
    "print(\"Loading audio encoder:\", audio_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(audio_model_name)\n",
    "whisper_encoder = WhisperModel.from_pretrained(audio_model_name).get_encoder()\n",
    "whisper_encoder.to(device)\n",
    "whisper_encoder.eval()\n",
    "for p in whisper_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature encoding helpers (match 02_alig_multi_mlp.ipynb)\n",
    "# ============================================================\n",
    "\n",
    "def encode_images_to_features(images: List) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of PIL images to CLIP patch features (B, T, 768).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = clip_vision(**inputs)\n",
    "        feats = outputs.last_hidden_state  # (B, T, 768)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def encode_texts_to_features(texts: List[str]) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of texts to token features (B, L, 384).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tokens = text_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        outputs = text_encoder(**tokens)\n",
    "        feats = outputs.last_hidden_state  # (B, L, 384)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def encode_audio_to_features(audio_batch: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of waveforms [B, T] at 16 kHz to Whisper features (B, T_feat, 512).\"\"\"\n",
    "    assert isinstance(audio_batch, torch.Tensor), \"audio_batch must be Tensor[B, T]\"\n",
    "    if sr != 16000:\n",
    "        raise ValueError(f\"Expected 16 kHz audio for Whisper, got sr={sr}\")\n",
    "\n",
    "    B, T_max = audio_batch.shape\n",
    "    features: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(B):\n",
    "            wav = audio_batch[i].detach().cpu().float().numpy()  # (T,)\n",
    "\n",
    "            inputs = whisper_processor(\n",
    "                wav,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            out = whisper_encoder(inputs.input_features)          # (1, T_feat_i, 512)\n",
    "            feat_i = out.last_hidden_state.squeeze(0)            # (T_feat_i, 512)\n",
    "            features.append(feat_i)\n",
    "\n",
    "    # Pad along time dimension to get (B, T_feat_max, 512)\n",
    "    max_T_feat = max(f.shape[0] for f in features)\n",
    "    hidden_dim = features[0].shape[1]\n",
    "\n",
    "    feats_batch = torch.zeros(B, max_T_feat, hidden_dim, device=device)\n",
    "    for i, f in enumerate(features):\n",
    "        feats_batch[i, : f.shape[0]] = f\n",
    "\n",
    "    return feats_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63694b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/mlp_mrl/best.pt\n",
      "\n",
      "Loading checkpoint from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Checkpoint loading â€“ MLP + Perceiver variants\n",
    "# ============================================================\n",
    "\n",
    "def load_alignment_model_from_dir(ckpt_dir: Path, tag: str = \"best\") -> Tuple[MultimodalAlignmentModel, Dict[str, Any]]:\n",
    "    \"\"\"Load MultimodalAlignmentModel + config dicts from a checkpoint directory.\"\"\"\n",
    "    ckpt_path = ckpt_dir / f\"{tag}.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    print(f\"\\nLoading checkpoint from: {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "\n",
    "    mm_cfg_dict = state.get(\"mm_config\", None)\n",
    "    exp_cfg_dict = state.get(\"exp_config\", None)\n",
    "\n",
    "    if mm_cfg_dict is not None:\n",
    "        mm_cfg_local = MultimodalAlignmentConfig(**mm_cfg_dict)\n",
    "    else:\n",
    "        mm_cfg_local = MultimodalAlignmentConfig()\n",
    "\n",
    "    mm_cfg_local.device = str(device)\n",
    "\n",
    "    model = MultimodalAlignmentModel(mm_cfg_local).to(device)\n",
    "    model.load_state_dict(state[\"model_state\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    return model, {\"mm_config\": mm_cfg_local, \"exp_config\": exp_cfg_dict, \"state\": state}\n",
    "\n",
    "\n",
    "# Try to load both variants if available\n",
    "model_mlp, info_mlp = None, None\n",
    "model_perceiver, info_perceiver = None, None\n",
    "\n",
    "if (MLP_DIR / \"best.pt\").exists():\n",
    "    model_mlp, info_mlp = load_alignment_model_from_dir(MLP_DIR, tag=\"best\")\n",
    "else:\n",
    "    print(\"âš ï¸ No MLP checkpoint found (best.pt).\")\n",
    "\n",
    "if (PERCEIVER_DIR / \"best.pt\").exists():\n",
    "    model_perceiver, info_perceiver = load_alignment_model_from_dir(PERCEIVER_DIR, tag=\"best\")\n",
    "else:\n",
    "    print(\"âš ï¸ No Perceiver checkpoint found (best.pt).\")\n",
    "\n",
    "assert model_mlp is not None or model_perceiver is not None, \"No checkpoints found â€“ cannot run eval.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5f94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Retrieval metrics (extend core.compute_retrieval_metrics)\n",
    "# ============================================================\n",
    "\n",
    "def compute_full_retrieval_metrics(z_q: torch.Tensor, z_k: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute recall@K plus rank-based metrics for 1-1 aligned pairs.\"\"\"\n",
    "    base = compute_retrieval_metrics(z_q, z_k)  # uses l2_normalize inside\n",
    "\n",
    "    # Similarity matrix\n",
    "    z_q_n = l2_normalize(z_q)\n",
    "    z_k_n = l2_normalize(z_k)\n",
    "    sims = z_q_n @ z_k_n.T  # (N, N)\n",
    "\n",
    "    N = sims.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(N):\n",
    "        scores = sims[i]\n",
    "        # Sort in descending order; get rank of the *true* index i\n",
    "        sorted_idx = torch.argsort(scores, descending=True)\n",
    "        rank_pos = (sorted_idx == i).nonzero(as_tuple=False).item()  # 0-based\n",
    "        ranks.append(rank_pos + 1)  # 1-based\n",
    "\n",
    "    ranks = torch.tensor(ranks, dtype=torch.float32)\n",
    "    mean_rank = ranks.mean().item()\n",
    "    median_rank = ranks.median().item()\n",
    "    mrr = (1.0 / ranks).mean().item()\n",
    "\n",
    "    def ndcg_at_k(k: int) -> float:\n",
    "        gains = []\n",
    "        for r in ranks.tolist():\n",
    "            if r <= k:\n",
    "                gains.append(1.0 / math.log2(r + 1.0))\n",
    "            else:\n",
    "                gains.append(0.0)\n",
    "        # IDCG = 1/log2(1+1) = 1\n",
    "        return float(sum(gains) / len(gains))\n",
    "\n",
    "    res = dict(base)\n",
    "    res[\"mean_rank\"] = mean_rank\n",
    "    res[\"median_rank\"] = median_rank\n",
    "    res[\"MRR\"] = mrr\n",
    "    res[\"NDCG@10\"] = ndcg_at_k(10)\n",
    "    res[\"NDCG@50\"] = ndcg_at_k(50)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe67adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Embedding collection helpers\n",
    "# ============================================================\n",
    "\n",
    "def get_model_module(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"Unwrap DataParallel if needed.\"\"\"\n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "\n",
    "def encode_modality(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    feats: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor],\n",
    "    modality: str,\n",
    "    use_perceiver: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encode features into aligned space (mirrors training helper).\"\"\"\n",
    "    m = get_model_module(model)\n",
    "\n",
    "    if modality == \"vision\":\n",
    "        adapter = m.vision_adapter\n",
    "    elif modality == \"audio\":\n",
    "        adapter = m.audio_adapter\n",
    "    elif modality == \"text\":\n",
    "        adapter = m.text_adapter\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    tokens = adapter(feats)  # (B, T, perceiver_dim)\n",
    "\n",
    "    if use_perceiver and hasattr(m, \"perceiver\") and m.perceiver is not None:\n",
    "        latents = m.perceiver(tokens, mask)  # (B, K, perceiver_dim)\n",
    "    else:\n",
    "        # MLP-only: treat tokens as latents and pool across sequence\n",
    "        latents = tokens  # (B, T, perceiver_dim)\n",
    "\n",
    "    z = m.alignment_projector(latents)  # (B, d_align)\n",
    "    return z\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_pair_embeddings(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    loader: DataLoader,\n",
    "    modality_a: str,\n",
    "    modality_b: str,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: Optional[int] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Collect aligned embeddings for a chosen pair: {vision,audio,text}.\"\"\"\n",
    "    model.eval()\n",
    "    all_a, all_b = [], []\n",
    "\n",
    "    for b_idx, batch in enumerate(tqdm(loader, desc=f\"Embeddings {modality_a}-{modality_b}\")):\n",
    "        # Move tensors to device\n",
    "        batch_t = {}\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch_t[k] = v.to(device)\n",
    "            else:\n",
    "                batch_t[k] = v\n",
    "\n",
    "        if modality_a == \"vision\":\n",
    "            feats_a = encode_images_to_features(batch_t[\"images\"])\n",
    "            mask_a = None\n",
    "        elif modality_a == \"audio\":\n",
    "            feats_a = encode_audio_to_features(batch_t[\"audio\"], int(batch_t[\"sr\"]))\n",
    "            mask_a = None\n",
    "        elif modality_a == \"text\":\n",
    "            feats_a = encode_texts_to_features(batch_t[\"captions\"])\n",
    "            mask_a = None\n",
    "        else:\n",
    "            raise ValueError(modality_a)\n",
    "\n",
    "        if modality_b == \"vision\":\n",
    "            feats_b = encode_images_to_features(batch_t[\"images\"])\n",
    "            mask_b = None\n",
    "        elif modality_b == \"audio\":\n",
    "            feats_b = encode_audio_to_features(batch_t[\"audio\"], int(batch_t[\"sr\"]))\n",
    "            mask_b = None\n",
    "        elif modality_b == \"text\":\n",
    "            feats_b = encode_texts_to_features(batch_t[\"captions\"])\n",
    "            mask_b = None\n",
    "        else:\n",
    "            raise ValueError(modality_b)\n",
    "\n",
    "        z_a = encode_modality(model, feats_a, mask_a, modality_a, use_perceiver)\n",
    "        z_b = encode_modality(model, feats_b, mask_b, modality_b, use_perceiver)\n",
    "\n",
    "        all_a.append(z_a.cpu())\n",
    "        all_b.append(z_b.cpu())\n",
    "\n",
    "        if max_batches is not None and (b_idx + 1) >= max_batches:\n",
    "            break\n",
    "\n",
    "    emb_a = torch.cat(all_a, dim=0)\n",
    "    emb_b = torch.cat(all_b, dim=0)\n",
    "    print(f\"Collected {emb_a.shape[0]} pairs for {modality_a}-{modality_b}\")\n",
    "    return emb_a, emb_b\n",
    "\n",
    "\n",
    "def truncate_embeddings(z: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"Matryoshka truncation: keep only the first `dim` dimensions.\"\"\"\n",
    "    return z[..., :dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da7bcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluation loops (R@K, ranks, MRL curves)\n",
    "# ============================================================\n",
    "\n",
    "def eval_alignment_pair_mrl(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    loader: DataLoader,\n",
    "    modality_a: str,\n",
    "    modality_b: str,\n",
    "    use_perceiver: bool,\n",
    "    mrl_dims: List[int],\n",
    "    prefix: str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval for a modality pair over MRL dims.\"\"\"\n",
    "    z_a_full, z_b_full = collect_pair_embeddings(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        modality_a=modality_a,\n",
    "        modality_b=modality_b,\n",
    "        use_perceiver=use_perceiver,\n",
    "        max_batches=None,\n",
    "    )\n",
    "\n",
    "    results: Dict[str, float] = {}\n",
    "\n",
    "    for d in mrl_dims:\n",
    "        z_a = truncate_embeddings(z_a_full, d)\n",
    "        z_b = truncate_embeddings(z_b_full, d)\n",
    "\n",
    "        # A -> B and B -> A\n",
    "        metrics_a2b = compute_full_retrieval_metrics(z_a, z_b)\n",
    "        metrics_b2a = compute_full_retrieval_metrics(z_b, z_a)\n",
    "\n",
    "        for k in (1, 5, 10, 50):\n",
    "            results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/R@{k}\"] = metrics_a2b.get(f\"R@{k}\", 0.0)\n",
    "            results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/R@{k}\"] = metrics_b2a.get(f\"R@{k}\", 0.0)\n",
    "\n",
    "        results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/mean_rank\"] = metrics_a2b[\"mean_rank\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/median_rank\"] = metrics_a2b[\"median_rank\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/MRR\"] = metrics_a2b[\"MRR\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/NDCG@10\"] = metrics_a2b[\"NDCG@10\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_a}_to_{modality_b}/NDCG@50\"] = metrics_a2b[\"NDCG@50\"]\n",
    "\n",
    "        results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/mean_rank\"] = metrics_b2a[\"mean_rank\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/median_rank\"] = metrics_b2a[\"median_rank\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/MRR\"] = metrics_b2a[\"MRR\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/NDCG@10\"] = metrics_b2a[\"NDCG@10\"]\n",
    "        results[f\"{prefix}/d{d}/{modality_b}_to_{modality_a}/NDCG@50\"] = metrics_b2a[\"NDCG@50\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_eval_for_model(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    tag: str,\n",
    "    mrl_dims: Optional[List[int]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Run full eval (vision-text + audio-text) for a given model variant.\"\"\"\n",
    "    if mrl_dims is None:\n",
    "        # fall back to model's config if available\n",
    "        mm_cfg_model = get_model_module(model).config if hasattr(get_model_module(model), \"config\") else mm_cfg\n",
    "        mrl_dims = list(mm_cfg_model.mrl_dims)\n",
    "\n",
    "    results_all: Dict[str, float] = {}\n",
    "\n",
    "    # Visionâ€“Text (PixMo)\n",
    "    if image_val_loader is not None:\n",
    "        print(f\"\\n[{tag}] Evaluating Visionâ€“Text alignment...\")\n",
    "        vt = eval_alignment_pair_mrl(\n",
    "            model=model,\n",
    "            loader=image_val_loader,\n",
    "            modality_a=\"vision\",\n",
    "            modality_b=\"text\",\n",
    "            use_perceiver=(tag == \"perceiver\"),  # convention\n",
    "            mrl_dims=mrl_dims,\n",
    "            prefix=f\"{tag}/vision_text\",\n",
    "        )\n",
    "        results_all.update(vt)\n",
    "    else:\n",
    "        print(\"No image_val_loader, skipping vision-text eval.\")\n",
    "\n",
    "    # Audioâ€“Text (Clotho)\n",
    "    if audio_val_loader is not None:\n",
    "        print(f\"\\n[{tag}] Evaluating Audioâ€“Text alignment...\")\n",
    "        at = eval_alignment_pair_mrl(\n",
    "            model=model,\n",
    "            loader=audio_val_loader,\n",
    "            modality_a=\"audio\",\n",
    "            modality_b=\"text\",\n",
    "            use_perceiver=(tag == \"perceiver\"),  # convention\n",
    "            mrl_dims=mrl_dims,\n",
    "            prefix=f\"{tag}/audio_text\",\n",
    "        )\n",
    "        results_all.update(at)\n",
    "    else:\n",
    "        print(\"No audio_val_loader, skipping audio-text eval.\")\n",
    "\n",
    "    return results_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d47c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phase1_multimodal_eval</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/icoybhgc' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/icoybhgc</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_223425-icoybhgc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251201_223803-320a0iys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/320a0iys' target=\"_blank\">phase1_multimodal_eval</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/320a0iys' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/320a0iys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[mlp] Evaluating Visionâ€“Text alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings vision-text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 5 pairs for vision-text\n",
      "\n",
      "[mlp] Evaluating Audioâ€“Text alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings audio-text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 pairs for audio-text\n",
      "\n",
      "[perceiver] Evaluating Visionâ€“Text alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings vision-text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 5 pairs for vision-text\n",
      "\n",
      "[perceiver] Evaluating Audioâ€“Text alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings audio-text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:14<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 pairs for audio-text\n",
      "\n",
      "===== SUMMARY METRICS =====\n",
      "mlp/audio_text/d128/audio_to_text/MRR: 0.1375\n",
      "mlp/audio_text/d128/audio_to_text/NDCG@10: 0.1628\n",
      "mlp/audio_text/d128/audio_to_text/NDCG@50: 0.2335\n",
      "mlp/audio_text/d128/audio_to_text/R@1: 0.0570\n",
      "mlp/audio_text/d128/audio_to_text/R@10: 0.3060\n",
      "mlp/audio_text/d128/audio_to_text/R@5: 0.1970\n",
      "mlp/audio_text/d128/audio_to_text/R@50: 0.0000\n",
      "mlp/audio_text/d128/audio_to_text/mean_rank: 70.6780\n",
      "mlp/audio_text/d128/audio_to_text/median_rank: 29.0000\n",
      "mlp/audio_text/d128/text_to_audio/MRR: 0.1609\n",
      "mlp/audio_text/d128/text_to_audio/NDCG@10: 0.1875\n",
      "mlp/audio_text/d128/text_to_audio/NDCG@50: 0.2662\n",
      "mlp/audio_text/d128/text_to_audio/R@1: 0.0700\n",
      "mlp/audio_text/d128/text_to_audio/R@10: 0.3380\n",
      "mlp/audio_text/d128/text_to_audio/R@5: 0.2390\n",
      "mlp/audio_text/d128/text_to_audio/R@50: 0.0000\n",
      "mlp/audio_text/d128/text_to_audio/mean_rank: 58.5580\n",
      "mlp/audio_text/d128/text_to_audio/median_rank: 22.0000\n",
      "mlp/audio_text/d256/audio_to_text/MRR: 0.1417\n",
      "mlp/audio_text/d256/audio_to_text/NDCG@10: 0.1699\n",
      "mlp/audio_text/d256/audio_to_text/NDCG@50: 0.2412\n",
      "mlp/audio_text/d256/audio_to_text/R@1: 0.0560\n",
      "mlp/audio_text/d256/audio_to_text/R@10: 0.3220\n",
      "mlp/audio_text/d256/audio_to_text/R@5: 0.2130\n",
      "mlp/audio_text/d256/audio_to_text/R@50: 0.0000\n",
      "mlp/audio_text/d256/audio_to_text/mean_rank: 67.8920\n",
      "mlp/audio_text/d256/audio_to_text/median_rank: 27.0000\n",
      "mlp/audio_text/d256/text_to_audio/MRR: 0.1725\n",
      "mlp/audio_text/d256/text_to_audio/NDCG@10: 0.1992\n",
      "mlp/audio_text/d256/text_to_audio/NDCG@50: 0.2787\n",
      "mlp/audio_text/d256/text_to_audio/R@1: 0.0820\n",
      "mlp/audio_text/d256/text_to_audio/R@10: 0.3520\n",
      "mlp/audio_text/d256/text_to_audio/R@5: 0.2460\n",
      "mlp/audio_text/d256/text_to_audio/R@50: 0.0000\n",
      "mlp/audio_text/d256/text_to_audio/mean_rank: 56.3080\n",
      "mlp/audio_text/d256/text_to_audio/median_rank: 19.0000\n",
      "mlp/audio_text/d512/audio_to_text/MRR: 0.1441\n",
      "mlp/audio_text/d512/audio_to_text/NDCG@10: 0.1755\n",
      "mlp/audio_text/d512/audio_to_text/NDCG@50: 0.2451\n",
      "mlp/audio_text/d512/audio_to_text/R@1: 0.0570\n",
      "mlp/audio_text/d512/audio_to_text/R@10: 0.3360\n",
      "mlp/audio_text/d512/audio_to_text/R@5: 0.2170\n",
      "mlp/audio_text/d512/audio_to_text/R@50: 0.0000\n",
      "mlp/audio_text/d512/audio_to_text/mean_rank: 65.3420\n",
      "mlp/audio_text/d512/audio_to_text/median_rank: 26.0000\n",
      "mlp/audio_text/d512/text_to_audio/MRR: 0.1758\n",
      "mlp/audio_text/d512/text_to_audio/NDCG@10: 0.2009\n",
      "mlp/audio_text/d512/text_to_audio/NDCG@50: 0.2809\n",
      "mlp/audio_text/d512/text_to_audio/R@1: 0.0810\n",
      "mlp/audio_text/d512/text_to_audio/R@10: 0.3500\n",
      "mlp/audio_text/d512/text_to_audio/R@5: 0.2500\n",
      "mlp/audio_text/d512/text_to_audio/R@50: 0.0000\n",
      "mlp/audio_text/d512/text_to_audio/mean_rank: 55.1620\n",
      "mlp/audio_text/d512/text_to_audio/median_rank: 18.0000\n",
      "mlp/audio_text/d64/audio_to_text/MRR: 0.1329\n",
      "mlp/audio_text/d64/audio_to_text/NDCG@10: 0.1591\n",
      "mlp/audio_text/d64/audio_to_text/NDCG@50: 0.2307\n",
      "mlp/audio_text/d64/audio_to_text/R@1: 0.0520\n",
      "mlp/audio_text/d64/audio_to_text/R@10: 0.3070\n",
      "mlp/audio_text/d64/audio_to_text/R@5: 0.1790\n",
      "mlp/audio_text/d64/audio_to_text/R@50: 0.0000\n",
      "mlp/audio_text/d64/audio_to_text/mean_rank: 71.7770\n",
      "mlp/audio_text/d64/audio_to_text/median_rank: 28.0000\n",
      "mlp/audio_text/d64/text_to_audio/MRR: 0.1560\n",
      "mlp/audio_text/d64/text_to_audio/NDCG@10: 0.1831\n",
      "mlp/audio_text/d64/text_to_audio/NDCG@50: 0.2609\n",
      "mlp/audio_text/d64/text_to_audio/R@1: 0.0700\n",
      "mlp/audio_text/d64/text_to_audio/R@10: 0.3370\n",
      "mlp/audio_text/d64/text_to_audio/R@5: 0.2220\n",
      "mlp/audio_text/d64/text_to_audio/R@50: 0.0000\n",
      "mlp/audio_text/d64/text_to_audio/mean_rank: 59.4990\n",
      "mlp/audio_text/d64/text_to_audio/median_rank: 22.0000\n",
      "mlp/vision_text/d128/text_to_vision/MRR: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/NDCG@10: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/NDCG@50: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/R@1: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/R@10: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/R@5: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/R@50: 0.0000\n",
      "mlp/vision_text/d128/text_to_vision/mean_rank: 1.0000\n",
      "mlp/vision_text/d128/text_to_vision/median_rank: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/MRR: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/NDCG@10: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/NDCG@50: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/R@1: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/R@10: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/R@5: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/R@50: 0.0000\n",
      "mlp/vision_text/d128/vision_to_text/mean_rank: 1.0000\n",
      "mlp/vision_text/d128/vision_to_text/median_rank: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/MRR: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/NDCG@10: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/NDCG@50: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/R@1: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/R@10: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/R@5: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/R@50: 0.0000\n",
      "mlp/vision_text/d256/text_to_vision/mean_rank: 1.0000\n",
      "mlp/vision_text/d256/text_to_vision/median_rank: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/MRR: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/NDCG@10: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/NDCG@50: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/R@1: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/R@10: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/R@5: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/R@50: 0.0000\n",
      "mlp/vision_text/d256/vision_to_text/mean_rank: 1.0000\n",
      "mlp/vision_text/d256/vision_to_text/median_rank: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/MRR: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/NDCG@10: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/NDCG@50: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/R@1: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/R@10: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/R@5: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/R@50: 0.0000\n",
      "mlp/vision_text/d512/text_to_vision/mean_rank: 1.0000\n",
      "mlp/vision_text/d512/text_to_vision/median_rank: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/MRR: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/NDCG@10: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/NDCG@50: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/R@1: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/R@10: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/R@5: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/R@50: 0.0000\n",
      "mlp/vision_text/d512/vision_to_text/mean_rank: 1.0000\n",
      "mlp/vision_text/d512/vision_to_text/median_rank: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/MRR: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/NDCG@10: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/NDCG@50: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/R@1: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/R@10: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/R@5: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/R@50: 0.0000\n",
      "mlp/vision_text/d64/text_to_vision/mean_rank: 1.0000\n",
      "mlp/vision_text/d64/text_to_vision/median_rank: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/MRR: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/NDCG@10: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/NDCG@50: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/R@1: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/R@10: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/R@5: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/R@50: 0.0000\n",
      "mlp/vision_text/d64/vision_to_text/mean_rank: 1.0000\n",
      "mlp/vision_text/d64/vision_to_text/median_rank: 1.0000\n",
      "perceiver/audio_text/d128/audio_to_text/MRR: 0.2379\n",
      "perceiver/audio_text/d128/audio_to_text/NDCG@10: 0.2830\n",
      "perceiver/audio_text/d128/audio_to_text/NDCG@50: 0.3551\n",
      "perceiver/audio_text/d128/audio_to_text/R@1: 0.1150\n",
      "perceiver/audio_text/d128/audio_to_text/R@10: 0.4860\n",
      "perceiver/audio_text/d128/audio_to_text/R@5: 0.3600\n",
      "perceiver/audio_text/d128/audio_to_text/R@50: 0.0000\n",
      "perceiver/audio_text/d128/audio_to_text/mean_rank: 37.9490\n",
      "perceiver/audio_text/d128/audio_to_text/median_rank: 11.0000\n",
      "perceiver/audio_text/d128/text_to_audio/MRR: 0.2373\n",
      "perceiver/audio_text/d128/text_to_audio/NDCG@10: 0.2847\n",
      "perceiver/audio_text/d128/text_to_audio/NDCG@50: 0.3568\n",
      "perceiver/audio_text/d128/text_to_audio/R@1: 0.1170\n",
      "perceiver/audio_text/d128/text_to_audio/R@10: 0.4970\n",
      "perceiver/audio_text/d128/text_to_audio/R@5: 0.3570\n",
      "perceiver/audio_text/d128/text_to_audio/R@50: 0.0000\n",
      "perceiver/audio_text/d128/text_to_audio/mean_rank: 35.9660\n",
      "perceiver/audio_text/d128/text_to_audio/median_rank: 11.0000\n",
      "perceiver/audio_text/d256/audio_to_text/MRR: 0.2384\n",
      "perceiver/audio_text/d256/audio_to_text/NDCG@10: 0.2817\n",
      "perceiver/audio_text/d256/audio_to_text/NDCG@50: 0.3553\n",
      "perceiver/audio_text/d256/audio_to_text/R@1: 0.1200\n",
      "perceiver/audio_text/d256/audio_to_text/R@10: 0.4810\n",
      "perceiver/audio_text/d256/audio_to_text/R@5: 0.3510\n",
      "perceiver/audio_text/d256/audio_to_text/R@50: 0.0000\n",
      "perceiver/audio_text/d256/audio_to_text/mean_rank: 37.8700\n",
      "perceiver/audio_text/d256/audio_to_text/median_rank: 11.0000\n",
      "perceiver/audio_text/d256/text_to_audio/MRR: 0.2394\n",
      "perceiver/audio_text/d256/text_to_audio/NDCG@10: 0.2853\n",
      "perceiver/audio_text/d256/text_to_audio/NDCG@50: 0.3587\n",
      "perceiver/audio_text/d256/text_to_audio/R@1: 0.1230\n",
      "perceiver/audio_text/d256/text_to_audio/R@10: 0.4950\n",
      "perceiver/audio_text/d256/text_to_audio/R@5: 0.3520\n",
      "perceiver/audio_text/d256/text_to_audio/R@50: 0.0000\n",
      "perceiver/audio_text/d256/text_to_audio/mean_rank: 35.4140\n",
      "perceiver/audio_text/d256/text_to_audio/median_rank: 11.0000\n",
      "perceiver/audio_text/d512/audio_to_text/MRR: 0.2409\n",
      "perceiver/audio_text/d512/audio_to_text/NDCG@10: 0.2844\n",
      "perceiver/audio_text/d512/audio_to_text/NDCG@50: 0.3575\n",
      "perceiver/audio_text/d512/audio_to_text/R@1: 0.1220\n",
      "perceiver/audio_text/d512/audio_to_text/R@10: 0.4840\n",
      "perceiver/audio_text/d512/audio_to_text/R@5: 0.3490\n",
      "perceiver/audio_text/d512/audio_to_text/R@50: 0.0000\n",
      "perceiver/audio_text/d512/audio_to_text/mean_rank: 37.6090\n",
      "perceiver/audio_text/d512/audio_to_text/median_rank: 11.0000\n",
      "perceiver/audio_text/d512/text_to_audio/MRR: 0.2368\n",
      "perceiver/audio_text/d512/text_to_audio/NDCG@10: 0.2855\n",
      "perceiver/audio_text/d512/text_to_audio/NDCG@50: 0.3570\n",
      "perceiver/audio_text/d512/text_to_audio/R@1: 0.1160\n",
      "perceiver/audio_text/d512/text_to_audio/R@10: 0.5020\n",
      "perceiver/audio_text/d512/text_to_audio/R@5: 0.3610\n",
      "perceiver/audio_text/d512/text_to_audio/R@50: 0.0000\n",
      "perceiver/audio_text/d512/text_to_audio/mean_rank: 35.2690\n",
      "perceiver/audio_text/d512/text_to_audio/median_rank: 10.0000\n",
      "perceiver/audio_text/d64/audio_to_text/MRR: 0.2362\n",
      "perceiver/audio_text/d64/audio_to_text/NDCG@10: 0.2809\n",
      "perceiver/audio_text/d64/audio_to_text/NDCG@50: 0.3538\n",
      "perceiver/audio_text/d64/audio_to_text/R@1: 0.1120\n",
      "perceiver/audio_text/d64/audio_to_text/R@10: 0.4830\n",
      "perceiver/audio_text/d64/audio_to_text/R@5: 0.3590\n",
      "perceiver/audio_text/d64/audio_to_text/R@50: 0.0000\n",
      "perceiver/audio_text/d64/audio_to_text/mean_rank: 37.8890\n",
      "perceiver/audio_text/d64/audio_to_text/median_rank: 11.0000\n",
      "perceiver/audio_text/d64/text_to_audio/MRR: 0.2316\n",
      "perceiver/audio_text/d64/text_to_audio/NDCG@10: 0.2770\n",
      "perceiver/audio_text/d64/text_to_audio/NDCG@50: 0.3523\n",
      "perceiver/audio_text/d64/text_to_audio/R@1: 0.1150\n",
      "perceiver/audio_text/d64/text_to_audio/R@10: 0.4860\n",
      "perceiver/audio_text/d64/text_to_audio/R@5: 0.3460\n",
      "perceiver/audio_text/d64/text_to_audio/R@50: 0.0000\n",
      "perceiver/audio_text/d64/text_to_audio/mean_rank: 35.7120\n",
      "perceiver/audio_text/d64/text_to_audio/median_rank: 11.0000\n",
      "perceiver/vision_text/d128/text_to_vision/MRR: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/R@1: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/R@10: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/R@5: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/R@50: 0.0000\n",
      "perceiver/vision_text/d128/text_to_vision/mean_rank: 1.0000\n",
      "perceiver/vision_text/d128/text_to_vision/median_rank: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/MRR: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/R@1: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/R@10: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/R@5: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/R@50: 0.0000\n",
      "perceiver/vision_text/d128/vision_to_text/mean_rank: 1.0000\n",
      "perceiver/vision_text/d128/vision_to_text/median_rank: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/MRR: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/R@1: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/R@10: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/R@5: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/R@50: 0.0000\n",
      "perceiver/vision_text/d256/text_to_vision/mean_rank: 1.0000\n",
      "perceiver/vision_text/d256/text_to_vision/median_rank: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/MRR: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/R@1: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/R@10: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/R@5: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/R@50: 0.0000\n",
      "perceiver/vision_text/d256/vision_to_text/mean_rank: 1.0000\n",
      "perceiver/vision_text/d256/vision_to_text/median_rank: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/MRR: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/R@1: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/R@10: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/R@5: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/R@50: 0.0000\n",
      "perceiver/vision_text/d512/text_to_vision/mean_rank: 1.0000\n",
      "perceiver/vision_text/d512/text_to_vision/median_rank: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/MRR: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/R@1: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/R@10: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/R@5: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/R@50: 0.0000\n",
      "perceiver/vision_text/d512/vision_to_text/mean_rank: 1.0000\n",
      "perceiver/vision_text/d512/vision_to_text/median_rank: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/MRR: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/R@1: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/R@10: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/R@5: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/R@50: 0.0000\n",
      "perceiver/vision_text/d64/text_to_vision/mean_rank: 1.0000\n",
      "perceiver/vision_text/d64/text_to_vision/median_rank: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/MRR: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/NDCG@10: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/NDCG@50: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/R@1: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/R@10: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/R@5: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/R@50: 0.0000\n",
      "perceiver/vision_text/d64/vision_to_text/mean_rank: 1.0000\n",
      "perceiver/vision_text/d64/vision_to_text/median_rank: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>mlp/audio_text/d128/audio_to_text/MRR</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/NDCG@10</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/NDCG@50</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@1</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@10</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@5</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@50</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/mean_rank</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/median_rank</td><td>â–</td></tr><tr><td>mlp/audio_text/d128/text_to_audio/MRR</td><td>â–</td></tr><tr><td>+278</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>mlp/audio_text/d128/audio_to_text/MRR</td><td>0.13751</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/NDCG@10</td><td>0.16276</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/NDCG@50</td><td>0.23348</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@1</td><td>0.057</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@10</td><td>0.306</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@5</td><td>0.197</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/R@50</td><td>0</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/mean_rank</td><td>70.678</td></tr><tr><td>mlp/audio_text/d128/audio_to_text/median_rank</td><td>29</td></tr><tr><td>mlp/audio_text/d128/text_to_audio/MRR</td><td>0.16094</td></tr><tr><td>+278</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phase1_multimodal_eval</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/320a0iys' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/320a0iys</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_223803-320a0iys/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main: run eval for available checkpoints + (optional) W&B\n",
    "# ============================================================\n",
    "\n",
    "use_wandb = cfg.use_wandb and WANDB_AVAILABLE\n",
    "\n",
    "run = None\n",
    "if use_wandb:\n",
    "    run = wandb.init(\n",
    "        project=cfg.wandb_project,\n",
    "        entity=cfg.wandb_entity,\n",
    "        mode=cfg.wandb_mode,\n",
    "        name=cfg.wandb_run_name,\n",
    "        config={\n",
    "            \"eval\": True,\n",
    "            \"ckpt_root\": str(CKPT_ROOT),\n",
    "            \"mlp_dir\": str(MLP_DIR),\n",
    "            \"perceiver_dir\": str(PERCEIVER_DIR),\n",
    "        },\n",
    "    )\n",
    "\n",
    "all_results: Dict[str, float] = {}\n",
    "\n",
    "if model_mlp is not None:\n",
    "    res_mlp = run_eval_for_model(model_mlp, tag=\"mlp\")\n",
    "    all_results.update(res_mlp)\n",
    "\n",
    "if model_perceiver is not None:\n",
    "    res_perc = run_eval_for_model(model_perceiver, tag=\"perceiver\")\n",
    "    all_results.update(res_perc)\n",
    "\n",
    "print(\"\\n===== SUMMARY METRICS =====\")\n",
    "for k in sorted(all_results.keys()):\n",
    "    print(f\"{k}: {all_results[k]:.4f}\")\n",
    "\n",
    "if run is not None:\n",
    "    wandb.log(all_results)\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a9baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Plotting Utilities ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_rank_histogram\u001b[39m(ranks: \u001b[43mTensor\u001b[49m, title: \u001b[38;5;28mstr\u001b[39m, max_rank: \u001b[38;5;28mint\u001b[39m = \u001b[32m100\u001b[39m):\n\u001b[32m      4\u001b[39m     ranks_np = ranks.cpu().numpy()\n\u001b[32m      5\u001b[39m     ranks_np = np.clip(ranks_np, \u001b[32m0\u001b[39m, max_rank)\n",
      "\u001b[31mNameError\u001b[39m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce9bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ab4e9a",
   "metadata": {},
   "source": [
    "## Diagnostics & Plots\n",
    "\n",
    "To interpret alignment quality (similar to plots in **Freezeâ€‘Align** and related work), we provide:\n",
    "\n",
    "- **Rank histograms** (already logged to W&B)\n",
    "- **Rank CDF plots** â€“ how often the correct match is in the topâ€‘K\n",
    "- **Recall@K curves** â€“ R@K vs. K\n",
    "- **Positive vs. Negative similarity distributions** â€“ to see separation of matched vs. unmatched pairs\n",
    "- **Matryoshka curves** â€“ R@K vs. embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcc8dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plotting Utilities ===\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "def plot_rank_histogram(ranks: Tensor, title: str, max_rank: int = 100):\n",
    "    ranks_np = ranks.cpu().numpy()\n",
    "    ranks_np = np.clip(ranks_np, 0, max_rank)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(ranks_np + 1, bins=min(max_rank, 100))\n",
    "    plt.xlabel('Rank (1â€‘indexed)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rank_cdf(ranks: Tensor, title: str, max_k: int = 50):\n",
    "    ranks_np = ranks.cpu().numpy() + 1  # 1â€‘indexed\n",
    "    ks = np.arange(1, max_k + 1)\n",
    "    cdf = [(ranks_np <= k).mean() * 100.0 for k in ks]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(ks, cdf, marker='o')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('P(rank â‰¤ K) [%]')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_recall_curve(results: Dict[str, float], base_prefix: str, direction: str):\n",
    "    ks = [1, 5, 10, 50]\n",
    "    vals = [results.get(f'{base_prefix}/{direction}/R@{k}', np.nan) for k in ks]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(ks, vals, marker='o')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Recall@K [%]')\n",
    "    plt.title(f'Recall Curve: {direction}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_similarity_distributions(q: Tensor, k: Tensor, title: str, num_neg: int = 1024):\n",
    "    q = l2_normalize(q)\n",
    "    k = l2_normalize(k)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sims = q @ k.t()  # (N, N)\n",
    "        N = sims.size(0)\n",
    "        pos = sims.diag().cpu().numpy()\n",
    "\n",
    "        # Sample a subset of negatives for visualization\n",
    "        mask = torch.eye(N, device=sims.device).bool()\n",
    "        neg = sims.masked_fill(mask, float('-inf'))\n",
    "        neg_vals = neg[neg > -1e9].cpu().numpy()\n",
    "        if len(neg_vals) > num_neg:\n",
    "            idx = np.random.choice(len(neg_vals), size=num_neg, replace=False)\n",
    "            neg_vals = neg_vals[idx]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(neg_vals, bins=50, alpha=0.6, label='Negatives')\n",
    "    plt.hist(pos, bins=50, alpha=0.6, label='Positives')\n",
    "    plt.xlabel('Cosine similarity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mrl_curves(results: Dict[str, float], base_prefix: str, direction: str):\n",
    "    \"\"\"\n",
    "    Plot Matryoshka curves (R@K vs embedding dim) for a given direction.\n",
    "\n",
    "    Expects keys of the form:\n",
    "      {base_prefix}/mrl_dim_{d}/{direction}/R@K\n",
    "\n",
    "    Example:\n",
    "      vision_text/image_text/mrl_dim_64/text_to_image/R@1\n",
    "    \"\"\"\n",
    "    dims = []\n",
    "\n",
    "    for key in results.keys():\n",
    "        # We only care about keys that:\n",
    "        #  - start with base_prefix\n",
    "        #  - contain '/mrl_dim_'\n",
    "        #  - end with '/{direction}/R@1'\n",
    "        if not key.startswith(base_prefix):\n",
    "            continue\n",
    "        if '/mrl_dim_' not in key:\n",
    "            continue\n",
    "        if not key.endswith(f'{direction}/R@1'):\n",
    "            continue\n",
    "\n",
    "        parts = key.split('/')\n",
    "        # Find the segment like \"mrl_dim_64\"\n",
    "        dim_part = None\n",
    "        for p in parts:\n",
    "            if p.startswith('mrl_dim_'):\n",
    "                dim_part = p\n",
    "                break\n",
    "\n",
    "        if dim_part is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            d = int(dim_part.split('_')[-1])\n",
    "            dims.append(d)\n",
    "        except ValueError:\n",
    "            # In case something weird slips through\n",
    "            continue\n",
    "\n",
    "    dims = sorted(set(dims))\n",
    "    if not dims:\n",
    "        print('No Matryoshka results found for', direction)\n",
    "        return\n",
    "\n",
    "    r1_vals, r5_vals, r10_vals = [], [], []\n",
    "    for d in dims:\n",
    "        r1_vals.append(results.get(f'{base_prefix}/mrl_dim_{d}/{direction}/R@1', np.nan))\n",
    "        r5_vals.append(results.get(f'{base_prefix}/mrl_dim_{d}/{direction}/R@5', np.nan))\n",
    "        r10_vals.append(results.get(f'{base_prefix}/mrl_dim_{d}/{direction}/R@10', np.nan))\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(dims, r1_vals, marker='o', label='R@1')\n",
    "    plt.plot(dims, r5_vals, marker='o', label='R@5')\n",
    "    plt.plot(dims, r10_vals, marker='o', label='R@10')\n",
    "    plt.xlabel('Matryoshka dimension (d)')\n",
    "    plt.ylabel('Recall [%]')\n",
    "    plt.title(f'Matryoshka Retrieval vs Dim ({direction})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da459a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run eval (Phase-1 visionâ€“text only)\n",
    "image_text_results, (img_emb, txt_emb, ranks_t2i, ranks_i2t) = eval_image_text_retrieval(\n",
    "    aligned_model,\n",
    "    aligned_cfg,\n",
    "    pixmo_eval_loader,\n",
    "    prefix=f\"{model_kind}/image_text\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83797e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ranks_t2i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m base_prefix = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mmultimodal\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/image_text\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Rank histograms & CDFs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m plot_rank_histogram(\u001b[43mranks_t2i\u001b[49m, title=\u001b[33m'\u001b[39m\u001b[33mTextâ†’Image Rank Histogram\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plot_rank_histogram(ranks_i2t, title=\u001b[33m'\u001b[39m\u001b[33mImageâ†’Text Rank Histogram\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m plot_rank_cdf(ranks_t2i, title=\u001b[33m'\u001b[39m\u001b[33mTextâ†’Image Rank CDF\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ranks_t2i' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === Visualize Imageâ€“Text Alignment Diagnostics ===\n",
    "\n",
    "base_prefix = f'{\"multimodal\"}/image_text'\n",
    "\n",
    "# Rank histograms & CDFs\n",
    "plot_rank_histogram(ranks_t2i, title='Textâ†’Image Rank Histogram')\n",
    "plot_rank_histogram(ranks_i2t, title='Imageâ†’Text Rank Histogram')\n",
    "plot_rank_cdf(ranks_t2i, title='Textâ†’Image Rank CDF')\n",
    "plot_rank_cdf(ranks_i2t, title='Imageâ†’Text Rank CDF')\n",
    "\n",
    "# Recall curves\n",
    "plot_recall_curve(image_text_results, base_prefix, direction='text_to_image')\n",
    "plot_recall_curve(image_text_results, base_prefix, direction='image_to_text')\n",
    "\n",
    "# Similarity distributions\n",
    "plot_similarity_distributions(txt_emb, img_emb, title='Textâ€“Image Similarity Distributions (Aligned Space)')\n",
    "\n",
    "# Matryoshka curves (if available)\n",
    "base_prefix = f'{model_kind}/image_text'\n",
    "plot_mrl_curves(image_text_results, base_prefix, direction='text_to_image')\n",
    "plot_mrl_curves(image_text_results, base_prefix, direction='image_to_text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
