{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf00016",
   "metadata": {},
   "source": [
    "# 03a ‚Äì Alignment Retrieval Evaluation\n",
    "\n",
    "This notebook evaluates **Phase‚Äë1 alignment checkpoints** for retrieval:\n",
    "\n",
    "- Vision ‚Üî Text retrieval (image‚Üítext, text‚Üíimage)\n",
    "- (Optional) Audio ‚Üî Text and Vision ‚Üî Audio retrieval for multimodal Perceiver models\n",
    "- **Matryoshka** multi‚Äëscale evaluation (performance vs. truncation dimensionality)\n",
    "- Comparison between **pre‚Äëalignment** (frozen encoders) and **post‚Äëalignment** (projectors / Perceiver)\n",
    "\n",
    "All metrics and plots are logged to **Weights & Biases** for easy comparison across:\n",
    "\n",
    "- Different encoder pairs (e.g., DINOv2 + MiniLM vs CLIP + MiniLM)\n",
    "- Different alignment heads (MLP‚Äëonly vs Perceiver+MLP+MRL)\n",
    "- Different Matryoshka scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca7b1e",
   "metadata": {},
   "source": [
    "## Evaluation Plan & Metrics\n",
    "\n",
    "We follow common evaluation practices from **Freeze‚ÄëAlign, ImageBind, Matryoshka Multimodal Models, and Unified‚ÄëIO 2**:\n",
    "\n",
    "### 1. Retrieval Metrics\n",
    "For each modality pair (e.g., image‚Üîtext, audio‚Üîtext):\n",
    "\n",
    "- **Recall@K** for K ‚àà {1, 5, 10, 50}\n",
    "  - Image‚ÜíText: given an image embedding, rank all texts.\n",
    "  - Text‚ÜíImage: given a caption embedding, rank all images.\n",
    "- **Mean Rank (MR)** and **Median Rank (MedR)**\n",
    "- **Mean Average Precision @K (mAP@K)** with K ‚àà {10, 50}\n",
    "- **NDCG@K** for ranking quality (K ‚àà {10, 50})\n",
    "\n",
    "These mirror the metrics used in CLIP/Freeze‚ÄëAlign style image‚Äëtext retrieval benchmarks.\n",
    "\n",
    "### 2. Matryoshka Multi‚ÄëScale Evaluation\n",
    "Given Matryoshka dimensions `mrl_dims = [d1, d2, ..., dN]` used during training:\n",
    "\n",
    "- Compute the full set of retrieval metrics at each scale by truncating embeddings to the first `d_i` dims.\n",
    "- This lets us draw **performance vs. dimension** curves (e.g., R@1 vs. dim), similar to Matryoshka.\n",
    "\n",
    "### 3. Baseline vs. Aligned Comparison\n",
    "\n",
    "If we can access frozen encoder outputs (without alignment projectors):\n",
    "- Evaluate retrieval in the **original encoder spaces** (e.g., DINOv2 CLS vs MiniLM sentence embedding).\n",
    "- Evaluate retrieval in the **aligned space** (after projectors / Perceiver).\n",
    "- Log relative gains (ŒîR@K, ŒîmAP, ŒîNDCG) for quick comparison.\n",
    "\n",
    "### 4. Diagnostics & Visualizations\n",
    "\n",
    "- Histograms of **positive ranks** (position of the true match in the ranked list).\n",
    "- Distributions of **intra‚Äëmodal** vs **cross‚Äëmodal** cosine similarities.\n",
    "- Optional: t‚ÄëSNE / UMAP scatter plot of aligned embeddings colored by class / concept (if labels exist).\n",
    "- Aggregated tables for quick copy‚Äëpaste into the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d1868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40d851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Imports & Environment Setup ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import math\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, l2_normalize\n",
    "from imports.multimodal_alignment_perceiver import MultimodalAlignmentModel, MultimodalAlignmentConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from imports.in_memory_datasets import (\n",
    "    InMemoryImageTextDataset,\n",
    "    collate_in_memory_images,\n",
    ")\n",
    "\n",
    "from imports.train import load_checkpoint as load_vt_checkpoint  # Phase-1 vision-text loader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daba1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251201_204904-17fqpzxk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/17fqpzxk' target=\"_blank\">phase1_alignment_retrieval_eval</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/17fqpzxk' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/17fqpzxk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Config: Paths, Checkpoints, and WandB ===\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Root of your EdgeGlass / alignment project\n",
    "ROOT_DIR = Path(os.environ.get('EDGE_GLASS_ROOT', '.')).resolve()\n",
    "\n",
    "# Directory where Phase‚Äë1 alignment checkpoints are saved\n",
    "# Example pattern (adjust as needed):\n",
    "#   ROOT_DIR / 'checkpoints' / 'phase1' / 'vision_text' / 'minilm_siglip' / 'best.pt'\n",
    "CHECKPOINT_PATH = ROOT_DIR / \"checkpoints\" / \"phase1\" / \"vision_text\" / \"best.pt\"\n",
    "\n",
    "# If you also want to evaluate the multimodal Perceiver alignment model,\n",
    "# set this path as well (optional).\n",
    "CHECKPOINT_MLP_MULTI = ROOT_DIR / \"checkpoints\" / \"phase1_multimodal\" / \"mlp_mrl\" / \"best.pt\"\n",
    "CHECKPOINT_PERCEIVER_MULTI = ROOT_DIR / \"checkpoints\" / \"phase1_multimodal\" / \"perceiver_mrl\" / \"best.pt\"\n",
    "\n",
    "\n",
    "# Dataset settings (evaluation split)\n",
    "# We reuse the same loader utilities as training; you can also swap in\n",
    "# your Parquet‚Äëbased feature datasets if desired.\n",
    "DATASET_NAME = 'pixmo_cap'  # or whatever you used in training\n",
    "EVAL_SPLIT = 'val'          # or 'test' if you created a test split\n",
    "MAX_EVAL_SAMPLES = 500    # cap for quick eval; set None for full\n",
    "\n",
    "# Dataloader settings\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# WandB config\n",
    "WANDB_PROJECT = 'edgeglass_phase1_alignment'\n",
    "WANDB_ENTITY = None  # set your entity if needed\n",
    "RUN_NAME = 'phase1_alignment_retrieval_eval'\n",
    "\n",
    "wandb_config = {\n",
    "    'checkpoint_path': str(CHECKPOINT_PATH),\n",
    "    'perceiver_checkpoint_path': str(CHECKPOINT_PERCEIVER_MULTI),\n",
    "    'mlp_checkpoint_path': str(CHECKPOINT_MLP_MULTI),\n",
    "    'dataset_name': DATASET_NAME,\n",
    "    'eval_split': EVAL_SPLIT,\n",
    "    'max_eval_samples': MAX_EVAL_SAMPLES,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "}\n",
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=RUN_NAME,\n",
    "    job_type='alignment_eval',\n",
    "    config=wandb_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2aab30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset  # at top with other imports\n",
    "\n",
    "# Path to the Parquet file that contains the image data + captions\n",
    "PIXMO_PARQUET_PATH = ROOT_DIR / \"data\" / \"alignment_offline\" / \"pixmocap_offline_20000.parquet\"\n",
    "\n",
    "# Column names inside the Parquet\n",
    "PIXMO_IMAGE_COL = \"image_bytes\"   # or \"image_path\" or whatever you used\n",
    "PIXMO_TEXT_COL = \"caption\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c226cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Loading PixMo-Cap from parquet: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_offline/pixmocap_offline_20000.parquet\n",
      "[Eval] Columns: ['image_bytes', 'caption', 'image_url', 'sample_id']\n",
      "[Eval] Capped eval samples to 500\n",
      "\n",
      "[Eval] Example row preview:\n",
      "{'image_bytes': \"b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x00\\\\xff\\\\xe2\\\\x\", 'caption': 'This aerial photograph showcases a meticulously organized array of travel essent', 'image_url': 'https://i.redd.it/wbibz0yne60c1.jpg', 'sample_id': 'pixmo_0004864'}\n",
      "\n",
      "üì• Pre-loading 500 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 32 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1957.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 500 images into memory\n",
      "   ‚ö†Ô∏è  500 images failed to load (using fallback)\n",
      "[Eval] Batches per epoch: 8\n",
      "Eval batches: 8\n"
     ]
    }
   ],
   "source": [
    "def build_eval_dataloader() -> DataLoader:\n",
    "    \"\"\"\n",
    "    Build evaluation dataloader using the local PixMo-Cap parquet file,\n",
    "    matching the in-memory setup used during training.\n",
    "    \"\"\"\n",
    "    assert PIXMO_PARQUET_PATH.exists(), f\"PixMo parquet not found at {PIXMO_PARQUET_PATH}\"\n",
    "\n",
    "    print(f\"[Eval] Loading PixMo-Cap from parquet: {PIXMO_PARQUET_PATH}\")\n",
    "    pixmo_local = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files={\"train\": str(PIXMO_PARQUET_PATH)},\n",
    "    )[\"train\"]\n",
    "\n",
    "    print(\"[Eval] Columns:\", pixmo_local.column_names)\n",
    "\n",
    "    if \"split\" in pixmo_local.column_names:\n",
    "        before = len(pixmo_local)\n",
    "        pixmo_local = pixmo_local.filter(lambda ex: ex[\"split\"] == EVAL_SPLIT)\n",
    "        print(f\"[Eval] Filtered by split='{EVAL_SPLIT}': {before} -> {len(pixmo_local)} samples\")\n",
    "\n",
    "    if MAX_EVAL_SAMPLES is not None:\n",
    "        n = min(MAX_EVAL_SAMPLES, len(pixmo_local))\n",
    "        pixmo_local = pixmo_local.select(range(n))\n",
    "        print(f\"[Eval] Capped eval samples to {len(pixmo_local)}\")\n",
    "\n",
    "    print(\"\\n[Eval] Example row preview:\")\n",
    "    ex0 = pixmo_local[0]\n",
    "    print({k: str(v)[:80] for k, v in ex0.items()})\n",
    "\n",
    "    dataset = InMemoryImageTextDataset(\n",
    "        hf_dataset=pixmo_local,\n",
    "        img_col=PIXMO_IMAGE_COL,\n",
    "        txt_col=PIXMO_TEXT_COL,\n",
    "        max_samples=None,\n",
    "        image_size=(224, 224),\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(f\"[Eval] Batches per epoch: {len(dataloader)}\")\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "eval_loader = build_eval_dataloader()\n",
    "print(\"Eval batches:\", len(eval_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca220a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "[TextEncoder] Loaded sentence-transformers/all-MiniLM-L6-v2, hidden_size=384\n",
      "[VisionTextAligner] d_vision=768, d_text=384, d_align=512\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1/vision_text/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m     model.eval()\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, cfg\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m vt_model, vt_cfg = \u001b[43mload_aligned_vision_text_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded VisionTextAligner with d_align =\u001b[39m\u001b[33m\"\u001b[39m, vt_cfg.d_align)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMatryoshka dims:\u001b[39m\u001b[33m\"\u001b[39m, vt_cfg.mrl_dims)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mload_aligned_vision_text_model\u001b[39m\u001b[34m(checkpoint_path)\u001b[39m\n\u001b[32m     13\u001b[39m model = VisionTextAligner(cfg).to(device)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# This populates vision_adapter and text_adapter from checkpoint\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mload_vt_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m model.eval()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, cfg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/imports/train.py:435\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(model, checkpoint_path, load_optimizer, optimizer)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(\n\u001b[32m    429\u001b[39m     model: VisionTextAligner,\n\u001b[32m    430\u001b[39m     checkpoint_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    431\u001b[39m     load_optimizer: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    432\u001b[39m     optimizer: Optional[AdamW] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    433\u001b[39m ):\n\u001b[32m    434\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load model checkpoint.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     model.vision_adapter.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mvision_adapter\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    438\u001b[39m     model.text_adapter.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mtext_adapter\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1/vision_text/best.pt'"
     ]
    }
   ],
   "source": [
    "def load_aligned_vision_text_model(checkpoint_path: Path) -> Tuple[VisionTextAligner, AlignmentConfig]:\n",
    "    \"\"\"\n",
    "    Load Phase-1 VisionTextAligner using train.py's checkpoint format.\n",
    "    \"\"\"\n",
    "    # IMPORTANT: mirror the config you used during training\n",
    "    cfg = AlignmentConfig(\n",
    "        vision_model_name=\"openai/clip-vit-base-patch32\",\n",
    "        text_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        d_align=512,\n",
    "        mrl_dims=[64, 128, 256, 512],\n",
    "        device=str(device),\n",
    "    )\n",
    "    model = VisionTextAligner(cfg).to(device)\n",
    "\n",
    "    # This populates vision_adapter and text_adapter from checkpoint\n",
    "    load_vt_checkpoint(model, str(checkpoint_path))\n",
    "\n",
    "    model.eval()\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "vt_model, vt_cfg = load_aligned_vision_text_model(CHECKPOINT_PATH)\n",
    "print(\"Loaded VisionTextAligner with d_align =\", vt_cfg.d_align)\n",
    "print(\"Matryoshka dims:\", vt_cfg.mrl_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper: Exhaustive Retrieval Metrics ===\n",
    "\n",
    "def compute_full_retrieval_metrics(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    ks: Tuple[int, ...] = (1, 5, 10, 50),\n",
    "    map_ks: Tuple[int, ...] = (10, 50),\n",
    "    ndcg_ks: Tuple[int, ...] = (10, 50),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute rich retrieval metrics for one‚Äëto‚Äëone matching.\n",
    "\n",
    "    Assumes q[i] should match k[i]. This is the typical setting for\n",
    "    image‚Äìcaption retrieval on captioned datasets.\n",
    "    \"\"\"\n",
    "    q = l2_normalize(q)\n",
    "    k = l2_normalize(k)\n",
    "\n",
    "    sims = q @ k.t()  # (N, N)\n",
    "    N = sims.size(0)\n",
    "    targets = torch.arange(N, device=sims.device)\n",
    "\n",
    "    # Sort in descending similarity\n",
    "    _, indices = sims.sort(dim=1, descending=True)\n",
    "\n",
    "    # Position of the correct item for each query\n",
    "    # ranks[i] is 0‚Äëindexed rank of the correct match\n",
    "    ranks = (indices == targets.unsqueeze(1)).nonzero(as_tuple=False)[:, 1]\n",
    "\n",
    "    metrics: Dict[str, float] = {}\n",
    "\n",
    "    # Recall@K\n",
    "    for k_val in ks:\n",
    "        hit_rate = (ranks < k_val).float().mean().item()\n",
    "        metrics[f'R@{k_val}'] = hit_rate * 100.0\n",
    "\n",
    "    # Rank statistics (1‚Äëindexed for readability)\n",
    "    metrics['mean_rank'] = (ranks.float() + 1).mean().item()\n",
    "    metrics['median_rank'] = (ranks.float() + 1).median().item()\n",
    "\n",
    "    # mAP@K (only one relevant item per query => AP = 1/rank if rank < K else 0)\n",
    "    for k_val in map_ks:\n",
    "        ap = torch.where(ranks < k_val, 1.0 / (ranks.float() + 1.0), torch.zeros_like(ranks, dtype=torch.float))\n",
    "        metrics[f'mAP@{k_val}'] = ap.mean().item()\n",
    "\n",
    "    # NDCG@K (single relevant item per query)\n",
    "    # DCG = 1 / log2(rank + 2) if rank < K else 0; IDCG = 1\n",
    "    for k_val in ndcg_ks:\n",
    "        gains = torch.where(\n",
    "            ranks < k_val,\n",
    "            1.0 / torch.log2(ranks.float() + 2.0),\n",
    "            torch.zeros_like(ranks, dtype=torch.float),\n",
    "        )\n",
    "        metrics[f'NDCG@{k_val}'] = gains.mean().item()\n",
    "\n",
    "    # Also return raw ranks for diagnostic plots\n",
    "    metrics['ranks_tensor'] = ranks\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Vision‚ÄìText Retrieval Evaluation ===\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_vision_text_embeddings(\n",
    "    model: VisionTextAligner,\n",
    "    loader: DataLoader,\n",
    "    use_text_as_query: bool = True,\n",
    "    max_batches: Optional[int] = None,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Collect aligned embeddings for image‚Äìtext pairs in the aligned space.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_img = []\n",
    "    all_txt = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Collecting embeddings\")):\n",
    "        images = batch[\"images\"]\n",
    "        texts = batch[\"captions\"]\n",
    "\n",
    "        z_img = model.encode_vision(images)  # (B, d_align)\n",
    "        z_txt = model.encode_text(texts)     # (B, d_align)\n",
    "\n",
    "        all_img.append(z_img.cpu())\n",
    "        all_txt.append(z_txt.cpu())\n",
    "\n",
    "        if max_batches is not None and (batch_idx + 1) >= max_batches:\n",
    "            break\n",
    "\n",
    "    emb_img = torch.cat(all_img, dim=0)\n",
    "    emb_txt = torch.cat(all_txt, dim=0)\n",
    "    print(\"Collected\", emb_img.shape[0], \"pairs\")\n",
    "\n",
    "    if use_text_as_query:\n",
    "        q, k = emb_txt, emb_img\n",
    "    else:\n",
    "        q, k = emb_img, emb_txt\n",
    "\n",
    "    return q, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_vision_text_retrieval(model: VisionTextAligner, cfg: AlignmentConfig, loader: DataLoader) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate image‚Üîtext retrieval (both directions) including Matryoshka scales.\n",
    "    Logs everything to WandB.\n",
    "    \"\"\"\n",
    "    # 1. Collect aligned embeddings\n",
    "    txt_queries, img_keys = collect_vision_text_embeddings(\n",
    "        model, loader, use_text_as_query=True,\n",
    "    )\n",
    "    img_queries, txt_keys = img_keys, txt_queries  # reuse for opposite direction\n",
    "\n",
    "    results: Dict[str, float] = {}\n",
    "\n",
    "    # 2. Full‚Äëdim metrics\n",
    "    txt_img_metrics = compute_full_retrieval_metrics(txt_queries, img_keys)\n",
    "    img_txt_metrics = compute_full_retrieval_metrics(img_queries, txt_keys)\n",
    "\n",
    "    # Log basic metrics (R@K etc.)\n",
    "    for k_val in (1, 5, 10, 50):\n",
    "        results[f'text_to_image/R@{k_val}'] = txt_img_metrics[f'R@{k_val}']\n",
    "        results[f'image_to_text/R@{k_val}'] = img_txt_metrics[f'R@{k_val}']\n",
    "\n",
    "    results['text_to_image/mean_rank'] = txt_img_metrics['mean_rank']\n",
    "    results['text_to_image/median_rank'] = txt_img_metrics['median_rank']\n",
    "    results['image_to_text/mean_rank'] = img_txt_metrics['mean_rank']\n",
    "    results['image_to_text/median_rank'] = img_txt_metrics['median_rank']\n",
    "\n",
    "    for k_val in (10, 50):\n",
    "        results[f'text_to_image/mAP@{k_val}'] = txt_img_metrics[f'mAP@{k_val}']\n",
    "        results[f'image_to_text/mAP@{k_val}'] = img_txt_metrics[f'mAP@{k_val}']\n",
    "        results[f'text_to_image/NDCG@{k_val}'] = txt_img_metrics[f'NDCG@{k_val}']\n",
    "        results[f'image_to_text/NDCG@{k_val}'] = img_txt_metrics[f'NDCG@{k_val}']\n",
    "\n",
    "    # Log histograms of ranks\n",
    "    wandb.log({\n",
    "        'text_to_image/rank_hist': wandb.Histogram(txt_img_metrics['ranks_tensor'].cpu().numpy()),\n",
    "        'image_to_text/rank_hist': wandb.Histogram(img_txt_metrics['ranks_tensor'].cpu().numpy()),\n",
    "    })\n",
    "\n",
    "    # 3. Matryoshka evaluation (if dims are defined)\n",
    "    if cfg.mrl_dims and len(cfg.mrl_dims) > 0:\n",
    "        dims_sorted = sorted(cfg.mrl_dims)\n",
    "        for d in dims_sorted:\n",
    "            qt = txt_queries[:, :d]\n",
    "            ki = img_keys[:, :d]\n",
    "            qi = img_queries[:, :d]\n",
    "            kt = txt_keys[:, :d]\n",
    "\n",
    "            t2i = compute_full_retrieval_metrics(qt, ki)\n",
    "            i2t = compute_full_retrieval_metrics(qi, kt)\n",
    "\n",
    "            for k_val in (1, 5, 10, 50):\n",
    "                results[f'mrl_dim_{d}/text_to_image/R@{k_val}'] = t2i[f'R@{k_val}']\n",
    "                results[f'mrl_dim_{d}/image_to_text/R@{k_val}'] = i2t[f'R@{k_val}']\n",
    "\n",
    "            for k_val in (10, 50):\n",
    "                results[f'mrl_dim_{d}/text_to_image/mAP@{k_val}'] = t2i[f'mAP@{k_val}']\n",
    "                results[f'mrl_dim_{d}/image_to_text/mAP@{k_val}'] = i2t[f'mAP@{k_val}']\n",
    "\n",
    "    # 4. Log all scalar metrics to WandB\n",
    "    wandb.log(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "vt_retrieval_results = eval_vision_text_retrieval(vt_model, vt_cfg, eval_loader)\n",
    "vt_retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Multimodal Perceiver Retrieval (Vision, Audio, Text) ===\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_multimodal_alignment_model(checkpoint_path: Path) -> Optional[MultimodalAlignmentModel]:\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"[Eval] Multimodal checkpoint not found at {checkpoint_path}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    mm_cfg_dict = state[\"mm_config\"]\n",
    "    mm_cfg = MultimodalAlignmentConfig(**mm_cfg_dict)\n",
    "    mm_cfg.device = str(device)\n",
    "\n",
    "    model = MultimodalAlignmentModel(mm_cfg).to(device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    model.eval()\n",
    "    print(f\"[Eval] Loaded multimodal alignment model from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_multimodal_pairs(model: MultimodalAlignmentModel, df: pd.DataFrame, max_samples: Optional[int] = None) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval for (vision, audio, text) feature triples.\n",
    "\n",
    "    Assumes `df` has columns like:\n",
    "        - 'vision_feats'  : (T_v, D_v) flattened or serialized\n",
    "        - 'audio_feats'   : (T_a, D_a)\n",
    "        - 'text_feats'    : (T_t, D_t)\n",
    "\n",
    "    You can adapt this to your actual feature storage format.\n",
    "    \"\"\"\n",
    "    # This is a template: you'll need to plug in your own loading logic\n",
    "    # for Perceiver feature datasets. For now we just show the metric\n",
    "    # computation pattern once you have aligned embeddings.\n",
    "    \n",
    "    # Placeholders for demonstration\n",
    "    # vision_aligned, audio_aligned, text_aligned = ...\n",
    "    # For now, we skip implementation to avoid breaking the notebook.\n",
    "    print('‚ö†Ô∏è Perceiver multimodal eval is a template; fill in feature loading for your dataset.')\n",
    "    return {}\n",
    "\n",
    "\n",
    "perceiver_model = load_multimodal_alignment_model(CHECKPOINT_PERCEIVER_MULTI)\n",
    "if perceiver_model is not None:\n",
    "    # TODO: replace this with real multimodal feature dataframe\n",
    "    dummy_df = pd.DataFrame()\n",
    "    perceiver_results = eval_multimodal_pairs(perceiver_model, dummy_df)\n",
    "else:\n",
    "    perceiver_results = {}\n",
    "\n",
    "perceiver_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692651d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Diagnostics: Plots for Ranks & Matryoshka Scales ===\n",
    "\n",
    "def plot_rank_histogram(ranks: Tensor, title: str, max_rank: int = 100):\n",
    "    ranks_np = ranks.cpu().numpy()\n",
    "    ranks_np = np.clip(ranks_np, 0, max_rank)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(ranks_np + 1, bins=min(max_rank, 100))\n",
    "    plt.xlabel('Rank (1‚Äëindexed)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mrl_curves(results: Dict[str, float], direction: str = 'text_to_image'):\n",
    "    dims = []\n",
    "    r1_vals = []\n",
    "    r5_vals = []\n",
    "    r10_vals = []\n",
    "\n",
    "    for key, val in results.items():\n",
    "        if key.startswith('mrl_dim_') and f'{direction}/R@1' in key:\n",
    "            dim = int(key.split('/')[0].split('_')[-1])\n",
    "            dims.append(dim)\n",
    "    \n",
    "    dims = sorted(set(dims))\n",
    "    if not dims:\n",
    "        print('No Matryoshka results found in metrics dict.')\n",
    "        return\n",
    "\n",
    "    for d in dims:\n",
    "        r1_vals.append(results[f'mrl_dim_{d}/{direction}/R@1'])\n",
    "        r5_vals.append(results[f'mrl_dim_{d}/{direction}/R@5'])\n",
    "        r10_vals.append(results[f'mrl_dim_{d}/{direction}/R@10'])\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(dims, r1_vals, marker='o', label='R@1')\n",
    "    plt.plot(dims, r5_vals, marker='o', label='R@5')\n",
    "    plt.plot(dims, r10_vals, marker='o', label='R@10')\n",
    "    plt.xlabel('Matryoshka dimension (d)')\n",
    "    plt.ylabel('Recall (%)')\n",
    "    plt.title(f'Matryoshka Retrieval vs Dim ({direction})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage once `vt_retrieval_results` is computed:\n",
    "if 'text_to_image/R@1' in vt_retrieval_results:\n",
    "    print('Text‚ÜíImage R@1:', vt_retrieval_results['text_to_image/R@1'])\n",
    "    plot_mrl_curves(vt_retrieval_results, direction='text_to_image')\n",
    "    plot_mrl_curves(vt_retrieval_results, direction='image_to_text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b791d4",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "This notebook gives a **rich retrieval evaluation suite** for Phase‚Äë1 alignment models:\n",
    "\n",
    "- Standard image‚Üîtext retrieval metrics (R@K, MR, MedR, mAP, NDCG)\n",
    "- Matryoshka multi‚Äëscale analysis vs embedding dimension\n",
    "- Hooks to extend to multimodal Perceiver models (vision, audio, text)\n",
    "- All results logged to **Weights & Biases** for cross‚Äëexperiment comparison.\n",
    "\n",
    "**Next:**\n",
    "- Plug in your Parquet‚Äëbased PixMo‚ÄëCap / audio feature loaders into `build_eval_dataloader` and the Perceiver template.\n",
    "- Add baselines (e.g., CLIP, raw DINOv2 + MiniLM retrieval) and log them with a different `job_type` / `group` in WandB.\n",
    "- Use the logged tables & plots directly in your report, comparing against Freeze‚ÄëAlign, ImageBind, Matryoshka, and Unified‚ÄëIO 2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
