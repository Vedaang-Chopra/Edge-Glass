{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bda7c4f",
   "metadata": {},
   "source": [
    "# 02a Â· LLM Decoder Alignment\n",
    "\n",
    "Phase **2a** notebook: take a **pre-aligned visionâ€“text encoder** (Phase 1),\n",
    "freeze it, and train a **visionâ†’LLM projector** so that a causal LLM can\n",
    "generate captions from images.\n",
    "\n",
    "This notebook:\n",
    "1. Loads a Phaseâ€‘1 aligned `VisionTextAligner` checkpoint.\n",
    "2. Wraps it with `MultimodalLLM` (vision â†’ alignment â†’ LLM decoder).\n",
    "3. Uses PixMoâ€‘Cap (or your prebuilt inâ€‘memory loader) for imageâ€“caption pairs.\n",
    "4. Trains only the **visionâ†’LLM projector** (LLM + encoders frozen).\n",
    "5. Saves Phaseâ€‘2 checkpoints which can be reused by later TRM / MoE notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b74ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf7a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Phase 1 ckpt: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1/best.pt\n",
      "Phase 2 out dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase2_llm/qwen2p5_prefix_alignment_v1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Paths & Highâ€‘Level Config\n",
    "# -------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Root of your project (edit this to match your setup)\n",
    "ROOT_DIR = Path.cwd()\n",
    "\n",
    "# === Phase 1 (alignment) checkpoint ===\n",
    "# Point this to the *best* Phaseâ€‘1 alignment checkpoint (from `train_alignment`).\n",
    "# It should contain `vision_adapter` and `text_adapter` weights.\n",
    "PHASE1_CKPT_PATH = ROOT_DIR / \"checkpoints\" / \"phase1\" / \"best.pt\"  # <-- EDIT\n",
    "\n",
    "# === Phase 2 (LLM decoder alignment) output dir ===\n",
    "PHASE2_RUN_NAME = \"qwen2p5_prefix_alignment_v1\"  # change per experiment\n",
    "PHASE2_OUT_DIR = ROOT_DIR / \"checkpoints\" / \"phase2_llm\" / PHASE2_RUN_NAME\n",
    "PHASE2_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: where your PixMoâ€‘Cap *inâ€‘memory* / parquet data lives (if any).\n",
    "# This notebook defaults to the existing inâ€‘memory dataloader utilities,\n",
    "# but you can swap this for your parquet loader.\n",
    "PIXMO_PARQUET_PATH = None  # e.g. ROOT_DIR / \"data\" / \"pixmo_cap_train.parquet\"\n",
    "\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"Phase 1 ckpt:\", PHASE1_CKPT_PATH)\n",
    "print(\"Phase 2 out dir:\", PHASE2_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b186189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "W&B available: True\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Imports & Environment Setup\n",
    "# -------------------------------------------------------------\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, set_seed, count_parameters\n",
    "from imports.train import load_checkpoint\n",
    "from imports.llm_integration import (\n",
    "    LLMConfig,\n",
    "    MultimodalLLM,\n",
    "    create_caption_labels,\n",
    "    train_multimodal_step,\n",
    ")\n",
    "from imports.train_with_in_memory_datasets import create_in_memory_image_dataloader\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"W&B available:\", WANDB_AVAILABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f81bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Experiment Hyperparameters\n",
    "# -------------------------------------------------------------\n",
    "class Phase2Config:\n",
    "    \"\"\"Lightweight config object for Phaseâ€‘2 training.\"\"\"\n",
    "\n",
    "    # Randomness\n",
    "    seed = 42\n",
    "\n",
    "    # Data\n",
    "    max_samples = 20_000          # number of imageâ€“caption pairs to load\n",
    "    image_size = (224, 224)\n",
    "    batch_size_per_gpu = 8        # effective batch = batch_size_per_gpu * n_gpus\n",
    "    num_workers = 4\n",
    "\n",
    "    # Text / LLM\n",
    "    llm_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    max_caption_tokens = 128\n",
    "    num_prefix_tokens = 16        # how many soft prefix tokens from vision\n",
    "    freeze_llm = True             # Phase 2a: ONLY train projector\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 1.0\n",
    "    num_epochs = 3\n",
    "\n",
    "    # Logging\n",
    "    log_every = 50\n",
    "    use_wandb = True\n",
    "    wandb_project = \"edgeglass_phase2_llm_alignment\"\n",
    "    wandb_run_name = PHASE2_RUN_NAME\n",
    "\n",
    "cfg2 = Phase2Config()\n",
    "\n",
    "set_seed(cfg2.seed)\n",
    "\n",
    "# Device & multiâ€‘GPU\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    n_gpus = 0\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}, GPUs: {n_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Optional: Weights & Biases Init\n",
    "# -------------------------------------------------------------\n",
    "wandb_run = None\n",
    "if cfg2.use_wandb and WANDB_AVAILABLE:\n",
    "    wandb_run = wandb.init(\n",
    "        project=cfg2.wandb_project,\n",
    "        name=cfg2.wandb_run_name,\n",
    "        config={\n",
    "            \"phase\": \"02a_llm_decoder_alignment\",\n",
    "            \"phase1_ckpt\": str(PHASE1_CKPT_PATH),\n",
    "            \"llm_model\": cfg2.llm_model_name,\n",
    "            \"batch_size_per_gpu\": cfg2.batch_size_per_gpu,\n",
    "            \"num_prefix_tokens\": cfg2.num_prefix_tokens,\n",
    "            \"learning_rate\": cfg2.learning_rate,\n",
    "            \"num_epochs\": cfg2.num_epochs,\n",
    "        },\n",
    "    )\n",
    "    print(\"âœ… WandB run initialized:\", wandb_run.name)\n",
    "elif cfg2.use_wandb and not WANDB_AVAILABLE:\n",
    "    print(\"âš ï¸ cfg2.use_wandb=True but wandb is not installed. Logging disabled.\")\n",
    "else:\n",
    "    print(\"W&B logging disabled in config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Load Phaseâ€‘1 Aligned Visionâ€“Text Model\n",
    "# -------------------------------------------------------------\n",
    "assert PHASE1_CKPT_PATH.is_file(), f\"Phaseâ€‘1 checkpoint not found: {PHASE1_CKPT_PATH}\"\n",
    "\n",
    "print(\"Loading Phaseâ€‘1 checkpoint from:\", PHASE1_CKPT_PATH)\n",
    "\n",
    "# The Phaseâ€‘1 checkpoint saved the AlignmentConfig object; we reuse it.\n",
    "checkpoint = torch.load(PHASE1_CKPT_PATH, map_location=\"cpu\")\n",
    "phase1_cfg: AlignmentConfig = checkpoint[\"config\"]\n",
    "\n",
    "# Update device in config in case you moved from CPUâ†’GPU or vice versa\n",
    "phase1_cfg.device = device\n",
    "\n",
    "# Instantiate aligner and load adapters\n",
    "aligner = VisionTextAligner(phase1_cfg).to(device)\n",
    "load_checkpoint(aligner, str(PHASE1_CKPT_PATH), load_optimizer=False)\n",
    "\n",
    "params_align = count_parameters(aligner)\n",
    "print(\"Phaseâ€‘1 model parameters:\", params_align)\n",
    "\n",
    "# We freeze the entire aligner in Phase 2a (adapters + encoders)\n",
    "for p in aligner.parameters():\n",
    "    p.requires_grad = False\n",
    "aligner.eval()\n",
    "\n",
    "print(\"Aligner frozen for Phaseâ€‘2 training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Build Phaseâ€‘2 Multimodal LLM Wrapper\n",
    "# -------------------------------------------------------------\n",
    "llm_cfg = LLMConfig(\n",
    "    model_name=cfg2.llm_model_name,\n",
    "    max_new_tokens=cfg2.max_caption_tokens,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_prefix_tokens=cfg2.num_prefix_tokens,\n",
    "    freeze_llm=cfg2.freeze_llm,\n",
    ")\n",
    "\n",
    "print(\"Initializing MultimodalLLM (this will download the LLM if needed)...\")\n",
    "mm_model = MultimodalLLM(aligner=aligner, llm_config=llm_cfg)\n",
    "mm_model.to(device)\n",
    "\n",
    "# Multiâ€‘GPU: wrap in DataParallel if more than 1 GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Wrapping MultimodalLLM with DataParallel across {torch.cuda.device_count()} GPUs\")\n",
    "    mm_model = torch.nn.DataParallel(mm_model)\n",
    "\n",
    "# For counting params, unwrap if needed\n",
    "model_for_params = mm_model.module if isinstance(mm_model, torch.nn.DataParallel) else mm_model\n",
    "trainable_params = sum(p.numel() for p in model_for_params.get_trainable_params())\n",
    "total_params = sum(p.numel() for p in model_for_params.parameters())\n",
    "\n",
    "print(f\"Total parameters (Phaseâ€‘2 model): {total_params:,}\")\n",
    "print(f\"Trainable parameters (projector only): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef67edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# DataLoader: PixMoâ€‘Cap Inâ€‘Memory Images + Captions\n",
    "# -------------------------------------------------------------\n",
    "effective_batch_size = cfg2.batch_size_per_gpu * max(1, torch.cuda.device_count())\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "# NOTE:\n",
    "#   We reuse `create_in_memory_image_dataloader` which already:\n",
    "#     â€¢ Loads PixMoâ€‘Cap via HuggingFace\n",
    "#     â€¢ Preâ€‘downloads & caches images in RAM\n",
    "#     â€¢ Returns batches with keys: 'images' (PIL list), 'captions' (str list)\n",
    "#   If you have a parquetâ€‘based loader, you can swap it in here.\n",
    "\n",
    "image_dataloader = create_in_memory_image_dataloader(\n",
    "    max_samples=cfg2.max_samples,\n",
    "    batch_size=effective_batch_size,\n",
    "    num_workers=cfg2.num_workers,\n",
    ")\n",
    "\n",
    "len(image_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d943679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Phaseâ€‘2 Training Loop (Instruction Tuning with Frozen LLM)\n",
    "# -------------------------------------------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_for_params.get_trainable_params(),\n",
    "    lr=cfg2.learning_rate,\n",
    "    weight_decay=cfg2.weight_decay,\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "history = {\"train_loss\": []}\n",
    "\n",
    "for epoch in range(cfg2.num_epochs):\n",
    "    model_for_params.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(image_dataloader, desc=f\"Epoch {epoch+1}/{cfg2.num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(pbar, start=1):\n",
    "        images = batch[\"images\"]                 # list of PIL images\n",
    "        captions = batch[\"captions\"]             # list of strings\n",
    "\n",
    "        # Tokenize captions into input_ids, attention_mask, labels\n",
    "        label_tensors = create_caption_labels(\n",
    "            tokenizer=model_for_params.llm.tokenizer,\n",
    "            captions=captions,\n",
    "            max_length=cfg2.max_caption_tokens,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        train_batch: Dict[str, Any] = {\n",
    "            \"images\": images,\n",
    "            \"input_ids\": label_tensors[\"input_ids\"],\n",
    "            \"attention_mask\": label_tensors[\"attention_mask\"],\n",
    "            \"labels\": label_tensors[\"labels\"],\n",
    "        }\n",
    "\n",
    "        metrics = train_multimodal_step(mm_model, train_batch, optimizer)\n",
    "\n",
    "        loss_val = metrics[\"loss\"]\n",
    "        epoch_loss += loss_val\n",
    "        n_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if batch_idx % cfg2.log_every == 0:\n",
    "            avg_loss = epoch_loss / max(1, n_batches)\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "            if wandb_run is not None:\n",
    "                wandb.log({\n",
    "                    \"train/loss\": loss_val,\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/epoch\": epoch,\n",
    "                    \"global_step\": global_step,\n",
    "                })\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / max(1, n_batches)\n",
    "    history[\"train_loss\"].append(avg_epoch_loss)\n",
    "    print(f\"\\nâœ… Epoch {epoch+1} complete | avg loss = {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Save checkpoint at end of each epoch\n",
    "    # We save only the projector + configs (aligner & LLM are reusable)\n",
    "    # ---------------------------------------------------------\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"global_step\": global_step,\n",
    "        \"phase1_config\": phase1_cfg,\n",
    "        \"llm_config\": llm_cfg,\n",
    "        \"projector_state_dict\": model_for_params.projector.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"history\": history,\n",
    "    }\n",
    "\n",
    "    ckpt_path = PHASE2_OUT_DIR / f\"projector_epoch_{epoch+1:02d}.pt\"\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "    print(\"ðŸ’¾ Saved Phaseâ€‘2 checkpoint:\", ckpt_path)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Phaseâ€‘2 LLM decoder alignment training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ff86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Quick Qualitative Check: Generate Captions\n",
    "# -------------------------------------------------------------\n",
    "mm_model.eval()\n",
    "\n",
    "sample_batch = next(iter(image_dataloader))\n",
    "sample_images = sample_batch[\"images\"][:2]\n",
    "sample_captions = sample_batch[\"captions\"][:2]\n",
    "\n",
    "print(\"Reference captions:\")\n",
    "for i, c in enumerate(sample_captions):\n",
    "    print(f\"[{i}] {c[:120]}...\")\n",
    "\n",
    "print(\"\\nGenerated captions:\")\n",
    "for i, img in enumerate(sample_images):\n",
    "    gen = model_for_params.generate(\n",
    "        images=img,\n",
    "        prompt=\"Describe this image in one sentence:\",\n",
    "        max_new_tokens=cfg2.max_caption_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(f\"[{i}] {gen}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Save Training History & Finish\n",
    "# -------------------------------------------------------------\n",
    "import json\n",
    "\n",
    "history_path = PHASE2_OUT_DIR / \"train_history_phase2_llm.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "\n",
    "if wandb_run is not None:\n",
    "    wandb_run.finish()\n",
    "    print(\"Closed W&B run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
