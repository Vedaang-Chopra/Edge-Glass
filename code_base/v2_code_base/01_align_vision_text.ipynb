{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838e8cf4",
   "metadata": {},
   "source": [
    "\n",
    "# 01 \u2014 Vision\u2013Text Alignment on PixMo-Cap (Phase 1, Improved)\n",
    "\n",
    "This notebook trains a **vision\u2013text alignment model** on a **local PixMo-Cap Parquet subset**\n",
    "(created by `00_build_alignment_datasets.ipynb`).\n",
    "\n",
    "It incorporates the key Phase\u20111 features from your original code:\n",
    "\n",
    "- Uses the same **`AlignmentConfig` + `VisionTextAligner`** (CLIP-style model)\n",
    "- Uses **Matryoshka (MRL) + CLIP** contrastive losses from `core.py`\n",
    "- Adds a **warmup + cosine learning rate schedule** (Phase\u20111 improvement)\n",
    "- Uses **in\u2011memory image\u2013text dataset** for fast training\n",
    "- Evaluates **image \u2194 text retrieval** on a validation split each epoch\n",
    "- Saves **`last`** and **`best`** checkpoints into a common directory:\n",
    "  - `artifacts/phase1_alignment/vision_text/`\n",
    "- Optionally resumes from an existing checkpoint\n",
    "- Logs training & validation metrics to **Weights & Biases (W&B)**\n",
    "- Supports **multi\u2011GPU** via `torch.nn.DataParallel` (1\u20132 GPUs)\n",
    "\n",
    "These checkpoints will be the **Phase\u20111 encoder** artifacts used by Phase\u20112 experiments\n",
    "(normal LLM decoder, TRM decoder, MoE decoder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a2628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ca11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root      : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Data dir          : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets\n",
      "Phase 1 dir       : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment\n",
      "Vision-text dir   : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# Local modules\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, compute_retrieval_metrics, set_seed, get_device\n",
    "from imports.in_memory_datasets import InMemoryImageTextDataset, collate_in_memory_images\n",
    "from imports.train import save_checkpoint, load_checkpoint\n",
    "\n",
    "# ---- Paths ----\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"alignment_subsets\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "PHASE1_DIR = ARTIFACTS_DIR / \"phase1_alignment\"\n",
    "VISION_TEXT_DIR = PHASE1_DIR / \"vision_text\"\n",
    "\n",
    "VISION_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root      : {PROJECT_ROOT}\")\n",
    "print(f\"Data dir          : {DATA_DIR}\")\n",
    "print(f\"Phase 1 dir       : {PHASE1_DIR}\")\n",
    "print(f\"Vision-text dir   : {VISION_TEXT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d31d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Dataset & training config\n",
    "# =========================\n",
    "\n",
    "class VTDataConfig:\n",
    "    'Config for PixMo-Cap subset used in Phase-1 alignment.'\n",
    "    # Path created by 00_build_alignment_datasets.ipynb\n",
    "    pixmocap_parquet = DATA_DIR / 'pixmocap_train_subset_50000.parquet'  # adjust if needed\n",
    "\n",
    "    # Subsampling and split\n",
    "    max_train_samples = 40_000\n",
    "    max_val_samples = 5_000\n",
    "    val_ratio = 0.1\n",
    "\n",
    "    # DataLoader\n",
    "    num_workers = 8\n",
    "    image_size = (224, 224)\n",
    "\n",
    "\n",
    "class VTTrainConfig:\n",
    "    'High-level training hyperparameters for Phase-1.'\n",
    "    seed = 42\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 1.0\n",
    "    use_amp = True\n",
    "\n",
    "    temperature = 0.07\n",
    "    mrl_weight = 1.0\n",
    "    clip_weight = 0.5\n",
    "\n",
    "    # LR scheduler: warmup fraction of total steps\n",
    "    warmup_fraction = 0.1\n",
    "\n",
    "    # Logging\n",
    "    use_wandb = True\n",
    "    wandb_project = 'edgeglass_phase1'\n",
    "    wandb_entity = None  # or your W&B username/org\n",
    "    wandb_group = 'vision_text_alignment'\n",
    "    wandb_run_name = 'vt_align_pixmocap_phase1'\n",
    "\n",
    "    # Checkpointing / resume\n",
    "    resume_from = None  # e.g., VISION_TEXT_DIR / 'vision_text_best.pt'\n",
    "    save_every_epochs = 1\n",
    "\n",
    "\n",
    "data_cfg = VTDataConfig()\n",
    "train_cfg = VTTrainConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4346155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTDataConfig:\n",
      "\n",
      "VTTrainConfig:\n"
     ]
    }
   ],
   "source": [
    "print(\"VTDataConfig:\")\n",
    "for k, v in data_cfg.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nVTTrainConfig:\")\n",
    "for k, v in train_cfg.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da73a3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AlignmentConfig.__init__() got an unexpected keyword argument 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# AlignmentConfig (model)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# You can adjust these to match your original Phase-1 experiments.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m align_cfg = \u001b[43mAlignmentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Keep your chosen encoders here (examples):\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# vision_model_name=\"openai/clip-vit-base-patch32\",\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# text_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Temperature & MRL settings from your original experiments:\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmrl_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmrl_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAlignmentConfig:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m asdict(align_cfg).items():\n",
      "\u001b[31mTypeError\u001b[39m: AlignmentConfig.__init__() got an unexpected keyword argument 'temperature'"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# AlignmentConfig (model)\n",
    "# =========================\n",
    "\n",
    "# You can adjust these to match your original Phase-1 experiments.\n",
    "align_cfg = AlignmentConfig(\n",
    "    # Keep your chosen encoders here (examples):\n",
    "    # vision_model_name='openai/clip-vit-base-patch32',\n",
    "    # text_model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    num_epochs=train_cfg.num_epochs,\n",
    "    learning_rate=train_cfg.lr,\n",
    "    weight_decay=train_cfg.weight_decay,\n",
    "    warmup_ratio=train_cfg.warmup_fraction,\n",
    "    mrl_temperature=train_cfg.temperature,\n",
    "    clip_temperature=train_cfg.temperature,\n",
    "    # Temperature & MRL settings from your original experiments:\n",
    "    mrl_dims=(64, 128, 256, 512),\n",
    "    mrl_weight=train_cfg.mrl_weight,\n",
    "    clip_weight=train_cfg.clip_weight,\n",
    ")\n",
    "\n",
    "print('AlignmentConfig:')\n",
    "for k, v in asdict(align_cfg).items():\n",
    "    print(f'  {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Weights & Biases init\n",
    "# =========================\n",
    "\n",
    "run = None\n",
    "if train_cfg.use_wandb:\n",
    "    def _to_serializable(v):\n",
    "        if isinstance(v, Path):\n",
    "            return str(v)\n",
    "        if 'torch' in globals() and (isinstance(v, torch.device) or isinstance(v, torch.dtype)):\n",
    "            return str(v)\n",
    "        return v\n",
    "\n",
    "    wandb_kwargs = dict(\n",
    "        project=train_cfg.wandb_project,\n",
    "        name=train_cfg.wandb_run_name,\n",
    "        group=train_cfg.wandb_group,\n",
    "        config={\n",
    "            'phase': 'phase1_alignment',\n",
    "            'task': 'vision_text',\n",
    "            'data_cfg': {k: _to_serializable(v) for k, v in data_cfg.__dict__.items()},\n",
    "            'align_cfg': {k: _to_serializable(v) for k, v in asdict(align_cfg).items()},\n",
    "            'train_cfg': {k: _to_serializable(v) for k, v in train_cfg.__dict__.items()},\n",
    "        },\n",
    "    )\n",
    "    if train_cfg.wandb_entity is not None:\n",
    "        wandb_kwargs['entity'] = train_cfg.wandb_entity\n",
    "\n",
    "    run = wandb.init(**wandb_kwargs)\n",
    "    print('\u2705 W&B run initialized:', run.name)\n",
    "else:\n",
    "    print('W&B logging disabled.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Load PixMo-Cap subset from Parquet\n",
    "# =========================\n",
    "\n",
    "assert data_cfg.pixmocap_parquet.exists(), f\"Parquet file not found: {data_cfg.pixmocap_parquet}\"\n",
    "\n",
    "print(f\"\ud83d\udce5 Loading PixMo-Cap subset from: {data_cfg.pixmocap_parquet}\")\n",
    "pixmo_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": str(data_cfg.pixmocap_parquet)},\n",
    ")[\"train\"]\n",
    "\n",
    "print(f\"Total PixMo-Cap subset size: {len(pixmo_ds):,}\")\n",
    "\n",
    "# Train/val split\n",
    "split = pixmo_ds.train_test_split(test_size=data_cfg.val_ratio, seed=train_cfg.seed)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "if data_cfg.max_train_samples is not None and data_cfg.max_train_samples < len(train_ds):\n",
    "    train_ds = train_ds.shuffle(seed=train_cfg.seed).select(range(data_cfg.max_train_samples))\n",
    "\n",
    "if data_cfg.max_val_samples is not None and data_cfg.max_val_samples < len(val_ds):\n",
    "    val_ds = val_ds.shuffle(seed=train_cfg.seed).select(range(data_cfg.max_val_samples))\n",
    "\n",
    "print(f\"Train split: {len(train_ds):,} samples\")\n",
    "print(f\"Val split  : {len(val_ds):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75630a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Build In-Memory Datasets & DataLoaders\n",
    "# =========================\n",
    "\n",
    "train_mem = InMemoryImageTextDataset(\n",
    "    hf_dataset=train_ds,\n",
    "    img_col=\"image_url\",\n",
    "    txt_col=\"caption\",\n",
    "    max_samples=None,  # already subselected\n",
    "    image_size=data_cfg.image_size,\n",
    "    num_workers=data_cfg.num_workers,\n",
    ")\n",
    "\n",
    "val_mem = InMemoryImageTextDataset(\n",
    "    hf_dataset=val_ds,\n",
    "    img_col=\"image_url\",\n",
    "    txt_col=\"caption\",\n",
    "    max_samples=None,\n",
    "    image_size=data_cfg.image_size,\n",
    "    num_workers=data_cfg.num_workers,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_mem,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=data_cfg.num_workers,\n",
    "    collate_fn=collate_in_memory_images,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_mem,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=data_cfg.num_workers,\n",
    "    collate_fn=collate_in_memory_images,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches  :\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21998fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Model, optimizer, scheduler, device\n",
    "# =========================\n",
    "\n",
    "set_seed(train_cfg.seed)\n",
    "device = get_device()\n",
    "align_cfg.device = device\n",
    "align_cfg.dtype = torch.float16 if train_cfg.use_amp and device.type == 'cuda' else torch.float32\n",
    "print('Using device:', device)\n",
    "print('Using dtype:', align_cfg.dtype)\n",
    "\n",
    "model = VisionTextAligner(align_cfg)\n",
    "model.to(device)\n",
    "\n",
    "# Multi-GPU (DataParallel) if 2 GPUs are visible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'\u2705 Using DataParallel on {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    print('Using single GPU or CPU.')\n",
    "\n",
    "vt_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "optimizer = AdamW(\n",
    "    vt_model.get_trainable_params(),\n",
    "    lr=train_cfg.lr,\n",
    "    weight_decay=train_cfg.weight_decay,\n",
    ")\n",
    "\n",
    "# ----- Warmup + cosine scheduler -----\n",
    "num_training_steps = train_cfg.num_epochs * len(train_loader)\n",
    "warmup_steps = int(train_cfg.warmup_fraction * num_training_steps)\n",
    "\n",
    "def lr_lambda(step: int):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / max(1, warmup_steps)\n",
    "    progress = (step - warmup_steps) / max(1, num_training_steps - warmup_steps)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scaler = GradScaler(enabled=train_cfg.use_amp)\n",
    "\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "# Optionally resume\n",
    "if train_cfg.resume_from is not None and Path(train_cfg.resume_from).exists():\n",
    "    print(f'\ud83d\udd04 Resuming from checkpoint: {train_cfg.resume_from}')\n",
    "    start_epoch = load_checkpoint(train_cfg.resume_from, vt_model, optimizer=optimizer, load_optimizer=True)\n",
    "    # Note: scheduler/global_step are not restored here; adjust if you save them in the future.\n",
    "else:\n",
    "    print('No resume checkpoint specified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7267c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Validation / retrieval evaluation\n",
    "# =========================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_retrieval(model, loader, device):\n",
    "    model.eval()\n",
    "    vt_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "    all_vision = []\n",
    "    all_text = []\n",
    "\n",
    "    for batch in loader:\n",
    "        images = batch[\"images\"]\n",
    "        captions = batch[\"captions\"]\n",
    "\n",
    "        z_v = vt_model.encode_vision(images)\n",
    "        z_t = vt_model.encode_text(captions)\n",
    "\n",
    "        all_vision.append(z_v.cpu())\n",
    "        all_text.append(z_t.cpu())\n",
    "\n",
    "    z_v_all = torch.cat(all_vision, dim=0)\n",
    "    z_t_all = torch.cat(all_text, dim=0)\n",
    "\n",
    "    metrics_i2t = compute_retrieval_metrics(z_v_all, z_t_all)\n",
    "    metrics_t2i = compute_retrieval_metrics(z_t_all, z_v_all)\n",
    "\n",
    "    metrics = {}\n",
    "    for k, v in metrics_i2t.items():\n",
    "        metrics[f\"i2t_{k}\"] = v\n",
    "    for k, v in metrics_t2i.items():\n",
    "        metrics[f\"t2i_{k}\"] = v\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Training loop (Phase-1)\n",
    "# =========================\n",
    "\n",
    "best_r1 = 0.0\n",
    "\n",
    "print(\"Trainable parameter count:\", vt_model.count_trainable_params())\n",
    "\n",
    "for epoch in range(start_epoch, train_cfg.num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_mrl = 0.0\n",
    "    epoch_clip = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        images = batch[\"images\"]\n",
    "        captions = batch[\"captions\"]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=device.type if device.type != \"mps\" else \"cuda\",\n",
    "                      enabled=train_cfg.use_amp):\n",
    "            outputs = model(images=images, texts=captions)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss_mrl = outputs.get(\"loss_mrl\", torch.tensor(0.0, device=device))\n",
    "            loss_clip = outputs.get(\"loss_clip\", torch.tensor(0.0, device=device))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(vt_model.get_trainable_params(), train_cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_mrl += loss_mrl.item()\n",
    "        epoch_clip += loss_clip.item()\n",
    "        num_steps += 1\n",
    "\n",
    "        if train_cfg.use_wandb and step % 10 == 0:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/loss_mrl\": loss_mrl.item(),\n",
    "                \"train/loss_clip\": loss_clip.item(),\n",
    "                \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"train/epoch_progress\": epoch + (step + 1) / len(train_loader),\n",
    "                \"global_step\": global_step,\n",
    "            })\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"[Epoch {epoch+1}/{train_cfg.num_epochs}] \"\n",
    "                  f\"Step {step}/{len(train_loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | MRL: {loss_mrl.item():.4f} | CLIP: {loss_clip.item():.4f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    epoch_time = time.time() - t0\n",
    "    avg_loss = epoch_loss / max(num_steps, 1)\n",
    "    avg_mrl = epoch_mrl / max(num_steps, 1)\n",
    "    avg_clip = epoch_clip / max(num_steps, 1)\n",
    "\n",
    "    print(f\"\\n=== Epoch {epoch+1} finished in {epoch_time/60:.2f} min ===\")\n",
    "    print(f\"Train avg loss: {avg_loss:.4f} | MRL: {avg_mrl:.4f} | CLIP: {avg_clip:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_metrics = evaluate_retrieval(model, val_loader, device)\n",
    "    print(\"Val retrieval metrics:\")\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    r1_mean = 0.5 * (val_metrics[\"i2t_r@1\"] + val_metrics[\"t2i_r@1\"])\n",
    "\n",
    "    if train_cfg.use_wandb:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/avg_loss\": avg_loss,\n",
    "            \"train/avg_loss_mrl\": avg_mrl,\n",
    "            \"train/avg_loss_clip\": avg_clip,\n",
    "            \"val/i2t_r@1\": val_metrics[\"i2t_r@1\"],\n",
    "            \"val/i2t_r@5\": val_metrics[\"i2t_r@5\"],\n",
    "            \"val/i2t_r@10\": val_metrics[\"i2t_r@10\"],\n",
    "            \"val/t2i_r@1\": val_metrics[\"t2i_r@1\"],\n",
    "            \"val/t2i_r@5\": val_metrics[\"t2i_r@5\"],\n",
    "            \"val/t2i_r@10\": val_metrics[\"t2i_r@10\"],\n",
    "            \"val/r1_mean\": r1_mean,\n",
    "        })\n",
    "\n",
    "    # Save last checkpoint\n",
    "    save_checkpoint(\n",
    "        vt_model,\n",
    "        optimizer,\n",
    "        epoch=epoch + 1,\n",
    "        save_dir=str(VISION_TEXT_DIR),\n",
    "        name=\"vision_text_last\",\n",
    "    )\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if r1_mean > best_r1:\n",
    "        best_r1 = r1_mean\n",
    "        print(f\"\u2705 New best mean R@1: {best_r1:.4f} \u2014 saving best checkpoint\")\n",
    "        save_checkpoint(\n",
    "            vt_model,\n",
    "            optimizer,\n",
    "            epoch=epoch + 1,\n",
    "            save_dir=str(VISION_TEXT_DIR),\n",
    "            name=\"vision_text_best\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No improvement over best mean R@1: {best_r1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best mean R@1 achieved: {best_r1:.4f}\")\n",
    "\n",
    "if run is not None:\n",
    "    run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}