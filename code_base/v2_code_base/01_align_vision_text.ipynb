{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838e8cf4",
   "metadata": {},
   "source": [
    "\n",
    "# 01 â€” Visionâ€“Text Alignment on PixMo-Cap (Phase 1, Improved)\n",
    "\n",
    "This notebook trains a **visionâ€“text alignment model** on a **local PixMo-Cap Parquet subset**\n",
    "(created by `00_build_alignment_datasets.ipynb`).\n",
    "\n",
    "It incorporates the key Phaseâ€‘1 features from your original code:\n",
    "\n",
    "- Uses the same **`AlignmentConfig` + `VisionTextAligner`** (CLIP-style model)\n",
    "- Uses **Matryoshka (MRL) + CLIP** contrastive losses from `core.py`\n",
    "- Adds a **warmup + cosine learning rate schedule** (Phaseâ€‘1 improvement)\n",
    "- Uses **inâ€‘memory imageâ€“text dataset** for fast training\n",
    "- Evaluates **image â†” text retrieval** on a validation split each epoch\n",
    "- Saves **`last`** and **`best`** checkpoints into a common directory:\n",
    "  - `artifacts/phase1_alignment/vision_text/`\n",
    "- Optionally resumes from an existing checkpoint\n",
    "- Logs training & validation metrics to **Weights & Biases (W&B)**\n",
    "- Supports **multiâ€‘GPU** via `torch.nn.DataParallel` (1â€“2 GPUs)\n",
    "\n",
    "These checkpoints will be the **Phaseâ€‘1 encoder** artifacts used by Phaseâ€‘2 experiments\n",
    "(normal LLM decoder, TRM decoder, MoE decoder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a2628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f60ca11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root      : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Data dir          : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data\n",
      "Phase 1 dir       : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment\n",
      "Vision-text dir   : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "from typing import Optional, Tuple\n",
    "# Local modules\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, compute_retrieval_metrics, set_seed, get_device\n",
    "from imports.in_memory_datasets import InMemoryImageTextDataset, collate_in_memory_images\n",
    "from imports.train import save_checkpoint, load_checkpoint\n",
    "\n",
    "# ---- Paths ----\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" \n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "PHASE1_DIR = ARTIFACTS_DIR / \"phase1_alignment\"\n",
    "VISION_TEXT_DIR = PHASE1_DIR / \"vision_text\"\n",
    "\n",
    "VISION_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root      : {PROJECT_ROOT}\")\n",
    "print(f\"Data dir          : {DATA_DIR}\")\n",
    "print(f\"Phase 1 dir       : {PHASE1_DIR}\")\n",
    "print(f\"Vision-text dir   : {VISION_TEXT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05d31d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Dataset & training config\n",
    "# =========================\n",
    "\n",
    "class VTDataConfig:\n",
    "    'Config for PixMo-Cap subset used in Phase-1 alignment.'\n",
    "    # Path created by 00_build_alignment_datasets.ipynb\n",
    "    pixmocap_parquet = DATA_DIR / 'final_dataset'/ 'pixmo'/ 'pixmo_train.parquet'  # adjust if needed\n",
    "\n",
    "    # Subsampling and split\n",
    "    max_train_samples = 40_000\n",
    "    max_val_samples = 5_000\n",
    "    val_ratio = 0.1\n",
    "\n",
    "    # DataLoader\n",
    "    num_workers = 8\n",
    "    image_size = (224, 224)\n",
    "\n",
    "\n",
    "class VTTrainConfig:\n",
    "    'High-level training hyperparameters for Phase-1.'\n",
    "    seed = 42\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 1.0\n",
    "    use_amp = False\n",
    "\n",
    "    temperature = 0.07\n",
    "    mrl_weight = 1.0\n",
    "    clip_weight = 0.5\n",
    "\n",
    "    # LR scheduler: warmup fraction of total steps\n",
    "    warmup_fraction = 0.1\n",
    "\n",
    "    # Logging\n",
    "    use_wandb = True\n",
    "    wandb_project = 'edgeglass_phase1_alignment'\n",
    "    wandb_entity = None  # or your W&B username/org\n",
    "    wandb_group = 'vision_text_alignment'\n",
    "    wandb_run_name = 'vt_align_pixmocap_phase1'\n",
    "    wandb_run_name: Optional[str] = \"vision_only\"  # if None, auto-generated\n",
    "\n",
    "    # Checkpointing / resume\n",
    "    resume_from = None  # e.g., VISION_TEXT_DIR / 'vision_text_best.pt'\n",
    "    save_every_epochs = 1\n",
    "\n",
    "\n",
    "data_cfg = VTDataConfig()\n",
    "train_cfg = VTTrainConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4346155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTDataConfig:\n",
      "\n",
      "VTTrainConfig:\n"
     ]
    }
   ],
   "source": [
    "print(\"VTDataConfig:\")\n",
    "for k, v in data_cfg.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nVTTrainConfig:\")\n",
    "for k, v in train_cfg.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da73a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignmentConfig:\n",
      "  vision_model_name: openai/clip-vit-base-patch32\n",
      "  text_model_name: sentence-transformers/all-MiniLM-L6-v2\n",
      "  llm_model_name: Qwen/Qwen2.5-1.5B-Instruct\n",
      "  d_align: 512\n",
      "  adapter_hidden_factor: 2.0\n",
      "  dropout: 0.1\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.01\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.1\n",
      "  max_grad_norm: 1.0\n",
      "  mrl_dims: (64, 128, 256, 512)\n",
      "  mrl_temperature: 0.07\n",
      "  clip_temperature: 0.07\n",
      "  mrl_weight: 1.0\n",
      "  clip_weight: 0.5\n",
      "  seed: 42\n",
      "  log_every: 50\n",
      "  max_text_length: 128\n",
      "  d_vision: 0\n",
      "  d_text: 0\n",
      "  device: cpu\n",
      "  dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# AlignmentConfig (model)\n",
    "# =========================\n",
    "\n",
    "# You can adjust these to match your original Phase-1 experiments.\n",
    "align_cfg = AlignmentConfig(\n",
    "    # Keep your chosen encoders here (examples):\n",
    "    # vision_model_name='openai/clip-vit-base-patch32',\n",
    "    # text_model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    num_epochs=train_cfg.num_epochs,\n",
    "    learning_rate=train_cfg.lr,\n",
    "    weight_decay=train_cfg.weight_decay,\n",
    "    warmup_ratio=train_cfg.warmup_fraction,\n",
    "    mrl_temperature=train_cfg.temperature,\n",
    "    clip_temperature=train_cfg.temperature,\n",
    "    # Temperature & MRL settings from your original experiments:\n",
    "    mrl_dims=(64, 128, 256, 512),\n",
    "    mrl_weight=train_cfg.mrl_weight,\n",
    "    clip_weight=train_cfg.clip_weight,\n",
    ")\n",
    "\n",
    "print('AlignmentConfig:')\n",
    "for k, v in asdict(align_cfg).items():\n",
    "    print(f'  {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6445135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vision_only</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/6kskw2xo' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/6kskw2xo</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_030108-6kskw2xo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251202_035841-fz8lsjga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/fz8lsjga' target=\"_blank\">vision_only</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/fz8lsjga' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/fz8lsjga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… W&B run initialized: vision_only\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Weights & Biases init\n",
    "# =========================\n",
    "\n",
    "run = None\n",
    "if train_cfg.use_wandb:\n",
    "    def _to_serializable(v):\n",
    "        if isinstance(v, Path):\n",
    "            return str(v)\n",
    "        if 'torch' in globals() and (isinstance(v, torch.device) or isinstance(v, torch.dtype)):\n",
    "            return str(v)\n",
    "        return v\n",
    "\n",
    "    wandb_kwargs = dict(\n",
    "        project=train_cfg.wandb_project,\n",
    "        name=train_cfg.wandb_run_name,\n",
    "        group=train_cfg.wandb_group,\n",
    "        config={\n",
    "            'phase': 'phase1_alignment',\n",
    "            'task': 'vision_text',\n",
    "            'data_cfg': {k: _to_serializable(v) for k, v in data_cfg.__dict__.items()},\n",
    "            'align_cfg': {k: _to_serializable(v) for k, v in asdict(align_cfg).items()},\n",
    "            'train_cfg': {k: _to_serializable(v) for k, v in train_cfg.__dict__.items()},\n",
    "        },\n",
    "    )\n",
    "    if train_cfg.wandb_entity is not None:\n",
    "        wandb_kwargs['entity'] = train_cfg.wandb_entity\n",
    "\n",
    "    run = wandb.init(**wandb_kwargs)\n",
    "    print('âœ… W&B run initialized:', run.name)\n",
    "else:\n",
    "    print('W&B logging disabled.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14e3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading PixMo-Cap subset from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/final_dataset/pixmo/pixmo_train.parquet\n",
      "Total PixMo-Cap subset size: 14,000\n",
      "Train split: 12,600 samples\n",
      "Val split  : 1,400 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Load PixMo-Cap subset from Parquet\n",
    "# =========================\n",
    "\n",
    "assert data_cfg.pixmocap_parquet.exists(), f\"Parquet file not found: {data_cfg.pixmocap_parquet}\"\n",
    "\n",
    "print(f\"ğŸ“¥ Loading PixMo-Cap subset from: {data_cfg.pixmocap_parquet}\")\n",
    "pixmo_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": str(data_cfg.pixmocap_parquet)},\n",
    ")[\"train\"]\n",
    "\n",
    "print(f\"Total PixMo-Cap subset size: {len(pixmo_ds):,}\")\n",
    "\n",
    "# Train/val split\n",
    "split = pixmo_ds.train_test_split(test_size=data_cfg.val_ratio, seed=train_cfg.seed)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "if data_cfg.max_train_samples is not None and data_cfg.max_train_samples < len(train_ds):\n",
    "    train_ds = train_ds.shuffle(seed=train_cfg.seed).select(range(data_cfg.max_train_samples))\n",
    "\n",
    "if data_cfg.max_val_samples is not None and data_cfg.max_val_samples < len(val_ds):\n",
    "    val_ds = val_ds.shuffle(seed=train_cfg.seed).select(range(data_cfg.max_val_samples))\n",
    "\n",
    "print(f\"Train split: {len(train_ds):,} samples\")\n",
    "print(f\"Val split  : {len(val_ds):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b75630a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¥ Pre-loading 12600 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 8 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/12600 [00:00<?, ?it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  16%|â–ˆâ–Œ        | 1966/12600 [03:12<07:38, 23.19it/s]  /home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (130382142 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 8647/12600 [08:52<02:15, 29.19it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (161569818 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "Loading images:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 9433/12600 [10:09<03:21, 15.69it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (100000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12600/12600 [11:55<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 12600 images into memory\n",
      "   âš ï¸  87 images failed to load (using fallback)\n",
      "\n",
      "ğŸ“¥ Pre-loading 1400 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 8 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   6%|â–Œ         | 87/1400 [00:19<03:50,  5.71it/s] /home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 646/1400 [00:41<00:35, 21.17it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 732/1400 [00:43<00:26, 25.58it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [01:15<00:00, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1400 images into memory\n",
      "   âš ï¸  8 images failed to load (using fallback)\n",
      "Train batches: 394\n",
      "Val batches  : 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Build In-Memory Datasets & DataLoaders\n",
    "# =========================\n",
    "\n",
    "train_mem = InMemoryImageTextDataset(\n",
    "    hf_dataset=train_ds,\n",
    "    img_col=\"image_url\",\n",
    "    txt_col=\"caption\",\n",
    "    max_samples=None,  # already subselected\n",
    "    image_size=data_cfg.image_size,\n",
    "    num_workers=data_cfg.num_workers,\n",
    ")\n",
    "\n",
    "val_mem = InMemoryImageTextDataset(\n",
    "    hf_dataset=val_ds,\n",
    "    img_col=\"image_url\",\n",
    "    txt_col=\"caption\",\n",
    "    max_samples=None,\n",
    "    image_size=data_cfg.image_size,\n",
    "    num_workers=data_cfg.num_workers,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_mem,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=data_cfg.num_workers,\n",
    "    collate_fn=collate_in_memory_images,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_mem,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=data_cfg.num_workers,\n",
    "    collate_fn=collate_in_memory_images,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches  :\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21998fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using dtype: torch.float32\n",
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "[TextEncoder] Loaded sentence-transformers/all-MiniLM-L6-v2, hidden_size=384\n",
      "[VisionTextAligner] d_vision=768, d_text=384, d_align=512\n",
      "Using single GPU or CPU.\n",
      "No resume checkpoint specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_1365851/380270250.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=train_cfg.use_amp)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Model, optimizer, scheduler, device\n",
    "# =========================\n",
    "\n",
    "set_seed(train_cfg.seed)\n",
    "device = get_device()\n",
    "align_cfg.device = device\n",
    "align_cfg.dtype = torch.float16 if train_cfg.use_amp and device.type == 'cuda' else torch.float32\n",
    "print('Using device:', device)\n",
    "print('Using dtype:', align_cfg.dtype)\n",
    "\n",
    "model = VisionTextAligner(align_cfg)\n",
    "model.to(device)\n",
    "\n",
    "# Multi-GPU (DataParallel) if 2 GPUs are visible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'âœ… Using DataParallel on {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    print('Using single GPU or CPU.')\n",
    "\n",
    "vt_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "optimizer = AdamW(\n",
    "    vt_model.get_trainable_params(),\n",
    "    lr=train_cfg.lr,\n",
    "    weight_decay=train_cfg.weight_decay,\n",
    ")\n",
    "\n",
    "# ----- Warmup + cosine scheduler -----\n",
    "num_training_steps = train_cfg.num_epochs * len(train_loader)\n",
    "warmup_steps = int(train_cfg.warmup_fraction * num_training_steps)\n",
    "\n",
    "def lr_lambda(step: int):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / max(1, warmup_steps)\n",
    "    progress = (step - warmup_steps) / max(1, num_training_steps - warmup_steps)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scaler = GradScaler(enabled=train_cfg.use_amp)\n",
    "\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "# Optionally resume\n",
    "if train_cfg.resume_from is not None and Path(train_cfg.resume_from).exists():\n",
    "    print(f'ğŸ”„ Resuming from checkpoint: {train_cfg.resume_from}')\n",
    "    start_epoch = load_checkpoint(train_cfg.resume_from, vt_model, optimizer=optimizer, load_optimizer=True)\n",
    "    # Note: scheduler/global_step are not restored here; adjust if you save them in the future.\n",
    "else:\n",
    "    print('No resume checkpoint specified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d7267c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Validation / retrieval evaluation\n",
    "# =========================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_retrieval(model, loader, device):\n",
    "    model.eval()\n",
    "    vt_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "    all_vision = []\n",
    "    all_text = []\n",
    "\n",
    "    for batch in loader:\n",
    "        images = batch[\"images\"]\n",
    "        captions = batch[\"captions\"]\n",
    "\n",
    "        z_v = vt_model.encode_vision(images)\n",
    "        z_t = vt_model.encode_text(captions)\n",
    "\n",
    "        all_vision.append(z_v.cpu())\n",
    "        all_text.append(z_t.cpu())\n",
    "\n",
    "    z_v_all = torch.cat(all_vision, dim=0)\n",
    "    z_t_all = torch.cat(all_text, dim=0)\n",
    "\n",
    "    metrics_i2t = compute_retrieval_metrics(z_v_all, z_t_all)\n",
    "    metrics_t2i = compute_retrieval_metrics(z_t_all, z_v_all)\n",
    "\n",
    "    metrics = {}\n",
    "    for k, v in metrics_i2t.items():\n",
    "        metrics[f\"i2t_{k.lower()}\"] = v\n",
    "    for k, v in metrics_t2i.items():\n",
    "        metrics[f\"t2i_{k.lower()}\"] = v\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62ec4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter count: 2659840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_1365851/3446720765.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=train_cfg.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Step 0/394 | Loss: 5.7046 | MRL: 3.9265 | CLIP: 3.5562 | LR: 5.08e-07\n",
      "[Epoch 1/5] Step 50/394 | Loss: 5.4212 | MRL: 3.7204 | CLIP: 3.4016 | LR: 2.59e-05\n",
      "[Epoch 1/5] Step 100/394 | Loss: 3.8152 | MRL: 2.5336 | CLIP: 2.5632 | LR: 5.13e-05\n",
      "[Epoch 1/5] Step 150/394 | Loss: 2.6539 | MRL: 1.7854 | CLIP: 1.7369 | LR: 7.66e-05\n",
      "[Epoch 1/5] Step 200/394 | Loss: 1.8900 | MRL: 1.2733 | CLIP: 1.2334 | LR: 1.00e-04\n",
      "[Epoch 1/5] Step 250/394 | Loss: 1.6525 | MRL: 1.1286 | CLIP: 1.0478 | LR: 9.98e-05\n",
      "[Epoch 1/5] Step 300/394 | Loss: 1.4517 | MRL: 0.9693 | CLIP: 0.9648 | LR: 9.92e-05\n",
      "[Epoch 1/5] Step 350/394 | Loss: 1.3135 | MRL: 0.9076 | CLIP: 0.8118 | LR: 9.82e-05\n",
      "\n",
      "=== Epoch 1 finished in 0.54 min ===\n",
      "Train avg loss: 2.7412 | MRL: 1.8675 | CLIP: 1.7473\n",
      "Val retrieval metrics:\n",
      "  i2t_r@1: 0.3471\n",
      "  i2t_r@5: 0.6336\n",
      "  i2t_r@10: 0.7443\n",
      "  t2i_r@1: 0.3871\n",
      "  t2i_r@5: 0.6721\n",
      "  t2i_r@10: 0.7750\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_last.pt\n",
      "âœ… New best mean R@1: 0.3671 â€” saving best checkpoint\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_best.pt\n",
      "[Epoch 2/5] Step 0/394 | Loss: 0.6509 | MRL: 0.4408 | CLIP: 0.4201 | LR: 9.70e-05\n",
      "[Epoch 2/5] Step 50/394 | Loss: 0.7593 | MRL: 0.5175 | CLIP: 0.4835 | LR: 9.52e-05\n",
      "[Epoch 2/5] Step 100/394 | Loss: 0.8104 | MRL: 0.5473 | CLIP: 0.5263 | LR: 9.32e-05\n",
      "[Epoch 2/5] Step 150/394 | Loss: 0.5763 | MRL: 0.3986 | CLIP: 0.3553 | LR: 9.08e-05\n",
      "[Epoch 2/5] Step 200/394 | Loss: 0.6721 | MRL: 0.4528 | CLIP: 0.4385 | LR: 8.81e-05\n",
      "[Epoch 2/5] Step 250/394 | Loss: 1.0457 | MRL: 0.7237 | CLIP: 0.6440 | LR: 8.51e-05\n",
      "[Epoch 2/5] Step 300/394 | Loss: 0.6552 | MRL: 0.4629 | CLIP: 0.3846 | LR: 8.18e-05\n",
      "[Epoch 2/5] Step 350/394 | Loss: 0.4654 | MRL: 0.3113 | CLIP: 0.3082 | LR: 7.82e-05\n",
      "\n",
      "=== Epoch 2 finished in 0.50 min ===\n",
      "Train avg loss: 0.8007 | MRL: 0.5448 | CLIP: 0.5118\n",
      "Val retrieval metrics:\n",
      "  i2t_r@1: 0.4486\n",
      "  i2t_r@5: 0.7364\n",
      "  i2t_r@10: 0.8164\n",
      "  t2i_r@1: 0.4750\n",
      "  t2i_r@5: 0.7564\n",
      "  t2i_r@10: 0.8371\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_last.pt\n",
      "âœ… New best mean R@1: 0.4618 â€” saving best checkpoint\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_best.pt\n",
      "[Epoch 3/5] Step 0/394 | Loss: 0.7322 | MRL: 0.5006 | CLIP: 0.4633 | LR: 7.49e-05\n",
      "[Epoch 3/5] Step 50/394 | Loss: 0.5095 | MRL: 0.3359 | CLIP: 0.3473 | LR: 7.10e-05\n",
      "[Epoch 3/5] Step 100/394 | Loss: 0.6363 | MRL: 0.4362 | CLIP: 0.4002 | LR: 6.69e-05\n",
      "[Epoch 3/5] Step 150/394 | Loss: 0.3949 | MRL: 0.2676 | CLIP: 0.2547 | LR: 6.27e-05\n",
      "[Epoch 3/5] Step 200/394 | Loss: 0.5303 | MRL: 0.3558 | CLIP: 0.3490 | LR: 5.83e-05\n",
      "[Epoch 3/5] Step 250/394 | Loss: 0.4033 | MRL: 0.2658 | CLIP: 0.2751 | LR: 5.39e-05\n",
      "[Epoch 3/5] Step 300/394 | Loss: 0.3517 | MRL: 0.2412 | CLIP: 0.2211 | LR: 4.95e-05\n",
      "[Epoch 3/5] Step 350/394 | Loss: 0.4990 | MRL: 0.3338 | CLIP: 0.3303 | LR: 4.51e-05\n",
      "\n",
      "=== Epoch 3 finished in 0.48 min ===\n",
      "Train avg loss: 0.5506 | MRL: 0.3745 | CLIP: 0.3521\n",
      "Val retrieval metrics:\n",
      "  i2t_r@1: 0.4764\n",
      "  i2t_r@5: 0.7593\n",
      "  i2t_r@10: 0.8414\n",
      "  t2i_r@1: 0.5100\n",
      "  t2i_r@5: 0.7807\n",
      "  t2i_r@10: 0.8550\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_last.pt\n",
      "âœ… New best mean R@1: 0.4932 â€” saving best checkpoint\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_best.pt\n",
      "[Epoch 4/5] Step 0/394 | Loss: 0.5381 | MRL: 0.3629 | CLIP: 0.3505 | LR: 4.12e-05\n",
      "[Epoch 4/5] Step 50/394 | Loss: 0.3526 | MRL: 0.2282 | CLIP: 0.2487 | LR: 3.69e-05\n",
      "[Epoch 4/5] Step 100/394 | Loss: 0.5730 | MRL: 0.3897 | CLIP: 0.3667 | LR: 3.27e-05\n",
      "[Epoch 4/5] Step 150/394 | Loss: 0.2968 | MRL: 0.2097 | CLIP: 0.1741 | LR: 2.86e-05\n",
      "[Epoch 4/5] Step 200/394 | Loss: 0.5656 | MRL: 0.3805 | CLIP: 0.3701 | LR: 2.47e-05\n",
      "[Epoch 4/5] Step 250/394 | Loss: 0.4049 | MRL: 0.2770 | CLIP: 0.2557 | LR: 2.10e-05\n",
      "[Epoch 4/5] Step 300/394 | Loss: 0.6922 | MRL: 0.4926 | CLIP: 0.3993 | LR: 1.75e-05\n",
      "[Epoch 4/5] Step 350/394 | Loss: 0.5177 | MRL: 0.3479 | CLIP: 0.3394 | LR: 1.43e-05\n",
      "\n",
      "=== Epoch 4 finished in 0.48 min ===\n",
      "Train avg loss: 0.4374 | MRL: 0.2980 | CLIP: 0.2788\n",
      "Val retrieval metrics:\n",
      "  i2t_r@1: 0.4957\n",
      "  i2t_r@5: 0.7714\n",
      "  i2t_r@10: 0.8507\n",
      "  t2i_r@1: 0.5121\n",
      "  t2i_r@5: 0.7907\n",
      "  t2i_r@10: 0.8600\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_last.pt\n",
      "âœ… New best mean R@1: 0.5039 â€” saving best checkpoint\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_best.pt\n",
      "[Epoch 5/5] Step 0/394 | Loss: 0.4249 | MRL: 0.2867 | CLIP: 0.2765 | LR: 1.16e-05\n",
      "[Epoch 5/5] Step 50/394 | Loss: 0.2716 | MRL: 0.1748 | CLIP: 0.1935 | LR: 8.95e-06\n",
      "[Epoch 5/5] Step 100/394 | Loss: 0.3086 | MRL: 0.2122 | CLIP: 0.1929 | LR: 6.59e-06\n",
      "[Epoch 5/5] Step 150/394 | Loss: 0.4438 | MRL: 0.3080 | CLIP: 0.2718 | LR: 4.56e-06\n",
      "[Epoch 5/5] Step 200/394 | Loss: 0.4195 | MRL: 0.2964 | CLIP: 0.2463 | LR: 2.90e-06\n",
      "[Epoch 5/5] Step 250/394 | Loss: 0.4151 | MRL: 0.2789 | CLIP: 0.2724 | LR: 1.60e-06\n",
      "[Epoch 5/5] Step 300/394 | Loss: 0.6190 | MRL: 0.4299 | CLIP: 0.3783 | LR: 6.77e-07\n",
      "[Epoch 5/5] Step 350/394 | Loss: 0.2134 | MRL: 0.1471 | CLIP: 0.1326 | LR: 1.45e-07\n",
      "\n",
      "=== Epoch 5 finished in 0.55 min ===\n",
      "Train avg loss: 0.4000 | MRL: 0.2728 | CLIP: 0.2544\n",
      "Val retrieval metrics:\n",
      "  i2t_r@1: 0.4986\n",
      "  i2t_r@5: 0.7729\n",
      "  i2t_r@10: 0.8479\n",
      "  t2i_r@1: 0.5229\n",
      "  t2i_r@5: 0.7907\n",
      "  t2i_r@10: 0.8614\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_last.pt\n",
      "âœ… New best mean R@1: 0.5107 â€” saving best checkpoint\n",
      "Saved checkpoint: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/artifacts/phase1_alignment/vision_text/vision_text_best.pt\n",
      "\n",
      "Training complete.\n",
      "Best mean R@1 achieved: 0.5107\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–ƒâ–…â–†â–ˆ</td></tr><tr><td>global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>train/avg_loss</td><td>â–ˆâ–‚â–â–â–</td></tr><tr><td>train/avg_loss_clip</td><td>â–ˆâ–‚â–â–â–</td></tr><tr><td>train/avg_loss_mrl</td><td>â–ˆâ–‚â–â–â–</td></tr><tr><td>train/epoch_progress</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/loss_clip</td><td>â–ˆâ–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/loss_mrl</td><td>â–ˆâ–ˆâ–‡â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lr</td><td>â–â–‚â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>global_step</td><td>1967</td></tr><tr><td>train/avg_loss</td><td>0.40001</td></tr><tr><td>train/avg_loss_clip</td><td>0.25439</td></tr><tr><td>train/avg_loss_mrl</td><td>0.27281</td></tr><tr><td>train/epoch_progress</td><td>4.99239</td></tr><tr><td>train/loss</td><td>0.23203</td></tr><tr><td>train/loss_clip</td><td>0.15025</td></tr><tr><td>train/loss_mrl</td><td>0.1569</td></tr><tr><td>train/lr</td><td>0.0</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vision_only</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/fz8lsjga' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/fz8lsjga</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_035841-fz8lsjga/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Training loop (Phase-1)\n",
    "# =========================\n",
    "\n",
    "best_r1 = 0.0\n",
    "\n",
    "\n",
    "trainable_params = sum(p.numel() for p in vt_model.parameters() if p.requires_grad)\n",
    "print(\"Trainable parameter count:\", trainable_params)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, train_cfg.num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_mrl = 0.0\n",
    "    epoch_clip = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        images = batch[\"images\"]\n",
    "        captions = batch[\"captions\"]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=train_cfg.use_amp):\n",
    "            outputs = vt_model(images=images, texts=captions)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss_mrl = outputs.get(\"loss_mrl\", torch.tensor(0.0, device=device))\n",
    "            loss_clip = outputs.get(\"loss_clip\", torch.tensor(0.0, device=device))\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(vt_model.get_trainable_params(), train_cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        epoch_mrl += float(loss_mrl)\n",
    "        epoch_clip += float(loss_clip)\n",
    "        num_steps += 1\n",
    "\n",
    "        if train_cfg.use_wandb and step % 10 == 0:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss,\n",
    "                \"train/loss_mrl\": loss_mrl,\n",
    "                \"train/loss_clip\": loss_clip,\n",
    "                \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"train/epoch_progress\": epoch + (step + 1) / len(train_loader),\n",
    "                \"global_step\": global_step,\n",
    "            })\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"[Epoch {epoch+1}/{train_cfg.num_epochs}] \"\n",
    "                  f\"Step {step}/{len(train_loader)} | \"\n",
    "                  f\"Loss: {loss:.4f} | MRL: {loss_mrl:.4f} | CLIP: {loss_clip:.4f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    epoch_time = time.time() - t0\n",
    "    avg_loss = epoch_loss / max(num_steps, 1)\n",
    "    avg_mrl = epoch_mrl / max(num_steps, 1)\n",
    "    avg_clip = epoch_clip / max(num_steps, 1)\n",
    "\n",
    "    print(f\"\\n=== Epoch {epoch+1} finished in {epoch_time/60:.2f} min ===\")\n",
    "    print(f\"Train avg loss: {avg_loss:.4f} | MRL: {avg_mrl:.4f} | CLIP: {avg_clip:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_metrics = evaluate_retrieval(model, val_loader, device)\n",
    "    print(\"Val retrieval metrics:\")\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    r1_mean = 0.5 * (val_metrics[\"i2t_r@1\"] + val_metrics[\"t2i_r@1\"])\n",
    "\n",
    "    if train_cfg.use_wandb:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/avg_loss\": avg_loss,\n",
    "            \"train/avg_loss_mrl\": avg_mrl,\n",
    "            \"train/avg_loss_clip\": avg_clip,\n",
    "            \"val/i2t_r@1\": val_metrics[\"i2t_r@1\"],\n",
    "            \"val/i2t_r@5\": val_metrics[\"i2t_r@5\"],\n",
    "            \"val/i2t_r@10\": val_metrics[\"i2t_r@10\"],\n",
    "            \"val/t2i_r@1\": val_metrics[\"t2i_r@1\"],\n",
    "            \"val/t2i_r@5\": val_metrics[\"t2i_r@5\"],\n",
    "            \"val/t2i_r@10\": val_metrics[\"t2i_r@10\"],\n",
    "            \"val/r1_mean\": r1_mean,\n",
    "        })\n",
    "\n",
    "    # Save last checkpoint\n",
    "    save_checkpoint(\n",
    "        vt_model,\n",
    "        optimizer,\n",
    "        epoch=epoch + 1,\n",
    "        save_dir=str(VISION_TEXT_DIR),\n",
    "        name=\"vision_text_last\",\n",
    "    )\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if r1_mean > best_r1:\n",
    "        best_r1 = r1_mean\n",
    "        print(f\"âœ… New best mean R@1: {best_r1:.4f} â€” saving best checkpoint\")\n",
    "        save_checkpoint(\n",
    "            vt_model,\n",
    "            optimizer,\n",
    "            epoch=epoch + 1,\n",
    "            save_dir=str(VISION_TEXT_DIR),\n",
    "            name=\"vision_text_best\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No improvement over best mean R@1: {best_r1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best mean R@1 achieved: {best_r1:.4f}\")\n",
    "\n",
    "if run is not None:\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef5576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
