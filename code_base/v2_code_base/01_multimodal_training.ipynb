{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Multimodal Alignment Training with Perceiver Resampler\n",
    "\n",
    "**Complete training pipeline for multimodal alignment**\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "## Training Phases\n",
    "\n",
    "| Phase | Objective | Trainable | Loss |\n",
    "|-------|-----------|-----------|------|\n",
    "| **Level 1** | Align modalities | Adapters + Perceiver + Projector | MRL + CLIP Contrastive |\n",
    "| **Level 2** | LLM Integration | LLM Projector (+ LoRA) | Language Modeling |\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Image/Audio/Text ‚Üí Frozen Encoder ‚Üí Adapter ‚Üí Perceiver ‚Üí Projector ‚Üí Aligned Embedding\n",
    "                                                              ‚Üì\n",
    "                                              (Phase 2) ‚Üí LLM Projector ‚Üí LLM ‚Üí Generated Text\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install torch transformers datasets accelerate sentencepiece\n",
    "# !pip install librosa soundfile\n",
    "# !pip install wandb  # Optional: for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    WhisperModel,\n",
    "    WhisperProcessor,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Audio\n",
    "try:\n",
    "    import librosa\n",
    "    HAS_LIBROSA = True\n",
    "except ImportError:\n",
    "    HAS_LIBROSA = False\n",
    "    print(\"‚ö†Ô∏è librosa not installed\")\n",
    "\n",
    "# Logging (optional)\n",
    "try:\n",
    "    import wandb\n",
    "    HAS_WANDB = True\n",
    "except ImportError:\n",
    "    HAS_WANDB = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Complete configuration for multimodal alignment training.\n",
    "    \n",
    "    Adjust these parameters based on your:\n",
    "    - GPU memory (reduce batch_size if OOM)\n",
    "    - Dataset size (increase epochs for small datasets)\n",
    "    - Quality requirements (increase perceiver_layers for better quality)\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Experiment ===\n",
    "    experiment_name: str = \"multimodal_align_v1\"\n",
    "    seed: int = 42\n",
    "    \n",
    "    # === Model Names ===\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    text_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Small for training\n",
    "    \n",
    "    # === Encoder Dimensions ===\n",
    "    d_vision: int = 768      # CLIP ViT-B/32\n",
    "    d_audio: int = 512       # Whisper-base\n",
    "    d_text: int = 384        # MiniLM\n",
    "    \n",
    "    # === Perceiver Architecture ===\n",
    "    perceiver_dim: int = 512\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 4\n",
    "    num_attn_heads: int = 8\n",
    "    perceiver_mlp_ratio: float = 4.0\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # === Alignment ===\n",
    "    d_align: int = 512\n",
    "    mrl_dims: Tuple[int, ...] = (64, 128, 256, 512)\n",
    "    \n",
    "    # === LLM ===\n",
    "    llm_hidden_size: int = 896  # Qwen2.5-0.5B\n",
    "    \n",
    "    # === Phase 1: Alignment Training ===\n",
    "    phase1_epochs: int = 5\n",
    "    phase1_batch_size: int = 32\n",
    "    phase1_lr: float = 1e-4\n",
    "    phase1_weight_decay: float = 0.01\n",
    "    phase1_warmup_ratio: float = 0.1\n",
    "    phase1_max_samples: int = 10000  # Set to None for full dataset\n",
    "    \n",
    "    # Loss weights\n",
    "    mrl_weight: float = 1.0\n",
    "    clip_weight: float = 0.5\n",
    "    temperature: float = 0.07\n",
    "    \n",
    "    # === Phase 2: LLM Training ===\n",
    "    phase2_epochs: int = 3\n",
    "    phase2_batch_size: int = 8\n",
    "    phase2_lr: float = 2e-5\n",
    "    phase2_max_samples: int = 5000\n",
    "    max_seq_length: int = 128\n",
    "    \n",
    "    # === Training ===\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_amp: bool = True  # Mixed precision\n",
    "    num_workers: int = 0\n",
    "    \n",
    "    # === Checkpointing ===\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    save_every_n_steps: int = 500\n",
    "    eval_every_n_steps: int = 250\n",
    "    \n",
    "    # === Logging ===\n",
    "    use_wandb: bool = False\n",
    "    log_every_n_steps: int = 50\n",
    "\n",
    "\n",
    "# Create config\n",
    "cfg = TrainingConfig()\n",
    "\n",
    "# Set seed\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Create directories\n",
    "Path(cfg.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{cfg.checkpoint_dir}/phase1\").mkdir(exist_ok=True)\n",
    "Path(f\"{cfg.checkpoint_dir}/phase2\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   Experiment: {cfg.experiment_name}\")\n",
    "print(f\"   Perceiver: {cfg.num_latents} latents, {cfg.num_perceiver_layers} layers\")\n",
    "print(f\"   Phase 1: {cfg.phase1_epochs} epochs, batch={cfg.phase1_batch_size}\")\n",
    "print(f\"   Phase 2: {cfg.phase2_epochs} epochs, batch={cfg.phase2_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERCEIVER COMPONENTS\n",
    "# ============================================================\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"FFN with GELU activation.\"\"\"\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PerceiverAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with pre-LayerNorm.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.ln_q = nn.LayerNorm(dim)\n",
    "        self.ln_kv = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, kv, mask=None):\n",
    "        B, N_q, D = q.shape\n",
    "        N_kv = kv.shape[1]\n",
    "        \n",
    "        q = self.ln_q(q)\n",
    "        kv = self.ln_kv(kv)\n",
    "        \n",
    "        Q = self.q_proj(q).view(B, N_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(kv).view(B, N_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(kv).view(B, N_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(~mask.bool().unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = (attn @ V).transpose(1, 2).reshape(B, N_q, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    \"\"\"Single Perceiver layer: cross-attn + self-attn + FFN.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 8, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.cross_attn = PerceiverAttention(dim, num_heads, dropout)\n",
    "        self.self_attn = PerceiverAttention(dim, num_heads, dropout)\n",
    "        self.ffn = FeedForward(dim, mlp_ratio, dropout)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, latents, tokens, mask=None):\n",
    "        latents = latents + self.cross_attn(latents, tokens, mask)\n",
    "        latents = latents + self.self_attn(latents, latents)\n",
    "        latents = latents + self.ffn(self.ln(latents))\n",
    "        return latents\n",
    "\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"Perceiver Resampler: compress variable-length to fixed latents.\"\"\"\n",
    "    def __init__(self, dim: int, num_latents: int = 64, num_layers: int = 4, \n",
    "                 num_heads: int = 8, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) * (dim ** -0.5))\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_out = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, tokens, mask=None):\n",
    "        B = tokens.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, mask)\n",
    "        \n",
    "        return self.ln_out(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADAPTERS AND PROJECTORS\n",
    "# ============================================================\n",
    "\n",
    "class MLPAdapter(nn.Module):\n",
    "    \"\"\"MLP adapter for projecting encoder outputs.\"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden_factor: float = 2.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden = int(in_dim * hidden_factor)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AlignmentProjector(nn.Module):\n",
    "    \"\"\"Project latents to alignment space with mean pooling.\"\"\"\n",
    "    def __init__(self, dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, latents):\n",
    "        pooled = latents.mean(dim=1)  # (B, K, D) -> (B, D)\n",
    "        return self.proj(pooled)\n",
    "\n",
    "\n",
    "class LLMProjector(nn.Module):\n",
    "    \"\"\"Project latents to LLM embedding space.\"\"\"\n",
    "    def __init__(self, dim: int, llm_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, llm_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, latents):\n",
    "        return self.proj(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE MODEL\n",
    "# ============================================================\n",
    "\n",
    "class MultimodalAlignmentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Multimodal Alignment Model with Perceiver Resampler.\n",
    "    \n",
    "    Phase 1 (Alignment): Uses adapters + perceiver + alignment_proj\n",
    "    Phase 2 (LLM): Adds llm_proj for generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Modality adapters\n",
    "        self.vision_adapter = MLPAdapter(cfg.d_vision, cfg.perceiver_dim, dropout=cfg.dropout)\n",
    "        self.audio_adapter = MLPAdapter(cfg.d_audio, cfg.perceiver_dim, dropout=cfg.dropout)\n",
    "        self.text_adapter = MLPAdapter(cfg.d_text, cfg.perceiver_dim, dropout=cfg.dropout)\n",
    "        \n",
    "        # Shared Perceiver\n",
    "        self.perceiver = PerceiverResampler(\n",
    "            dim=cfg.perceiver_dim,\n",
    "            num_latents=cfg.num_latents,\n",
    "            num_layers=cfg.num_perceiver_layers,\n",
    "            num_heads=cfg.num_attn_heads,\n",
    "            mlp_ratio=cfg.perceiver_mlp_ratio,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "        \n",
    "        # Alignment projector (Phase 1)\n",
    "        self.alignment_proj = AlignmentProjector(cfg.perceiver_dim, cfg.d_align)\n",
    "        \n",
    "        # LLM projector (Phase 2)\n",
    "        self.llm_proj = LLMProjector(cfg.perceiver_dim, cfg.llm_hidden_size)\n",
    "    \n",
    "    def encode_modality(self, features, adapter, mask=None):\n",
    "        \"\"\"Encode features through adapter + perceiver.\"\"\"\n",
    "        tokens = adapter(features)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        return latents\n",
    "    \n",
    "    def encode_vision(self, features, mask=None):\n",
    "        latents = self.encode_modality(features, self.vision_adapter, mask)\n",
    "        z = self.alignment_proj(latents)\n",
    "        return z, latents\n",
    "    \n",
    "    def encode_audio(self, features, mask=None):\n",
    "        latents = self.encode_modality(features, self.audio_adapter, mask)\n",
    "        z = self.alignment_proj(latents)\n",
    "        return z, latents\n",
    "    \n",
    "    def encode_text(self, features, mask=None):\n",
    "        latents = self.encode_modality(features, self.text_adapter, mask)\n",
    "        z = self.alignment_proj(latents)\n",
    "        return z, latents\n",
    "    \n",
    "    def project_to_llm(self, latents):\n",
    "        \"\"\"Project latents to LLM embedding space.\"\"\"\n",
    "        return self.llm_proj(latents)\n",
    "    \n",
    "    def get_phase1_params(self):\n",
    "        \"\"\"Get parameters for Phase 1 training.\"\"\"\n",
    "        return list(self.vision_adapter.parameters()) + \\\n",
    "               list(self.audio_adapter.parameters()) + \\\n",
    "               list(self.text_adapter.parameters()) + \\\n",
    "               list(self.perceiver.parameters()) + \\\n",
    "               list(self.alignment_proj.parameters())\n",
    "    \n",
    "    def get_phase2_params(self):\n",
    "        \"\"\"Get parameters for Phase 2 training.\"\"\"\n",
    "        return list(self.llm_proj.parameters())\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = MultimodalAlignmentModel(cfg).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Frozen Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD FROZEN ENCODERS\n",
    "# ============================================================\n",
    "\n",
    "class FrozenEncoders:\n",
    "    \"\"\"Container for frozen encoders.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: TrainingConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        \n",
    "        print(\"\\nüì¶ Loading frozen encoders...\")\n",
    "        \n",
    "        # Vision (CLIP)\n",
    "        self.vision_processor = CLIPImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "        self.vision_encoder = CLIPVisionModel.from_pretrained(cfg.vision_model_name)\n",
    "        self.vision_encoder.to(device).eval()\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   ‚úì Vision: {cfg.vision_model_name}\")\n",
    "        \n",
    "        # Audio (Whisper)\n",
    "        self.audio_processor = WhisperProcessor.from_pretrained(cfg.audio_model_name)\n",
    "        self.audio_encoder = WhisperModel.from_pretrained(cfg.audio_model_name).encoder\n",
    "        self.audio_encoder.to(device).eval()\n",
    "        for p in self.audio_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   ‚úì Audio: {cfg.audio_model_name}\")\n",
    "        \n",
    "        # Text (Sentence-BERT)\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "        self.text_encoder = AutoModel.from_pretrained(cfg.text_model_name)\n",
    "        self.text_encoder.to(device).eval()\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   ‚úì Text: {cfg.text_model_name}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"Encode images with CLIP.\"\"\"\n",
    "        inputs = self.vision_processor(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
    "        outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        return outputs.last_hidden_state  # (B, 50, 768)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_audio(self, waveforms: List[np.ndarray], sr: int = 16000) -> torch.Tensor:\n",
    "        \"\"\"Encode audio with Whisper.\"\"\"\n",
    "        inputs = self.audio_processor(\n",
    "            waveforms, sampling_rate=sr, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_features = inputs[\"input_features\"].to(self.device)\n",
    "        outputs = self.audio_encoder(input_features)\n",
    "        return outputs.last_hidden_state  # (B, T, 512)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode texts with Sentence-BERT.\"\"\"\n",
    "        tokens = self.text_tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        outputs = self.text_encoder(**tokens)\n",
    "        return outputs.last_hidden_state, tokens[\"attention_mask\"]  # (B, L, 384), (B, L)\n",
    "\n",
    "\n",
    "# Load encoders\n",
    "encoders = FrozenEncoders(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def contrastive_loss(z_a: torch.Tensor, z_b: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Symmetric InfoNCE contrastive loss.\n",
    "    Brings matching pairs together, pushes non-matching apart.\n",
    "    \"\"\"\n",
    "    z_a = F.normalize(z_a, dim=-1)\n",
    "    z_b = F.normalize(z_b, dim=-1)\n",
    "    \n",
    "    logits = z_a @ z_b.T / temperature\n",
    "    labels = torch.arange(z_a.size(0), device=z_a.device)\n",
    "    \n",
    "    loss_a2b = F.cross_entropy(logits, labels)\n",
    "    loss_b2a = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_a2b + loss_b2a) / 2\n",
    "\n",
    "\n",
    "def matryoshka_loss(\n",
    "    z_a: torch.Tensor, \n",
    "    z_b: torch.Tensor, \n",
    "    dims: Tuple[int, ...] = (64, 128, 256, 512),\n",
    "    temperature: float = 0.07,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Matryoshka Representation Learning loss.\n",
    "    Trains at multiple embedding dimensions for flexible inference.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for dim in dims:\n",
    "        if dim > z_a.size(-1):\n",
    "            continue\n",
    "        z_a_trunc = z_a[:, :dim]\n",
    "        z_b_trunc = z_b[:, :dim]\n",
    "        total_loss += contrastive_loss(z_a_trunc, z_b_trunc, temperature)\n",
    "        count += 1\n",
    "    \n",
    "    return total_loss / count if count > 0 else total_loss\n",
    "\n",
    "\n",
    "def compute_alignment_metrics(z_a: torch.Tensor, z_b: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute alignment quality metrics.\"\"\"\n",
    "    z_a = F.normalize(z_a, dim=-1)\n",
    "    z_b = F.normalize(z_b, dim=-1)\n",
    "    \n",
    "    # Alignment: average distance between pairs (lower = better)\n",
    "    alignment = (z_a - z_b).pow(2).sum(dim=-1).mean().item()\n",
    "    \n",
    "    # Similarity of matching pairs (higher = better)\n",
    "    pos_sim = (z_a * z_b).sum(dim=-1).mean().item()\n",
    "    \n",
    "    # Recall@1\n",
    "    sim_matrix = z_a @ z_b.T\n",
    "    preds = sim_matrix.argmax(dim=-1)\n",
    "    targets = torch.arange(z_a.size(0), device=z_a.device)\n",
    "    recall_at_1 = (preds == targets).float().mean().item() * 100\n",
    "    \n",
    "    return {\n",
    "        \"alignment\": alignment,\n",
    "        \"pos_sim\": pos_sim,\n",
    "        \"R@1\": recall_at_1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 1 DATASET: IMAGE-TEXT PAIRS\n",
    "# ============================================================\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"Dataset for image-text pairs from COCO or similar.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoders: FrozenEncoders, max_samples: int = None):\n",
    "        self.encoders = encoders\n",
    "        \n",
    "        print(\"\\nüìö Loading image-text dataset...\")\n",
    "        \n",
    "        # Try different datasets\n",
    "        try:\n",
    "            self.dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"train\")\n",
    "            self.img_col = \"image\"\n",
    "            self.txt_col = \"sentences\"\n",
    "            print(\"   Using: yerevann/coco-karpathy\")\n",
    "        except:\n",
    "            try:\n",
    "                self.dataset = load_dataset(\"nlphuji/flickr30k\", split=\"test\")\n",
    "                self.img_col = \"image\"\n",
    "                self.txt_col = \"caption\"\n",
    "                print(\"   Using: nlphuji/flickr30k\")\n",
    "            except:\n",
    "                # Fallback: create dummy data\n",
    "                print(\"   ‚ö†Ô∏è Using dummy data (no dataset available)\")\n",
    "                self.dataset = None\n",
    "        \n",
    "        if self.dataset and max_samples:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "        \n",
    "        if self.dataset:\n",
    "            print(f\"   Samples: {len(self.dataset)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset else 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset is None:\n",
    "            # Return dummy data\n",
    "            return {\n",
    "                \"image\": Image.new(\"RGB\", (224, 224), color=\"white\"),\n",
    "                \"caption\": \"A dummy caption for testing.\",\n",
    "            }\n",
    "        \n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Get image\n",
    "        img = item[self.img_col]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.open(BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        # Get caption\n",
    "        caption = item[self.txt_col]\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0] if caption else \"\"\n",
    "        if isinstance(caption, dict):\n",
    "            caption = caption.get(\"raw\", str(caption))\n",
    "        \n",
    "        return {\"image\": img, \"caption\": str(caption)}\n",
    "\n",
    "\n",
    "def collate_phase1(batch, encoders):\n",
    "    \"\"\"Collate function for Phase 1 training.\"\"\"\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    \n",
    "    # Encode with frozen encoders\n",
    "    vision_features = encoders.encode_images(images)\n",
    "    text_features, text_mask = encoders.encode_texts(captions)\n",
    "    \n",
    "    return {\n",
    "        \"vision_features\": vision_features,\n",
    "        \"text_features\": text_features,\n",
    "        \"text_mask\": text_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 2 DATASET: CAPTIONING\n",
    "# ============================================================\n",
    "\n",
    "class CaptioningDataset(Dataset):\n",
    "    \"\"\"Dataset for image captioning (Phase 2 LLM training).\"\"\"\n",
    "    \n",
    "    def __init__(self, encoders: FrozenEncoders, llm_tokenizer, max_samples: int = None):\n",
    "        self.encoders = encoders\n",
    "        self.tokenizer = llm_tokenizer\n",
    "        \n",
    "        print(\"\\nüìö Loading captioning dataset...\")\n",
    "        \n",
    "        try:\n",
    "            self.dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"train\")\n",
    "            self.img_col = \"image\"\n",
    "            self.txt_col = \"sentences\"\n",
    "        except:\n",
    "            self.dataset = None\n",
    "            print(\"   ‚ö†Ô∏è Using dummy data\")\n",
    "        \n",
    "        if self.dataset and max_samples:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "        \n",
    "        if self.dataset:\n",
    "            print(f\"   Samples: {len(self.dataset)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset else 500\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset is None:\n",
    "            return {\n",
    "                \"image\": Image.new(\"RGB\", (224, 224)),\n",
    "                \"caption\": \"A sample image.\",\n",
    "            }\n",
    "        \n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        img = item[self.img_col]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.open(BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        caption = item[self.txt_col]\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0] if caption else \"\"\n",
    "        if isinstance(caption, dict):\n",
    "            caption = caption.get(\"raw\", str(caption))\n",
    "        \n",
    "        return {\"image\": img, \"caption\": str(caption)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Simple logging for training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_wandb: bool = False, project_name: str = \"multimodal_align\"):\n",
    "        self.use_wandb = use_wandb and HAS_WANDB\n",
    "        self.metrics_history = defaultdict(list)\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            wandb.init(project=project_name)\n",
    "    \n",
    "    def log(self, metrics: Dict[str, float], step: int):\n",
    "        for k, v in metrics.items():\n",
    "            self.metrics_history[k].append((step, v))\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "    \n",
    "    def plot(self, metric_name: str):\n",
    "        if metric_name not in self.metrics_history:\n",
    "            return\n",
    "        \n",
    "        steps, values = zip(*self.metrics_history[metric_name])\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(steps, values)\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f\"Training: {metric_name}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, step, loss, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": step,\n",
    "        \"loss\": loss,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"   üíæ Saved checkpoint: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    \"\"\"Load training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    return checkpoint[\"epoch\"], checkpoint[\"step\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ PHASE 1: Alignment Training\n",
    "\n",
    "**Objective**: Align vision, audio, and text in a shared embedding space.\n",
    "\n",
    "**Trainable Components**:\n",
    "- Vision Adapter\n",
    "- Audio Adapter  \n",
    "- Text Adapter\n",
    "- Perceiver Resampler\n",
    "- Alignment Projector\n",
    "\n",
    "**Loss**: Matryoshka + CLIP Contrastive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 1: ALIGNMENT TRAINING\n",
    "# ============================================================\n",
    "\n",
    "def train_phase1(model, encoders, cfg, resume_from=None):\n",
    "    \"\"\"\n",
    "    Phase 1: Train multimodal alignment.\n",
    "    \n",
    "    This trains the adapters, perceiver, and alignment projector\n",
    "    to align vision and text embeddings in a shared space.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ PHASE 1: ALIGNMENT TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = ImageTextDataset(encoders, max_samples=cfg.phase1_max_samples)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.phase1_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        collate_fn=lambda b: collate_phase1(b, encoders),\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    # Optimizer (only Phase 1 params)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.get_phase1_params(),\n",
    "        lr=cfg.phase1_lr,\n",
    "        weight_decay=cfg.phase1_weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(dataloader) * cfg.phase1_epochs\n",
    "    warmup_steps = int(total_steps * cfg.phase1_warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Mixed precision\n",
    "    scaler = GradScaler() if cfg.use_amp else None\n",
    "    \n",
    "    # Logger\n",
    "    logger = TrainingLogger(use_wandb=cfg.use_wandb)\n",
    "    \n",
    "    # Resume if specified\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    if resume_from and Path(resume_from).exists():\n",
    "        start_epoch, global_step = load_checkpoint(model, optimizer, resume_from)\n",
    "        print(f\"   Resumed from epoch {start_epoch}, step {global_step}\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(start_epoch, cfg.phase1_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_metrics = defaultdict(float)\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{cfg.phase1_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Get features\n",
    "            vision_feats = batch[\"vision_features\"].to(device)\n",
    "            text_feats = batch[\"text_features\"].to(device)\n",
    "            text_mask = batch[\"text_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with autocast(enabled=cfg.use_amp):\n",
    "                z_vision, _ = model.encode_vision(vision_feats)\n",
    "                z_text, _ = model.encode_text(text_feats, text_mask)\n",
    "                \n",
    "                # Compute losses\n",
    "                loss_mrl = matryoshka_loss(z_vision, z_text, cfg.mrl_dims, cfg.temperature)\n",
    "                loss_clip = contrastive_loss(z_vision, z_text, cfg.temperature)\n",
    "                \n",
    "                loss = cfg.mrl_weight * loss_mrl + cfg.clip_weight * loss_clip\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_metrics[\"loss_mrl\"] += loss_mrl.item()\n",
    "            epoch_metrics[\"loss_clip\"] += loss_clip.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            })\n",
    "            \n",
    "            # Logging\n",
    "            if global_step % cfg.log_every_n_steps == 0:\n",
    "                metrics = compute_alignment_metrics(z_vision.detach(), z_text.detach())\n",
    "                logger.log({\n",
    "                    \"phase1/loss\": loss.item(),\n",
    "                    \"phase1/loss_mrl\": loss_mrl.item(),\n",
    "                    \"phase1/loss_clip\": loss_clip.item(),\n",
    "                    \"phase1/R@1\": metrics[\"R@1\"],\n",
    "                    \"phase1/lr\": scheduler.get_last_lr()[0],\n",
    "                }, global_step)\n",
    "            \n",
    "            # Checkpointing\n",
    "            if global_step % cfg.save_every_n_steps == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, epoch, global_step, loss.item(),\n",
    "                    f\"{cfg.checkpoint_dir}/phase1/step_{global_step}.pt\"\n",
    "                )\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"\\n   Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Loss: {avg_loss:.4f}\")\n",
    "        print(f\"   MRL: {epoch_metrics['loss_mrl']/len(dataloader):.4f}\")\n",
    "        print(f\"   CLIP: {epoch_metrics['loss_clip']/len(dataloader):.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, epoch, global_step, avg_loss,\n",
    "                f\"{cfg.checkpoint_dir}/phase1/best.pt\"\n",
    "            )\n",
    "    \n",
    "    # Final save\n",
    "    save_checkpoint(\n",
    "        model, optimizer, cfg.phase1_epochs, global_step, avg_loss,\n",
    "        f\"{cfg.checkpoint_dir}/phase1/final.pt\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Phase 1 Training Complete!\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 1 Training\n",
    "phase1_logger = train_phase1(model, encoders, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "phase1_logger.plot(\"phase1/loss\")\n",
    "phase1_logger.plot(\"phase1/R@1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† PHASE 2: LLM Integration Training\n",
    "\n",
    "**Objective**: Enable LLM to understand and generate from multimodal inputs.\n",
    "\n",
    "**Trainable Components**:\n",
    "- LLM Projector (maps perceiver latents to LLM embedding space)\n",
    "- Optionally: LoRA adapters on LLM\n",
    "\n",
    "**Frozen**:\n",
    "- All Phase 1 components (adapters, perceiver, alignment projector)\n",
    "- LLM base weights\n",
    "\n",
    "**Loss**: Language Modeling (next-token prediction)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD LLM FOR PHASE 2\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüì¶ Loading LLM for Phase 2...\")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model_name)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.llm_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Freeze LLM\n",
    "for p in llm_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"   ‚úì LLM: {cfg.llm_model_name}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in llm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 2: LLM INTEGRATION TRAINING\n",
    "# ============================================================\n",
    "\n",
    "def train_phase2(model, llm_model, llm_tokenizer, encoders, cfg):\n",
    "    \"\"\"\n",
    "    Phase 2: Train LLM integration.\n",
    "    \n",
    "    This trains the LLM projector to map perceiver latents\n",
    "    to the LLM embedding space for generation tasks.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üß† PHASE 2: LLM INTEGRATION TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Freeze Phase 1 components\n",
    "    for p in model.vision_adapter.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.audio_adapter.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.text_adapter.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.perceiver.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.alignment_proj.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Only train LLM projector\n",
    "    for p in model.llm_proj.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Trainable parameters: {trainable:,}\")\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = CaptioningDataset(encoders, llm_tokenizer, max_samples=cfg.phase2_max_samples)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.get_phase2_params(),\n",
    "        lr=cfg.phase2_lr,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    # Logger\n",
    "    logger = TrainingLogger(use_wandb=cfg.use_wandb)\n",
    "    \n",
    "    # Mixed precision\n",
    "    scaler = GradScaler() if cfg.use_amp else None\n",
    "    \n",
    "    # Get LLM device and dtype\n",
    "    llm_device = llm_model.model.embed_tokens.weight.device\n",
    "    llm_dtype = llm_model.model.embed_tokens.weight.dtype\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.phase2_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Manual batching for captioning\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        pbar = tqdm(\n",
    "            range(0, len(indices), cfg.phase2_batch_size),\n",
    "            desc=f\"Epoch {epoch+1}/{cfg.phase2_epochs}\"\n",
    "        )\n",
    "        \n",
    "        for batch_start in pbar:\n",
    "            batch_indices = indices[batch_start:batch_start + cfg.phase2_batch_size]\n",
    "            \n",
    "            # Collect batch\n",
    "            images = []\n",
    "            captions = []\n",
    "            for idx in batch_indices:\n",
    "                item = dataset[idx]\n",
    "                images.append(item[\"image\"])\n",
    "                captions.append(item[\"caption\"])\n",
    "            \n",
    "            try:\n",
    "                # Encode images\n",
    "                with torch.no_grad():\n",
    "                    vision_feats = encoders.encode_images(images)\n",
    "                \n",
    "                # Get perceiver latents and project to LLM space\n",
    "                with autocast(enabled=cfg.use_amp):\n",
    "                    _, latents = model.encode_vision(vision_feats)\n",
    "                    prefix_embeds = model.project_to_llm(latents)  # (B, K, D_llm)\n",
    "                \n",
    "                # Move to LLM device/dtype\n",
    "                prefix_embeds = prefix_embeds.to(device=llm_device, dtype=llm_dtype)\n",
    "                \n",
    "                # Tokenize captions\n",
    "                text_inputs = llm_tokenizer(\n",
    "                    captions,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=cfg.max_seq_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(llm_device)\n",
    "                \n",
    "                # Get text embeddings\n",
    "                text_embeds = llm_model.get_input_embeddings()(text_inputs.input_ids)\n",
    "                \n",
    "                # Concatenate: [prefix_embeds, text_embeds]\n",
    "                combined_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)\n",
    "                \n",
    "                # Create labels: -100 for prefix (don't compute loss), then text tokens\n",
    "                prefix_len = prefix_embeds.size(1)\n",
    "                labels = torch.full((text_inputs.input_ids.size(0), prefix_len), -100, device=llm_device)\n",
    "                labels = torch.cat([labels, text_inputs.input_ids], dim=1)\n",
    "                \n",
    "                # Create attention mask\n",
    "                prefix_mask = torch.ones(prefix_embeds.size(0), prefix_len, device=llm_device)\n",
    "                combined_mask = torch.cat([prefix_mask, text_inputs.attention_mask], dim=1)\n",
    "                \n",
    "                # Forward through LLM\n",
    "                outputs = llm_model(\n",
    "                    inputs_embeds=combined_embeds,\n",
    "                    attention_mask=combined_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward\n",
    "                optimizer.zero_grad()\n",
    "                if scaler:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.llm_proj.parameters(), cfg.max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.llm_proj.parameters(), cfg.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % cfg.log_every_n_steps == 0:\n",
    "                    logger.log({\"phase2/loss\": loss.item()}, global_step)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Batch error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # End of epoch\n",
    "        num_batches = len(indices) // cfg.phase2_batch_size\n",
    "        avg_loss = epoch_loss / max(num_batches, 1)\n",
    "        print(f\"\\n   Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, epoch, global_step, avg_loss,\n",
    "                f\"{cfg.checkpoint_dir}/phase2/best.pt\"\n",
    "            )\n",
    "    \n",
    "    # Final save\n",
    "    save_checkpoint(\n",
    "        model, optimizer, cfg.phase2_epochs, global_step, avg_loss,\n",
    "        f\"{cfg.checkpoint_dir}/phase2/final.pt\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Phase 2 Training Complete!\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 2 Training\n",
    "phase2_logger = train_phase2(model, llm_model, llm_tokenizer, encoders, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Phase 2 training curve\n",
    "phase2_logger.plot(\"phase2/loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Evaluation & Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image(image: Image.Image, model, encoders):\n",
    "    \"\"\"Encode a single image to aligned embedding.\"\"\"\n",
    "    model.eval()\n",
    "    vision_feats = encoders.encode_images([image])\n",
    "    z, _ = model.encode_vision(vision_feats)\n",
    "    return F.normalize(z, dim=-1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_query(text: str, model, encoders):\n",
    "    \"\"\"Encode a single text to aligned embedding.\"\"\"\n",
    "    model.eval()\n",
    "    text_feats, mask = encoders.encode_texts([text])\n",
    "    z, _ = model.encode_text(text_feats, mask)\n",
    "    return F.normalize(z, dim=-1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(image: Image.Image, model, encoders, llm_model, llm_tokenizer, max_new_tokens=50):\n",
    "    \"\"\"Generate caption for an image.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get LLM device/dtype\n",
    "    llm_device = llm_model.model.embed_tokens.weight.device\n",
    "    llm_dtype = llm_model.model.embed_tokens.weight.dtype\n",
    "    \n",
    "    # Encode image\n",
    "    vision_feats = encoders.encode_images([image])\n",
    "    _, latents = model.encode_vision(vision_feats)\n",
    "    prefix_embeds = model.project_to_llm(latents)\n",
    "    prefix_embeds = prefix_embeds.to(device=llm_device, dtype=llm_dtype)\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = \"Describe this image:\"\n",
    "    prompt_tokens = llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_device)\n",
    "    prompt_embeds = llm_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "    \n",
    "    # Concatenate\n",
    "    combined = torch.cat([prefix_embeds, prompt_embeds], dim=1)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = llm_model.generate(\n",
    "        inputs_embeds=combined,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=llm_tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    return llm_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST RETRIEVAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüß™ Testing Image-Text Retrieval...\")\n",
    "\n",
    "# Create test data\n",
    "test_images = [Image.new(\"RGB\", (224, 224), color=c) for c in [\"red\", \"green\", \"blue\", \"yellow\"]]\n",
    "test_texts = [\"a red image\", \"a green image\", \"a blue image\", \"a yellow image\"]\n",
    "\n",
    "# Encode\n",
    "model.eval()\n",
    "vision_feats = encoders.encode_images(test_images)\n",
    "text_feats, text_mask = encoders.encode_texts(test_texts)\n",
    "\n",
    "z_vision, _ = model.encode_vision(vision_feats)\n",
    "z_text, _ = model.encode_text(text_feats, text_mask)\n",
    "\n",
    "z_vision = F.normalize(z_vision, dim=-1)\n",
    "z_text = F.normalize(z_text, dim=-1)\n",
    "\n",
    "# Compute similarity\n",
    "sim_matrix = z_text @ z_vision.T\n",
    "print(\"\\nSimilarity Matrix (Text ‚Üí Image):\")\n",
    "print(sim_matrix.cpu().numpy().round(3))\n",
    "\n",
    "# Retrieval accuracy\n",
    "preds = sim_matrix.argmax(dim=-1)\n",
    "targets = torch.arange(len(test_texts), device=device)\n",
    "accuracy = (preds == targets).float().mean().item() * 100\n",
    "print(f\"\\nRetrieval Accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST GENERATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüß™ Testing Image Captioning...\")\n",
    "\n",
    "# Test with a sample image\n",
    "test_image = Image.new(\"RGB\", (224, 224), color=\"blue\")\n",
    "\n",
    "try:\n",
    "    caption = generate_caption(test_image, model, encoders, llm_model, llm_tokenizer)\n",
    "    print(f\"\\nGenerated Caption: {caption}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Generation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üíæ Save Final Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE COMPLETE MODEL\n",
    "# ============================================================\n",
    "\n",
    "def save_complete_model(model, cfg, path):\n",
    "    \"\"\"Save the complete trained model with config.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"architecture\": {\n",
    "            \"perceiver_dim\": cfg.perceiver_dim,\n",
    "            \"num_latents\": cfg.num_latents,\n",
    "            \"num_perceiver_layers\": cfg.num_perceiver_layers,\n",
    "            \"d_align\": cfg.d_align,\n",
    "            \"llm_hidden_size\": cfg.llm_hidden_size,\n",
    "        },\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"\\nüíæ Saved complete model: {path}\")\n",
    "\n",
    "\n",
    "# Save\n",
    "save_complete_model(model, cfg, f\"{cfg.checkpoint_dir}/multimodal_align_complete.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL FOR INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def load_complete_model(path, device=\"cuda\"):\n",
    "    \"\"\"Load a complete trained model.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Recreate config\n",
    "    cfg = TrainingConfig(**checkpoint[\"config\"])\n",
    "    \n",
    "    # Recreate model\n",
    "    model = MultimodalAlignmentModel(cfg).to(device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "# Test loading\n",
    "loaded_model, loaded_cfg = load_complete_model(f\"{cfg.checkpoint_dir}/multimodal_align_complete.pt\")\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Training Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Experiment: {cfg.experiment_name}\n",
    "\n",
    "Architecture:\n",
    "  ‚Ä¢ Perceiver: {cfg.num_latents} latents √ó {cfg.num_perceiver_layers} layers\n",
    "  ‚Ä¢ Alignment dim: {cfg.d_align}\n",
    "  ‚Ä¢ MRL dims: {cfg.mrl_dims}\n",
    "\n",
    "Phase 1 (Alignment):\n",
    "  ‚Ä¢ Epochs: {cfg.phase1_epochs}\n",
    "  ‚Ä¢ Batch size: {cfg.phase1_batch_size}\n",
    "  ‚Ä¢ Learning rate: {cfg.phase1_lr}\n",
    "  ‚Ä¢ Samples: {cfg.phase1_max_samples}\n",
    "\n",
    "Phase 2 (LLM Integration):\n",
    "  ‚Ä¢ Epochs: {cfg.phase2_epochs}\n",
    "  ‚Ä¢ Batch size: {cfg.phase2_batch_size}\n",
    "  ‚Ä¢ Learning rate: {cfg.phase2_lr}\n",
    "  ‚Ä¢ Samples: {cfg.phase2_max_samples}\n",
    "\n",
    "Checkpoints saved to: {cfg.checkpoint_dir}/\n",
    "  ‚Ä¢ phase1/best.pt - Best alignment model\n",
    "  ‚Ä¢ phase2/best.pt - Best LLM-integrated model  \n",
    "  ‚Ä¢ multimodal_align_complete.pt - Final complete model\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
