{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Comprehensive Multimodal Alignment Evaluation\n",
    "\n",
    "**A thorough evaluation suite for multimodal alignment models**\n",
    "\n",
    "This notebook evaluates your trained model against standard benchmarks and compares with published baselines.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Benchmarks Covered\n",
    "\n",
    "| Benchmark | Task | Metrics | Baselines |\n",
    "|-----------|------|---------|----------|\n",
    "| **COCO** | Image-Text Retrieval | R@1, R@5, R@10 | CLIP, BLIP, ImageBind |\n",
    "| **Flickr30K** | Image-Text Retrieval | R@1, R@5, R@10 | CLIP, BLIP |\n",
    "| **ESC-50** | Audio Zero-Shot Classification | Accuracy | AudioCLIP, ImageBind, CLAP |\n",
    "| **AudioCaps** | Audio-Text Retrieval | R@1, R@5, R@10 | CLAP, ImageBind |\n",
    "| **LibriSpeech** | ASR (via LLM) | WER | Whisper |\n",
    "| **Matryoshka** | Embedding Compression | R@1 vs Dim | - |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Reference Papers\n",
    "\n",
    "1. **CLIP** (Radford et al., 2021) - Learning Transferable Visual Models From Natural Language Supervision\n",
    "2. **ImageBind** (Girdhar et al., 2023) - One Embedding Space To Bind Them All\n",
    "3. **BLIP** (Li et al., 2022) - Bootstrapping Language-Image Pre-training\n",
    "4. **CLAP** (Elizalde et al., 2023) - CLAP: Learning Audio Concepts from Natural Language Supervision\n",
    "5. **Matryoshka** (Kusupati et al., 2022) - Matryoshka Representation Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers & Datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    WhisperModel,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Audio processing\n",
    "try:\n",
    "    import librosa\n",
    "    HAS_LIBROSA = True\n",
    "except ImportError:\n",
    "    HAS_LIBROSA = False\n",
    "    print(\"âš ï¸ librosa not installed. Run: pip install librosa\")\n",
    "\n",
    "# Metrics\n",
    "try:\n",
    "    from torchmetrics.text import WordErrorRate\n",
    "    HAS_WER = True\n",
    "except ImportError:\n",
    "    HAS_WER = False\n",
    "    print(\"âš ï¸ torchmetrics not installed. Run: pip install torchmetrics\")\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"âš ï¸ sklearn not installed. Run: pip install scikit-learn\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nğŸ–¥ï¸  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for comprehensive evaluation.\"\"\"\n",
    "    \n",
    "    # === Model Paths ===\n",
    "    checkpoint_path: str = \"./checkpoints/multimodal_adapter_poc_1.pt\"\n",
    "    \n",
    "    # === Encoder Models (must match training!) ===\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    text_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  # For generation tasks\n",
    "    \n",
    "    # === Architecture Dimensions ===\n",
    "    d_vision: int = 768      # CLIP ViT-B/32 output\n",
    "    d_text: int = 384        # MiniLM output\n",
    "    d_audio: int = 512       # Whisper-base output\n",
    "    d_align: int = 512       # Alignment embedding dimension\n",
    "    \n",
    "    # Perceiver (if used)\n",
    "    use_perceiver: bool = True\n",
    "    perceiver_dim: int = 512\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 2\n",
    "    num_attn_heads: int = 8\n",
    "    \n",
    "    # LLM projection\n",
    "    llm_hidden_size: int = 1536  # Qwen2.5-1.5B hidden size\n",
    "    \n",
    "    # === Evaluation Settings ===\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 0\n",
    "    use_fp16: bool = False\n",
    "    \n",
    "    # Sample sizes (set to None for full evaluation)\n",
    "    coco_samples: int = 1000\n",
    "    flickr_samples: int = 1000\n",
    "    audiocaps_samples: int = 500\n",
    "    esc50_samples: int = 400  # Full ESC-50 = 2000\n",
    "    librispeech_samples: int = 100\n",
    "    \n",
    "    # === Matryoshka Evaluation ===\n",
    "    mrl_dims: Tuple[int, ...] = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # === Output ===\n",
    "    seed: int = 42\n",
    "    save_results: bool = True\n",
    "    results_dir: str = \"./eval_results\"\n",
    "    \n",
    "\n",
    "cfg = EvalConfig()\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Create results directory\n",
    "if cfg.save_results:\n",
    "    Path(cfg.results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Configuration:\")\n",
    "print(f\"   Checkpoint: {cfg.checkpoint_path}\")\n",
    "print(f\"   d_align: {cfg.d_align}\")\n",
    "print(f\"   Perceiver: {cfg.use_perceiver}\")\n",
    "print(f\"   MRL dims: {cfg.mrl_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Published Baselines\n",
    "\n",
    "Reference numbers from CLIP, ImageBind, BLIP, CLAP papers for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PUBLISHED BASELINE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "BASELINES = {\n",
    "    # === COCO Image-Text Retrieval (5K test set) ===\n",
    "    # From original CLIP, BLIP, ImageBind papers\n",
    "    \"coco_retrieval\": {\n",
    "        \"CLIP-ViT-B/32\": {\n",
    "            \"T2I_R@1\": 30.4, \"T2I_R@5\": 56.0, \"T2I_R@10\": 67.0,\n",
    "            \"I2T_R@1\": 50.1, \"I2T_R@5\": 75.3, \"I2T_R@10\": 84.0,\n",
    "            \"source\": \"CLIP (Radford et al., 2021)\"\n",
    "        },\n",
    "        \"CLIP-ViT-L/14\": {\n",
    "            \"T2I_R@1\": 36.5, \"T2I_R@5\": 61.1, \"T2I_R@10\": 71.6,\n",
    "            \"I2T_R@1\": 56.3, \"I2T_R@5\": 79.4, \"I2T_R@10\": 86.7,\n",
    "            \"source\": \"CLIP (Radford et al., 2021)\"\n",
    "        },\n",
    "        \"BLIP-ViT-B\": {\n",
    "            \"T2I_R@1\": 39.7, \"T2I_R@5\": 63.8, \"T2I_R@10\": 74.0,\n",
    "            \"I2T_R@1\": 59.2, \"I2T_R@5\": 82.4, \"I2T_R@10\": 89.6,\n",
    "            \"source\": \"BLIP (Li et al., 2022)\"\n",
    "        },\n",
    "        \"ImageBind\": {\n",
    "            \"T2I_R@1\": 34.8, \"T2I_R@5\": 60.2, \"T2I_R@10\": 70.9,\n",
    "            \"I2T_R@1\": 53.2, \"I2T_R@5\": 77.8, \"I2T_R@10\": 85.6,\n",
    "            \"source\": \"ImageBind (Girdhar et al., 2023)\"\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # === Flickr30K Image-Text Retrieval (1K test set) ===\n",
    "    \"flickr_retrieval\": {\n",
    "        \"CLIP-ViT-B/32\": {\n",
    "            \"T2I_R@1\": 68.7, \"T2I_R@5\": 90.6, \"T2I_R@10\": 95.2,\n",
    "            \"I2T_R@1\": 88.0, \"I2T_R@5\": 98.7, \"I2T_R@10\": 99.4,\n",
    "            \"source\": \"CLIP (Radford et al., 2021)\"\n",
    "        },\n",
    "        \"CLIP-ViT-L/14\": {\n",
    "            \"T2I_R@1\": 75.6, \"T2I_R@5\": 93.2, \"T2I_R@10\": 96.5,\n",
    "            \"I2T_R@1\": 92.4, \"I2T_R@5\": 99.3, \"I2T_R@10\": 99.8,\n",
    "            \"source\": \"CLIP (Radford et al., 2021)\"\n",
    "        },\n",
    "        \"BLIP-ViT-B\": {\n",
    "            \"T2I_R@1\": 80.6, \"T2I_R@5\": 95.2, \"T2I_R@10\": 97.8,\n",
    "            \"I2T_R@1\": 94.8, \"I2T_R@5\": 99.7, \"I2T_R@10\": 99.9,\n",
    "            \"source\": \"BLIP (Li et al., 2022)\"\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # === ESC-50 Zero-Shot Audio Classification ===\n",
    "    \"esc50_zeroshot\": {\n",
    "        \"Random\": {\"accuracy\": 2.0, \"source\": \"1/50 classes\"},\n",
    "        \"Wav2CLIP\": {\"accuracy\": 41.4, \"source\": \"Wav2CLIP (Wu et al., 2022)\"},\n",
    "        \"AudioCLIP\": {\"accuracy\": 69.4, \"source\": \"AudioCLIP (Guzhov et al., 2022)\"},\n",
    "        \"ImageBind\": {\"accuracy\": 66.9, \"source\": \"ImageBind (Girdhar et al., 2023)\"},\n",
    "        \"CLAP\": {\"accuracy\": 82.6, \"source\": \"CLAP (Elizalde et al., 2023)\"},\n",
    "    },\n",
    "    \n",
    "    # === AudioCaps Text-Audio Retrieval ===\n",
    "    \"audiocaps_retrieval\": {\n",
    "        \"AudioCLIP\": {\n",
    "            \"T2A_R@1\": 6.8, \"T2A_R@5\": 19.2, \"T2A_R@10\": 28.8,\n",
    "            \"A2T_R@1\": 9.1, \"A2T_R@5\": 24.5, \"A2T_R@10\": 35.1,\n",
    "            \"source\": \"AudioCLIP (Guzhov et al., 2022)\"\n",
    "        },\n",
    "        \"ImageBind\": {\n",
    "            \"T2A_R@1\": 8.3, \"T2A_R@5\": 23.4, \"T2A_R@10\": 33.5,\n",
    "            \"A2T_R@1\": 11.2, \"A2T_R@5\": 28.7, \"A2T_R@10\": 40.2,\n",
    "            \"source\": \"ImageBind (Girdhar et al., 2023)\"\n",
    "        },\n",
    "        \"CLAP\": {\n",
    "            \"T2A_R@1\": 26.7, \"T2A_R@5\": 54.4, \"T2A_R@10\": 67.1,\n",
    "            \"A2T_R@1\": 33.9, \"A2T_R@5\": 61.8, \"A2T_R@10\": 73.8,\n",
    "            \"source\": \"CLAP (Elizalde et al., 2023)\"\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # === ImageNet Zero-Shot ===\n",
    "    \"imagenet_zeroshot\": {\n",
    "        \"CLIP-ViT-B/32\": {\"top1\": 63.2, \"top5\": 87.8, \"source\": \"CLIP\"},\n",
    "        \"CLIP-ViT-L/14\": {\"top1\": 75.5, \"top5\": 93.0, \"source\": \"CLIP\"},\n",
    "        \"ImageBind\": {\"top1\": 77.7, \"top5\": 94.3, \"source\": \"ImageBind\"},\n",
    "    },\n",
    "    \n",
    "    # === LibriSpeech ASR (WER - lower is better) ===\n",
    "    \"librispeech_asr\": {\n",
    "        \"Whisper-tiny\": {\"wer_clean\": 5.6, \"source\": \"Whisper (Radford et al., 2022)\"},\n",
    "        \"Whisper-base\": {\"wer_clean\": 4.2, \"source\": \"Whisper (Radford et al., 2022)\"},\n",
    "        \"Whisper-small\": {\"wer_clean\": 3.4, \"source\": \"Whisper (Radford et al., 2022)\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Loaded baselines from published papers:\")\n",
    "for benchmark, models in BASELINES.items():\n",
    "    print(f\"   â€¢ {benchmark}: {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Define the model architecture to match training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL ARCHITECTURE DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    \"\"\"Simple linear projection adapter.\"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class MLPAdapter(nn.Module):\n",
    "    \"\"\"2-layer MLP adapter with LayerNorm.\"\"\"\n",
    "    def __init__(self, d_in: int, d_out: int, hidden_factor: float = 2.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(d_in * hidden_factor)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(d_in),\n",
    "            nn.Linear(d_in, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, d_out),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"FFN block for Perceiver.\"\"\"\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    \"\"\"Single Perceiver layer with cross-attention and self-attention.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "        self.mlp = FeedForward(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, latents, tokens, token_mask=None):\n",
    "        # Cross-Attention: latents attend to tokens\n",
    "        q = self.ln_latents_1(latents)\n",
    "        kv = self.ln_tokens(tokens)\n",
    "        key_padding_mask = ~token_mask.bool() if token_mask is not None else None\n",
    "        attn_out, _ = self.cross_attn(q, kv, kv, key_padding_mask=key_padding_mask, need_weights=False)\n",
    "        latents = latents + attn_out\n",
    "        \n",
    "        # Self-Attention on latents\n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_out, _ = self.self_attn(q2, q2, q2, need_weights=False)\n",
    "        latents = latents + self_out\n",
    "        \n",
    "        # FFN\n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "        return latents\n",
    "\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"Perceiver Resampler for compressing variable-length sequences.\"\"\"\n",
    "    def __init__(self, dim: int, num_latents: int, num_layers: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads, mlp_ratio) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, tokens, token_mask=None):\n",
    "        B = tokens.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION MODEL WRAPPER\n",
    "# ============================================================\n",
    "\n",
    "class AlignedModelForEval(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete aligned model wrapper for evaluation.\n",
    "    \n",
    "    Provides unified interface:\n",
    "        - embed_image(images) -> (B, D)\n",
    "        - embed_text(texts) -> (B, D)\n",
    "        - embed_audio(waveforms) -> (B, D)\n",
    "        - generate(modality_feats, modality_type, prompt) -> str\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_adapter: nn.Module,\n",
    "        audio_adapter: nn.Module,\n",
    "        perceiver: nn.Module,\n",
    "        projector: nn.Module,\n",
    "        cfg: EvalConfig,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        \n",
    "        # Adapters (loaded from checkpoint)\n",
    "        self.vision_adapter = vision_adapter\n",
    "        self.audio_adapter = audio_adapter\n",
    "        self.perceiver = perceiver\n",
    "        self.projector = projector\n",
    "        \n",
    "        # Load frozen encoders\n",
    "        print(\"\\nğŸ“¦ Loading frozen encoders...\")\n",
    "        \n",
    "        # Vision (CLIP)\n",
    "        self.vision_processor = CLIPImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "        self.vision_encoder = CLIPVisionModel.from_pretrained(cfg.vision_model_name)\n",
    "        self.vision_encoder.to(device).eval()\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   âœ“ Vision: {cfg.vision_model_name}\")\n",
    "        \n",
    "        # Audio (Whisper)\n",
    "        self.audio_processor = WhisperProcessor.from_pretrained(cfg.audio_model_name)\n",
    "        self.audio_encoder = WhisperModel.from_pretrained(cfg.audio_model_name).encoder\n",
    "        self.audio_encoder.to(device).eval()\n",
    "        for p in self.audio_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   âœ“ Audio: {cfg.audio_model_name}\")\n",
    "        \n",
    "        # Text encoder for retrieval\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "        self.text_encoder = AutoModel.from_pretrained(cfg.text_model_name)\n",
    "        self.text_encoder.to(device).eval()\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"   âœ“ Text (retrieval): {cfg.text_model_name}\")\n",
    "        \n",
    "        # LLM for generation (loaded on demand)\n",
    "        self.llm_model = None\n",
    "        self.llm_tokenizer = None\n",
    "    \n",
    "    def _load_llm_if_needed(self):\n",
    "        \"\"\"Lazy load LLM for generation tasks.\"\"\"\n",
    "        if self.llm_model is None:\n",
    "            print(f\"   Loading LLM: {self.cfg.llm_model_name}...\")\n",
    "            self.llm_tokenizer = AutoTokenizer.from_pretrained(self.cfg.llm_model_name)\n",
    "            self.llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.cfg.llm_model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            self.llm_model.eval()\n",
    "            print(f\"   âœ“ LLM loaded\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_image_features(self, features: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project pre-extracted image features to alignment space.\"\"\"\n",
    "        features = features.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "        \n",
    "        tokens = self.vision_adapter(features)\n",
    "        if self.perceiver is not None:\n",
    "            latents = self.perceiver(tokens, mask)\n",
    "            z = self.projector(latents)\n",
    "            return z.mean(dim=1)  # Pool latents\n",
    "        else:\n",
    "            return tokens.mean(dim=1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_audio_features(self, features: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project pre-extracted audio features to alignment space.\"\"\"\n",
    "        features = features.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "        \n",
    "        tokens = self.audio_adapter(features)\n",
    "        if self.perceiver is not None:\n",
    "            latents = self.perceiver(tokens, mask)\n",
    "            z = self.projector(latents)\n",
    "            return z.mean(dim=1)\n",
    "        else:\n",
    "            return tokens.mean(dim=1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, images: Union[List[Image.Image], torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Encode images to aligned embeddings.\"\"\"\n",
    "        if isinstance(images, list):\n",
    "            inputs = self.vision_processor(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "        else:\n",
    "            pixel_values = images\n",
    "        \n",
    "        pixel_values = pixel_values.to(self.device)\n",
    "        \n",
    "        # Get features from CLIP\n",
    "        outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        features = outputs.last_hidden_state  # (B, T, D)\n",
    "        \n",
    "        # Project through adapter + perceiver\n",
    "        mask = torch.ones(features.shape[0], features.shape[1], dtype=torch.bool, device=self.device)\n",
    "        z = self.encode_image_features(features, mask)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed_audio(self, waveforms: np.ndarray, sr: int = 16000) -> torch.Tensor:\n",
    "        \"\"\"Encode audio waveforms to aligned embeddings.\"\"\"\n",
    "        # Process with Whisper\n",
    "        inputs = self.audio_processor(\n",
    "            waveforms,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_features = inputs[\"input_features\"].to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        outputs = self.audio_encoder(input_features)\n",
    "        features = outputs.last_hidden_state  # (B, T, D)\n",
    "        \n",
    "        # Project through adapter + perceiver\n",
    "        mask = torch.ones(features.shape[0], features.shape[1], dtype=torch.bool, device=self.device)\n",
    "        z = self.encode_audio_features(features, mask)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode texts using the text encoder.\n",
    "        \n",
    "        For retrieval, we use the sentence-transformer encoder.\n",
    "        For comparing with LLM embeddings, we use LLM embeddings.\n",
    "        \"\"\"\n",
    "        tokens = self.text_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        outputs = self.text_encoder(**tokens)\n",
    "        \n",
    "        # Mean pooling\n",
    "        hidden = outputs.last_hidden_state\n",
    "        mask = tokens[\"attention_mask\"].unsqueeze(-1).float()\n",
    "        pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed_text_llm(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Encode texts using LLM embeddings (for comparison with projected features).\"\"\"\n",
    "        self._load_llm_if_needed()\n",
    "        \n",
    "        tokens = self.llm_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.llm_model.device)\n",
    "        \n",
    "        token_embs = self.llm_model.get_input_embeddings()(tokens.input_ids)\n",
    "        mask = tokens.attention_mask.unsqueeze(-1)\n",
    "        pooled = (token_embs * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        \n",
    "        return pooled.float()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, modality_feats: torch.Tensor, modality_type: str, prompt: str, max_new_tokens: int = 50) -> str:\n",
    "        \"\"\"Generate text from modality features.\"\"\"\n",
    "        self._load_llm_if_needed()\n",
    "        \n",
    "        # Select adapter\n",
    "        if modality_type == \"vision\":\n",
    "            adapter = self.vision_adapter\n",
    "        elif modality_type == \"audio\":\n",
    "            adapter = self.audio_adapter\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown modality: {modality_type}\")\n",
    "        \n",
    "        modality_feats = modality_feats.to(self.device)\n",
    "        mask = torch.ones(1, modality_feats.shape[1], dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        # Project modality\n",
    "        tokens = adapter(modality_feats)\n",
    "        if self.perceiver is not None:\n",
    "            latents = self.perceiver(tokens, mask)\n",
    "            modality_embeds = self.projector(latents)\n",
    "        else:\n",
    "            modality_embeds = tokens\n",
    "        \n",
    "        # Move to LLM device/dtype\n",
    "        llm_dev = self.llm_model.model.embed_tokens.weight.device\n",
    "        llm_dtype = self.llm_model.model.embed_tokens.weight.dtype\n",
    "        modality_embeds = modality_embeds.to(device=llm_dev, dtype=llm_dtype)\n",
    "        \n",
    "        # Embed text prompt\n",
    "        text_inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_dev)\n",
    "        text_embeds = self.llm_model.get_input_embeddings()(text_inputs.input_ids)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([modality_embeds, text_embeds], dim=1)\n",
    "        \n",
    "        # Generate\n",
    "        out = self.llm_model.generate(\n",
    "            inputs_embeds=combined,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.llm_tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        return self.llm_tokenizer.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "def load_model(cfg: EvalConfig) -> AlignedModelForEval:\n",
    "    \"\"\"\n",
    "    Load trained model from checkpoint.\n",
    "    \n",
    "    Expected checkpoint structure:\n",
    "        - 'vision_adapter': state_dict\n",
    "        - 'audio_adapter': state_dict  \n",
    "        - 'perceiver': state_dict\n",
    "        - 'projector': state_dict\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“¦ Loading checkpoint: {cfg.checkpoint_path}\")\n",
    "    \n",
    "    # Initialize architectures\n",
    "    vision_adapter = ModalityAdapter(cfg.d_vision, cfg.perceiver_dim)\n",
    "    audio_adapter = ModalityAdapter(cfg.d_audio, cfg.perceiver_dim)\n",
    "    \n",
    "    if cfg.use_perceiver:\n",
    "        perceiver = PerceiverResampler(\n",
    "            dim=cfg.perceiver_dim,\n",
    "            num_latents=cfg.num_latents,\n",
    "            num_layers=cfg.num_perceiver_layers,\n",
    "            num_heads=cfg.num_attn_heads,\n",
    "        )\n",
    "        projector = nn.Linear(cfg.perceiver_dim, cfg.llm_hidden_size)\n",
    "    else:\n",
    "        perceiver = None\n",
    "        projector = nn.Linear(cfg.perceiver_dim, cfg.d_align)\n",
    "    \n",
    "    # Load weights if checkpoint exists\n",
    "    if Path(cfg.checkpoint_path).exists():\n",
    "        checkpoint = torch.load(cfg.checkpoint_path, map_location=\"cpu\")\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model' in checkpoint:\n",
    "            state = checkpoint['model']\n",
    "        else:\n",
    "            state = checkpoint\n",
    "        \n",
    "        # Load each component\n",
    "        if 'vision_adapter' in state:\n",
    "            vision_adapter.load_state_dict(state['vision_adapter'])\n",
    "            print(\"   âœ“ Loaded vision_adapter\")\n",
    "        \n",
    "        if 'audio_adapter' in state:\n",
    "            audio_adapter.load_state_dict(state['audio_adapter'])\n",
    "            print(\"   âœ“ Loaded audio_adapter\")\n",
    "        \n",
    "        if 'perceiver' in state and perceiver is not None:\n",
    "            perceiver.load_state_dict(state['perceiver'])\n",
    "            print(\"   âœ“ Loaded perceiver\")\n",
    "        \n",
    "        if 'projector' in state:\n",
    "            projector.load_state_dict(state['projector'])\n",
    "            print(\"   âœ“ Loaded projector\")\n",
    "        \n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"   Checkpoint from epoch: {checkpoint['epoch']}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Checkpoint not found! Using random weights.\")\n",
    "        print(f\"   This is for testing the evaluation pipeline only.\")\n",
    "    \n",
    "    # Move to device\n",
    "    vision_adapter.to(device).eval()\n",
    "    audio_adapter.to(device).eval()\n",
    "    if perceiver is not None:\n",
    "        perceiver.to(device).eval()\n",
    "    projector.to(device).eval()\n",
    "    \n",
    "    # Create wrapper\n",
    "    model = AlignedModelForEval(\n",
    "        vision_adapter=vision_adapter,\n",
    "        audio_adapter=audio_adapter,\n",
    "        perceiver=perceiver,\n",
    "        projector=projector,\n",
    "        cfg=cfg,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = load_model(cfg)\n",
    "model.eval()\n",
    "print(\"\\nâœ… Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETRIEVAL METRICS\n",
    "# ============================================================\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"L2 normalize along specified dimension.\"\"\"\n",
    "    return F.normalize(x, p=2, dim=dim)\n",
    "\n",
    "\n",
    "def recall_at_k(sim_matrix: torch.Tensor, k: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Compute Recall@K from similarity matrix.\n",
    "    Assumes diagonal entries are ground truth matches.\n",
    "    \"\"\"\n",
    "    N = sim_matrix.size(0)\n",
    "    k = min(k, N)\n",
    "    targets = torch.arange(N, device=sim_matrix.device)\n",
    "    topk_indices = sim_matrix.topk(k, dim=-1).indices\n",
    "    hits = (topk_indices == targets.unsqueeze(-1)).any(dim=-1)\n",
    "    return hits.float().mean().item() * 100\n",
    "\n",
    "\n",
    "def compute_retrieval_metrics(\n",
    "    query_embs: torch.Tensor,\n",
    "    target_embs: torch.Tensor,\n",
    "    ks: List[int] = [1, 5, 10],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute R@K, MeanRank, MedianRank.\"\"\"\n",
    "    q = l2_normalize(query_embs.float())\n",
    "    t = l2_normalize(target_embs.float())\n",
    "    sim = q @ t.T\n",
    "    N = sim.size(0)\n",
    "    targets = torch.arange(N, device=sim.device)\n",
    "    \n",
    "    metrics = {}\n",
    "    for k in ks:\n",
    "        metrics[f\"R@{k}\"] = recall_at_k(sim, k)\n",
    "    \n",
    "    # Rank statistics\n",
    "    sorted_indices = sim.argsort(dim=-1, descending=True)\n",
    "    ranks = (sorted_indices == targets.unsqueeze(-1)).nonzero(as_tuple=True)[1]\n",
    "    metrics[\"MeanRank\"] = ranks.float().mean().item()\n",
    "    metrics[\"MedianRank\"] = ranks.median().item()\n",
    "    \n",
    "    return metrics, sim\n",
    "\n",
    "\n",
    "def compute_bidirectional_retrieval(\n",
    "    embs_a: torch.Tensor,\n",
    "    embs_b: torch.Tensor,\n",
    "    name_a: str = \"A\",\n",
    "    name_b: str = \"B\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute A->B and B->A retrieval.\"\"\"\n",
    "    a2b, _ = compute_retrieval_metrics(embs_a, embs_b)\n",
    "    a2b_results = {f\"{name_a}2{name_b}_{k}\": v for k, v in a2b.items()}\n",
    "    \n",
    "    b2a, _ = compute_retrieval_metrics(embs_b, embs_a)\n",
    "    b2a_results = {f\"{name_b}2{name_a}_{k}\": v for k, v in b2a.items()}\n",
    "    \n",
    "    return {**a2b_results, **b2a_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EMBEDDING QUALITY METRICS\n",
    "# ============================================================\n",
    "\n",
    "def compute_alignment_uniformity(\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    "    t: float = 2.0,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Alignment & Uniformity metrics (Wang & Isola, 2020).\n",
    "    \n",
    "    - Alignment: avg distance between positive pairs (lower = better)\n",
    "    - Uniformity: log of avg pairwise Gaussian (lower = better)\n",
    "    \"\"\"\n",
    "    z_a = l2_normalize(z_a.float())\n",
    "    z_b = l2_normalize(z_b.float())\n",
    "    \n",
    "    # Alignment\n",
    "    alignment = (z_a - z_b).pow(2).sum(dim=-1).mean().item()\n",
    "    \n",
    "    # Uniformity\n",
    "    sq_pdist_a = torch.pdist(z_a, p=2).pow(2)\n",
    "    uniformity_a = sq_pdist_a.mul(-t).exp().mean().log().item()\n",
    "    \n",
    "    sq_pdist_b = torch.pdist(z_b, p=2).pow(2)\n",
    "    uniformity_b = sq_pdist_b.mul(-t).exp().mean().log().item()\n",
    "    \n",
    "    return {\n",
    "        \"alignment\": alignment,\n",
    "        \"uniformity_a\": uniformity_a,\n",
    "        \"uniformity_b\": uniformity_b,\n",
    "        \"uniformity_avg\": (uniformity_a + uniformity_b) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_gramian_volume(\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    "    z_c: Optional[torch.Tensor] = None,\n",
    "    n_samples: int = 256,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Gramian volume for multimodal alignment quality.\n",
    "    Lower volume = better alignment.\n",
    "    \"\"\"\n",
    "    N = min(n_samples, z_a.size(0))\n",
    "    volumes = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        a = l2_normalize(z_a[i:i+1]).squeeze()\n",
    "        b = l2_normalize(z_b[i:i+1]).squeeze()\n",
    "        \n",
    "        if z_c is not None:\n",
    "            c = l2_normalize(z_c[i:i+1]).squeeze()\n",
    "            G = torch.stack([\n",
    "                torch.stack([a @ a, a @ b, a @ c]),\n",
    "                torch.stack([b @ a, b @ b, b @ c]),\n",
    "                torch.stack([c @ a, c @ b, c @ c]),\n",
    "            ])\n",
    "        else:\n",
    "            G = torch.stack([\n",
    "                torch.stack([a @ a, a @ b]),\n",
    "                torch.stack([b @ a, b @ b]),\n",
    "            ])\n",
    "        \n",
    "        vol = torch.det(G).abs().item()\n",
    "        volumes.append(vol)\n",
    "    \n",
    "    return {\n",
    "        \"gramian_mean\": np.mean(volumes),\n",
    "        \"gramian_std\": np.std(volumes),\n",
    "        \"gramian_median\": np.median(volumes),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image-Text Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMAGE-TEXT RETRIEVAL (COCO / Flickr30K)\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_image_text_retrieval(\n",
    "    model: AlignedModelForEval,\n",
    "    dataset_name: str = \"coco\",\n",
    "    max_samples: int = 1000,\n",
    "    batch_size: int = 32,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate image-text retrieval on COCO or Flickr30K.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ–¼ï¸  Evaluating Image-Text Retrieval on {dataset_name.upper()}...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    if dataset_name.lower() == \"coco\":\n",
    "        try:\n",
    "            ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"validation\", streaming=True)\n",
    "            img_col, txt_col = \"image\", \"sentences\"\n",
    "        except:\n",
    "            print(\"   Using PixMo-Cap as fallback...\")\n",
    "            ds = load_dataset(\"allenai/pixmo-cap\", split=\"train\", streaming=True)\n",
    "            img_col, txt_col = \"image_url\", \"caption\"\n",
    "    elif dataset_name.lower() == \"flickr\":\n",
    "        ds = load_dataset(\"nlphuji/flickr30k\", split=\"test\", streaming=True)\n",
    "        img_col, txt_col = \"image\", \"caption\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Collect embeddings\n",
    "    all_img_embs, all_txt_embs = [], []\n",
    "    batch_imgs, batch_txts = [], []\n",
    "    count = 0\n",
    "    \n",
    "    pbar = tqdm(ds, total=max_samples, desc=f\"Encoding {dataset_name}\")\n",
    "    \n",
    "    for ex in pbar:\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Get image\n",
    "            if img_col == \"image_url\":\n",
    "                resp = requests.get(ex[img_col], timeout=5)\n",
    "                img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "            else:\n",
    "                img = ex[img_col]\n",
    "                if not isinstance(img, Image.Image):\n",
    "                    continue\n",
    "                img = img.convert(\"RGB\")\n",
    "            \n",
    "            # Get caption\n",
    "            caption = ex[txt_col]\n",
    "            if isinstance(caption, list):\n",
    "                caption = caption[0] if caption else \"\"\n",
    "            if isinstance(caption, dict):\n",
    "                caption = caption.get(\"raw\", str(caption))\n",
    "            \n",
    "            batch_imgs.append(img)\n",
    "            batch_txts.append(str(caption))\n",
    "            count += 1\n",
    "            \n",
    "            # Process batch\n",
    "            if len(batch_imgs) >= batch_size:\n",
    "                img_emb = model.embed_image(batch_imgs)\n",
    "                txt_emb = model.embed_text(batch_txts)\n",
    "                all_img_embs.append(img_emb.cpu())\n",
    "                all_txt_embs.append(txt_emb.cpu())\n",
    "                batch_imgs, batch_txts = [], []\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Process remaining\n",
    "    if batch_imgs:\n",
    "        img_emb = model.embed_image(batch_imgs)\n",
    "        txt_emb = model.embed_text(batch_txts)\n",
    "        all_img_embs.append(img_emb.cpu())\n",
    "        all_txt_embs.append(txt_emb.cpu())\n",
    "    \n",
    "    # Concatenate\n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    all_txt_embs = torch.cat(all_txt_embs, dim=0)\n",
    "    \n",
    "    print(f\"   Collected {all_img_embs.size(0)} pairs\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_bidirectional_retrieval(all_txt_embs, all_img_embs, \"T\", \"I\")\n",
    "    \n",
    "    # Embedding quality\n",
    "    quality = compute_alignment_uniformity(all_img_embs[:500], all_txt_embs[:500])\n",
    "    metrics.update({f\"quality_{k}\": v for k, v in quality.items()})\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n   ğŸ“Š {dataset_name.upper()} Results:\")\n",
    "    print(f\"      Textâ†’Image: R@1={metrics['T2I_R@1']:.1f}%, R@5={metrics['T2I_R@5']:.1f}%, R@10={metrics['T2I_R@10']:.1f}%\")\n",
    "    print(f\"      Imageâ†’Text: R@1={metrics['I2T_R@1']:.1f}%, R@5={metrics['I2T_R@5']:.1f}%, R@10={metrics['I2T_R@10']:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"img_embs\": all_img_embs,\n",
    "        \"txt_embs\": all_txt_embs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run COCO evaluation\n",
    "coco_results = evaluate_image_text_retrieval(\n",
    "    model,\n",
    "    dataset_name=\"coco\",\n",
    "    max_samples=cfg.coco_samples,\n",
    "    batch_size=cfg.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ESC-50 Zero-Shot Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESC-50 ZERO-SHOT CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "ESC50_CLASSES = [\n",
    "    'dog', 'rooster', 'pig', 'cow', 'frog', 'cat', 'hen', 'insects', 'sheep', 'crow',\n",
    "    'rain', 'sea_waves', 'crackling_fire', 'crickets', 'chirping_birds', \n",
    "    'water_drops', 'wind', 'pouring_water', 'toilet_flush', 'thunderstorm',\n",
    "    'crying_baby', 'sneezing', 'clapping', 'breathing', 'coughing', \n",
    "    'footsteps', 'laughing', 'brushing_teeth', 'snoring', 'drinking_sipping',\n",
    "    'door_wood_knock', 'mouse_click', 'keyboard_typing', 'door_wood_creaks', 'can_opening', \n",
    "    'washing_machine', 'vacuum_cleaner', 'clock_alarm', 'clock_tick', 'glass_breaking',\n",
    "    'helicopter', 'chainsaw', 'siren', 'car_horn', 'engine', \n",
    "    'train', 'church_bells', 'airplane', 'fireworks', 'hand_saw'\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_esc50_zeroshot(\n",
    "    model: AlignedModelForEval,\n",
    "    max_samples: int = 400,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Zero-shot audio classification on ESC-50.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”Š Evaluating ESC-50 Zero-Shot Classification...\")\n",
    "    \n",
    "    if not HAS_LIBROSA:\n",
    "        print(\"   âš ï¸ librosa not installed. Skipping.\")\n",
    "        return {\"accuracy\": 0.0, \"num_samples\": 0}\n",
    "    \n",
    "    # Create text embeddings for each class\n",
    "    prompts = [f\"The sound of {c.replace('_', ' ')}.\" for c in ESC50_CLASSES]\n",
    "    class_embs = model.embed_text(prompts)\n",
    "    class_embs = l2_normalize(class_embs)\n",
    "    \n",
    "    # Load ESC-50\n",
    "    try:\n",
    "        ds = load_dataset(\"ashraq/esc50\", split=\"train\", streaming=True)\n",
    "        ds = ds.cast_column(\"audio\", load_dataset.features.Audio(decode=False))\n",
    "    except:\n",
    "        try:\n",
    "            ds = load_dataset(\"ashraq/esc50\", split=\"train\", streaming=True).decode(False)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Could not load ESC-50: {e}\")\n",
    "            return {\"accuracy\": 0.0, \"num_samples\": 0}\n",
    "    \n",
    "    predictions, ground_truth = [], []\n",
    "    count = 0\n",
    "    \n",
    "    for ex in tqdm(ds, total=max_samples, desc=\"ESC-50\"):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Decode audio\n",
    "            audio_bytes = ex[\"audio\"][\"bytes\"]\n",
    "            wav, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
    "            category = ex[\"category\"]\n",
    "            \n",
    "            # Embed audio\n",
    "            audio_emb = model.embed_audio(wav, sr=16000)\n",
    "            audio_emb = l2_normalize(audio_emb)\n",
    "            \n",
    "            # Classify\n",
    "            sims = audio_emb @ class_embs.T\n",
    "            pred_idx = sims.argmax(dim=-1).item()\n",
    "            pred_class = ESC50_CLASSES[pred_idx]\n",
    "            \n",
    "            predictions.append(pred_class)\n",
    "            ground_truth.append(category)\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(ground_truth, predictions) * 100 if predictions else 0.0\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š ESC-50 Results:\")\n",
    "    print(f\"      Zero-Shot Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"      Samples evaluated: {len(predictions)}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"num_samples\": len(predictions),\n",
    "        \"predictions\": predictions,\n",
    "        \"ground_truth\": ground_truth,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ESC-50 evaluation\n",
    "esc50_results = evaluate_esc50_zeroshot(model, max_samples=cfg.esc50_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Matryoshka Representation Learning Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MATRYOSHKA EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_matryoshka(\n",
    "    img_embs: torch.Tensor,\n",
    "    txt_embs: torch.Tensor,\n",
    "    dims: Tuple[int, ...] = (32, 64, 128, 256, 512),\n",
    ") -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval at different embedding truncations (Matryoshka).\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“ Evaluating Matryoshka Representations...\")\n",
    "    \n",
    "    full_dim = img_embs.size(-1)\n",
    "    valid_dims = [d for d in dims if d <= full_dim]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim in valid_dims:\n",
    "        img_trunc = img_embs[:, :dim]\n",
    "        txt_trunc = txt_embs[:, :dim]\n",
    "        \n",
    "        metrics = compute_bidirectional_retrieval(txt_trunc, img_trunc, \"T\", \"I\")\n",
    "        \n",
    "        results[dim] = {\n",
    "            \"T2I_R@1\": metrics[\"T2I_R@1\"],\n",
    "            \"T2I_R@5\": metrics[\"T2I_R@5\"],\n",
    "            \"I2T_R@1\": metrics[\"I2T_R@1\"],\n",
    "        }\n",
    "        \n",
    "        print(f\"   Dim={dim:4d}: T2I R@1={metrics['T2I_R@1']:5.1f}%, I2T R@1={metrics['I2T_R@1']:5.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_matryoshka_curve(mrl_results: Dict, save_path: Optional[str] = None):\n",
    "    \"\"\"Plot MRL accuracy vs dimension curve.\"\"\"\n",
    "    dims = sorted(mrl_results.keys())\n",
    "    t2i_r1 = [mrl_results[d][\"T2I_R@1\"] for d in dims]\n",
    "    i2t_r1 = [mrl_results[d][\"I2T_R@1\"] for d in dims]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(dims, t2i_r1, 'b-o', label='Textâ†’Image R@1', linewidth=2, markersize=8)\n",
    "    ax.plot(dims, i2t_r1, 'r-s', label='Imageâ†’Text R@1', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
    "    ax.set_ylabel('Recall@1 (%)', fontsize=12)\n",
    "    ax.set_title('Matryoshka Representation Learning\\nAccuracy vs Embedding Dimension', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.set_xticks(dims)\n",
    "    ax.set_xticklabels(dims)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"   Saved: {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Matryoshka evaluation\n",
    "if coco_results:\n",
    "    mrl_results = evaluate_matryoshka(\n",
    "        coco_results[\"img_embs\"],\n",
    "        coco_results[\"txt_embs\"],\n",
    "        dims=cfg.mrl_dims,\n",
    "    )\n",
    "    \n",
    "    plot_matryoshka_curve(\n",
    "        mrl_results,\n",
    "        save_path=f\"{cfg.results_dir}/matryoshka_curve.png\" if cfg.save_results else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Against Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BASELINE COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "def create_comparison_table(\n",
    "    our_results: Dict[str, float],\n",
    "    baselines: Dict[str, Dict[str, float]],\n",
    "    metrics: List[str],\n",
    "    our_name: str = \"Ours\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create comparison DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name, model_results in baselines.items():\n",
    "        row = {\"Model\": model_name}\n",
    "        for metric in metrics:\n",
    "            row[metric] = model_results.get(metric, \"-\")\n",
    "        rows.append(row)\n",
    "    \n",
    "    our_row = {\"Model\": f\"ğŸ”¥ {our_name}\"}\n",
    "    for metric in metrics:\n",
    "        our_row[metric] = our_results.get(metric, \"-\")\n",
    "    rows.append(our_row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_comparison_chart(\n",
    "    our_results: Dict[str, float],\n",
    "    baselines: Dict[str, Dict[str, float]],\n",
    "    metric: str,\n",
    "    title: str,\n",
    "    save_path: Optional[str] = None,\n",
    "    higher_is_better: bool = True,\n",
    "):\n",
    "    \"\"\"Create bar chart comparison.\"\"\"\n",
    "    models = list(baselines.keys()) + [\"Ours\"]\n",
    "    values = [baselines[m].get(metric, 0) for m in baselines.keys()] + [our_results.get(metric, 0)]\n",
    "    \n",
    "    colors = ['steelblue'] * len(baselines) + ['coral']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(models, values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{val:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim(0, max(values) * 1.15)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add \"better\" indicator\n",
    "    direction = \"â†‘ Higher is better\" if higher_is_better else \"â†“ Lower is better\"\n",
    "    ax.text(0.02, 0.98, direction, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', style='italic', color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE COMPARISON TABLES & CHARTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š COMPARISON WITH PUBLISHED BASELINES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- COCO Retrieval ---\n",
    "if coco_results:\n",
    "    our_coco = {\n",
    "        \"T2I_R@1\": coco_results[\"metrics\"][\"T2I_R@1\"],\n",
    "        \"T2I_R@5\": coco_results[\"metrics\"][\"T2I_R@5\"],\n",
    "        \"T2I_R@10\": coco_results[\"metrics\"][\"T2I_R@10\"],\n",
    "        \"I2T_R@1\": coco_results[\"metrics\"][\"I2T_R@1\"],\n",
    "        \"I2T_R@5\": coco_results[\"metrics\"][\"I2T_R@5\"],\n",
    "        \"I2T_R@10\": coco_results[\"metrics\"][\"I2T_R@10\"],\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“· COCO Image-Text Retrieval:\")\n",
    "    coco_table = create_comparison_table(\n",
    "        our_coco,\n",
    "        BASELINES[\"coco_retrieval\"],\n",
    "        [\"T2I_R@1\", \"T2I_R@5\", \"I2T_R@1\", \"I2T_R@5\"],\n",
    "    )\n",
    "    print(coco_table.to_string(index=False))\n",
    "    \n",
    "    plot_comparison_chart(\n",
    "        our_coco,\n",
    "        BASELINES[\"coco_retrieval\"],\n",
    "        \"T2I_R@1\",\n",
    "        \"COCO Textâ†’Image Retrieval R@1 Comparison\",\n",
    "        save_path=f\"{cfg.results_dir}/coco_comparison.png\" if cfg.save_results else None,\n",
    "    )\n",
    "\n",
    "# --- ESC-50 ---\n",
    "if esc50_results and esc50_results.get(\"accuracy\", 0) > 0:\n",
    "    our_esc50 = {\"accuracy\": esc50_results[\"accuracy\"]}\n",
    "    \n",
    "    print(\"\\nğŸ”Š ESC-50 Zero-Shot Audio Classification:\")\n",
    "    esc50_table = create_comparison_table(\n",
    "        our_esc50,\n",
    "        BASELINES[\"esc50_zeroshot\"],\n",
    "        [\"accuracy\"],\n",
    "    )\n",
    "    print(esc50_table.to_string(index=False))\n",
    "    \n",
    "    plot_comparison_chart(\n",
    "        our_esc50,\n",
    "        BASELINES[\"esc50_zeroshot\"],\n",
    "        \"accuracy\",\n",
    "        \"ESC-50 Zero-Shot Accuracy Comparison\",\n",
    "        save_path=f\"{cfg.results_dir}/esc50_comparison.png\" if cfg.save_results else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def plot_similarity_heatmap(\n",
    "    img_embs: torch.Tensor,\n",
    "    txt_embs: torch.Tensor,\n",
    "    n_samples: int = 32,\n",
    "    save_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Plot similarity matrix heatmap.\"\"\"\n",
    "    img_sub = l2_normalize(img_embs[:n_samples].float())\n",
    "    txt_sub = l2_normalize(txt_embs[:n_samples].float())\n",
    "    sim_matrix = (txt_sub @ img_sub.T).cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        sim_matrix, ax=ax, cmap='RdBu_r', center=0,\n",
    "        vmin=-1, vmax=1, square=True, cbar_kws={'shrink': 0.8},\n",
    "    )\n",
    "    ax.set_xlabel('Image Index', fontsize=12)\n",
    "    ax.set_ylabel('Text Index', fontsize=12)\n",
    "    ax.set_title('Text-Image Similarity Matrix\\n(Diagonal should be highest)', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tsne_embeddings(\n",
    "    img_embs: torch.Tensor,\n",
    "    txt_embs: torch.Tensor,\n",
    "    n_samples: int = 200,\n",
    "    save_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Plot t-SNE visualization.\"\"\"\n",
    "    if not HAS_SKLEARN:\n",
    "        print(\"   sklearn not available for t-SNE\")\n",
    "        return\n",
    "    \n",
    "    n = min(n_samples, img_embs.size(0))\n",
    "    img_sub = img_embs[:n].numpy()\n",
    "    txt_sub = txt_embs[:n].numpy()\n",
    "    combined = np.vstack([img_sub, txt_sub])\n",
    "    \n",
    "    print(\"   Running t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=cfg.seed)\n",
    "    coords = tsne.fit_transform(combined)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    img_coords = coords[:n]\n",
    "    txt_coords = coords[n:]\n",
    "    \n",
    "    ax.scatter(img_coords[:, 0], img_coords[:, 1], c='coral', label='Image', alpha=0.6, s=50)\n",
    "    ax.scatter(txt_coords[:, 0], txt_coords[:, 1], c='steelblue', label='Text', alpha=0.6, s=50)\n",
    "    \n",
    "    # Connect paired samples\n",
    "    for i in range(min(30, n)):\n",
    "        ax.plot([img_coords[i, 0], txt_coords[i, 0]],\n",
    "                [img_coords[i, 1], txt_coords[i, 1]],\n",
    "                'gray', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('t-SNE Dim 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Dim 2', fontsize=12)\n",
    "    ax.set_title('t-SNE of Image & Text Embeddings\\n(Lines connect paired samples)', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if coco_results:\n",
    "    print(\"\\nğŸ“ˆ Creating Visualizations...\")\n",
    "    \n",
    "    plot_similarity_heatmap(\n",
    "        coco_results[\"img_embs\"],\n",
    "        coco_results[\"txt_embs\"],\n",
    "        n_samples=32,\n",
    "        save_path=f\"{cfg.results_dir}/similarity_heatmap.png\" if cfg.save_results else None,\n",
    "    )\n",
    "    \n",
    "    plot_tsne_embeddings(\n",
    "        coco_results[\"img_embs\"],\n",
    "        coco_results[\"txt_embs\"],\n",
    "        n_samples=200,\n",
    "        save_path=f\"{cfg.results_dir}/tsne_embeddings.png\" if cfg.save_results else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "def generate_summary(coco_results, mrl_results, esc50_results, cfg):\n",
    "    \"\"\"Generate final summary report.\"\"\"\n",
    "    summary = {\n",
    "        \"config\": {\n",
    "            \"checkpoint\": cfg.checkpoint_path,\n",
    "            \"d_align\": cfg.d_align,\n",
    "            \"mrl_dims\": cfg.mrl_dims,\n",
    "        },\n",
    "        \"results\": {},\n",
    "    }\n",
    "    \n",
    "    if coco_results:\n",
    "        summary[\"results\"][\"coco\"] = {\n",
    "            k: v for k, v in coco_results[\"metrics\"].items()\n",
    "            if not k.startswith(\"quality\")\n",
    "        }\n",
    "    \n",
    "    if mrl_results:\n",
    "        summary[\"results\"][\"matryoshka\"] = mrl_results\n",
    "    \n",
    "    if esc50_results:\n",
    "        summary[\"results\"][\"esc50\"] = {\n",
    "            \"accuracy\": esc50_results.get(\"accuracy\", 0),\n",
    "            \"num_samples\": esc50_results.get(\"num_samples\", 0),\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Generate and print summary\n",
    "summary = generate_summary(\n",
    "    coco_results if 'coco_results' in dir() else None,\n",
    "    mrl_results if 'mrl_results' in dir() else None,\n",
    "    esc50_results if 'esc50_results' in dir() else None,\n",
    "    cfg,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Pretty print\n",
    "if 'coco_results' in dir() and coco_results:\n",
    "    m = coco_results[\"metrics\"]\n",
    "    print(f\"\\nğŸ“· Image-Text Retrieval (COCO):\")\n",
    "    print(f\"   Textâ†’Image: R@1={m['T2I_R@1']:.1f}% | R@5={m['T2I_R@5']:.1f}% | R@10={m['T2I_R@10']:.1f}%\")\n",
    "    print(f\"   Imageâ†’Text: R@1={m['I2T_R@1']:.1f}% | R@5={m['I2T_R@5']:.1f}% | R@10={m['I2T_R@10']:.1f}%\")\n",
    "\n",
    "if 'mrl_results' in dir() and mrl_results:\n",
    "    print(f\"\\nğŸ“ Matryoshka (MRL):\")\n",
    "    for dim in sorted(mrl_results.keys()):\n",
    "        r = mrl_results[dim]\n",
    "        print(f\"   Dim {dim:4d}: T2I R@1={r['T2I_R@1']:5.1f}%\")\n",
    "\n",
    "if 'esc50_results' in dir() and esc50_results.get(\"accuracy\", 0) > 0:\n",
    "    print(f\"\\nğŸ”Š Audio Classification (ESC-50):\")\n",
    "    print(f\"   Zero-Shot Accuracy: {esc50_results['accuracy']:.2f}%\")\n",
    "\n",
    "# Save report\n",
    "if cfg.save_results:\n",
    "    report_path = f\"{cfg.results_dir}/evaluation_report.json\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    print(f\"\\nâœ… Report saved: {report_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Evaluation Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Benchmark Reference Table\n",
    "\n",
    "| Paper | Model | COCO T2I R@1 | COCO I2T R@1 | ESC-50 Acc | Year |\n",
    "|-------|-------|--------------|--------------|------------|------|\n",
    "| CLIP | ViT-B/32 | 30.4% | 50.1% | - | 2021 |\n",
    "| CLIP | ViT-L/14 | 36.5% | 56.3% | - | 2021 |\n",
    "| BLIP | ViT-B | 39.7% | 59.2% | - | 2022 |\n",
    "| ImageBind | ViT-H | 34.8% | 53.2% | 66.9% | 2023 |\n",
    "| CLAP | - | - | - | 82.6% | 2023 |\n",
    "| **Ours** | - | **?%** | **?%** | **?%** | 2024 |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
