{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29210eb0",
   "metadata": {},
   "source": [
    "\n",
    "# 03 · Multimodal LLM Decoder Training on VALOR (Phase 2a)\n",
    "\n",
    "This notebook implements **Notebook 3 – Aligned Model + “Normal” Decoder (standard LLM)**,\n",
    "adapted to your **VALOR multi-modal instruction-tuning data** stored in multiple parquet shards.\n",
    "\n",
    "**What this notebook does:**\n",
    "\n",
    "1. **Loads the Phase‑1 aligned model** (`VisionTextAligner`), using the best checkpoint.\n",
    "2. Wraps it with `MultimodalLLM` (vision → alignment → LLM prefix → decoder).\n",
    "3. Loads **all VALOR shards** (multi-frame, audio+caption) and groups rows by `video_id`.\n",
    "4. Builds a **clip-level dataset**:\n",
    "   - Each sample = one `video_id` with multiple `image_jpeg` frames, one `audio_wav`, one `caption`.\n",
    "5. Trains only the **vision→LLM projector** (Phase‑2a), with:\n",
    "   - **Single-frame mode** by default: randomly pick a frame per clip each step.\n",
    "   - Easy to extend later to **multi-frame pooling** / Perceiver usage.\n",
    "6. Uses **multi‑GPU** (`DataParallel` if 2 GPUs visible).\n",
    "7. Logs to **Weights & Biases (wandb)**:\n",
    "   - Training / validation loss\n",
    "   - Learning rate\n",
    "   - Qualitative generations (GT caption vs generated caption)\n",
    "8. Saves:\n",
    "   - `best_phase3_valor.pt` (by validation loss)\n",
    "   - `final_phase3_valor.pt`\n",
    "\n",
    "> ⚠️ You will need to edit **paths** to match your environment:\n",
    "> - `VALOR_SHARDS_DIR`\n",
    "> - `PHASE1_CKPT_PATH`\n",
    "> - `W&B` project / run names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc1389",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ade5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a5a7e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (3391005567.py, line 23)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom imports.core import AlignmentConfig, VisionTextAligner, set_seed, count_parameters,\u001b[39m\n                                                                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image as PILImage\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Project imports\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, set_seed, count_parameters, \n",
    "from imports.train import load_checkpoint\n",
    "from imports.llm_integration import LLMConfig, MultimodalLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e4cf0",
   "metadata": {},
   "source": [
    "## 2. Paths & High-Level Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaeb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR       : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "PHASE1_CKPT    : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "VALOR_SHARDS   : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards\n",
      "PHASE3_OUT_DIR : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase3_llm_valor/valor_qwen2p5_phase3_v1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root directory of your project (edit if needed)\n",
    "ROOT_DIR = Path.cwd()\n",
    "\n",
    "# === Phase 1 (alignment) checkpoint ===\n",
    "# This should be the *best* alignment checkpoint (Phase 1, tri-modal or vision-text only).\n",
    "PHASE1_CKPT_PATH = ROOT_DIR / \"checkpoints\" / \"phase1_multimodal\" / \"perceiver_mrl\" / \"best.pt\"  # <-- EDIT\n",
    "\n",
    "# === VALOR parquet shards (train split) ===\n",
    "# Directory containing multiple parquet shards like:\n",
    "#   valor32k_train_batch000_shard000.parquet\n",
    "VALOR_SHARDS_DIR = ROOT_DIR / \"data\" / \"alignment_subsets\" / \"valor32k_train_shards\"  # <-- EDIT\n",
    "\n",
    "assert PHASE1_CKPT_PATH.is_file(), f\"Phase-1 checkpoint not found: {PHASE1_CKPT_PATH}\"\n",
    "assert VALOR_SHARDS_DIR.is_dir(), f\"VALOR shards dir not found: {VALOR_SHARDS_DIR}\"\n",
    "\n",
    "print(\"ROOT_DIR       :\", ROOT_DIR)\n",
    "print(\"PHASE1_CKPT    :\", PHASE1_CKPT_PATH)\n",
    "print(\"VALOR_SHARDS   :\", VALOR_SHARDS_DIR)\n",
    "\n",
    "# === Phase 3 (LLM decoder alignment on VALOR) output dir ===\n",
    "PHASE3_RUN_NAME = \"valor_qwen2p5_phase3_v1\"  # change per experiment\n",
    "PHASE3_OUT_DIR = ROOT_DIR / \"checkpoints\" / \"phase3_llm_valor\" / PHASE3_RUN_NAME\n",
    "PHASE3_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"PHASE3_OUT_DIR :\", PHASE3_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256101d",
   "metadata": {},
   "source": [
    "## 3. Training Hyperparameters & Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase3ValorConfig(wandb_project='edgeglass_model_training', wandb_run_name='valor_qwen2p5_phase3_v1', wandb_entity=None, llm_model_name='Qwen/Qwen2.5-3B-Instruct', freeze_llm=True, num_prefix_tokens=8, max_clips=None, val_ratio=0.05, max_caption_len=96, frame_limit_per_clip=None, num_epochs=3, batch_size_per_gpu=2, learning_rate=5e-05, weight_decay=0.01, max_grad_norm=1.0, warmup_ratio=0.1, log_every=50, eval_every=500, num_eval_samples=8, seed=42, device='cuda', dtype='bfloat16')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class Phase3ValorConfig:\n",
    "    # Wandb\n",
    "    wandb_project: str = \"edgeglass_model_training\"\n",
    "    wandb_run_name: str = PHASE3_RUN_NAME\n",
    "    wandb_entity: Optional[str] = None  # set if you use a team account\n",
    "    \n",
    "    # LLM\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    freeze_llm: bool = True\n",
    "    num_prefix_tokens: int = 8\n",
    "    \n",
    "    # Data\n",
    "    max_clips: Optional[int] = None  # cap for debugging; None = use all\n",
    "    val_ratio: float = 0.05\n",
    "    max_caption_len: int = 96\n",
    "    frame_limit_per_clip: Optional[int] = None  # e.g. 8; None = all frames\n",
    "    \n",
    "    # Training\n",
    "    num_epochs: int = 3\n",
    "    batch_size_per_gpu: int = 2\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # Logging\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 500\n",
    "    num_eval_samples: int = 8\n",
    "    \n",
    "    # Device / precision\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: str = \"bfloat16\" if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else \"float16\"\n",
    "\n",
    "cfg = Phase3ValorConfig()\n",
    "\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3049f1",
   "metadata": {},
   "source": [
    "## 4. Device & Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6687f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, dtype: torch.bfloat16\n",
      "GPUs available: 2\n",
      "  GPU 0: NVIDIA H200\n",
      "  GPU 1: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(cfg.device)\n",
    "\n",
    "def get_dtype() -> torch.dtype:\n",
    "    if cfg.dtype == \"bfloat16\":\n",
    "        return torch.bfloat16\n",
    "    if cfg.dtype == \"float16\":\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = get_device()\n",
    "dtype = get_dtype()\n",
    "\n",
    "print(f\"Using device: {device}, dtype: {dtype}\")\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.device_count() > 0:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3372a",
   "metadata": {},
   "source": [
    "## 5. Load Phase‑1 Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "[TextEncoder] Loaded sentence-transformers/all-MiniLM-L6-v2, hidden_size=384\n",
      "[VisionTextAligner] d_vision=768, d_text=384, d_align=512\n",
      "Loading Phase‑1 checkpoint from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_checkpoint() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m aligner = VisionTextAligner(phase1_cfg).to(device)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Phase‑1 checkpoint from:\u001b[39m\u001b[33m\"\u001b[39m, PHASE1_CKPT_PATH)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m _ = \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPHASE1_CKPT_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Freeze aligner in Phase 2a\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m aligner.parameters():\n",
      "\u001b[31mTypeError\u001b[39m: load_checkpoint() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: This assumes a VisionTextAligner-based Phase 1.\n",
    "# If you used a Perceiver-based tri-modal aligner, you can adapt this cell to\n",
    "# import and instantiate `MultimodalAlignmentModel` instead.\n",
    "\n",
    "phase1_cfg = MultimodalAlignmentModel()\n",
    "phase1_cfg.device = device\n",
    "phase1_cfg.dtype = dtype\n",
    "\n",
    "aligner = VisionTextAligner(phase1_cfg).to(device)\n",
    "print(\"Loading Phase‑1 checkpoint from:\", PHASE1_CKPT_PATH)\n",
    "_ = load_checkpoint(\n",
    "    model=aligner,\n",
    "    checkpoint_path=str(PHASE1_CKPT_PATH),\n",
    "    load_optimizer=False,\n",
    "    optimizer=None,\n",
    "    device=cfg.device,\n",
    ")\n",
    "\n",
    "# Freeze aligner in Phase 2a\n",
    "for p in aligner.parameters():\n",
    "    p.requires_grad = False\n",
    "aligner.eval()\n",
    "\n",
    "print(\"Aligner trainable params:\", count_parameters(aligner, trainable_only=True))\n",
    "print(\"Aligner total params    :\", count_parameters(aligner))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d687c56",
   "metadata": {},
   "source": [
    "## 6. Build Multimodal LLM (Aligned Model + Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecda45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_cfg = LLMConfig(\n",
    "    model_name=cfg.llm_model_name,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_prefix_tokens=cfg.num_prefix_tokens,\n",
    "    freeze_llm=cfg.freeze_llm,\n",
    ")\n",
    "\n",
    "multimodal_model = MultimodalLLM(\n",
    "    aligner=aligner,\n",
    "    llm_config=llm_cfg,\n",
    ")\n",
    "\n",
    "multimodal_model.to(device)\n",
    "multimodal_model = multimodal_model.to(dtype=dtype)\n",
    "\n",
    "# Wrap with DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Wrapping multimodal model in DataParallel over {torch.cuda.device_count()} GPUs\")\n",
    "    multimodal_model = nn.DataParallel(multimodal_model)\n",
    "\n",
    "mm_module = multimodal_model.module if isinstance(multimodal_model, nn.DataParallel) else multimodal_model\n",
    "\n",
    "print(\"Multimodal model ready.\")\n",
    "print(\"Trainable params (Phase‑2a projector):\", sum(p.numel() for p in mm_module.get_trainable_params()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a60f89",
   "metadata": {},
   "source": [
    "## 7. Load VALOR Parquet Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shard_paths = sorted(VALOR_SHARDS_DIR.glob(\"*.parquet\"))\n",
    "assert shard_paths, f\"No parquet shards found in {VALOR_SHARDS_DIR}\"\n",
    "\n",
    "print(\"Found\", len(shard_paths), \"shards:\")\n",
    "for p in shard_paths[:5]:\n",
    "    print(\" -\", p.name)\n",
    "if len(shard_paths) > 5:\n",
    "    print(\" ...\")\n",
    "\n",
    "dfs = []\n",
    "for p in shard_paths:\n",
    "    print(\"Loading shard:\", p)\n",
    "    df_shard = pd.read_parquet(p)\n",
    "    dfs.append(df_shard)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total VALOR rows (frames):\", len(df_all))\n",
    "print(\"Columns:\", list(df_all.columns))\n",
    "\n",
    "# Sanity: expected columns from 00_inspect_valor.ipynb\n",
    "for col in [\"video_id\", \"caption\", \"image_jpeg\", \"audio_wav\"]:\n",
    "    assert col in df_all.columns, f\"Missing column in VALOR data: {col}\"\n",
    "\n",
    "# Optional cap for debugging\n",
    "if cfg.max_clips is not None:\n",
    "    # we'll apply it after grouping by video_id\n",
    "    print(\"max_clips specified; will cap after grouping by video_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69377ccb",
   "metadata": {},
   "source": [
    "## 8. Clip-Level Dataset (Multi-Frame VALOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069aca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ValorMultiFrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Clip-level dataset for VALOR multi-frame, audio+caption data.\n",
    "    \n",
    "    Each row in df_all is a single frame from a video (`video_id`).\n",
    "    We group by `video_id` so that each sample is:\n",
    "      - video_id\n",
    "      - list[image_jpeg] bytes (frames)\n",
    "      - audio_wav bytes (first occurrence)\n",
    "      - caption (first occurrence)\n",
    "    \n",
    "    For Phase‑3 (LLM decoder alignment), we'll use only image+caption by default,\n",
    "    but audio is kept for potential future use.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_all: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length: int = 96,\n",
    "        frame_limit_per_clip: Optional[int] = None,\n",
    "        max_clips: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.frame_limit_per_clip = frame_limit_per_clip\n",
    "\n",
    "        grouped = df_all.groupby(\"video_id\")\n",
    "        video_ids = list(grouped.groups.keys())\n",
    "        random.shuffle(video_ids)\n",
    "\n",
    "        if max_clips is not None:\n",
    "            video_ids = video_ids[:max_clips]\n",
    "\n",
    "        self.clips = []\n",
    "        for vid in tqdm(video_ids, desc=\"Building clip index\"):\n",
    "            group = grouped.get_group(vid)\n",
    "            frames = list(group[\"image_jpeg\"])\n",
    "            audio = group[\"audio_wav\"].iloc[0]\n",
    "            caption = group[\"caption\"].iloc[0]\n",
    "\n",
    "            if frame_limit_per_clip is not None:\n",
    "                frames = frames[:frame_limit_per_clip]\n",
    "\n",
    "            self.clips.append(\n",
    "                {\n",
    "                    \"video_id\": vid,\n",
    "                    \"frames\": frames,   # list[bytes]\n",
    "                    \"audio_wav\": audio,\n",
    "                    \"caption\": str(caption),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Pre-tokenize captions for efficiency\n",
    "        self._tok = []\n",
    "        for clip in tqdm(self.clips, desc=\"Tokenizing captions\"):\n",
    "            text = clip[\"caption\"]\n",
    "            toks = tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = toks[\"input_ids\"][0]\n",
    "            attn = toks[\"attention_mask\"][0]\n",
    "            labels = input_ids.clone()\n",
    "            labels[attn == 0] = -100\n",
    "            self._tok.append(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attn,\n",
    "                    \"labels\": labels,\n",
    "                    \"text\": text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(\"Total clips (videos):\", len(self.clips))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clips)\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_image(jpeg_bytes: bytes) -> PILImage.Image:\n",
    "        return PILImage.open(io.BytesIO(jpeg_bytes)).convert(\"RGB\")\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        clip = self.clips[idx]\n",
    "        toks = self._tok[idx]\n",
    "\n",
    "        frames = [self._decode_image(b) for b in clip[\"frames\"]]\n",
    "\n",
    "        return {\n",
    "            \"video_id\": clip[\"video_id\"],\n",
    "            \"frames\": frames,  # list[PIL.Image]\n",
    "            \"audio_wav\": clip[\"audio_wav\"],\n",
    "            \"input_ids\": toks[\"input_ids\"],\n",
    "            \"attention_mask\": toks[\"attention_mask\"],\n",
    "            \"labels\": toks[\"labels\"],\n",
    "            \"caption\": toks[\"text\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def valor_multiframe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"video_ids\": [b[\"video_id\"] for b in batch],\n",
    "        \"frames\": [b[\"frames\"] for b in batch],  # list[list[Image]]\n",
    "        \"audio_wav\": [b[\"audio_wav\"] for b in batch],\n",
    "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch], dim=0),\n",
    "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch], dim=0),\n",
    "        \"labels\": torch.stack([b[\"labels\"] for b in batch], dim=0),\n",
    "        \"captions\": [b[\"caption\"] for b in batch],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aa2ae",
   "metadata": {},
   "source": [
    "## 9. Build Train / Val Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = mm_module.llm.tokenizer\n",
    "\n",
    "# Build a clip-level dataset from all rows\n",
    "full_dataset = ValorMultiFrameDataset(\n",
    "    df_all=df_all,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.max_caption_len,\n",
    "    frame_limit_per_clip=cfg.frame_limit_per_clip,\n",
    "    max_clips=cfg.max_clips,\n",
    ")\n",
    "\n",
    "num_clips = len(full_dataset)\n",
    "num_val = max(1, int(num_clips * cfg.val_ratio))\n",
    "num_train = num_clips - num_val\n",
    "print(f\"Total clips: {num_clips} -> train={num_train}, val={num_val}\")\n",
    "\n",
    "# Simple split by index (clips already shuffled)\n",
    "train_indices = list(range(num_train))\n",
    "val_indices = list(range(num_train, num_clips))\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "global_batch_size = cfg.batch_size_per_gpu * max(1, torch.cuda.device_count())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=global_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=valor_multiframe_collate,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=global_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=valor_multiframe_collate,\n",
    ")\n",
    "\n",
    "print(f\"Batches per epoch: train={len(train_loader)}, val={len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970db64",
   "metadata": {},
   "source": [
    "## 10. Optimizer & LR Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_params = list(mm_module.get_trainable_params())\n",
    "print(\"Trainable parameter count:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * cfg.num_epochs\n",
    "warmup_steps = int(num_training_steps * cfg.warmup_ratio)\n",
    "print(f\"Total training steps: {num_training_steps}, warmup steps: {warmup_steps}\")\n",
    "\n",
    "# We'll use a simple linear warmup + cosine decay implemented manually\n",
    "def get_lr(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return cfg.learning_rate * (step + 1) / max(1, warmup_steps)\n",
    "    progress = (step - warmup_steps) / max(1, num_training_steps - warmup_steps)\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    # Cosine from LR -> 0\n",
    "    return cfg.learning_rate * 0.5 * (1.0 + math.cos(math.pi * progress))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b9cfb",
   "metadata": {},
   "source": [
    "## 11. Weights & Biases Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=cfg.wandb_project,\n",
    "    name=cfg.wandb_run_name,\n",
    "    entity=cfg.wandb_entity,\n",
    "    config=asdict(cfg),\n",
    ")\n",
    "\n",
    "wandb.watch(mm_module.projector, log=\"all\", log_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f33b0",
   "metadata": {},
   "source": [
    "## 12. Training & Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_single_frame_per_clip(frames_batch: List[List[PILImage.Image]]) -> List[PILImage.Image]:\n",
    "    \"\"\"\n",
    "    Given batch of clips (each clip is a list of frames),\n",
    "    randomly pick ONE frame from each clip.\n",
    "    \"\"\"\n",
    "    picked = []\n",
    "    for frames in frames_batch:\n",
    "        if len(frames) == 0:\n",
    "            raise RuntimeError(\"Clip has zero frames; check VALOR data.\")\n",
    "        idx = random.randint(0, len(frames) - 1)\n",
    "        picked.append(frames[idx])\n",
    "    return picked\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(step: int) -> Dict[str, float]:\n",
    "    mm_module.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    all_samples = []\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=f\"Eval @ step {step}\", leave=False):\n",
    "        # single-frame mode\n",
    "        images = sample_single_frame_per_clip(batch[\"frames\"])\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = mm_module(\n",
    "            images=images,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        # qualitative samples\n",
    "        if len(all_samples) < cfg.num_eval_samples:\n",
    "            for img, cap in zip(images, batch[\"captions\"]):\n",
    "                if len(all_samples) >= cfg.num_eval_samples:\n",
    "                    break\n",
    "                try:\n",
    "                    gen = mm_module.generate(\n",
    "                        images=img,\n",
    "                        prompt=\"Describe this video frame in one or two sentences:\",\n",
    "                        max_new_tokens=64,\n",
    "                        temperature=0.7,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    gen = f\"[GEN_ERROR: {e}]\"\n",
    "                all_samples.append(\n",
    "                    {\n",
    "                        \"image\": wandb.Image(img),\n",
    "                        \"gt_caption\": cap,\n",
    "                        \"gen_caption\": gen,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    metrics = {\"val_loss\": avg_loss}\n",
    "\n",
    "    if all_samples:\n",
    "        table = wandb.Table(columns=[\"image\", \"gt_caption\", \"gen_caption\"])\n",
    "        for s in all_samples:\n",
    "            table.add_data(s[\"image\"], s[\"gt_caption\"], s[\"gen_caption\"])\n",
    "        wandb.log({\"eval_samples\": table, \"global_step\": step})\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9193e",
   "metadata": {},
   "source": [
    "## 13. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62844043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_step = 0\n",
    "best_val_loss = math.inf\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{cfg.num_epochs} =====\")\n",
    "    mm_module.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        images = sample_single_frame_per_clip(batch[\"frames\"])\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # update LR\n",
    "        lr = get_lr(global_step)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        outputs = mm_module(\n",
    "            images=images,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, cfg.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        step_loss = loss.item()\n",
    "        epoch_loss += step_loss\n",
    "        n_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % cfg.log_every == 0:\n",
    "            avg_loss = epoch_loss / max(1, n_batches)\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": step_loss,\n",
    "                    \"train_loss_avg\": avg_loss,\n",
    "                    \"lr\": lr,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"global_step\": global_step,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if global_step % cfg.eval_every == 0:\n",
    "            print(f\"\\n>>> Running evaluation at step {global_step} ...\")\n",
    "            val_metrics = evaluate_epoch(global_step)\n",
    "            val_loss = val_metrics[\"val_loss\"]\n",
    "            wandb.log({**val_metrics, \"global_step\": global_step})\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_path = PHASE3_OUT_DIR / \"best_phase3_valor.pt\"\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"step\": global_step,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": mm_module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"cfg\": asdict(cfg),\n",
    "                    },\n",
    "                    best_path,\n",
    "                )\n",
    "                print(f\"[BEST] Saved new best checkpoint to {best_path} (val_loss={val_loss:.4f})\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / max(1, n_batches)\n",
    "    history.append({\"epoch\": epoch + 1, \"avg_train_loss\": avg_epoch_loss})\n",
    "    print(f\"Epoch {epoch+1} complete | avg_train_loss={avg_epoch_loss:.4f}\")\n",
    "    wandb.log({\"epoch_train_loss\": avg_epoch_loss, \"epoch\": epoch + 1, \"global_step\": global_step})\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n>>> Final evaluation after training ...\")\n",
    "final_metrics = evaluate_epoch(global_step)\n",
    "wandb.log({**final_metrics, \"global_step\": global_step})\n",
    "\n",
    "final_path = PHASE3_OUT_DIR / \"final_phase3_valor.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"step\": global_step,\n",
    "        \"epoch\": cfg.num_epochs,\n",
    "        \"model_state_dict\": mm_module.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"cfg\": asdict(cfg),\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "print(f\"Saved final Phase‑3 checkpoint to {final_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc10167",
   "metadata": {},
   "source": [
    "## 14. Save History & Quick Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save training history\n",
    "history_path = PHASE3_OUT_DIR / \"train_history_phase3_valor.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "\n",
    "if wandb_run is not None:\n",
    "    wandb_run.finish()\n",
    "    print(\"Closed W&B run.\")\n",
    "\n",
    "# Quick helper for manual inspection in the notebook\n",
    "def debug_generate_for_clip(idx: int = 0, prompt: str = \"Describe this video frame:\") -> Dict[str, Any]:\n",
    "    mm_module.eval()\n",
    "    clip = full_dataset.clips[idx]\n",
    "    frames = [ValorMultiFrameDataset._decode_image(b) for b in clip[\"frames\"]]\n",
    "    img = random.choice(frames)\n",
    "\n",
    "    gen = mm_module.generate(\n",
    "        images=img,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=96,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return {\n",
    "        \"video_id\": clip[\"video_id\"],\n",
    "        \"image\": img,\n",
    "        \"caption_gt\": clip[\"caption\"],\n",
    "        \"caption_gen\": gen,\n",
    "    }\n",
    "\n",
    "print(\"You can now call `debug_generate_for_clip(0)` in the notebook to inspect a sample.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
