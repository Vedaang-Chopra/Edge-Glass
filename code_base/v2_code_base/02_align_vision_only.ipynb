{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ea2fd3",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Vision-Text Alignment (Vision Only) + Alignment Plots (W&B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6082a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9226126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Basic imports & environment setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Local modules\n",
    "from imports.in_memory_datasets import *\n",
    "from imports.multimodal_alignment_perceiver import (\n",
    "    MultimodalAlignmentConfig,\n",
    "    MultimodalAlignmentModel,\n",
    "    contrastive_loss,\n",
    "    matryoshka_loss,\n",
    ")\n",
    "from imports.core import set_seed\n",
    "\n",
    "# Transformers encoders\n",
    "from transformers import (\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperModel,\n",
    "    WhisperProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ac2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Num GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Checkpoint root: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal_vision\n",
      "MLP run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal_vision/mlp_mrl\n",
      "Perceiver run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal_vision/perceiver_mrl\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # General\n",
    "    seed: int = 42\n",
    "    num_epochs_mlp: int = 3          # was 3\n",
    "    num_epochs_perceiver: int = 5   # was 5\n",
    "    log_every: int = 25\n",
    "\n",
    "    # Data\n",
    "    pixmo_parquet_dir: Path = Path.cwd() / \"data\" / \"final_dataset\" / \"pixmo\"\n",
    "    clotho_parquet_dir: Path = Path.cwd() / \"data\" / \"final_dataset\" / \"clotho\"\n",
    "    pixmo_parquet_glob: str = \"pixmo_train.parquet\"\n",
    "    clotho_parquet_glob: str = \"clotho_train.parquet\"\n",
    "\n",
    "    image_hf_dataset: str = \"allenai/pixmo-cap\"\n",
    "    audio_hf_dataset: str = \"clotho_v2\"\n",
    "\n",
    "    max_image_samples: Optional[int] = None   # keep None for now; if debugging, use e.g. 20_000\n",
    "    max_audio_samples: Optional[int] = None\n",
    "    image_size: Tuple[int, int] = (224, 224)\n",
    "    sample_val_fraction: float = 0.10   # was 0.05 ‚Üí more stable validation\n",
    "\n",
    "    # Training\n",
    "    base_batch_size: int = 64           # was 32; scale by num_gpus anyway\n",
    "    learning_rate: float = 2e-4         # was 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # WandB\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"edgeglass_final_align\"\n",
    "    wandb_entity: Optional[str] = None\n",
    "    wandb_mode: str = \"online\"\n",
    "    wandb_run_name: Optional[str] = \"final_align_perciever_1\"\n",
    "\n",
    "    # Checkpoints (Phase 1)\n",
    "    root_dir: Path = Path(\".\").resolve()\n",
    "    ckpt_root: str = \"checkpoints/phase1_multimodal_vision\"\n",
    "\n",
    "    # Alignment / model\n",
    "    mrl_weight: float = 1.0\n",
    "    clip_weight: float = 0.25          # was 0.5\n",
    "\n",
    "cfg = ExperimentConfig()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "ROOT_DIR = cfg.root_dir\n",
    "CKPT_ROOT = ROOT_DIR / cfg.ckpt_root\n",
    "MLP_DIR = CKPT_ROOT / \"mlp_mrl\"\n",
    "PERCEIVER_DIR = CKPT_ROOT / \"perceiver_mrl\"\n",
    "\n",
    "for d in [CKPT_ROOT, MLP_DIR, PERCEIVER_DIR, cfg.pixmo_parquet_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"Checkpoint root:\", CKPT_ROOT)\n",
    "print(\"MLP run dir:\", MLP_DIR)\n",
    "print(\"Perceiver run dir:\", PERCEIVER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aba699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases helpers\n",
    "\n",
    "def init_wandb(run_name: str, variant: str, extra_config: Dict[str, Any] = None):\n",
    "    if not cfg.use_wandb:\n",
    "        return None\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    base_config = asdict(cfg)\n",
    "    base_config[\"variant\"] = variant\n",
    "    if extra_config:\n",
    "        base_config.update(extra_config)\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=cfg.wandb_project,\n",
    "        entity=cfg.wandb_entity,\n",
    "        name=run_name,\n",
    "        mode=cfg.wandb_mode,\n",
    "        config=base_config,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "def log_alignment_histograms(\n",
    "    run,\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    "    prefix: str,\n",
    "    max_points: int = 512,\n",
    "):\n",
    "    \"\"\"Log positive vs negative cosine similarity histograms to W&B.\n",
    "\n",
    "    This is a lightweight way to 'see' alignment progress:\n",
    "    - Positive sims: cosine between matching pairs\n",
    "    - Negative sims: cosine between random mismatched pairs\n",
    "    \"\"\"\n",
    "    if run is None:\n",
    "        return\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_a = F.normalize(z_a, dim=-1)\n",
    "        z_b = F.normalize(z_b, dim=-1)\n",
    "\n",
    "        # Sample subset to keep logging light\n",
    "        n = min(z_a.size(0), max_points)\n",
    "        idx = torch.randperm(z_a.size(0))[:n]\n",
    "        za = z_a[idx]\n",
    "        zb = z_b[idx]\n",
    "\n",
    "        # Positive similarities\n",
    "        pos_sims = (za * zb).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "        # Negative similarities (shuffle)\n",
    "        shuffle_idx = torch.randperm(n)\n",
    "        neg_sims = (za * zb[shuffle_idx]).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "        run.log({\n",
    "            f\"{prefix}/pos_sim\": wandb.Histogram(pos_sims),\n",
    "            f\"{prefix}/neg_sim\": wandb.Histogram(neg_sims),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4449e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: load PixMo-Cap and MusicCaps from local Parquet (or HF fallback)\n",
    "\n",
    "import glob\n",
    "\n",
    "def get_pixmo_dataset():\n",
    "    \"\"\"Load PixMo-Cap 'train' split from Parquet if available, otherwise from HF.\"\"\"\n",
    "    parquet_pattern = cfg.pixmo_parquet_dir / cfg.pixmo_parquet_glob\n",
    "    matches = sorted(glob.glob(str(parquet_pattern)))\n",
    "    if matches:\n",
    "        print(f\"Loading PixMo-Cap from Parquet: {matches[-1]}\")\n",
    "        ds_dict = load_dataset(\"parquet\", data_files={\"train\": matches[-1]})\n",
    "        ds = ds_dict[\"train\"]\n",
    "    else:\n",
    "        print(f\"No PixMo Parquet found at pattern {parquet_pattern}, loading from HF: {cfg.image_hf_dataset}\")\n",
    "        ds = load_dataset(cfg.image_hf_dataset, split=\"train\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01120de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Image-Text Datasets (PixMo-Cap) ===\n",
      "Loading PixMo-Cap from Parquet: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/final_dataset/pixmo/pixmo_train.parquet\n",
      "\n",
      "üì• Pre-loading 14000 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 32 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/14000 [00:00<?, ?it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (161569818 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:   2%|‚ñè         | 328/14000 [01:07<33:17,  6.84it/s]  /home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (130382142 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "Loading images:  18%|‚ñà‚ñä        | 2508/14000 [01:34<04:56, 38.71it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  18%|‚ñà‚ñä        | 2508/14000 [01:45<04:56, 38.71it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (100000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6650/14000 [02:08<01:16, 95.61it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6650/14000 [02:19<01:16, 95.61it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8285/14000 [02:39<01:15, 75.92it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8612/14000 [02:48<01:24, 63.93it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10029/14000 [02:52<00:36, 109.98it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10683/14000 [02:59<00:31, 104.07it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14000/14000 [03:44<00:00, 62.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 14000 images into memory\n",
      "   ‚ö†Ô∏è  99 images failed to load (using fallback)\n",
      "Image train size: 12600 | val size: 1400 | batch size: 64\n"
     ]
    }
   ],
   "source": [
    "def build_image_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create train/val DataLoaders for image-text data using PixMo-Cap.\"\"\"\n",
    "    print(\"\\n=== Building Image-Text Datasets (PixMo-Cap) ===\")\n",
    "    hf_ds = get_pixmo_dataset()\n",
    "\n",
    "    if cfg.max_image_samples is not None:\n",
    "        hf_ds = hf_ds.select(range(min(cfg.max_image_samples, len(hf_ds))))\n",
    "\n",
    "    # Expect PixMo-Cap columns: image_url + caption\n",
    "    colnames = hf_ds.column_names\n",
    "    if \"image_url\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'image_url' column in PixMo dataset, found: {colnames}\")\n",
    "    if \"caption\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'caption' column in PixMo dataset, found: {colnames}\")\n",
    "\n",
    "    ds = InMemoryImageTextDataset(\n",
    "        hf_dataset=hf_ds,\n",
    "        img_col=\"image_url\",\n",
    "        txt_col=\"caption\",\n",
    "        max_samples=None,  # already limited above\n",
    "        image_size=cfg.image_size,\n",
    "    )\n",
    "\n",
    "    # Train/val split via indices\n",
    "    n = len(ds)\n",
    "    idx = np.random.permutation(n)\n",
    "    split = int(n * (1.0 - cfg.sample_val_fraction))\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "    train_ds = Subset(ds, train_idx)\n",
    "    val_ds = Subset(ds, val_idx)\n",
    "\n",
    "    # Scale batch size with number of GPUs\n",
    "    num_gpus = max(1, torch.cuda.device_count())\n",
    "    batch_size = cfg.base_batch_size * num_gpus\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Image train size: {len(train_ds)} | val size: {len(val_ds)} | batch size: {batch_size}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "image_train_loader, image_val_loader = build_image_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56803f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# cache resamplers per original sr\n",
    "_resamplers: Dict[int, torchaudio.transforms.Resample] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14224b",
   "metadata": {},
   "source": [
    "## Loading Model Architectures for Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2e053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalAlignmentConfig(vision_model_name='openai/clip-vit-base-patch32', audio_model_name='openai/whisper-base', text_model_name='sentence-transformers/all-MiniLM-L6-v2', llm_model_name='Qwen/Qwen2.5-1.5B-Instruct', d_vision=768, d_audio=512, d_text=384, perceiver_dim=512, num_latents=64, num_perceiver_layers=4, num_attn_heads=8, perceiver_mlp_ratio=4.0, perceiver_dropout=0.1, d_align=512, mrl_dims=(64, 128, 256, 512), llm_hidden_size=1536, num_prefix_tokens=64, batch_size=32, learning_rate=0.0001, weight_decay=0.01, num_epochs=10, warmup_ratio=0.1, max_grad_norm=1.0, temperature=0.07, mrl_weight=1.0, clip_weight=0.25, seed=42, device='cuda', dtype='float32')\n"
     ]
    }
   ],
   "source": [
    "# Frozen encoders: CLIP (vision), Whisper (audio), MiniLM (text)\n",
    "\n",
    "# Multimodal alignment config (from multimodal_alignment_perceiver.py)\n",
    "\n",
    "mm_cfg = MultimodalAlignmentConfig()\n",
    "mm_cfg.device = str(device)\n",
    "mm_cfg.mrl_weight = cfg.mrl_weight\n",
    "mm_cfg.clip_weight = cfg.clip_weight\n",
    "\n",
    "print(mm_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07bbc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIP vision encoder: openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "# Vision encoder (CLIP)\n",
    "vision_model_name = mm_cfg.vision_model_name\n",
    "print(\"\\nLoading CLIP vision encoder:\", vision_model_name)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n",
    "clip_vision = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "clip_vision.to(device)\n",
    "clip_vision.eval()\n",
    "for p in clip_vision.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e9b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading audio encoder: openai/whisper-base\n"
     ]
    }
   ],
   "source": [
    "# Text encoder (MiniLM)\n",
    "text_model_name = mm_cfg.text_model_name\n",
    "print(\"Loading text encoder:\", text_model_name)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Audio encoder (Whisper)\n",
    "audio_model_name = mm_cfg.audio_model_name\n",
    "print(\"Loading audio encoder:\", audio_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(audio_model_name)\n",
    "whisper_encoder = WhisperModel.from_pretrained(audio_model_name).get_encoder()\n",
    "whisper_encoder.to(device)\n",
    "whisper_encoder.eval()\n",
    "for p in whisper_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772e0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_images_to_features(images: List) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of PIL images to CLIP patch features (B, T, 768).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = clip_vision(**inputs)\n",
    "        feats = outputs.last_hidden_state  # (B, T, 768)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def encode_texts_to_features(texts: List[str]) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of texts to token features (B, L, 384).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tokens = text_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        outputs = text_encoder(**tokens)\n",
    "        feats = outputs.last_hidden_state  # (B, L, 384)\n",
    "    return feats\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def encode_audio_to_features(audio_batch: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode a batch of padded 16 kHz audio waveforms using Whisper.\n",
    "\n",
    "    Args:\n",
    "        audio_batch: Tensor of shape (B, T_max_16k)\n",
    "        sr: sampling rate (should be 16000 after collate)\n",
    "\n",
    "    Returns:\n",
    "        feats: Tensor of shape (B, T_feat_max, 512)\n",
    "    \"\"\"\n",
    "    assert isinstance(audio_batch, torch.Tensor), \"audio_batch must be Tensor[B, T]\"\n",
    "    if sr != 16000:\n",
    "        # In case something upstream goes wrong, fail loudly\n",
    "        raise ValueError(f\"Expected 16 kHz audio for Whisper, got sr={sr}\")\n",
    "\n",
    "    B, T_max = audio_batch.shape\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(B):\n",
    "            # 1) Take single waveform [T]\n",
    "            wav = audio_batch[i].detach().cpu().float().numpy()  # (T,)\n",
    "\n",
    "            # 2) Run WhisperProcessor on this single example at 16k\n",
    "            inputs = whisper_processor(\n",
    "                wav,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            # 3) Encode with Whisper encoder\n",
    "            out = whisper_encoder(inputs.input_features)          # (1, T_feat_i, 512)\n",
    "            feat_i = out.last_hidden_state.squeeze(0)            # (T_feat_i, 512)\n",
    "            features.append(feat_i)\n",
    "\n",
    "    # 4) Pad along time dimension to get (B, T_feat_max, 512)\n",
    "    max_T_feat = max(f.shape[0] for f in features)\n",
    "    hidden_dim = features[0].shape[1]\n",
    "\n",
    "    feats_batch = torch.zeros(B, max_T_feat, hidden_dim, device=device)\n",
    "    for i, f in enumerate(features):\n",
    "        feats_batch[i, : f.shape[0]] = f\n",
    "\n",
    "    return feats_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "317a4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalAlignmentModel created.\n"
     ]
    }
   ],
   "source": [
    "# Multimodal alignment model (Perceiver backbone)\n",
    "\n",
    "model = MultimodalAlignmentModel(mm_cfg).to(device)\n",
    "print(\"MultimodalAlignmentModel created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e97fc206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 21,621,760\n"
     ]
    }
   ],
   "source": [
    "# Multi-GPU support\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Wrapping model in DataParallel for {num_gpus} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "def get_model_module(m: nn.Module) -> nn.Module:\n",
    "    \"\"\"Return underlying module if wrapped in DataParallel.\"\"\"\n",
    "    return m.module if isinstance(m, nn.DataParallel) else m\n",
    "\n",
    "\n",
    "def count_trainable_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Trainable parameters:\", f\"{count_trainable_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5079cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint utilities\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    best_metric: float,\n",
    "    out_dir: Path,\n",
    "    tag: str,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_metric\": best_metric,\n",
    "        \"model_state\": get_model_module(model).state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"mm_config\": mm_cfg.__dict__,\n",
    "        \"exp_config\": asdict(cfg),\n",
    "    }\n",
    "    ckpt_path = out_dir / f\"{tag}.pt\"\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(f\"Saved checkpoint to: {ckpt_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    ckpt_path: Path,\n",
    "    strict: bool = True,\n",
    "):\n",
    "    state = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    get_model_module(model).load_state_dict(state[\"model_state\"], strict=strict)\n",
    "    if optimizer is not None and \"optimizer_state\" in state:\n",
    "        optimizer.load_state_dict(state[\"optimizer_state\"])\n",
    "    print(f\"Loaded checkpoint from epoch {state.get('epoch', 'N/A')} at {ckpt_path}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e39590ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_2239409/2129190948.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=cfg.use_amp)\n"
     ]
    }
   ],
   "source": [
    "# Training helpers ‚Äì MLP-only and Perceiver variants\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "\n",
    "def encode_modality(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    feats: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor],\n",
    "    modality: str,\n",
    "    use_perceiver: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encode features into aligned space.\n",
    "\n",
    "    If use_perceiver=True, uses full Perceiver pipeline.\n",
    "    If False, bypasses Perceiver and uses adapters + alignment projector directly.\n",
    "    \"\"\"\n",
    "    m = get_model_module(model)\n",
    "\n",
    "    if modality == \"vision\":\n",
    "        adapter = m.vision_adapter\n",
    "    elif modality == \"audio\":\n",
    "        adapter = m.audio_adapter\n",
    "    elif modality == \"text\":\n",
    "        adapter = m.text_adapter\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    tokens = adapter(feats)  # (B, T, perceiver_dim)\n",
    "\n",
    "    if use_perceiver:\n",
    "        latents = m.perceiver(tokens, mask)  # (B, K, perceiver_dim)\n",
    "    else:\n",
    "        # MLP-only: treat tokens as latents and pool across sequence\n",
    "        latents = tokens  # (B, T, perceiver_dim)\n",
    "\n",
    "    z = m.alignment_projector(latents)  # (B, d_align)\n",
    "    return z\n",
    "\n",
    "\n",
    "def compute_alignment_losses(\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Compute MRL + CLIP contrastive losses between two modalities.\"\"\"\n",
    "    loss_mrl = matryoshka_loss(\n",
    "        z_a,\n",
    "        z_b,\n",
    "        dims=mm_cfg.mrl_dims,\n",
    "        temperature=mm_cfg.temperature,\n",
    "    )\n",
    "    loss_clip = contrastive_loss(\n",
    "        z_a,\n",
    "        z_b,\n",
    "        temperature=mm_cfg.temperature,\n",
    "    )\n",
    "    loss_total = cfg.mrl_weight * loss_mrl + cfg.clip_weight * loss_clip\n",
    "    return {\n",
    "        \"loss_total\": loss_total,\n",
    "        \"loss_mrl\": loss_mrl,\n",
    "        \"loss_clip\": loss_clip,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2098a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_alignment(\n",
    "    model: nn.Module,\n",
    "    image_val_loader: DataLoader,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 20,\n",
    "    run=None,\n",
    "    prefix: str = \"val\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate *vision‚Äìtext only* alignment.\n",
    "\n",
    "    Computes VT embeddings, logs histograms to W&B, and returns summary stats.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_z_v = []\n",
    "    all_z_t_img = []\n",
    "\n",
    "    for b_idx, batch in enumerate(image_val_loader):\n",
    "        if b_idx >= max_batches:\n",
    "            break\n",
    "        images = batch[\"images\"]\n",
    "        texts = batch[\"captions\"]\n",
    "\n",
    "        vision_feats = encode_images_to_features(images)\n",
    "        text_feats = encode_texts_to_features(texts)\n",
    "\n",
    "        z_v = encode_modality(model, vision_feats, None, \"vision\", use_perceiver)\n",
    "        z_t = encode_modality(model, text_feats, None, \"text\", use_perceiver)\n",
    "\n",
    "        all_z_v.append(z_v.cpu())\n",
    "        all_z_t_img.append(z_t.cpu())\n",
    "\n",
    "    if not all_z_v:\n",
    "        return {}\n",
    "\n",
    "    z_v_all = torch.cat(all_z_v, dim=0)\n",
    "    z_t_all = torch.cat(all_z_t_img, dim=0)\n",
    "\n",
    "    # Log histograms for VT\n",
    "    log_alignment_histograms(run, z_v_all.to(device), z_t_all.to(device), f\"{prefix}/vision_text\")\n",
    "\n",
    "    metrics = {\n",
    "        f\"{prefix}/num_samples_vt\": float(z_v_all.size(0)),\n",
    "    }\n",
    "\n",
    "    if run is not None:\n",
    "        run.log(metrics)\n",
    "\n",
    "    print(f\"Validation ({prefix}) | VT samples: {metrics[f'{prefix}/num_samples_vt']}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37aa49de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceiver model trainable params: 21621760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">02_multimodal_alignment_perceiver_mrl</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/fsadc8b4' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/fsadc8b4</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_034903-fsadc8b4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251202_035201-9z97ph2v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/9z97ph2v' target=\"_blank\">02_multimodal_alignment_perceiver_mrl</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/9z97ph2v' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_final_align/runs/9z97ph2v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Variant B: Perceiver + MLP + MRL (full model) ===\n",
    "\n",
    "# Re-instantiate model to avoid interference from MLP-only run\n",
    "model_perceiver = MultimodalAlignmentModel(mm_cfg).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_perceiver = nn.DataParallel(model_perceiver)\n",
    "\n",
    "print(\"Perceiver model trainable params:\", count_trainable_params(model_perceiver))\n",
    "\n",
    "optimizer_perceiver = torch.optim.AdamW(\n",
    "    get_model_module(model_perceiver).parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "best_metric_perceiver = -float(\"inf\")\n",
    "\n",
    "run_perceiver = init_wandb(run_name=\"02_multimodal_alignment_perceiver_mrl\", variant=\"perceiver_mrl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b9f81c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, cfg.num_epochs_perceiver + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     stats = \u001b[43mtrain_one_epoch\u001b[49m(\n\u001b[32m      3\u001b[39m         epoch=epoch,\n\u001b[32m      4\u001b[39m         model=model_perceiver,\n\u001b[32m      5\u001b[39m         optimizer=optimizer_perceiver,\n\u001b[32m      6\u001b[39m         image_loader=image_train_loader,\n\u001b[32m      7\u001b[39m         use_perceiver=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m         run=run_perceiver,\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m     val_metrics = validate_alignment(\n\u001b[32m     12\u001b[39m         model=model_perceiver,\n\u001b[32m     13\u001b[39m         image_val_loader=image_val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         prefix=\u001b[33m\"\u001b[39m\u001b[33mval_perceiver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m     current_metric = val_metrics.get(\u001b[33m\"\u001b[39m\u001b[33mval_perceiver/num_samples_vt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, cfg.num_epochs_perceiver + 1):\n",
    "    stats = train_one_epoch(\n",
    "        epoch=epoch,\n",
    "        model=model_perceiver,\n",
    "        optimizer=optimizer_perceiver,\n",
    "        image_loader=image_train_loader,\n",
    "        use_perceiver=True,\n",
    "        run=run_perceiver,\n",
    "    )\n",
    "\n",
    "    val_metrics = validate_alignment(\n",
    "        model=model_perceiver,\n",
    "        image_val_loader=image_val_loader,\n",
    "        use_perceiver=True,\n",
    "        max_batches=20,\n",
    "        run=run_perceiver,\n",
    "        prefix=\"val_perceiver\",\n",
    "    )\n",
    "\n",
    "    current_metric = val_metrics.get(\"val_perceiver/num_samples_vt\", 0.0)\n",
    "    if current_metric >= best_metric_perceiver:\n",
    "        best_metric_perceiver = current_metric\n",
    "        save_checkpoint(\n",
    "            model=model_perceiver,\n",
    "            optimizer=optimizer_perceiver,\n",
    "            epoch=epoch,\n",
    "            best_metric=best_metric_perceiver,\n",
    "            out_dir=PERCEIVER_DIR,\n",
    "            tag=\"best\",\n",
    "        )\n",
    "\n",
    "# Save final checkpoint\n",
    "save_checkpoint(\n",
    "    model=model_perceiver,\n",
    "    optimizer=optimizer_perceiver,\n",
    "    epoch=cfg.num_epochs_perceiver,\n",
    "    best_metric=best_metric_perceiver,\n",
    "    out_dir=PERCEIVER_DIR,\n",
    "    tag=\"final\",\n",
    ")\n",
    "\n",
    "if run_perceiver is not None:\n",
    "    run_perceiver.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Detailed alignment analysis for best checkpoints ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import wandb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_val_embeddings(\n",
    "    model: nn.Module,\n",
    "    image_val_loader: DataLoader,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 10,\n",
    "):\n",
    "    \"\"\"Collect aligned embeddings for a subset of the image-text val set.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_z_v = []\n",
    "    all_z_t = []\n",
    "\n",
    "    for b_idx, batch in enumerate(image_val_loader):\n",
    "        if b_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        images = batch[\"images\"]\n",
    "        texts = batch[\"captions\"]\n",
    "\n",
    "        vision_feats = encode_images_to_features(images)\n",
    "        text_feats = encode_texts_to_features(texts)\n",
    "\n",
    "        z_v = encode_modality(model, vision_feats, None, \"vision\", use_perceiver)\n",
    "        z_t = encode_modality(model, text_feats, None, \"text\", use_perceiver)\n",
    "\n",
    "        all_z_v.append(z_v.cpu())\n",
    "        all_z_t.append(z_t.cpu())\n",
    "\n",
    "    if not all_z_v:\n",
    "        raise RuntimeError(\"No validation batches collected for embeddings.\")\n",
    "\n",
    "    z_v_all = torch.cat(all_z_v, dim=0)\n",
    "    z_t_all = torch.cat(all_z_t, dim=0)\n",
    "\n",
    "    return z_v_all, z_t_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289aa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_similarity_heatmap(z_v: torch.Tensor, z_t: torch.Tensor, title: str = \"\"):\n",
    "    \"\"\"Create a cosine similarity heatmap between image and text embeddings.\"\"\"\n",
    "    z_v_norm = F.normalize(z_v, dim=-1)\n",
    "    z_t_norm = F.normalize(z_t, dim=-1)\n",
    "\n",
    "    sim_matrix = z_v_norm @ z_t_norm.T  # (N, N)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(sim_matrix.numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "    ax.set_title(title or \"Cosine Similarity (vision ‚Üî text)\")\n",
    "    ax.set_xlabel(\"Text index\")\n",
    "    ax.set_ylabel(\"Image index\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    return fig, sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68895159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_tsne_plot(z_v: torch.Tensor, z_t: torch.Tensor, title: str = \"\"):\n",
    "    \"\"\"Create a t-SNE 2D scatter of vision and text aligned embeddings.\"\"\"\n",
    "    z_v_np = z_v.numpy()\n",
    "    z_t_np = z_t.numpy()\n",
    "\n",
    "    n_v = z_v_np.shape[0]\n",
    "    n_t = z_t_np.shape[0]\n",
    "\n",
    "    all_embeds = np.concatenate([z_v_np, z_t_np], axis=0)\n",
    "    perplexity = min(30, max(5, (n_v + n_t) // 5))\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        init=\"random\",\n",
    "        learning_rate=\"auto\",\n",
    "    )\n",
    "    all_2d = tsne.fit_transform(all_embeds)\n",
    "\n",
    "    v_2d = all_2d[:n_v]\n",
    "    t_2d = all_2d[n_v:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.scatter(v_2d[:, 0], v_2d[:, 1], label=\"vision\", alpha=0.7, s=20)\n",
    "    ax.scatter(t_2d[:, 0], t_2d[:, 1], label=\"text\", alpha=0.7, s=20, marker=\"x\")\n",
    "    ax.set_title(title or \"t-SNE of aligned embeddings (vision & text)\")\n",
    "    ax.set_xlabel(\"t-SNE dim 1\")\n",
    "    ax.set_ylabel(\"t-SNE dim 2\")\n",
    "    ax.legend()\n",
    "\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_best_checkpoint(\n",
    "    model: nn.Module,\n",
    "    ckpt_dir: Path,\n",
    "    variant_name: str,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 10,\n",
    "):\n",
    "    \"\"\"Load best checkpoint, compute alignment diagnostics, and log plots to W&B.\"\"\"\n",
    "    best_ckpt_path = ckpt_dir / \"best.pt\"\n",
    "    if not best_ckpt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Best checkpoint not found at {best_ckpt_path}, skipping analysis.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Alignment analysis for variant: {variant_name} ===\")\n",
    "    print(f\"Loading best checkpoint from: {best_ckpt_path}\")\n",
    "    _ = load_checkpoint(model, optimizer=None, ckpt_path=best_ckpt_path, strict=True)\n",
    "\n",
    "    # Collect embeddings\n",
    "    z_v_all, z_t_all = collect_val_embeddings(\n",
    "        model=model,\n",
    "        image_val_loader=image_val_loader,\n",
    "        use_perceiver=use_perceiver,\n",
    "        max_batches=max_batches,\n",
    "    )\n",
    "\n",
    "    analysis_run = init_wandb(\n",
    "        run_name=f\"02_alignment_analysis_{variant_name}\",\n",
    "        variant=f\"{variant_name}_analysis\",\n",
    "        extra_config={\"checkpoint_path\": str(best_ckpt_path)},\n",
    "    )\n",
    "\n",
    "    # Histograms\n",
    "    if analysis_run is not None:\n",
    "        log_alignment_histograms(\n",
    "            analysis_run,\n",
    "            z_a=z_v_all.to(device),\n",
    "            z_b=z_t_all.to(device),\n",
    "            prefix=f\"analysis/{variant_name}/vision_text\",\n",
    "            max_points=512,\n",
    "        )\n",
    "\n",
    "    # Heatmap\n",
    "    fig_heatmap, sim_matrix = make_similarity_heatmap(\n",
    "        z_v_all,\n",
    "        z_t_all,\n",
    "        title=f\"{variant_name}: cosine similarity (vision ‚Üî text)\",\n",
    "    )\n",
    "\n",
    "    sim_np = sim_matrix.numpy()\n",
    "    diag = np.diag(sim_np)\n",
    "    off_diag = sim_np[~np.eye(sim_np.shape[0], dtype=bool)]\n",
    "\n",
    "    diag_mean = float(diag.mean())\n",
    "    diag_std = float(diag.std())\n",
    "    off_mean = float(off_diag.mean())\n",
    "    off_std = float(off_diag.std())\n",
    "\n",
    "    if analysis_run is not None:\n",
    "        analysis_run.log(\n",
    "            {\n",
    "                f\"analysis/{variant_name}/sim_heatmap\": wandb.Image(fig_heatmap),\n",
    "                f\"analysis/{variant_name}/diag_mean\": diag_mean,\n",
    "                f\"analysis/{variant_name}/diag_std\": diag_std,\n",
    "                f\"analysis/{variant_name}/offdiag_mean\": off_mean,\n",
    "                f\"analysis/{variant_name}/offdiag_std\": off_std,\n",
    "            }\n",
    "        )\n",
    "    plt.close(fig_heatmap)\n",
    "\n",
    "    print(\n",
    "        f\"Diagonal similarity: mean={diag_mean:.3f}, std={diag_std:.3f} | \"\n",
    "        f\"Off-diagonal: mean={off_mean:.3f}, std={off_std:.3f}\"\n",
    "    )\n",
    "\n",
    "    # t-SNE\n",
    "    fig_tsne = make_tsne_plot(\n",
    "        z_v_all,\n",
    "        z_t_all,\n",
    "        title=f\"{variant_name}: t-SNE of aligned embeddings\",\n",
    "    )\n",
    "    if analysis_run is not None:\n",
    "        analysis_run.log({f\"analysis/{variant_name}/tsne\": wandb.Image(fig_tsne)})\n",
    "        analysis_run.log(\n",
    "            {\n",
    "                f\"analysis/{variant_name}/num_samples\": float(z_v_all.size(0)),\n",
    "                f\"analysis/{variant_name}/d_align\": float(z_v_all.size(1)),\n",
    "            }\n",
    "        )\n",
    "        analysis_run.finish()\n",
    "    plt.close(fig_tsne)\n",
    "\n",
    "\n",
    "analyze_best_checkpoint(\n",
    "    model=model_perceiver,\n",
    "    ckpt_dir=PERCEIVER_DIR,\n",
    "    variant_name=\"perceiver_mrl\",\n",
    "    use_perceiver=True,\n",
    "    max_batches=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727c7f",
   "metadata": {},
   "source": [
    "## Outputs & Next Steps\n",
    "\n",
    "This notebook produces **Phase 1 multimodal alignment checkpoints**:\n",
    "\n",
    "- **MLP + MRL (no Perceiver)**: `checkpoints/phase1_multimodal/mlp_mrl/`\n",
    "- **Perceiver + MLP + MRL**: `checkpoints/phase1_multimodal/perceiver_mrl/`\n",
    "\n",
    "Each directory contains:\n",
    "\n",
    "- `best.pt` ‚Äì best model according to a validation proxy metric (placeholder for now)\n",
    "- `final.pt` ‚Äì final epoch checkpoint\n",
    "\n",
    "Additionally, W&B runs include:\n",
    "\n",
    "- Training curves (`train/loss_*`, `val_*/num_samples_*`).\n",
    "- Cosine similarity **histograms** (pos vs neg pairs) for vision‚Äìtext and audio‚Äìtext.\n",
    "- Cosine similarity **heatmaps** (vision ‚Üî text) for best checkpoints.\n",
    "- **t-SNE visualizations** of aligned embeddings (vision & text) for best checkpoints.\n",
    "\n",
    "These checkpoints and diagnostics are the foundation for Phase 2 experiments:\n",
    "\n",
    "- LLM decoder alignment (normal decoder).\n",
    "- TRM decoder alignment.\n",
    "- MoE decoder alignment.\n",
    "- Full retrieval-based evaluation and Matryoshka ablations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
