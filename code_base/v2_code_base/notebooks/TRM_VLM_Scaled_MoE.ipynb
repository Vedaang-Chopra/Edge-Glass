{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRM-Enhanced Vision-Language Model with MoE\n",
    "\n",
    "## A Scalable VLM using Tiny Recursive Models, Mixture of Experts, and Multi-GPU Training\n",
    "\n",
    "This notebook implements a complete Vision-Language Model with:\n",
    "\n",
    "### Architecture\n",
    "- **Language Backbone**: Qwen2.5-3B with TRM recursive reasoning\n",
    "- **MoE Enhancement**: Mixture of Experts layers for increased capacity\n",
    "- **Vision Encoders**: Configurable SigLIP / DINOv2 (for ablation studies)\n",
    "- **Projection**: 2-layer MLP connector\n",
    "\n",
    "### Training\n",
    "- **Dataset**: Pixmo-Cap (712K images, 196-word avg captions)\n",
    "- **Infrastructure**: 2x H200 GPUs with FSDP\n",
    "- **Comparison**: Qwen2.5-VL-7B benchmark\n",
    "\n",
    "### Key TRM Concepts Applied\n",
    "- Recursive reasoning with tiny networks\n",
    "- Deep supervision across cycles\n",
    "- Progressive answer refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install dependencies\n",
    "# ! uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
    "# ! uv pip install transformers>=4.45.0 accelerate>=0.34.0 datasets -q\n",
    "# ! uv pip install bitsandbytes peft flash-attn --no-build-isolation -q\n",
    "# ! uv pip install einops timm sentencepiece tiktoken -q\n",
    "# ! uv pip install wandb matplotlib seaborn tqdm pillow requests -q\n",
    "# # ! uv pip install qwen-vl-utils -q  # For Qwen2.5-VL comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Tuple, List, Dict, Any, Union\n",
    "from enum import Enum\n",
    "from io import BytesIO\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp import ShardingStrategy, MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    AutoImageProcessor,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set seeds\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoderType(Enum):\n",
    "    \"\"\"Supported vision encoder types for ablation studies\"\"\"\n",
    "    SIGLIP = \"siglip\"\n",
    "    DINOV2 = \"dinov2\"\n",
    "    SIGLIP_DINOV2 = \"siglip_dinov2\"  # Dual encoder\n",
    "\n",
    "\n",
    "class LLMBackboneType(Enum):\n",
    "    \"\"\"Supported LLM backbone types\"\"\"\n",
    "    QWEN2_5_0_5B = \"Qwen/Qwen2.5-0.5B\"\n",
    "    QWEN2_5_1_5B = \"Qwen/Qwen2.5-1.5B\"\n",
    "    QWEN2_5_3B = \"Qwen/Qwen2.5-3B\"\n",
    "    QWEN2_5_7B = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VisionEncoderConfig:\n",
    "    \"\"\"Configuration for vision encoder\"\"\"\n",
    "    encoder_type: VisionEncoderType = VisionEncoderType.SIGLIP\n",
    "    \n",
    "    # SigLIP config\n",
    "    siglip_model_name: str = \"google/siglip-so400m-patch14-384\"\n",
    "    siglip_image_size: int = 384\n",
    "    siglip_hidden_size: int = 1152\n",
    "    siglip_num_patches: int = 729  # (384/14)^2\n",
    "    \n",
    "    # DINOv2 config\n",
    "    dinov2_model_name: str = \"facebook/dinov2-large\"\n",
    "    dinov2_image_size: int = 224\n",
    "    dinov2_hidden_size: int = 1024\n",
    "    dinov2_num_patches: int = 256  # (224/14)^2\n",
    "    \n",
    "    # Shared config\n",
    "    freeze_vision: bool = True\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        if self.encoder_type == VisionEncoderType.SIGLIP:\n",
    "            return self.siglip_hidden_size\n",
    "        elif self.encoder_type == VisionEncoderType.DINOV2:\n",
    "            return self.dinov2_hidden_size\n",
    "        else:  # Dual encoder\n",
    "            return self.siglip_hidden_size + self.dinov2_hidden_size\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MoEConfig:\n",
    "    \"\"\"Configuration for Mixture of Experts\"\"\"\n",
    "    enabled: bool = True\n",
    "    num_experts: int = 8\n",
    "    num_experts_per_token: int = 2  # top-k\n",
    "    num_shared_experts: int = 1  # DeepSeek-style shared experts\n",
    "    capacity_factor: float = 1.25  # For load balancing\n",
    "    router_jitter_noise: float = 0.1  # Training stability\n",
    "    load_balance_loss_weight: float = 0.01\n",
    "    router_z_loss_weight: float = 0.001\n",
    "    moe_layer_frequency: int = 2  # Apply MoE every N layers\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TRMConfig:\n",
    "    \"\"\"Configuration for TRM recursive reasoning\"\"\"\n",
    "    enabled: bool = True\n",
    "    n_recursions: int = 4  # Latent reasoning iterations\n",
    "    t_cycles: int = 2  # Deep supervision cycles\n",
    "    apply_to_layers: List[int] = field(default_factory=lambda: [-4, -3, -2, -1])  # Last N layers\n",
    "    residual_scale: float = 0.1  # Scale for residual connections\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ProjectorConfig:\n",
    "    \"\"\"Configuration for vision-language projector\"\"\"\n",
    "    projector_type: str = \"mlp\"  # 'mlp', 'linear', 'resampler'\n",
    "    num_layers: int = 2\n",
    "    activation: str = \"gelu\"\n",
    "    dropout: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TRMVLMConfig:\n",
    "    \"\"\"Master configuration for TRM VLM\"\"\"\n",
    "    # Model components\n",
    "    llm_backbone: LLMBackboneType = LLMBackboneType.QWEN2_5_3B\n",
    "    vision_config: VisionEncoderConfig = field(default_factory=VisionEncoderConfig)\n",
    "    moe_config: MoEConfig = field(default_factory=MoEConfig)\n",
    "    trm_config: TRMConfig = field(default_factory=TRMConfig)\n",
    "    projector_config: ProjectorConfig = field(default_factory=ProjectorConfig)\n",
    "    \n",
    "    # LLM config (will be loaded from pretrained)\n",
    "    llm_hidden_size: int = 2048  # Qwen2.5-3B default\n",
    "    \n",
    "    # Training config\n",
    "    max_seq_length: int = 2048\n",
    "    image_token_id: int = 151655  # <|image_pad|> in Qwen\n",
    "    num_image_tokens: int = 256  # Number of visual tokens\n",
    "    \n",
    "    # Precision\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    use_flash_attention: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Set LLM hidden size based on backbone\n",
    "        llm_hidden_sizes = {\n",
    "            LLMBackboneType.QWEN2_5_0_5B: 896,\n",
    "            LLMBackboneType.QWEN2_5_1_5B: 1536,\n",
    "            LLMBackboneType.QWEN2_5_3B: 2048,\n",
    "            LLMBackboneType.QWEN2_5_7B: 3584,\n",
    "        }\n",
    "        self.llm_hidden_size = llm_hidden_sizes.get(self.llm_backbone, 2048)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    # Basic training\n",
    "    num_epochs: int = 3\n",
    "    per_device_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate: float = 1e-4\n",
    "    llm_learning_rate: float = 2e-5  # Lower LR for LLM backbone\n",
    "    vision_learning_rate: float = 0.0  # Frozen by default\n",
    "    warmup_ratio: float = 0.03\n",
    "    weight_decay: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Scheduler\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 250\n",
    "    logging_steps: int = 10\n",
    "    output_dir: str = \"./outputs/trm_vlm\"\n",
    "    \n",
    "    # Mixed precision\n",
    "    bf16: bool = True\n",
    "    \n",
    "    # Distributed\n",
    "    fsdp_sharding_strategy: str = \"SHARD_GRAD_OP\"  # Best for 2 GPUs\n",
    "    \n",
    "    # Wandb\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"trm-vlm\"\n",
    "    wandb_run_name: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default configuration\n",
    "model_config = TRMVLMConfig(\n",
    "    llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "    vision_config=VisionEncoderConfig(\n",
    "        encoder_type=VisionEncoderType.SIGLIP,\n",
    "        freeze_vision=True\n",
    "    ),\n",
    "    moe_config=MoEConfig(\n",
    "        enabled=True,\n",
    "        num_experts=8,\n",
    "        num_experts_per_token=2\n",
    "    ),\n",
    "    trm_config=TRMConfig(\n",
    "        enabled=True,\n",
    "        n_recursions=4,\n",
    "        t_cycles=2\n",
    "    )\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    num_epochs=3,\n",
    "    per_device_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  LLM Backbone: {model_config.llm_backbone.value}\")\n",
    "print(f\"  Vision Encoder: {model_config.vision_config.encoder_type.value}\")\n",
    "print(f\"  MoE Enabled: {model_config.moe_config.enabled}\")\n",
    "print(f\"  TRM Enabled: {model_config.trm_config.enabled}\")\n",
    "print(f\"  LLM Hidden Size: {model_config.llm_hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Vision Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLIPVisionEncoder(nn.Module):\n",
    "    \"\"\"SigLIP Vision Encoder wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VisionEncoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load SigLIP model\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            config.siglip_model_name,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        ).vision_model\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(config.siglip_model_name)\n",
    "        \n",
    "        # Freeze if specified\n",
    "        if config.freeze_vision:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Enable gradient checkpointing\n",
    "        if config.use_gradient_checkpointing:\n",
    "            if hasattr(self.model, \"gradient_checkpointing_enable\"):\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "            elif hasattr(self.model, \"set_grad_checkpointing\"):\n",
    "                self.model.set_grad_checkpointing(True)\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"Requested gradient checkpointing for {self.__class__.__name__}, but the encoder backend does not expose a compatible hook.\"\n",
    "                )\n",
    "    \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Preprocessed images (B, C, H, W)\n",
    "        Returns:\n",
    "            Patch embeddings (B, num_patches, hidden_size)\n",
    "        \"\"\"\n",
    "        outputs = self.model(pixel_values=images)\n",
    "        # Return all patch embeddings (excluding CLS if present)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        return hidden_states\n",
    "    \n",
    "    def preprocess(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"Preprocess PIL images\"\"\"\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        return inputs.pixel_values\n",
    "\n",
    "\n",
    "class DINOv2VisionEncoder(nn.Module):\n",
    "    \"\"\"DINOv2 Vision Encoder wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VisionEncoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load DINOv2 model\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            config.dinov2_model_name,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        self.processor = AutoImageProcessor.from_pretrained(config.dinov2_model_name)\n",
    "        \n",
    "        # Freeze if specified\n",
    "        if config.freeze_vision:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Enable gradient checkpointing\n",
    "        if config.use_gradient_checkpointing:\n",
    "            if hasattr(self.model, \"gradient_checkpointing_enable\"):\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "            elif hasattr(self.model, \"set_grad_checkpointing\"):\n",
    "                self.model.set_grad_checkpointing(True)\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"Requested gradient checkpointing for {self.__class__.__name__}, but the encoder backend does not expose a compatible hook.\"\n",
    "                )\n",
    "    \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Preprocessed images (B, C, H, W)\n",
    "        Returns:\n",
    "            Patch embeddings (B, num_patches, hidden_size)\n",
    "        \"\"\"\n",
    "        outputs = self.model(pixel_values=images)\n",
    "        # Skip CLS token, return patch embeddings\n",
    "        hidden_states = outputs.last_hidden_state[:, 1:, :]\n",
    "        return hidden_states\n",
    "    \n",
    "    def preprocess(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"Preprocess PIL images\"\"\"\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        return inputs.pixel_values\n",
    "\n",
    "\n",
    "class DualVisionEncoder(nn.Module):\n",
    "    \"\"\"Dual SigLIP + DINOv2 encoder for enhanced representations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VisionEncoderConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.siglip = SigLIPVisionEncoder(config)\n",
    "        self.dinov2 = DINOv2VisionEncoder(config)\n",
    "        \n",
    "        # Align spatial dimensions through adaptive pooling or interpolation\n",
    "        # SigLIP: 729 patches, DINOv2: 256 patches -> use 256\n",
    "        self.target_num_patches = config.dinov2_num_patches\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        siglip_images: torch.Tensor,\n",
    "        dinov2_images: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            siglip_images: Images for SigLIP (B, C, 384, 384)\n",
    "            dinov2_images: Images for DINOv2 (B, C, 224, 224)\n",
    "        Returns:\n",
    "            Concatenated features (B, num_patches, siglip_dim + dinov2_dim)\n",
    "        \"\"\"\n",
    "        # Get features from both encoders\n",
    "        siglip_features = self.siglip(siglip_images)  # (B, 729, 1152)\n",
    "        dinov2_features = self.dinov2(dinov2_images)  # (B, 256, 1024)\n",
    "        \n",
    "        # Interpolate SigLIP features to match DINOv2 patch count\n",
    "        B, N_sig, D_sig = siglip_features.shape\n",
    "        H_sig = W_sig = int(math.sqrt(N_sig))\n",
    "        \n",
    "        siglip_features = siglip_features.view(B, H_sig, W_sig, D_sig).permute(0, 3, 1, 2)\n",
    "        siglip_features = F.interpolate(\n",
    "            siglip_features, \n",
    "            size=(16, 16),  # 256 patches\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        siglip_features = siglip_features.permute(0, 2, 3, 1).view(B, 256, D_sig)\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        combined = torch.cat([siglip_features, dinov2_features], dim=-1)\n",
    "        return combined\n",
    "\n",
    "\n",
    "def create_vision_encoder(config: VisionEncoderConfig) -> nn.Module:\n",
    "    \"\"\"Factory function to create vision encoder based on config\"\"\"\n",
    "    if config.encoder_type == VisionEncoderType.SIGLIP:\n",
    "        return SigLIPVisionEncoder(config)\n",
    "    elif config.encoder_type == VisionEncoderType.DINOV2:\n",
    "        return DINOv2VisionEncoder(config)\n",
    "    elif config.encoder_type == VisionEncoderType.SIGLIP_DINOV2:\n",
    "        return DualVisionEncoder(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoder type: {config.encoder_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Mixture of Experts Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoERouter(nn.Module):\n",
    "    \"\"\"Router for Mixture of Experts with load balancing\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        jitter_noise: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.jitter_noise = jitter_noise\n",
    "        \n",
    "        # Router linear layer\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch, seq_len, hidden_size)\n",
    "        Returns:\n",
    "            router_probs: (batch, seq_len, top_k)\n",
    "            expert_indices: (batch, seq_len, top_k)\n",
    "            router_logits: (batch, seq_len, num_experts) - for aux loss\n",
    "            expert_mask: (batch, seq_len, num_experts, top_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        # Compute router logits in float32 for stability\n",
    "        router_logits = self.gate(hidden_states.float())\n",
    "        \n",
    "        # Add jitter noise during training\n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            noise = torch.randn_like(router_logits) * self.jitter_noise\n",
    "            router_logits = router_logits + noise\n",
    "        \n",
    "        # Get top-k experts\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        top_k_probs, expert_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize top-k probabilities\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Create expert mask for dispatching\n",
    "        expert_mask = F.one_hot(expert_indices, num_classes=self.num_experts)\n",
    "        expert_mask = expert_mask.permute(0, 1, 3, 2)  # (batch, seq, num_experts, top_k)\n",
    "        \n",
    "        return top_k_probs, expert_indices, router_logits, expert_mask\n",
    "\n",
    "\n",
    "class MoEExpert(nn.Module):\n",
    "    \"\"\"Single expert (SwiGLU FFN)\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer replacing standard FFN\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        config: MoEConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = config.num_experts\n",
    "        self.top_k = config.num_experts_per_token\n",
    "        \n",
    "        # Router\n",
    "        self.router = MoERouter(\n",
    "            hidden_size=hidden_size,\n",
    "            num_experts=config.num_experts,\n",
    "            top_k=config.num_experts_per_token,\n",
    "            jitter_noise=config.router_jitter_noise\n",
    "        )\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            MoEExpert(hidden_size, intermediate_size)\n",
    "            for _ in range(config.num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Shared experts (DeepSeek-style)\n",
    "        if config.num_shared_experts > 0:\n",
    "            self.shared_experts = nn.ModuleList([\n",
    "                MoEExpert(hidden_size, intermediate_size)\n",
    "                for _ in range(config.num_shared_experts)\n",
    "            ])\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch, seq_len, hidden_size)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, hidden_size)\n",
    "            router_logits: for computing aux loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        \n",
    "        # Route tokens\n",
    "        top_k_probs, expert_indices, router_logits, expert_mask = self.router(hidden_states)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(hidden_states)\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Find tokens routed to this expert\n",
    "            # expert_mask: (batch, seq, num_experts, top_k)\n",
    "            token_mask = expert_mask[:, :, expert_idx, :].any(dim=-1)  # (batch, seq)\n",
    "            \n",
    "            if token_mask.any():\n",
    "                # Get the expert's weight for these tokens\n",
    "                expert_probs = torch.zeros(batch_size, seq_len, device=hidden_states.device)\n",
    "                for k in range(self.top_k):\n",
    "                    mask = (expert_indices[:, :, k] == expert_idx)\n",
    "                    expert_probs[mask] = top_k_probs[:, :, k][mask]\n",
    "                \n",
    "                # Compute expert output for all tokens (will mask later)\n",
    "                expert_output = expert(hidden_states)\n",
    "                \n",
    "                # Weighted addition\n",
    "                output = output + expert_output * expert_probs.unsqueeze(-1)\n",
    "        \n",
    "        # Add shared expert output\n",
    "        if self.shared_experts is not None:\n",
    "            for shared_expert in self.shared_experts:\n",
    "                output = output + shared_expert(hidden_states) / len(self.shared_experts)\n",
    "        \n",
    "        return output, router_logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_load_balancing_loss(\n",
    "        router_logits: torch.Tensor,\n",
    "        expert_indices: torch.Tensor,\n",
    "        num_experts: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute load balancing auxiliary loss\"\"\"\n",
    "        # router_logits: (batch, seq, num_experts)\n",
    "        router_probs = F.softmax(router_logits.float(), dim=-1)\n",
    "        \n",
    "        # Expert importance: mean probability assigned to each expert\n",
    "        expert_importance = router_probs.mean(dim=[0, 1])  # (num_experts,)\n",
    "        \n",
    "        # Expert load: fraction of tokens assigned to each expert\n",
    "        one_hot = F.one_hot(expert_indices, num_classes=num_experts).float()\n",
    "        expert_load = one_hot.mean(dim=[0, 1, 2])  # (num_experts,)\n",
    "        \n",
    "        # Loss encourages uniform distribution\n",
    "        return num_experts * (expert_importance * expert_load).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_router_z_loss(router_logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute router z-loss for stability\"\"\"\n",
    "        # Penalize large logits\n",
    "        return torch.logsumexp(router_logits.float(), dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: TRM Recursive Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMReasoningBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TRM Reasoning Block that can wrap any transformer layer.\n",
    "    Applies recursive reasoning: updates latent z given (x, y, z).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Module,\n",
    "        hidden_size: int,\n",
    "        config: TRMConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.config = config\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Learnable initial state for z\n",
    "        self.z_init = nn.Parameter(torch.randn(hidden_size) * 0.02)\n",
    "        \n",
    "        # Projection for combining x, y, z\n",
    "        self.input_fusion = nn.Linear(hidden_size * 3, hidden_size, bias=False)\n",
    "        \n",
    "        # Output gate (controls how much TRM reasoning affects output)\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer norm for z updates\n",
    "        self.z_norm = nn.RMSNorm(hidden_size, eps=1e-6)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Forward with TRM recursive reasoning.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        # Initialize z (latent reasoning state)\n",
    "        z = self.z_init.unsqueeze(0).unsqueeze(0).expand(batch_size, seq_len, -1)\n",
    "        z = z.to(hidden_states.dtype)\n",
    "        \n",
    "        # Store x (input) and initialize y (answer draft)\n",
    "        x = hidden_states\n",
    "        y = hidden_states.clone()\n",
    "        \n",
    "        all_outputs = []\n",
    "        \n",
    "        # TRM recursive reasoning loop\n",
    "        for t in range(self.config.t_cycles):\n",
    "            # Determine if we need gradients (only last cycle)\n",
    "            use_grad = (t == self.config.t_cycles - 1) or self.training\n",
    "            \n",
    "            with torch.set_grad_enabled(use_grad):\n",
    "                # n recursions to update z\n",
    "                for n in range(self.config.n_recursions):\n",
    "                    # Combine x, y, z\n",
    "                    combined = torch.cat([x, y, z], dim=-1)\n",
    "                    fused = self.input_fusion(combined)\n",
    "                    \n",
    "                    # Pass through original transformer layer\n",
    "                    layer_outputs = self.original_layer(\n",
    "                        fused,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=None,  # Don't use cache during recursion\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=False,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                    \n",
    "                    # Update z with residual\n",
    "                    z_update = layer_outputs[0]\n",
    "                    z = self.z_norm(z + self.config.residual_scale * z_update)\n",
    "                \n",
    "                # Update y (answer) based on refined z\n",
    "                gate = self.output_gate(torch.cat([y, z], dim=-1))\n",
    "                y = y + gate * (z - y)\n",
    "                \n",
    "                all_outputs.append(y)\n",
    "            \n",
    "            # Detach for next cycle\n",
    "            if t < self.config.t_cycles - 1:\n",
    "                z = z.detach()\n",
    "                y = y.detach()\n",
    "        \n",
    "        # Final output\n",
    "        final_output = all_outputs[-1]\n",
    "        \n",
    "        # Return in same format as original layer\n",
    "        outputs = (final_output,)\n",
    "        if output_attentions:\n",
    "            outputs += (None,)  # Placeholder for attention weights\n",
    "        if use_cache:\n",
    "            outputs += (past_key_value,)  # Pass through cache\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Vision-Language Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageProjector(nn.Module):\n",
    "    \"\"\"Projects vision features to language model space\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_dim: int,\n",
    "        llm_dim: int,\n",
    "        config: ProjectorConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        if config.projector_type == \"linear\":\n",
    "            self.projector = nn.Linear(vision_dim, llm_dim)\n",
    "        \n",
    "        elif config.projector_type == \"mlp\":\n",
    "            layers = []\n",
    "            in_dim = vision_dim\n",
    "            \n",
    "            for i in range(config.num_layers):\n",
    "                out_dim = llm_dim\n",
    "                layers.append(nn.Linear(in_dim, out_dim))\n",
    "                \n",
    "                if i < config.num_layers - 1:  # No activation on last layer\n",
    "                    if config.activation == \"gelu\":\n",
    "                        layers.append(nn.GELU())\n",
    "                    elif config.activation == \"silu\":\n",
    "                        layers.append(nn.SiLU())\n",
    "                    elif config.activation == \"relu\":\n",
    "                        layers.append(nn.ReLU())\n",
    "                    \n",
    "                    if config.dropout > 0:\n",
    "                        layers.append(nn.Dropout(config.dropout))\n",
    "                \n",
    "                in_dim = out_dim\n",
    "            \n",
    "            self.projector = nn.Sequential(*layers)\n",
    "        \n",
    "        elif config.projector_type == \"resampler\":\n",
    "            # Perceiver-style resampler\n",
    "            self.projector = PerceiverResampler(\n",
    "                dim=vision_dim,\n",
    "                output_dim=llm_dim,\n",
    "                num_latents=64,\n",
    "                depth=2\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown projector type: {config.projector_type}\")\n",
    "    \n",
    "    def forward(self, vision_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vision_features: (batch, num_patches, vision_dim)\n",
    "        Returns:\n",
    "            projected: (batch, num_patches, llm_dim)\n",
    "        \"\"\"\n",
    "        return self.projector(vision_features)\n",
    "\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"Perceiver-style resampler for flexible token compression\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        output_dim: int,\n",
    "        num_latents: int = 64,\n",
    "        depth: int = 2,\n",
    "        num_heads: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) * 0.02)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'cross_attn': nn.MultiheadAttention(dim, num_heads, batch_first=True),\n",
    "                'cross_norm': nn.LayerNorm(dim),\n",
    "                'ff': nn.Sequential(\n",
    "                    nn.Linear(dim, dim * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(dim * 4, dim)\n",
    "                ),\n",
    "                'ff_norm': nn.LayerNorm(dim)\n",
    "            })\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(dim, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # Cross attention\n",
    "            attn_out, _ = layer['cross_attn'](latents, x, x)\n",
    "            latents = layer['cross_norm'](latents + attn_out)\n",
    "            \n",
    "            # Feed-forward\n",
    "            ff_out = layer['ff'](latents)\n",
    "            latents = layer['ff_norm'](latents + ff_out)\n",
    "        \n",
    "        return self.output_proj(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Complete TRM-VLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete TRM Vision-Language Model.\n",
    "    \n",
    "    Components:\n",
    "    - Vision Encoder: SigLIP / DINOv2 / Dual\n",
    "    - Projector: MLP/Resampler\n",
    "    - LLM: Qwen2.5 with optional MoE and TRM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TRMVLMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 1. Vision Encoder\n",
    "        logger.info(f\"Loading vision encoder: {config.vision_config.encoder_type.value}\")\n",
    "        self.vision_encoder = create_vision_encoder(config.vision_config)\n",
    "        \n",
    "        # 2. Vision-Language Projector\n",
    "        self.projector = VisionLanguageProjector(\n",
    "            vision_dim=config.vision_config.output_dim,\n",
    "            llm_dim=config.llm_hidden_size,\n",
    "            config=config.projector_config\n",
    "        )\n",
    "        \n",
    "        # 3. Load LLM backbone\n",
    "        logger.info(f\"Loading LLM backbone: {config.llm_backbone.value}\")\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            config.llm_backbone.value,\n",
    "            torch_dtype=getattr(torch, config.torch_dtype),\n",
    "            # attn_implementation=\"flash_attention_2\" if config.use_flash_attention else \"eager\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.llm_backbone.value,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 4. Apply MoE modifications if enabled\n",
    "        if config.moe_config.enabled:\n",
    "            self._apply_moe_modifications()\n",
    "        \n",
    "        # 5. Apply TRM modifications if enabled\n",
    "        if config.trm_config.enabled:\n",
    "            self._apply_trm_modifications()\n",
    "        \n",
    "        # 6. Enable gradient checkpointing\n",
    "        self.llm.gradient_checkpointing_enable()\n",
    "        \n",
    "        # 7. Store auxiliary losses\n",
    "        self.aux_losses = {}\n",
    "    \n",
    "    def _apply_moe_modifications(self):\n",
    "        \"\"\"Replace FFN layers with MoE layers\"\"\"\n",
    "        moe_config = self.config.moe_config\n",
    "        \n",
    "        # Get model layers\n",
    "        layers = self.llm.model.layers\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            # Apply MoE every N layers\n",
    "            if i % moe_config.moe_layer_frequency == 0:\n",
    "                # Get original FFN dimensions\n",
    "                original_mlp = layer.mlp\n",
    "                hidden_size = self.config.llm_hidden_size\n",
    "                \n",
    "                # Infer intermediate size from original MLP\n",
    "                if hasattr(original_mlp, 'gate_proj'):\n",
    "                    intermediate_size = original_mlp.gate_proj.out_features\n",
    "                else:\n",
    "                    intermediate_size = hidden_size * 4\n",
    "                \n",
    "                # Create MoE layer\n",
    "                moe_layer = MoELayer(\n",
    "                    hidden_size=hidden_size,\n",
    "                    intermediate_size=intermediate_size,\n",
    "                    config=moe_config\n",
    "                )\n",
    "                \n",
    "                # Initialize experts from original FFN (sparse upcycling)\n",
    "                with torch.no_grad():\n",
    "                    for expert in moe_layer.experts:\n",
    "                        if hasattr(original_mlp, 'gate_proj'):\n",
    "                            expert.gate_proj.weight.copy_(original_mlp.gate_proj.weight)\n",
    "                            expert.up_proj.weight.copy_(original_mlp.up_proj.weight)\n",
    "                            expert.down_proj.weight.copy_(original_mlp.down_proj.weight)\n",
    "                \n",
    "                # Replace MLP with MoE\n",
    "                layer.mlp = moe_layer\n",
    "                logger.info(f\"Replaced layer {i} FFN with MoE ({moe_config.num_experts} experts)\")\n",
    "    \n",
    "    def _apply_trm_modifications(self):\n",
    "        \"\"\"Wrap specified layers with TRM reasoning\"\"\"\n",
    "        trm_config = self.config.trm_config\n",
    "        layers = self.llm.model.layers\n",
    "        num_layers = len(layers)\n",
    "        \n",
    "        # Convert negative indices to positive\n",
    "        layer_indices = [\n",
    "            idx if idx >= 0 else num_layers + idx\n",
    "            for idx in trm_config.apply_to_layers\n",
    "        ]\n",
    "        \n",
    "        for idx in layer_indices:\n",
    "            if 0 <= idx < num_layers:\n",
    "                original_layer = layers[idx]\n",
    "                trm_layer = TRMReasoningBlock(\n",
    "                    original_layer=original_layer,\n",
    "                    hidden_size=self.config.llm_hidden_size,\n",
    "                    config=trm_config\n",
    "                )\n",
    "                layers[idx] = trm_layer\n",
    "                logger.info(f\"Wrapped layer {idx} with TRM reasoning block\")\n",
    "    \n",
    "    def encode_images(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode images to LLM embedding space.\n",
    "        \n",
    "        Args:\n",
    "            images: Preprocessed images (B, C, H, W)\n",
    "        Returns:\n",
    "            image_embeds: (B, num_tokens, llm_dim)\n",
    "        \"\"\"\n",
    "        # Get vision features\n",
    "        with torch.no_grad() if self.config.vision_config.freeze_vision else torch.enable_grad():\n",
    "            vision_features = self.vision_encoder(images)\n",
    "        \n",
    "        # Project to LLM space\n",
    "        image_embeds = self.projector(vision_features)\n",
    "        \n",
    "        return image_embeds\n",
    "    \n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        image_positions: Optional[torch.Tensor] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare inputs by inserting image embeddings into text sequence.\n",
    "        \"\"\"\n",
    "        # Get text embeddings\n",
    "        text_embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        if images is not None:\n",
    "            # Get image embeddings\n",
    "            image_embeds = self.encode_images(images)\n",
    "            \n",
    "            # Replace image token positions with image embeddings\n",
    "            batch_size, seq_len, hidden_dim = text_embeds.shape\n",
    "            num_image_tokens = image_embeds.shape[1]\n",
    "            \n",
    "            # Find image token positions\n",
    "            image_token_mask = (input_ids == self.config.image_token_id)\n",
    "            \n",
    "            # Insert image embeddings\n",
    "            for b in range(batch_size):\n",
    "                image_positions = image_token_mask[b].nonzero(as_tuple=True)[0]\n",
    "                if len(image_positions) > 0:\n",
    "                    start_pos = image_positions[0].item()\n",
    "                    text_embeds[b, start_pos:start_pos + num_image_tokens] = image_embeds[b]\n",
    "        \n",
    "        return {\n",
    "            'inputs_embeds': text_embeds,\n",
    "            'attention_mask': (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "        **kwargs\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        \"\"\"\n",
    "        # Prepare inputs with image embeddings\n",
    "        if images is not None:\n",
    "            inputs = self.prepare_inputs_for_generation(input_ids, images)\n",
    "            inputs_embeds = inputs['inputs_embeds']\n",
    "            \n",
    "            # Forward through LLM with embeddings\n",
    "            outputs = self.llm(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            # Text-only forward\n",
    "            outputs = self.llm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        # Compute auxiliary losses (MoE load balancing)\n",
    "        if self.config.moe_config.enabled and self.training:\n",
    "            total_aux_loss = self._compute_moe_aux_losses()\n",
    "            if outputs.loss is not None:\n",
    "                outputs.loss = outputs.loss + total_aux_loss\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _compute_moe_aux_losses(self) -> torch.Tensor:\n",
    "        \"\"\"Compute and accumulate MoE auxiliary losses\"\"\"\n",
    "        total_loss = 0.0\n",
    "        moe_config = self.config.moe_config\n",
    "        \n",
    "        for name, module in self.llm.named_modules():\n",
    "            if isinstance(module, MoELayer):\n",
    "                # Note: Would need to store router_logits during forward\n",
    "                # This is a simplified version\n",
    "                pass\n",
    "        \n",
    "        return torch.tensor(total_loss, device=next(self.parameters()).device)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text from the model.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        if images is not None:\n",
    "            inputs = self.prepare_inputs_for_generation(input_ids, images)\n",
    "            \n",
    "            return self.llm.generate(\n",
    "                inputs_embeds=inputs['inputs_embeds'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            return self.llm.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                **kwargs\n",
    "            )\n",
    "    \n",
    "    def save_pretrained(self, save_path: str):\n",
    "        \"\"\"Save model weights and config\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Save config\n",
    "        import pickle\n",
    "        with open(os.path.join(save_path, \"config.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.config, f)\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(self.state_dict(), os.path.join(save_path, \"model.pt\"))\n",
    "        \n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_path: str):\n",
    "        \"\"\"Load model from saved weights\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        # Load config\n",
    "        with open(os.path.join(load_path, \"config.pkl\"), \"rb\") as f:\n",
    "            config = pickle.load(f)\n",
    "        \n",
    "        # Create model\n",
    "        model = cls(config)\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(load_path, \"model.pt\"))\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Pixmo-Cap Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixmoCapDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pixmo-Cap dataset for VLM training.\n",
    "    \n",
    "    Features:\n",
    "    - 712K images with dense captions (avg 196 words)\n",
    "    - URL-based image loading with caching\n",
    "    - Multiple transcripts per image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str = \"train\",\n",
    "        max_samples: Optional[int] = None,\n",
    "        image_processor = None,\n",
    "        tokenizer = None,\n",
    "        max_length: int = 2048,\n",
    "        image_size: int = 384,\n",
    "        cache_dir: str = \"./cache/pixmo_images\",\n",
    "        num_image_tokens: int = 256\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.max_length = max_length\n",
    "        self.image_size = image_size\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        \n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load dataset\n",
    "        logger.info(\"Loading Pixmo-Cap dataset...\")\n",
    "        self.dataset = load_dataset(\"allenai/pixmo-cap\", split=split)\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.dataset)} samples from Pixmo-Cap {split}\")\n",
    "        \n",
    "        # Define prompt template\n",
    "        self.prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant that describes images in detail.<|im_end|>\n",
    "<|im_start|>user\n",
    "<image>\n",
    "Describe this image in detail.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{caption}<|im_end|>\"\"\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def _download_image(self, url: str, idx: int) -> Optional[Image.Image]:\n",
    "        \"\"\"Download image with caching\"\"\"\n",
    "        cache_path = self.cache_dir / f\"{idx}.jpg\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_path.exists():\n",
    "            try:\n",
    "                return Image.open(cache_path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                cache_path.unlink(missing_ok=True)\n",
    "        \n",
    "        # Download image\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            \n",
    "            # Cache the image\n",
    "            image.save(cache_path, \"JPEG\", quality=95)\n",
    "            \n",
    "            return image\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to download image {idx}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _process_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Process image for model input\"\"\"\n",
    "        if self.image_processor is not None:\n",
    "            processed = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            return processed.pixel_values.squeeze(0)\n",
    "        else:\n",
    "            # Default preprocessing\n",
    "            from torchvision import transforms\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "            return transform(image)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Get image\n",
    "        image_url = item.get('image_url', '')\n",
    "        image = self._download_image(image_url, idx)\n",
    "        \n",
    "        if image is None:\n",
    "            # Return a dummy sample if image download fails\n",
    "            return self._get_dummy_sample()\n",
    "        \n",
    "        # Process image\n",
    "        pixel_values = self._process_image(image)\n",
    "        \n",
    "        # Get caption\n",
    "        caption = item.get('caption', '')\n",
    "        \n",
    "        # Create full text with prompt\n",
    "        full_text = self.prompt_template.format(caption=caption)\n",
    "        \n",
    "        # Tokenize\n",
    "        if self.tokenizer is not None:\n",
    "            # Replace <image> placeholder with image tokens\n",
    "            image_placeholder = \"<image>\"\n",
    "            image_tokens = \"<|image_pad|>\" * self.num_image_tokens\n",
    "            full_text = full_text.replace(image_placeholder, image_tokens)\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                full_text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding.input_ids.squeeze(0)\n",
    "            attention_mask = encoding.attention_mask.squeeze(0)\n",
    "            \n",
    "            # Create labels (mask prompt, only train on completion)\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Find where assistant response starts\n",
    "            assistant_token = self.tokenizer.encode(\"<|im_start|>assistant\", add_special_tokens=False)\n",
    "            for i in range(len(input_ids) - len(assistant_token)):\n",
    "                if input_ids[i:i+len(assistant_token)].tolist() == assistant_token:\n",
    "                    # Mask everything before the response\n",
    "                    labels[:i+len(assistant_token)] = -100\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels,\n",
    "                'pixel_values': pixel_values\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'pixel_values': pixel_values\n",
    "            }\n",
    "    \n",
    "    def _get_dummy_sample(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return a dummy sample for failed image downloads\"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            'labels': torch.full((self.max_length,), -100, dtype=torch.long),\n",
    "            'pixel_values': torch.zeros(3, self.image_size, self.image_size)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    distributed: bool = False\n",
    ") -> DataLoader:\n",
    "    \"\"\"Create dataloader with optional distributed sampling\"\"\"\n",
    "    sampler = None\n",
    "    if distributed:\n",
    "        sampler = DistributedSampler(dataset, shuffle=shuffle)\n",
    "        shuffle = False\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Training Loop with FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVLMTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for TRM-VLM with FSDP support.\n",
    "    \n",
    "    Features:\n",
    "    - FSDP for multi-GPU training\n",
    "    - Mixed precision (bf16)\n",
    "    - Gradient checkpointing\n",
    "    - Learning rate scheduling\n",
    "    - Wandb logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: TRMVLM,\n",
    "        train_dataset: Dataset,\n",
    "        eval_dataset: Optional[Dataset],\n",
    "        training_config: TrainingConfig,\n",
    "        model_config: TRMVLMConfig\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.training_config = training_config\n",
    "        self.model_config = model_config\n",
    "        \n",
    "        # Initialize accelerator for distributed training\n",
    "        self.accelerator = Accelerator(\n",
    "            mixed_precision=\"bf16\" if training_config.bf16 else None,\n",
    "            gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "            log_with=\"wandb\" if training_config.use_wandb else None,\n",
    "            project_dir=training_config.output_dir\n",
    "        )\n",
    "        \n",
    "        # Setup training\n",
    "        self._setup_training()\n",
    "    \n",
    "    def _setup_training(self):\n",
    "        \"\"\"Setup optimizer, scheduler, and dataloaders\"\"\"\n",
    "        config = self.training_config\n",
    "        \n",
    "        # Create parameter groups with different learning rates\n",
    "        param_groups = [\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() \n",
    "                          if 'vision_encoder' in n and p.requires_grad],\n",
    "                'lr': config.vision_learning_rate,\n",
    "                'name': 'vision_encoder'\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() \n",
    "                          if 'llm' in n and p.requires_grad],\n",
    "                'lr': config.llm_learning_rate,\n",
    "                'name': 'llm'\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() \n",
    "                          if 'projector' in n and p.requires_grad],\n",
    "                'lr': config.learning_rate,\n",
    "                'name': 'projector'\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        # Filter out empty groups\n",
    "        param_groups = [g for g in param_groups if len(list(g['params'])) > 0]\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            param_groups,\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=(0.9, 0.95)\n",
    "        )\n",
    "        \n",
    "        # Dataloader\n",
    "        self.train_dataloader = create_dataloader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config.per_device_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        if self.eval_dataset is not None:\n",
    "            self.eval_dataloader = create_dataloader(\n",
    "                self.eval_dataset,\n",
    "                batch_size=config.per_device_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4\n",
    "            )\n",
    "        else:\n",
    "            self.eval_dataloader = None\n",
    "        \n",
    "        # Calculate total steps\n",
    "        num_update_steps_per_epoch = len(self.train_dataloader) // config.gradient_accumulation_steps\n",
    "        self.total_steps = num_update_steps_per_epoch * config.num_epochs\n",
    "        warmup_steps = int(self.total_steps * config.warmup_ratio)\n",
    "        \n",
    "        # Scheduler\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=self.total_steps\n",
    "        )\n",
    "        \n",
    "        # Prepare for distributed training\n",
    "        self.model, self.optimizer, self.train_dataloader, self.scheduler = self.accelerator.prepare(\n",
    "            self.model, self.optimizer, self.train_dataloader, self.scheduler\n",
    "        )\n",
    "        \n",
    "        if self.eval_dataloader is not None:\n",
    "            self.eval_dataloader = self.accelerator.prepare(self.eval_dataloader)\n",
    "        \n",
    "        # Initialize wandb\n",
    "        if self.training_config.use_wandb and self.accelerator.is_main_process:\n",
    "            self.accelerator.init_trackers(\n",
    "                project_name=config.wandb_project,\n",
    "                config={\n",
    "                    'model_config': str(self.model_config),\n",
    "                    'training_config': str(config)\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        config = self.training_config\n",
    "        \n",
    "        self.model.train()\n",
    "        global_step = 0\n",
    "        best_eval_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(config.num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            pbar = tqdm(\n",
    "                self.train_dataloader,\n",
    "                desc=f\"Epoch {epoch + 1}/{config.num_epochs}\",\n",
    "                disable=not self.accelerator.is_main_process\n",
    "            )\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                with self.accelerator.accumulate(self.model):\n",
    "                    # Forward pass\n",
    "                    outputs = self.model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'],\n",
    "                        images=batch.get('pixel_values')\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    self.accelerator.backward(loss)\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    if self.accelerator.sync_gradients:\n",
    "                        self.accelerator.clip_grad_norm_(\n",
    "                            self.model.parameters(),\n",
    "                            config.max_grad_norm\n",
    "                        )\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Logging\n",
    "                if self.accelerator.sync_gradients:\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    if global_step % config.logging_steps == 0:\n",
    "                        avg_loss = epoch_loss / num_batches\n",
    "                        lr = self.scheduler.get_last_lr()[0]\n",
    "                        \n",
    "                        pbar.set_postfix({\n",
    "                            'loss': f'{avg_loss:.4f}',\n",
    "                            'lr': f'{lr:.2e}'\n",
    "                        })\n",
    "                        \n",
    "                        if self.training_config.use_wandb:\n",
    "                            self.accelerator.log({\n",
    "                                'train/loss': avg_loss,\n",
    "                                'train/lr': lr,\n",
    "                                'train/epoch': epoch,\n",
    "                                'train/global_step': global_step\n",
    "                            })\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    if global_step % config.eval_steps == 0 and self.eval_dataloader is not None:\n",
    "                        eval_loss = self.evaluate()\n",
    "                        \n",
    "                        if eval_loss < best_eval_loss:\n",
    "                            best_eval_loss = eval_loss\n",
    "                            self.save_checkpoint(f\"best_model\")\n",
    "                        \n",
    "                        self.model.train()\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if global_step % config.save_steps == 0:\n",
    "                        self.save_checkpoint(f\"checkpoint-{global_step}\")\n",
    "            \n",
    "            # End of epoch logging\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            logger.info(f\"Epoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_checkpoint(\"final_model\")\n",
    "        \n",
    "        if self.training_config.use_wandb:\n",
    "            self.accelerator.end_training()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(self.eval_dataloader, desc=\"Evaluating\", \n",
    "                         disable=not self.accelerator.is_main_process):\n",
    "            outputs = self.model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels'],\n",
    "                images=batch.get('pixel_values')\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        if self.training_config.use_wandb:\n",
    "            self.accelerator.log({'eval/loss': avg_loss})\n",
    "        \n",
    "        logger.info(f\"Evaluation loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_checkpoint(self, name: str):\n",
    "        \"\"\"Save training checkpoint\"\"\"\n",
    "        save_path = os.path.join(self.training_config.output_dir, name)\n",
    "        \n",
    "        self.accelerator.wait_for_everyone()\n",
    "        \n",
    "        if self.accelerator.is_main_process:\n",
    "            unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "            unwrapped_model.save_pretrained(save_path)\n",
    "            logger.info(f\"Checkpoint saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Comparison with Qwen2.5-VL-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2VLComparison:\n",
    "    \"\"\"\n",
    "    Comparison utilities for benchmarking against Qwen2.5-VL-7B.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.qwen_vl_model = None\n",
    "        self.qwen_vl_processor = None\n",
    "    \n",
    "    def load_qwen_vl(self):\n",
    "        \"\"\"Load Qwen2.5-VL-7B model\"\"\"\n",
    "        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "        \n",
    "        logger.info(\"Loading Qwen2.5-VL-7B...\")\n",
    "        \n",
    "        self.qwen_vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # attn_implementation=\"flash_attention_2\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.qwen_vl_processor = AutoProcessor.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Qwen2.5-VL-7B loaded successfully\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_caption_qwen_vl(\n",
    "        self, \n",
    "        image: Image.Image,\n",
    "        prompt: str = \"Describe this image in detail.\"\n",
    "    ) -> str:\n",
    "        \"\"\"Generate caption using Qwen2.5-VL-7B\"\"\"\n",
    "        if self.qwen_vl_model is None:\n",
    "            self.load_qwen_vl()\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        text = self.qwen_vl_processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = self.qwen_vl_processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        output_ids = self.qwen_vl_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        caption = self.qwen_vl_processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_caption_trm_vlm(\n",
    "        self,\n",
    "        model: TRMVLM,\n",
    "        image: Image.Image,\n",
    "        prompt: str = \"Describe this image in detail.\"\n",
    "    ) -> str:\n",
    "        \"\"\"Generate caption using TRM-VLM\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Process image\n",
    "        if hasattr(model.vision_encoder, 'processor'):\n",
    "            pixel_values = model.vision_encoder.preprocess([image]).to(self.device)\n",
    "        else:\n",
    "            from torchvision import transforms\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((384, 384)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            pixel_values = transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Create prompt\n",
    "        full_prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant that describes images in detail.<|im_end|>\n",
    "<|im_start|>user\n",
    "{\"<|image_pad|>\" * model.config.num_image_tokens}\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = model.tokenizer(\n",
    "            full_prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            images=pixel_values,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        caption = model.tokenizer.decode(\n",
    "            output_ids[0, inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def compare_models(\n",
    "        self,\n",
    "        trm_vlm_model: TRMVLM,\n",
    "        test_images: List[Image.Image],\n",
    "        prompts: Optional[List[str]] = None\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        \"\"\"Compare outputs from both models on test images\"\"\"\n",
    "        if prompts is None:\n",
    "            prompts = [\"Describe this image in detail.\"] * len(test_images)\n",
    "        \n",
    "        results = {\n",
    "            'trm_vlm': [],\n",
    "            'qwen_vl': []\n",
    "        }\n",
    "        \n",
    "        for image, prompt in tqdm(zip(test_images, prompts), total=len(test_images)):\n",
    "            # TRM-VLM caption\n",
    "            trm_caption = self.generate_caption_trm_vlm(trm_vlm_model, image, prompt)\n",
    "            results['trm_vlm'].append(trm_caption)\n",
    "            \n",
    "            # Qwen-VL caption\n",
    "            qwen_caption = self.generate_caption_qwen_vl(image, prompt)\n",
    "            results['qwen_vl'].append(qwen_caption)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_metrics(\n",
    "        self,\n",
    "        predictions: List[str],\n",
    "        references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        try:\n",
    "            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "            from rouge_score import rouge_scorer\n",
    "        except ImportError:\n",
    "            logger.warning(\"nltk or rouge_score not installed. Skipping metrics.\")\n",
    "            return {}\n",
    "        \n",
    "        # BLEU scores\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            ref_tokens = ref.split()\n",
    "            pred_tokens = pred.split()\n",
    "            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(score)\n",
    "        \n",
    "        # ROUGE scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = scorer.score(ref, pred)\n",
    "            for key in rouge_scores:\n",
    "                rouge_scores[key].append(scores[key].fmeasure)\n",
    "        \n",
    "        return {\n",
    "            'bleu': np.mean(bleu_scores),\n",
    "            'rouge1': np.mean(rouge_scores['rouge1']),\n",
    "            'rouge2': np.mean(rouge_scores['rouge2']),\n",
    "            'rougeL': np.mean(rouge_scores['rougeL'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 11: Ablation Study Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblationStudy:\n",
    "    \"\"\"\n",
    "    Framework for running ablation studies on TRM-VLM.\n",
    "    \n",
    "    Ablations:\n",
    "    1. Vision Encoder: SigLIP vs DINOv2 vs Dual\n",
    "    2. MoE: Enabled vs Disabled\n",
    "    3. TRM: Enabled vs Disabled\n",
    "    4. LLM Backbone: 0.5B vs 1.5B vs 3B\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_training_config: TrainingConfig):\n",
    "        self.base_training_config = base_training_config\n",
    "        self.results = {}\n",
    "    \n",
    "    def create_ablation_configs(self) -> Dict[str, TRMVLMConfig]:\n",
    "        \"\"\"Create configurations for each ablation\"\"\"\n",
    "        configs = {}\n",
    "        \n",
    "        # 1. Vision Encoder Ablations\n",
    "        for encoder_type in VisionEncoderType:\n",
    "            name = f\"vision_{encoder_type.value}\"\n",
    "            configs[name] = TRMVLMConfig(\n",
    "                llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "                vision_config=VisionEncoderConfig(encoder_type=encoder_type),\n",
    "                moe_config=MoEConfig(enabled=True),\n",
    "                trm_config=TRMConfig(enabled=True)\n",
    "            )\n",
    "        \n",
    "        # 2. MoE Ablation\n",
    "        configs[\"no_moe\"] = TRMVLMConfig(\n",
    "            llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "            vision_config=VisionEncoderConfig(encoder_type=VisionEncoderType.SIGLIP),\n",
    "            moe_config=MoEConfig(enabled=False),\n",
    "            trm_config=TRMConfig(enabled=True)\n",
    "        )\n",
    "        \n",
    "        # 3. TRM Ablation\n",
    "        configs[\"no_trm\"] = TRMVLMConfig(\n",
    "            llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "            vision_config=VisionEncoderConfig(encoder_type=VisionEncoderType.SIGLIP),\n",
    "            moe_config=MoEConfig(enabled=True),\n",
    "            trm_config=TRMConfig(enabled=False)\n",
    "        )\n",
    "        \n",
    "        # 4. LLM Backbone Ablations\n",
    "        for backbone in [LLMBackboneType.QWEN2_5_0_5B, LLMBackboneType.QWEN2_5_1_5B]:\n",
    "            name = f\"backbone_{backbone.value.split('/')[-1].lower()}\"\n",
    "            configs[name] = TRMVLMConfig(\n",
    "                llm_backbone=backbone,\n",
    "                vision_config=VisionEncoderConfig(encoder_type=VisionEncoderType.SIGLIP),\n",
    "                moe_config=MoEConfig(enabled=True),\n",
    "                trm_config=TRMConfig(enabled=True)\n",
    "            )\n",
    "        \n",
    "        # 5. Baseline (full model)\n",
    "        configs[\"full_model\"] = TRMVLMConfig(\n",
    "            llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "            vision_config=VisionEncoderConfig(encoder_type=VisionEncoderType.SIGLIP),\n",
    "            moe_config=MoEConfig(enabled=True),\n",
    "            trm_config=TRMConfig(enabled=True)\n",
    "        )\n",
    "        \n",
    "        return configs\n",
    "    \n",
    "    def run_single_ablation(\n",
    "        self,\n",
    "        name: str,\n",
    "        config: TRMVLMConfig,\n",
    "        train_dataset: Dataset,\n",
    "        eval_dataset: Dataset\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Run a single ablation experiment\"\"\"\n",
    "        logger.info(f\"Running ablation: {name}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = TRMVLM(config)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        logger.info(f\"  Total params: {total_params:,}\")\n",
    "        logger.info(f\"  Trainable params: {trainable_params:,}\")\n",
    "        \n",
    "        # Update training config\n",
    "        training_config = TrainingConfig(\n",
    "            **{**self.base_training_config.__dict__,\n",
    "               'output_dir': f\"{self.base_training_config.output_dir}/{name}\",\n",
    "               'wandb_run_name': name}\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer = TRMVLMTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            training_config=training_config,\n",
    "            model_config=config\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_loss = trainer.evaluate()\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'final_loss': final_loss,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "    \n",
    "    def run_all_ablations(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        eval_dataset: Dataset,\n",
    "        ablations_to_run: Optional[List[str]] = None\n",
    "    ):\n",
    "        \"\"\"Run all ablation studies\"\"\"\n",
    "        configs = self.create_ablation_configs()\n",
    "        \n",
    "        if ablations_to_run is not None:\n",
    "            configs = {k: v for k, v in configs.items() if k in ablations_to_run}\n",
    "        \n",
    "        for name, config in configs.items():\n",
    "            try:\n",
    "                result = self.run_single_ablation(\n",
    "                    name, config, train_dataset, eval_dataset\n",
    "                )\n",
    "                self.results[name] = result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ablation {name} failed: {e}\")\n",
    "                self.results[name] = {'error': str(e)}\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def summarize_results(self) -> str:\n",
    "        \"\"\"Generate summary of ablation results\"\"\"\n",
    "        summary = \"\\n\" + \"=\"*60 + \"\\n\"\n",
    "        summary += \"ABLATION STUDY RESULTS\\n\"\n",
    "        summary += \"=\"*60 + \"\\n\\n\"\n",
    "        \n",
    "        # Sort by loss\n",
    "        sorted_results = sorted(\n",
    "            [(k, v) for k, v in self.results.items() if 'error' not in v],\n",
    "            key=lambda x: x[1].get('final_loss', float('inf'))\n",
    "        )\n",
    "        \n",
    "        for name, result in sorted_results:\n",
    "            summary += f\"{name}:\\n\"\n",
    "            summary += f\"  Loss: {result['final_loss']:.4f}\\n\"\n",
    "            summary += f\"  Params: {result['total_params']:,}\\n\"\n",
    "            summary += f\"  Trainable: {result['trainable_params']:,}\\n\\n\"\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 12: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for TRM-VLM training.\n",
    "    \"\"\"\n",
    "    # 1. Create configurations\n",
    "    model_config = TRMVLMConfig(\n",
    "        llm_backbone=LLMBackboneType.QWEN2_5_3B,\n",
    "        vision_config=VisionEncoderConfig(\n",
    "            encoder_type=VisionEncoderType.SIGLIP,\n",
    "            freeze_vision=True\n",
    "        ),\n",
    "        moe_config=MoEConfig(\n",
    "            enabled=True,\n",
    "            num_experts=8,\n",
    "            num_experts_per_token=2,\n",
    "            num_shared_experts=1\n",
    "        ),\n",
    "        trm_config=TRMConfig(\n",
    "            enabled=True,\n",
    "            n_recursions=4,\n",
    "            t_cycles=2\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        num_epochs=3,\n",
    "        per_device_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=1e-4,\n",
    "        llm_learning_rate=2e-5,\n",
    "        warmup_ratio=0.03,\n",
    "        output_dir=\"./outputs/trm_vlm_full\",\n",
    "        use_wandb=True,\n",
    "        wandb_project=\"trm-vlm\"\n",
    "    )\n",
    "    \n",
    "    print(\"Configuration Summary:\")\n",
    "    print(f\"  LLM: {model_config.llm_backbone.value}\")\n",
    "    print(f\"  Vision: {model_config.vision_config.encoder_type.value}\")\n",
    "    print(f\"  MoE: {model_config.moe_config.enabled} ({model_config.moe_config.num_experts} experts)\")\n",
    "    print(f\"  TRM: {model_config.trm_config.enabled}\")\n",
    "    print(f\"  Effective batch size: {training_config.per_device_batch_size * training_config.gradient_accumulation_steps * 2}\")\n",
    "    \n",
    "    return model_config, training_config\n",
    "\n",
    "model_config, training_config = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating TRM-VLM model...\")\n",
    "model = TRMVLM(model_config)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "print(f\"  Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating Pixmo-Cap datasets...\")\n",
    "\n",
    "# Use smaller subset for testing\n",
    "MAX_TRAIN_SAMPLES = 10000  # Set to None for full dataset\n",
    "MAX_EVAL_SAMPLES = 500\n",
    "\n",
    "train_dataset = PixmoCapDataset(\n",
    "    split=\"train\",\n",
    "    max_samples=MAX_TRAIN_SAMPLES,\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=model_config.max_seq_length,\n",
    "    num_image_tokens=model_config.num_image_tokens\n",
    ")\n",
    "\n",
    "# For evaluation, use a subset\n",
    "eval_dataset = PixmoCapDataset(\n",
    "    split=\"train\",  # Using train split subset for eval since no explicit eval split\n",
    "    max_samples=MAX_EVAL_SAMPLES,\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=model_config.max_seq_length,\n",
    "    num_image_tokens=model_config.num_image_tokens\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = TRMVLMTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    training_config=training_config,\n",
    "    model_config=model_config\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Total training steps: {trainer.total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 13: Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images for comparison\n",
    "def load_test_images(num_images: int = 10) -> List[Image.Image]:\n",
    "    \"\"\"Load test images from the dataset\"\"\"\n",
    "    test_dataset = PixmoCapDataset(\n",
    "        split=\"train\",\n",
    "        max_samples=num_images * 2  # Load extra in case some fail\n",
    "    )\n",
    "    \n",
    "    images = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        item = test_dataset.dataset[i]\n",
    "        image_url = item.get('image_url', '')\n",
    "        image = test_dataset._download_image(image_url, i + 100000)  # Different cache\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "        if len(images) >= num_images:\n",
    "            break\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load test images\n",
    "test_images = load_test_images(10)\n",
    "print(f\"Loaded {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = Qwen2VLComparison()\n",
    "\n",
    "# Generate captions from both models\n",
    "results = comparison.compare_models(\n",
    "    trm_vlm_model=model,\n",
    "    test_images=test_images\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CAPTION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (trm_cap, qwen_cap) in enumerate(zip(results['trm_vlm'], results['qwen_vl'])):\n",
    "    print(f\"\\nImage {i+1}:\")\n",
    "    print(f\"  TRM-VLM: {trm_cap[:200]}...\" if len(trm_cap) > 200 else f\"  TRM-VLM: {trm_cap}\")\n",
    "    print(f\"  Qwen-VL: {qwen_cap[:200]}...\" if len(qwen_cap) > 200 else f\"  Qwen-VL: {qwen_cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i, (ax, image) in enumerate(zip(axes.flat, test_images[:10])):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"Image {i+1}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Test Images for Comparison\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 14: Run Ablation Studies (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ablation studies (this will take a long time)\n",
    "RUN_ABLATIONS = False  # Set to True to run ablations\n",
    "\n",
    "if RUN_ABLATIONS:\n",
    "    ablation_training_config = TrainingConfig(\n",
    "        num_epochs=1,  # Reduced for ablation\n",
    "        per_device_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-4,\n",
    "        output_dir=\"./outputs/ablations\",\n",
    "        use_wandb=True,\n",
    "        wandb_project=\"trm-vlm-ablations\"\n",
    "    )\n",
    "    \n",
    "    ablation_study = AblationStudy(ablation_training_config)\n",
    "    \n",
    "    # Run specific ablations\n",
    "    ablations_to_run = [\n",
    "        \"vision_siglip\",\n",
    "        \"vision_dinov2\",\n",
    "        \"no_moe\",\n",
    "        \"no_trm\",\n",
    "        \"full_model\"\n",
    "    ]\n",
    "    \n",
    "    # Create smaller datasets for ablations\n",
    "    ablation_train = PixmoCapDataset(\n",
    "        split=\"train\",\n",
    "        max_samples=5000,\n",
    "        max_length=1024\n",
    "    )\n",
    "    ablation_eval = PixmoCapDataset(\n",
    "        split=\"train\",\n",
    "        max_samples=500,\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    # Run ablations\n",
    "    results = ablation_study.run_all_ablations(\n",
    "        train_dataset=ablation_train,\n",
    "        eval_dataset=ablation_eval,\n",
    "        ablations_to_run=ablations_to_run\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(ablation_study.summarize_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 15: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = \"./outputs/trm_vlm_final\"\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Save training metrics\n",
    "import json\n",
    "\n",
    "metrics = {\n",
    "    'model_config': str(model_config),\n",
    "    'training_config': str(training_config),\n",
    "    'total_params': total_params,\n",
    "    'trainable_params': trainable_params\n",
    "}\n",
    "\n",
    "with open(f\"{save_path}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook implements a complete TRM-enhanced Vision-Language Model with:\n",
    "\n",
    "### Architecture Components\n",
    "1. **Vision Encoders**: SigLIP, DINOv2, or Dual (configurable)\n",
    "2. **Projector**: 2-layer MLP or Perceiver Resampler\n",
    "3. **LLM Backbone**: Qwen2.5 (0.5B/1.5B/3B/7B configurable)\n",
    "4. **MoE Enhancement**: 8 experts with top-2 routing + shared experts\n",
    "5. **TRM Reasoning**: Recursive refinement on last N layers\n",
    "\n",
    "### Training Features\n",
    "- FSDP for 2x H200 GPUs\n",
    "- Mixed precision (bf16)\n",
    "- Gradient checkpointing\n",
    "- Wandb logging\n",
    "- Pixmo-Cap dataset (712K images)\n",
    "\n",
    "### Evaluation\n",
    "- Comparison with Qwen2.5-VL-7B\n",
    "- BLEU/ROUGE metrics\n",
    "- Ablation study framework\n",
    "\n",
    "### Key Hyperparameters\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Effective Batch Size | 64 |\n",
    "| Learning Rate | 1e-4 (projector), 2e-5 (LLM) |\n",
    "| MoE Experts | 8 (top-2) |\n",
    "| TRM Recursions | 4 |\n",
    "| TRM Cycles | 2 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
