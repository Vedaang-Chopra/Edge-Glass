# ğŸ§  Multimodal Alignment Architecture: Complete Guide

## Overview

This document explains the complete multimodal alignment system with Perceiver Resampler, covering:
1. **What** each component does
2. **Why** it's designed this way
3. **How** the data flows through the system

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 INPUT LAYER                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   ğŸ–¼ï¸ IMAGE              ğŸ”Š AUDIO                ğŸ“ TEXT                         â”‚
â”‚   (224Ã—224Ã—3)           (waveform)              (string)                        â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                        â”‚
         â–¼                       â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FROZEN ENCODERS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   CLIP ViT-B/32         Whisper-base            Sentence-BERT                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚   â€¢ 12 layers           â€¢ 6 layers              â€¢ 6 layers                      â”‚
â”‚   â€¢ 768 hidden dim      â€¢ 512 hidden dim        â€¢ 384 hidden dim                â”‚
â”‚   â€¢ 86M params          â€¢ 74M params            â€¢ 22M params                    â”‚
â”‚                                                                                  â”‚
â”‚   Output: (B,50,768)    Output: (B,1500,512)    Output: (B,L,384)               â”‚
â”‚   50 patch tokens       1500 audio frames       L text tokens                   â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                        â”‚
         â–¼                       â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRAINABLE ADAPTERS                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   Vision Adapter        Audio Adapter           Text Adapter                    â”‚
â”‚   768 â†’ 512             512 â†’ 512               384 â†’ 512                       â”‚
â”‚                                                                                  â”‚
â”‚   LayerNorm             LayerNorm               LayerNorm                       â”‚
â”‚   Linear(768,1536)      Linear(512,1024)        Linear(384,768)                 â”‚
â”‚   GELU                  GELU                    GELU                            â”‚
â”‚   Linear(1536,512)      Linear(1024,512)        Linear(768,512)                 â”‚
â”‚                                                                                  â”‚
â”‚   Output: (B,50,512)    Output: (B,1500,512)    Output: (B,L,512)               â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PERCEIVER RESAMPLER                                       â”‚
â”‚                    (Shared across all modalities)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                     LEARNED LATENT QUERIES                          â”‚       â”‚
â”‚   â”‚                        (64 Ã— 512)                                   â”‚       â”‚
â”‚   â”‚                                                                     â”‚       â”‚
â”‚   â”‚   These are trainable parameters that learn to "ask questions"     â”‚       â”‚
â”‚   â”‚   about the input. Think of them as 64 specialized "experts"       â”‚       â”‚
â”‚   â”‚   that each focus on extracting different types of information.    â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                                  â”‚
â”‚   FOR EACH LAYER (Ã—4):                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ 1. CROSS-ATTENTION                                                  â”‚       â”‚
â”‚   â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚       â”‚
â”‚   â”‚    Q = Latent Queries (64 tokens)                                   â”‚       â”‚
â”‚   â”‚    K = V = Input Tokens (50/1500/L tokens)                          â”‚       â”‚
â”‚   â”‚                                                                     â”‚       â”‚
â”‚   â”‚    Latents "read" from input, extracting relevant information       â”‚       â”‚
â”‚   â”‚    Complexity: O(64 Ã— T) - efficient for long sequences!           â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚   â”‚ 2. SELF-ATTENTION                                                   â”‚       â”‚
â”‚   â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚       â”‚
â”‚   â”‚    Q = K = V = Latents (64 tokens)                                  â”‚       â”‚
â”‚   â”‚                                                                     â”‚       â”‚
â”‚   â”‚    Latents communicate with each other to share information         â”‚       â”‚
â”‚   â”‚    Complexity: O(64Â²) - constant regardless of input length        â”‚       â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚   â”‚ 3. FEED-FORWARD NETWORK                                             â”‚       â”‚
â”‚   â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚       â”‚
â”‚   â”‚    FFN(x) = Linear(GELU(Linear(x)))                                 â”‚       â”‚
â”‚   â”‚                                                                     â”‚       â”‚
â”‚   â”‚    Non-linear transformation applied to each latent independently   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                                  â”‚
â”‚   Output: (B, 64, 512) - FIXED SIZE regardless of input!                        â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                â”‚                â”‚
                    â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALIGNMENT PROJECTOR  â”‚ â”‚   LLM PROJECTOR   â”‚ â”‚   RAW LATENTS         â”‚
â”‚  (For Retrieval)      â”‚ â”‚   (For Generation)â”‚ â”‚   (For Analysis)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       â”‚ â”‚                   â”‚ â”‚                       â”‚
â”‚  Mean Pool + Linear   â”‚ â”‚  Linear           â”‚ â”‚  Direct output        â”‚
â”‚  (B,64,512)â†’(B,512)   â”‚ â”‚  512 â†’ 1536       â”‚ â”‚  (B,64,512)           â”‚
â”‚                       â”‚ â”‚  (B,64,1536)      â”‚ â”‚                       â”‚
â”‚  L2 Normalize         â”‚ â”‚                   â”‚ â”‚  Use for:             â”‚
â”‚                       â”‚ â”‚  Becomes LLM      â”‚ â”‚  â€¢ Visualization      â”‚
â”‚  Use for:             â”‚ â”‚  prefix tokens    â”‚ â”‚  â€¢ Probing            â”‚
â”‚  â€¢ Retrieval          â”‚ â”‚                   â”‚ â”‚  â€¢ Debugging          â”‚
â”‚  â€¢ Classification     â”‚ â”‚  Use for:         â”‚ â”‚                       â”‚
â”‚  â€¢ Similarity         â”‚ â”‚  â€¢ Captioning     â”‚ â”‚                       â”‚
â”‚                       â”‚ â”‚  â€¢ VQA            â”‚ â”‚                       â”‚
â”‚                       â”‚ â”‚  â€¢ ASR            â”‚ â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Why Perceiver Resampler?

### Problem: Variable-Length Sequences

Different modalities produce different sequence lengths:
- **Image**: 50 patches (7Ã—7 grid + CLS)
- **Audio**: ~1500 frames (30 seconds at 50Hz)
- **Text**: Variable (depends on sentence length)

This creates problems:
1. Can't directly compare embeddings
2. Self-attention is O(TÂ²) - expensive for long audio
3. LLMs expect fixed-size inputs

### Solution: Perceiver Resampler

The Perceiver uses **learned queries** to compress any input to fixed size:

```
Input: Variable length T          Output: Fixed length K (e.g., 64)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image:  50 patches      â”‚       â”‚                         â”‚
â”‚ Audio:  1500 frames     â”‚  â”€â”€â”€â–º â”‚   64 latent vectors     â”‚
â”‚ Text:   32 tokens       â”‚       â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Cross-Attention Achieves This

```
Latent Queries (64 Ã— 512):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ qâ‚ â”‚ qâ‚‚ â”‚ qâ‚ƒ â”‚ qâ‚„ â”‚ ... â”‚qâ‚†â‚„â”‚  â† Learned "questions"
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
  â”‚    â”‚    â”‚    â”‚         â”‚
  â–¼    â–¼    â–¼    â–¼         â–¼
  
Input Tokens (T Ã— 512):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ tâ‚ â”‚ tâ‚‚ â”‚ tâ‚ƒ â”‚ tâ‚„ â”‚ ... â”‚ tâ‚œâ”‚  â† Encoder output
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

Cross-Attention:
  â€¢ Each query attends to ALL input tokens
  â€¢ Attention weights determine "how much" to read from each token
  â€¢ Output: 64 latent vectors, each a weighted combination of inputs
```

### Benefits

| Aspect | Without Perceiver | With Perceiver |
|--------|-------------------|----------------|
| Memory | O(TÂ²) for self-attn | O(KÃ—T) cross-attn |
| Output Size | Variable | Fixed (64) |
| Long Audio | Very expensive | Efficient |
| Modality Comparison | Difficult | Easy |
| LLM Integration | Need pooling | Natural prefix |

---

## ğŸ“Š Loss Functions Explained

### 1. Contrastive Loss (InfoNCE / CLIP Loss)

**Goal**: Bring matching pairs together, push non-matching apart.

```
Similarity Matrix (B Ã— B):

              Text Embeddings
           tâ‚    tâ‚‚    tâ‚ƒ    tâ‚„
         â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    iâ‚   â”‚ 0.9 â”‚ 0.1 â”‚ 0.2 â”‚ 0.1 â”‚  â† Should be highest on diagonal
Image    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
Embs iâ‚‚  â”‚ 0.2 â”‚ 0.8 â”‚ 0.1 â”‚ 0.3 â”‚
         â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    iâ‚ƒ   â”‚ 0.1 â”‚ 0.2 â”‚ 0.85â”‚ 0.1 â”‚
         â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    iâ‚„   â”‚ 0.3 â”‚ 0.1 â”‚ 0.2 â”‚ 0.7 â”‚
         â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Loss = CrossEntropy(logits, diagonal_labels)
     = -log(exp(sim[i,i]) / Î£â±¼ exp(sim[i,j]))
```

**Temperature** (Ï„ = 0.07): Controls sharpness of softmax
- Lower Ï„ â†’ Sharper distribution â†’ Harder negatives
- Higher Ï„ â†’ Softer distribution â†’ Easier training

### 2. Matryoshka Representation Learning (MRL)

**Goal**: Pack important information in early dimensions.

```
Full embedding: [dâ‚, dâ‚‚, dâ‚ƒ, dâ‚„, ..., dâ‚…â‚â‚‚]
                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         512 dimensions

MRL trains at multiple truncations:
  â€¢ dims[:64]  â†’ Lossâ‚   (hardest, most compressed)
  â€¢ dims[:128] â†’ Lossâ‚‚
  â€¢ dims[:256] â†’ Lossâ‚ƒ
  â€¢ dims[:512] â†’ Lossâ‚„   (full, easiest)

Total Loss = (Lossâ‚ + Lossâ‚‚ + Lossâ‚ƒ + Lossâ‚„) / 4
```

**Benefits**:
- **Flexible deployment**: Use 64 dims for fast search, 512 for accuracy
- **Better gradients**: Multiple objectives = richer training signal
- **Implicit curriculum**: Small dims are harder, provide challenge

---

## ğŸ¯ Training Strategy

### Phase 1: Multimodal Alignment

**Objective**: Create unified embedding space for all modalities.

```python
# What's trained
trainable = [
    vision_adapter,    # 768 â†’ 512
    audio_adapter,     # 512 â†’ 512
    text_adapter,      # 384 â†’ 512
    perceiver,         # Shared resampler
    alignment_proj,    # Latents â†’ aligned embeddings
]

# What's frozen
frozen = [
    clip_encoder,      # ~86M params
    whisper_encoder,   # ~74M params
    text_encoder,      # ~22M params
]

# Loss
loss = mrl_weight * matryoshka_loss(z_vision, z_text) + 
       clip_weight * contrastive_loss(z_vision, z_text)
```

**Data**: Image-text pairs (COCO, Conceptual Captions) + Audio-text pairs (AudioCaps)

### Phase 2: LLM Integration

**Objective**: Enable LLM to understand multimodal inputs.

```python
# What's trained
trainable = [
    llm_projector,     # 512 â†’ D_llm
    # Optional: LoRA adapters on LLM
]

# What's frozen
frozen = [
    everything_from_phase1,  # Keep alignment intact
    llm_base_weights,        # Keep language abilities
]

# Loss
loss = language_modeling_loss(
    input=concat(multimodal_prefix, text_tokens),
    target=text_tokens
)
```

---

## ğŸ” Component Deep Dive

### Modality Adapters

**Purpose**: Bridge dimension gap between encoders and Perceiver.

```python
class MLPAdapter:
    """
    Why MLP instead of Linear?
    
    Linear: y = Wx + b
      â€¢ Fast, but limited expressivity
      â€¢ Okay for small dimension changes
    
    MLP: y = Wâ‚‚(GELU(Wâ‚(LN(x))))
      â€¢ More expressive transformation
      â€¢ LayerNorm stabilizes training
      â€¢ GELU adds non-linearity
      â€¢ Better for cross-domain mapping
    """
    
    def __init__(self, in_dim, out_dim, hidden_factor=2.0):
        hidden = int(in_dim * hidden_factor)
        self.net = nn.Sequential(
            nn.LayerNorm(in_dim),      # Normalize input
            nn.Linear(in_dim, hidden),  # Expand
            nn.GELU(),                  # Non-linearity
            nn.Dropout(0.1),            # Regularization
            nn.Linear(hidden, out_dim), # Project to target
        )
```

### Perceiver Layer

**Purpose**: Extract and refine information from input.

```python
class PerceiverLayer:
    """
    Three-step process per layer:
    
    1. Cross-Attention: READ from input
       - Latents query the input tokens
       - Each latent decides what to pay attention to
       - Information flows: Input â†’ Latents
    
    2. Self-Attention: COMMUNICATE among latents
       - Latents share information with each other
       - Helps coordinate what each latent represents
       - Information flows: Latents â†” Latents
    
    3. FFN: TRANSFORM latent representations
       - Non-linear transformation
       - Each latent processed independently
       - Adds model capacity
    """
    
    def forward(self, latents, tokens, mask):
        # Step 1: Cross-attention
        latents = latents + self.cross_attn(
            q=latents,      # 64 queries
            kv=tokens,      # T keys/values
            mask=mask
        )
        
        # Step 2: Self-attention
        latents = latents + self.self_attn(
            q=latents,
            kv=latents      # Same source
        )
        
        # Step 3: FFN
        latents = latents + self.ffn(self.ln(latents))
        
        return latents
```

### Number of Layers

**How many Perceiver layers?**

| Layers | Quality | Speed | Use Case |
|--------|---------|-------|----------|
| 1-2 | Lower | Fast | Quick prototyping |
| 4 | Good | Balanced | **Recommended** |
| 6-8 | Best | Slow | Maximum quality |

More layers = more refinement of latents, but diminishing returns.

---

## ğŸ“ˆ Data Flow Example

Let's trace an image through the system:

```
INPUT: Photo of a golden retriever playing fetch in a park

Step 1: CLIP Encoding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image (224Ã—224Ã—3) 
  â†’ Patch embed (196 patches + CLS = 197 tokens)
  â†’ 12 Transformer layers
  â†’ Output: (1, 50, 768)  # Taking 50 patches
  
  Patches encode: [grass, dog_head, dog_body, ball, sky, trees, ...]

Step 2: Vision Adapter
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(1, 50, 768) â†’ MLP â†’ (1, 50, 512)

  Transforms CLIP features to Perceiver dimension
  Learns task-relevant transformations

Step 3: Perceiver Resampler (4 layers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initialize: 64 learned latent queries

Layer 1:
  Cross-Attn: Latents attend to patches
    - Query 1 might focus on "animal" patches
    - Query 2 might focus on "action" patches
    - Query 3 might focus on "environment" patches
  Self-Attn: Latents share findings
  FFN: Refine representations

Layer 2-4: Continue refining...

Output: (1, 64, 512)
  - 64 latent vectors, each capturing different aspects
  - Some latents: "golden retriever"
  - Some latents: "playing/running"
  - Some latents: "park setting"
  - Some latents: "ball/toy"

Step 4a: For Retrieval
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(1, 64, 512) â†’ MeanPool â†’ (1, 512) â†’ L2 Norm

Final embedding captures: "golden retriever playing fetch in park"
Can compare with text embedding: "A dog playing in the grass"

Step 4b: For Generation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(1, 64, 512) â†’ Linear â†’ (1, 64, 1536)

64 prefix tokens for LLM, each carrying different visual information
LLM generates: "A golden retriever is playing fetch with a ball 
                in a sunny park with green grass."
```

---

## ğŸ› ï¸ Implementation Tips

### 1. Start Simple, Then Add Complexity

```python
# Week 1: Linear adapters, 2 Perceiver layers
config = Config(
    num_perceiver_layers=2,
    adapter_type='linear',
)

# Week 2: MLP adapters, 4 Perceiver layers
config = Config(
    num_perceiver_layers=4,
    adapter_type='mlp',
)

# Week 3: Add MRL loss
config = Config(
    mrl_dims=(64, 128, 256, 512),
    mrl_weight=1.0,
)
```

### 2. Monitor These Metrics

```python
# During training
metrics = {
    'loss': total_loss,
    'loss_mrl': mrl_loss,
    'loss_clip': clip_loss,
    'alignment': compute_alignment(z_v, z_t),  # Should decrease
    'uniformity': compute_uniformity(z_v),      # Should decrease
}

# During evaluation
eval_metrics = {
    'R@1': recall_at_1,     # Primary metric
    'R@5': recall_at_5,
    'R@10': recall_at_10,
    'MRR': mean_reciprocal_rank,
}
```

### 3. Debugging Checklist

```
â–¡ Check embedding norms (should be ~1 after L2 norm)
â–¡ Check similarity distribution (diagonal should be highest)
â–¡ Check gradient norms (should be stable, not exploding/vanishing)
â–¡ Check attention weights (should be meaningful, not uniform)
â–¡ Check latent diversity (latents should specialize)
```

---

## ğŸ“š References

1. **Perceiver**: Jaegle et al., "Perceiver: General Perception with Iterative Attention" (2021)
2. **CLIP**: Radford et al., "Learning Transferable Visual Models From Natural Language Supervision" (2021)
3. **Flamingo**: Alayrac et al., "Flamingo: a Visual Language Model for Few-Shot Learning" (2022)
4. **ImageBind**: Girdhar et al., "ImageBind: One Embedding Space To Bind Them All" (2023)
5. **Matryoshka**: Kusupati et al., "Matryoshka Representation Learning" (2022)

---

## ğŸ“ Summary

| Component | Purpose | Input â†’ Output |
|-----------|---------|----------------|
| **Frozen Encoders** | Extract rich features | Raw â†’ (B, T, D_enc) |
| **Adapters** | Bridge to Perceiver | (B, T, D_enc) â†’ (B, T, 512) |
| **Perceiver** | Compress to fixed size | (B, T, 512) â†’ (B, 64, 512) |
| **Alignment Proj** | For retrieval | (B, 64, 512) â†’ (B, 512) |
| **LLM Proj** | For generation | (B, 64, 512) â†’ (B, 64, D_llm) |

**Key Insight**: The Perceiver is the bottleneck that enables efficient, unified multimodal representations regardless of input modality or sequence length.
