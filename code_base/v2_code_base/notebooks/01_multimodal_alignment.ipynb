{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Alignment Training\n",
    "\n",
    "This notebook demonstrates a clean, modular implementation of:\n",
    "\n",
    "1. **Phase 1**: Vision-Text Alignment (CLIP-style contrastive learning)\n",
    "2. **Phase 2**: Connect aligned embeddings to LLM decoder\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Phase 1: Alignment\n",
    "==================\n",
    "Vision Encoder (CLIP, frozen) → MLP Adapter (trainable) → z_vision\n",
    "Text Encoder (frozen)         → MLP Adapter (trainable) → z_text\n",
    "                                      ↓\n",
    "                              Contrastive Loss (MRL + CLIP)\n",
    "\n",
    "Phase 2: LLM Integration\n",
    "========================\n",
    "z_vision → Vision-to-LLM Projector (trainable) → prefix tokens\n",
    "                                                      ↓\n",
    "                                               LLM Decoder → Generated text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "# Add module path if needed\n",
    "# sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from imports.core import AlignmentConfig, get_device, set_seed\n",
    "\n",
    "# Create configuration\n",
    "cfg = AlignmentConfig(\n",
    "    # Models\n",
    "    vision_model_name=\"openai/clip-vit-base-patch32\",\n",
    "    text_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    llm_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # Use smaller for testing\n",
    "    \n",
    "    # Architecture\n",
    "    d_align=512,\n",
    "    adapter_hidden_factor=2.0,\n",
    "    dropout=0.1,\n",
    "    \n",
    "    # Training\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Loss\n",
    "    mrl_dims=(128, 256, 512),\n",
    "    mrl_temperature=0.07,\n",
    "    clip_temperature=0.07,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    log_every=20,\n",
    ")\n",
    "\n",
    "# Set device and dtype\n",
    "cfg.device = get_device()\n",
    "cfg.dtype = torch.float32  # Use float32 for stability (float16 for GPU if memory is tight)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Dtype: {cfg.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Vision-Text Alignment\n",
    "\n",
    "### 1.1 Create the Alignment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "[TextEncoder] Loaded sentence-transformers/all-MiniLM-L6-v2, hidden_size=384\n",
      "[VisionTextAligner] d_vision=768, d_text=384, d_align=512\n",
      "\n",
      "Parameter count:\n",
      "  Total: 112,829,056\n",
      "  Trainable: 2,659,840\n",
      "  Frozen: 110,169,216\n"
     ]
    }
   ],
   "source": [
    "from imports.core import VisionTextAligner, count_parameters\n",
    "\n",
    "# Create alignment model\n",
    "model = VisionTextAligner(cfg)\n",
    "\n",
    "# Count parameters\n",
    "params = count_parameters(model)\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  Total: {params['total']:,}\")\n",
    "print(f\"  Trainable: {params['trainable']:,}\")\n",
    "print(f\"  Frozen: {params['frozen']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Test with a Quick Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test images\n",
      "Image sizes: [(200, 1), (200, 2)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load a test image\n",
    "def load_test_image(url: str) -> Image.Image:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Test images\n",
    "test_urls = [\n",
    "    \"https://picsum.photos/id/1/200/1\",\n",
    "    \"https://picsum.photos/id/2/200/2\",\n",
    "]\n",
    "\n",
    "test_images = [load_test_image(url) for url in test_urls]\n",
    "test_texts = [\n",
    "    \"A cat sitting and looking at the camera\",\n",
    "    \"A yellow labrador retriever dog\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_images)} test images\")\n",
    "print(f\"Image sizes: {[img.size for img in test_images]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3732\n",
      "MRL Loss: 0.6922\n",
      "CLIP Loss: 0.6811\n",
      "z_vision shape: torch.Size([2, 512])\n",
      "z_text shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Quick forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(test_images, test_texts)\n",
    "\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"MRL Loss: {output['loss_mrl']:.4f}\")\n",
    "print(f\"CLIP Loss: {output['loss_clip']:.4f}\")\n",
    "print(f\"z_vision shape: {output['z_vision'].shape}\")\n",
    "print(f\"z_text shape: {output['z_text'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Dataset\n",
    "\n",
    "You can use:\n",
    "- **Option A**: Pre-extracted features (faster, uses FeatureDataset)\n",
    "- **Option B**: On-the-fly loading from HuggingFace (more flexible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PIXMO dataset...\n",
      "Dataset size: 1000\n",
      "Columns: ['image_url', 'caption', 'transcripts']\n",
      "[ImageTextDataset] Using columns: image=image_url, text=caption\n"
     ]
    }
   ],
   "source": [
    "# Option B: Load from HuggingFace dataset (on-the-fly)\n",
    "from datasets import load_dataset\n",
    "from imports.data import ImageTextDataset, create_dataloader, collate_images\n",
    "\n",
    "print(\"Loading PIXMO dataset...\")\n",
    "\n",
    "# Load a small subset for quick testing\n",
    "# You can use other datasets like: allenai/pixmo-cap, conceptual_captions, etc.\n",
    "try:\n",
    "    hf_dataset = load_dataset(\n",
    "        \"allenai/pixmo-cap\",\n",
    "        split=\"train\",\n",
    "        # trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Take a subset for quick testing\n",
    "    subset_size = 1000\n",
    "    if len(hf_dataset) > subset_size:\n",
    "        hf_dataset = hf_dataset.shuffle(seed=cfg.seed).select(range(subset_size))\n",
    "    \n",
    "    print(f\"Dataset size: {len(hf_dataset)}\")\n",
    "    print(f\"Columns: {hf_dataset.column_names}\")\n",
    "    \n",
    "    # Create dataset wrapper\n",
    "    train_dataset = ImageTextDataset(\n",
    "        hf_dataset,\n",
    "        image_column=\"image_url\",\n",
    "        text_column=\"caption\",  # or \"caption\" depending on dataset\n",
    "    )\n",
    "    \n",
    "    USE_HF_DATASET = True\n",
    "except Exception as e:\n",
    "    print(f\"Could not load HF dataset: {e}\")\n",
    "    print(\"\\nFalling back to synthetic data for demo...\")\n",
    "    USE_HF_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fallback: Create synthetic dataset for demo\n",
    "# if not USE_HF_DATASET:\n",
    "#     from data import SimpleImageTextDataset\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Create random images and dummy captions\n",
    "#     n_samples = 200\n",
    "    \n",
    "#     synthetic_images = [\n",
    "#         Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "#         for _ in range(n_samples)\n",
    "#     ]\n",
    "#     synthetic_texts = [f\"This is synthetic image number {i}\" for i in range(n_samples)]\n",
    "    \n",
    "#     train_dataset = SimpleImageTextDataset(synthetic_images, synthetic_texts)\n",
    "#     print(f\"Created synthetic dataset with {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 31\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "train_loader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for debugging, increase for speed\n",
    "    collate_fn=collate_images,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train Phase 1 (Alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training\n",
      "  Epochs: 3\n",
      "  Train batches: 31\n",
      "  LR: 0.0001\n",
      "  Device: cuda\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057450f621e74a93930bc4aa1b63c956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 - Train Loss: 6.2982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9469813c16e4aef943cb1a9ea4bc692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imports.train import train_alignment\n",
    "\n",
    "# Train!\n",
    "history = train_alignment(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,  # Add validation loader if you have one\n",
    "    num_epochs=cfg.num_epochs,\n",
    "    log_every=cfg.log_every,\n",
    "    save_dir=\"./checkpoints/phase1\",\n",
    "    use_features=False,  # We're using on-the-fly images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "if history[\"R@1\"]:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"R@1\"], label=\"R@1\", marker='o')\n",
    "    plt.plot(history[\"R@5\"], label=\"R@5\", marker='s')\n",
    "    plt.plot(history[\"R@10\"], label=\"R@10\", marker='^')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Retrieval Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import compute_retrieval_metrics, l2_normalize\n",
    "\n",
    "# Encode our test images and texts\n",
    "with torch.no_grad():\n",
    "    z_img = model.encode_vision(test_images)\n",
    "    z_txt = model.encode_text(test_texts)\n",
    "\n",
    "# Compute similarity\n",
    "z_img_norm = l2_normalize(z_img)\n",
    "z_txt_norm = l2_normalize(z_txt)\n",
    "sims = z_img_norm @ z_txt_norm.T\n",
    "\n",
    "print(\"Similarity matrix (images × texts):\")\n",
    "print(sims.cpu().numpy().round(3))\n",
    "print(\"\\nExpected: High values on diagonal (matching pairs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: LLM Integration\n",
    "\n",
    "Now we connect the aligned vision embeddings to an LLM decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.llm_integration import LLMConfig, MultimodalLLM\n",
    "\n",
    "# Configure LLM integration\n",
    "llm_cfg = LLMConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # Use smaller for testing\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    num_prefix_tokens=8,\n",
    "    freeze_llm=True,  # Phase 2a: only train projector\n",
    ")\n",
    "\n",
    "# Create multimodal model\n",
    "mm_model = MultimodalLLM(\n",
    "    aligner=model,\n",
    "    llm_config=llm_cfg,\n",
    ")\n",
    "\n",
    "# Count trainable params\n",
    "trainable = sum(p.numel() for p in mm_model.get_trainable_params())\n",
    "print(f\"\\nPhase 2 trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Test Generation (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before any Phase 2 training\n",
    "print(\"Testing generation (before Phase 2 training)...\\n\")\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    output = mm_model.generate(\n",
    "        images=[img],\n",
    "        prompt=\"Describe this image in detail:\",\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"  Ground truth: {test_texts[i]}\")\n",
    "    print(f\"  Generated: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Phase 2 Training (Caption Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from imports.llm_integration import create_caption_labels, train_multimodal_step\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create optimizer for Phase 2 (projector only)\n",
    "optimizer_p2 = AdamW(\n",
    "    mm_model.get_trainable_params(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Simple training loop\n",
    "num_steps = 100  # Adjust based on your data\n",
    "mm_model.projector.train()\n",
    "\n",
    "print(\"Phase 2 Training (projector only)...\")\n",
    "running_loss = 0.0\n",
    "\n",
    "for step in tqdm(range(num_steps)):\n",
    "    # Sample a batch (using our test data for demo)\n",
    "    batch_images = test_images\n",
    "    batch_texts = test_texts\n",
    "    \n",
    "    # Create labels for caption training\n",
    "    labels = create_caption_labels(\n",
    "        mm_model.llm.tokenizer,\n",
    "        batch_texts,\n",
    "        max_length=128,\n",
    "        device=cfg.device,\n",
    "    )\n",
    "    \n",
    "    batch = {\n",
    "        \"images\": batch_images,\n",
    "        **labels,\n",
    "    }\n",
    "    \n",
    "    metrics = train_multimodal_step(mm_model, batch, optimizer_p2)\n",
    "    running_loss += metrics[\"loss\"]\n",
    "    \n",
    "    if (step + 1) % 20 == 0:\n",
    "        avg_loss = running_loss / 20\n",
    "        print(f\"Step {step+1}: loss = {avg_loss:.4f}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "print(\"\\nPhase 2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Generation (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after Phase 2 training\n",
    "mm_model.projector.eval()\n",
    "\n",
    "print(\"Testing generation (after Phase 2 training)...\\n\")\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    output = mm_model.generate(\n",
    "        images=[img],\n",
    "        prompt=\"Describe this image:\",\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"  Ground truth: {test_texts[i]}\")\n",
    "    print(f\"  Generated: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What we built:\n",
    "\n",
    "1. **Phase 1 - Alignment**\n",
    "   - Froze CLIP vision encoder\n",
    "   - Froze text encoder (sentence-transformers)\n",
    "   - Trained MLP adapters to align vision & text embeddings\n",
    "   - Used MRL + CLIP contrastive loss\n",
    "\n",
    "2. **Phase 2 - LLM Integration**\n",
    "   - Froze alignment model (from Phase 1)\n",
    "   - Added Vision-to-LLM projector\n",
    "   - Connected to Qwen LLM\n",
    "   - Trained projector for caption generation\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Add audio encoder (Whisper) for multimodal\n",
    "- Add Perceiver for better sequence handling\n",
    "- Use LoRA for efficient LLM fine-tuning\n",
    "- Scale up to larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    \"config\": cfg,\n",
    "    \"llm_config\": llm_cfg,\n",
    "    \"vision_adapter\": model.vision_adapter.state_dict(),\n",
    "    \"text_adapter\": model.text_adapter.state_dict(),\n",
    "    \"projector\": mm_model.projector.state_dict(),\n",
    "}, \"./checkpoints/multimodal_final.pt\")\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
