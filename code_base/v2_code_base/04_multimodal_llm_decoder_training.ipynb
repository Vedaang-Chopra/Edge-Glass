{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fb69cd",
   "metadata": {},
   "source": [
    "\n",
    "# 03_multimodal_llm_decoder_training.ipynb\n",
    "\n",
    "**Notebook 3 â€“ Aligned Model + â€œNormalâ€ Decoder (Standard LLM)**\n",
    "\n",
    "This notebook connects the **Phaseâ€‘1 aligned model** (visionâ€“text or triâ€‘modal Perceiver alignment) to a standard decoderâ€‘only LLM (e.g., Qwen2.5â€‘1.5B) and trains a **visionâ€‘conditioned captioning / multimodal generation head**.\n",
    "\n",
    "Key features:\n",
    "\n",
    "- Load **preâ€‘trained alignment checkpoint** (Phaseâ€‘1)\n",
    "- Build a **MultimodalLLM** that uses the aligned vision embedding as a **softâ€‘prompt prefix** for a frozen LLM\n",
    "- Use **Valor triâ€‘modal parquet** (or compatible) for training (image+text; extendable to audio)\n",
    "- **Multiâ€‘GPU training** via `DataParallel` (2 GPUs supported)\n",
    "- **Weights & Biases** logging for:\n",
    "  - Training & validation loss\n",
    "  - Learning rate\n",
    "  - Example generations (tables with GT vs predicted captions)\n",
    "- Clean experiment directory structure & checkpoint saving\n",
    "\n",
    "> ðŸ”§ You are expected to adjust a few paths (e.g., `VALOR_PARQUET_PATH`, `IMAGE_ROOT_DIR`, `ALIGN_CKPT_PATH`) to match your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf6288",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0479da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Local modules\n",
    "from imports.core import AlignmentConfig, VisionTextAligner, set_seed, count_parameters  # phaseâ€‘1 aligner\n",
    "from imports.llm_integration import (\n",
    "    LLMConfig,\n",
    "    MultimodalLLM,\n",
    "    create_caption_labels,\n",
    "    train_multimodal_step,\n",
    ")\n",
    "from imports.train import load_checkpoint  # for loading phaseâ€‘1 adapter weights\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f75e3",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Phase3Config:\n",
    "    # ----- Paths -----\n",
    "    project_name: str = \"edgeglass_phase3_normal_llm\"\n",
    "    run_name: str = \"valor_tri_modal_qwen_prefix\"\n",
    "    \n",
    "    # Base experiment directory (all artifacts under this)\n",
    "    base_dir: str = \"./checkpoints/phase1\"\n",
    "    \n",
    "    # Phaseâ€‘1 alignment checkpoint (visionâ€‘text or triâ€‘modal)\n",
    "    align_checkpoint_path: str = \"./checkpoints/phase1/best.pt\"\n",
    "    \n",
    "    # Valor / triâ€‘modal parquet path (image + text; optionally audio)\n",
    "    valor_parquet_path: str = \"./data/alignment_subsets/valor32k_train_shards/*.parquet\"\n",
    "    image_root_dir: str = \"./data/alignment_subsets/valor32k_train_shards/images\"  # root dir to prepend to relative image paths\n",
    "    \n",
    "    # Column names in Valor parquet\n",
    "    image_path_col: str = \"image_path\"    # path or relative path to image\n",
    "    caption_col: str = \"caption\"          # text caption; adjust if \"text\" etc.\n",
    "    \n",
    "    # Train/val split\n",
    "    val_ratio: float = 0.05\n",
    "    max_rows: Optional[int] = None  # set to e.g. 50_000 to cap rows\n",
    "    \n",
    "    # ----- LLM & prefix -----\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    freeze_llm: bool = True\n",
    "    num_prefix_tokens: int = 8\n",
    "    \n",
    "    # ----- Training -----\n",
    "    num_epochs: int = 3\n",
    "    batch_size_per_gpu: int = 4\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_caption_len: int = 64\n",
    "    \n",
    "    # Logging\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 500  # steps\n",
    "    num_eval_samples: int = 8  # qualitative generations\n",
    "    \n",
    "    # Device / precision\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: str = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\"\n",
    "\n",
    "cfg = Phase3Config()\n",
    "\n",
    "# Create directories\n",
    "base_dir = Path(cfg.base_dir)\n",
    "ckpt_dir = base_dir / \"checkpoints\"\n",
    "log_dir = base_dir / \"logs\"\n",
    "samples_dir = base_dir / \"samples\"\n",
    "\n",
    "for d in [ckpt_dir, log_dir, samples_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Base dir:\", base_dir)\n",
    "print(\"Checkpoint dir:\", ckpt_dir)\n",
    "print(\"Valor parquet:\", cfg.valor_parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4140da",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada777c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(cfg.device)\n",
    "\n",
    "def get_dtype() -> torch.dtype:\n",
    "    if cfg.dtype == \"bfloat16\":\n",
    "        return torch.bfloat16\n",
    "    if cfg.dtype == \"float16\":\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "def setup_seed():\n",
    "    set_seed(cfg.seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Seed set to {cfg.seed}\")\n",
    "\n",
    "setup_seed()\n",
    "\n",
    "device = get_device()\n",
    "dtype = get_dtype()\n",
    "print(f\"Using device: {device} | dtype: {dtype}\")\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.device_count() > 0:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1d952",
   "metadata": {},
   "source": [
    "## 4. Load Phaseâ€‘1 Aligner Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: AlignmentConfig here should match what you used in Phaseâ€‘1.\n",
    "align_cfg = AlignmentConfig()\n",
    "align_cfg.device = device\n",
    "align_cfg.dtype = dtype\n",
    "\n",
    "aligner = VisionTextAligner(align_cfg).to(device)\n",
    "\n",
    "print(\"Loading Phaseâ€‘1 alignment checkpoint from:\", cfg.align_checkpoint_path)\n",
    "_ = load_checkpoint(\n",
    "    model=aligner,\n",
    "    checkpoint_path=cfg.align_checkpoint_path,\n",
    "    load_optimizer=False,\n",
    "    optimizer=None,\n",
    ")\n",
    "\n",
    "# Freeze aligner completely for Phaseâ€‘3 (decoder training)\n",
    "for p in aligner.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Aligner parameter counts:\", count_parameters(aligner))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15890f",
   "metadata": {},
   "source": [
    "## 5. Build Multimodal LLM (Aligned Model + Normal Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_cfg = LLMConfig(\n",
    "    model_name=cfg.llm_model_name,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_prefix_tokens=cfg.num_prefix_tokens,\n",
    "    freeze_llm=cfg.freeze_llm,\n",
    ")\n",
    "\n",
    "multimodal_model = MultimodalLLM(\n",
    "    aligner=aligner,\n",
    "    llm_config=llm_cfg,\n",
    ")\n",
    "\n",
    "multimodal_model.to(device)\n",
    "multimodal_model = multimodal_model.to(dtype=dtype)\n",
    "\n",
    "# Multiâ€‘GPU (DataParallel)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Wrapping MultimodalLLM with DataParallel over {num_gpus} GPUs\")\n",
    "    multimodal_model = nn.DataParallel(multimodal_model)\n",
    "\n",
    "# Convenience handle to the underlying module for attributes (tokenizer, etc.)\n",
    "mm_module = multimodal_model.module if isinstance(multimodal_model, nn.DataParallel) else multimodal_model\n",
    "\n",
    "print(\"Multimodal model ready.\")\n",
    "print(\"Trainable params (projector only in Phaseâ€‘2a):\", sum(p.numel() for p in mm_module.get_trainable_params()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78cc86",
   "metadata": {},
   "source": [
    "## 6. Valor Triâ€‘Modal Dataset (Image + Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ValorImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset for Valorâ€‘style parquet with at least:\n",
    "      - image path column (absolute or relative to `image_root_dir`)\n",
    "      - caption/text column\n",
    "    \n",
    "    You can later extend this to include audio features, preâ€‘extracted encodings, etc.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_root_dir: Path,\n",
    "        image_path_col: str,\n",
    "        caption_col: str,\n",
    "        tokenizer,\n",
    "        max_length: int = 64,\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_root = Path(image_root_dir)\n",
    "        self.image_path_col = image_path_col\n",
    "        self.caption_col = caption_col\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Preâ€‘tokenize captions for efficiency & to make DataLoader workers simple\n",
    "        self._tokenized = []\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Tokenizing captions\"):\n",
    "            caption = str(row[self.caption_col])\n",
    "            tokens = self.tokenizer(\n",
    "                caption,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = tokens[\"input_ids\"][0]\n",
    "            attention_mask = tokens[\"attention_mask\"][0]\n",
    "            labels = input_ids.clone()\n",
    "            labels[attention_mask == 0] = -100\n",
    "            self._tokenized.append(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"labels\": labels,\n",
    "                    \"raw_caption\": caption,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, path: str) -> Image.Image:\n",
    "        p = Path(path)\n",
    "        if not p.is_absolute():\n",
    "            p = self.image_root / p\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[self.image_path_col]\n",
    "        img = self._load_image(path)\n",
    "        toks = self._tokenized[idx]\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"input_ids\": toks[\"input_ids\"],\n",
    "            \"attention_mask\": toks[\"attention_mask\"],\n",
    "            \"labels\": toks[\"labels\"],\n",
    "            \"caption\": toks[\"raw_caption\"],\n",
    "            \"image_path\": str(path),\n",
    "        }\n",
    "\n",
    "\n",
    "def valor_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch], dim=0)\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch], dim=0)\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "    image_paths = [b[\"image_path\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"captions\": captions,\n",
    "        \"image_paths\": image_paths,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731b167",
   "metadata": {},
   "source": [
    "## 7. Load Valor Parquet and Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading Valor parquet from:\", cfg.valor_parquet_path)\n",
    "valor_df = pd.read_parquet(cfg.valor_parquet_path)\n",
    "print(\"Raw Valor rows:\", len(valor_df))\n",
    "\n",
    "if cfg.max_rows is not None:\n",
    "    valor_df = valor_df.head(cfg.max_rows)\n",
    "    print(\"Capped Valor rows to:\", len(valor_df))\n",
    "\n",
    "# Basic sanity check\n",
    "print(\"Columns:\", list(valor_df.columns)[:20])\n",
    "assert cfg.image_path_col in valor_df.columns, f\"Missing image column: {cfg.image_path_col}\"\n",
    "assert cfg.caption_col in valor_df.columns, f\"Missing caption column: {cfg.caption_col}\"\n",
    "\n",
    "# Train/val split\n",
    "n_total = len(valor_df)\n",
    "n_val = max(1, int(n_total * cfg.val_ratio))\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_df = valor_df.iloc[:n_train].reset_index(drop=True)\n",
    "val_df = valor_df.iloc[n_train:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "\n",
    "\n",
    "tokenizer = mm_module.llm.tokenizer\n",
    "\n",
    "train_dataset = ValorImageTextDataset(\n",
    "    df=train_df,\n",
    "    image_root_dir=Path(cfg.image_root_dir),\n",
    "    image_path_col=cfg.image_path_col,\n",
    "    caption_col=cfg.caption_col,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.max_caption_len,\n",
    ")\n",
    "\n",
    "val_dataset = ValorImageTextDataset(\n",
    "    df=val_df,\n",
    "    image_root_dir=Path(cfg.image_root_dir),\n",
    "    image_path_col=cfg.image_path_col,\n",
    "    caption_col=cfg.caption_col,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.max_caption_len,\n",
    ")\n",
    "\n",
    "global_batch_size = cfg.batch_size_per_gpu * max(1, torch.cuda.device_count())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=global_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=valor_collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=global_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=valor_collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"Batches per epoch: train={len(train_loader)}, val={len(val_loader)}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50373505",
   "metadata": {},
   "source": [
    "## 8. Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only projector params are trainable in Phaseâ€‘2a\n",
    "trainable_params = list(mm_module.get_trainable_params())\n",
    "print(\"Trainable parameter count:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * cfg.num_epochs\n",
    "warmup_steps = int(num_training_steps * cfg.warmup_ratio)\n",
    "print(f\"Total training steps: {num_training_steps} | Warmup steps: {warmup_steps}\")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=max(10, num_training_steps - warmup_steps),\n",
    "    T_mult=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2796b",
   "metadata": {},
   "source": [
    "## 9. Weights & Biases Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf231a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=cfg.project_name,\n",
    "    name=cfg.run_name,\n",
    "    config=asdict(cfg),\n",
    ")\n",
    "\n",
    "wandb.watch(mm_module.projector, log=\"all\", log_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eba45",
   "metadata": {},
   "source": [
    "## 10. Training & Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_epoch(step: int) -> Dict[str, float]:\n",
    "    mm_module.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    all_samples = []  # for qualitative logging\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Eval @ step {step}\", leave=False)):\n",
    "            images = batch[\"images\"]\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = mm_module(\n",
    "                images=images,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Collect a few qualitative generations\n",
    "            if len(all_samples) < cfg.num_eval_samples:\n",
    "                for img, cap in zip(images, batch[\"captions\"]):\n",
    "                    if len(all_samples) >= cfg.num_eval_samples:\n",
    "                        break\n",
    "                    try:\n",
    "                        gen_text = mm_module.generate(\n",
    "                            images=img,\n",
    "                            prompt=\"Describe this scene in one or two sentences:\",\n",
    "                            max_new_tokens=64,\n",
    "                            temperature=0.7,\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        gen_text = f\"[GENERATION_ERROR: {e}]\"\n",
    "                    all_samples.append(\n",
    "                        {\n",
    "                            \"image\": wandb.Image(img),\n",
    "                            \"gt_caption\": cap,\n",
    "                            \"gen_caption\": gen_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    metrics = {\"val_loss\": avg_loss}\n",
    "\n",
    "    # Log qualitative samples as W&B Table\n",
    "    if all_samples:\n",
    "        table = wandb.Table(columns=[\"image\", \"gt_caption\", \"gen_caption\"])\n",
    "        for s in all_samples:\n",
    "            table.add_data(s[\"image\"], s[\"gt_caption\"], s[\"gen_caption\"])\n",
    "        wandb.log({\"eval_samples\": table, \"global_step\": step})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = math.inf\n",
    "\n",
    "mm_module.train()\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{cfg.num_epochs} =====\")\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\")):\n",
    "        multimodal_model.train()  # handles DataParallel as well\n",
    "\n",
    "        images = batch[\"images\"]\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        outputs = mm_module(\n",
    "            images=images,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, cfg.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step(global_step)\n",
    "\n",
    "        step_loss = loss.item()\n",
    "        epoch_loss += step_loss\n",
    "        n_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % cfg.log_every == 0:\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            avg_loss = epoch_loss / max(1, n_batches)\n",
    "            log_data = {\n",
    "                \"train_loss\": step_loss,\n",
    "                \"train_loss_avg\": avg_loss,\n",
    "                \"lr\": lr,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"global_step\": global_step,\n",
    "            }\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        if global_step % cfg.eval_every == 0:\n",
    "            print(f\"\\n>>> Running evaluation at step {global_step}...\")\n",
    "            val_metrics = evaluate_epoch(global_step)\n",
    "            wandb.log({**val_metrics, \"global_step\": global_step})\n",
    "            val_loss = val_metrics[\"val_loss\"]\n",
    "\n",
    "            # Save best checkpoint by val_loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                ckpt_path = ckpt_dir / \"best_phase3.pt\"\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"step\": global_step,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": mm_module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                        \"cfg\": asdict(cfg),\n",
    "                    },\n",
    "                    ckpt_path,\n",
    "                )\n",
    "                print(f\"[BEST] Saved new best checkpoint to {ckpt_path} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / max(1, n_batches)\n",
    "    print(f\"Epoch {epoch+1} complete | avg_train_loss={avg_epoch_loss:.4f}\")\n",
    "    wandb.log({\"epoch_train_loss\": avg_epoch_loss, \"epoch\": epoch + 1, \"global_step\": global_step})\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n>>> Final evaluation after training...\")\n",
    "final_val_metrics = evaluate_epoch(global_step)\n",
    "wandb.log({**final_val_metrics, \"global_step\": global_step})\n",
    "\n",
    "# Save final checkpoint\n",
    "final_ckpt_path = ckpt_dir / \"final_phase3.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"step\": global_step,\n",
    "        \"epoch\": cfg.num_epochs,\n",
    "        \"model_state_dict\": mm_module.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"cfg\": asdict(cfg),\n",
    "    },\n",
    "    final_ckpt_path,\n",
    ")\n",
    "print(f\"Saved final checkpoint to {final_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa99171",
   "metadata": {},
   "source": [
    "## 11. Inference Helper (Manual Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8135fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_for_single_row(row_idx: int = 0, prompt: str = \"Describe this scene:\") -> Dict[str, Any]:\n",
    "    \"\"\"Quick helper to inspect one Valor sample and generation.\"\"\"\n",
    "    mm_module.eval()\n",
    "    row = val_df.iloc[row_idx]\n",
    "    path = row[cfg.image_path_col]\n",
    "    p = Path(path)\n",
    "    if not p.is_absolute():\n",
    "        p = Path(cfg.image_root_dir) / p\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "\n",
    "    gen_text = mm_module.generate(\n",
    "        images=img,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=96,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return {\n",
    "        \"image\": img,\n",
    "        \"gt_caption\": str(row[cfg.caption_col]),\n",
    "        \"gen_caption\": gen_text,\n",
    "    }\n",
    "\n",
    "# Example (won't display in headless run, but useful in notebook):\n",
    "# sample = generate_for_single_row(0)\n",
    "# display(sample[\"image\"])\n",
    "# print(\"GT:\", sample[\"gt_caption\"])\n",
    "# print(\"GEN:\", sample[\"gen_caption\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
