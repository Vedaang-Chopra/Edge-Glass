{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579c01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_alignment_perceiver import MultimodalAlignmentConfig, MultimodalAlignmentModel, count_parameters\n",
    "from multimodal_alignment_perceiver import contrastive_loss, matryoshka_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2214631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ad109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTIMODAL ALIGNMENT WITH PERCEIVER RESAMPLER\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MULTIMODAL ALIGNMENT WITH PERCEIVER RESAMPLER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "config = MultimodalAlignmentConfig(\n",
    "    perceiver_dim=512,\n",
    "    num_latents=64,\n",
    "    num_perceiver_layers=4,\n",
    "    d_align=512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665ee12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MultimodalAlignmentModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5990f362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Architecture:\n",
      "   Total parameters: 21,621,760\n",
      "   Trainable: 21,621,760\n",
      "\n",
      "üß™ Testing forward pass...\n"
     ]
    }
   ],
   "source": [
    "# Print architecture\n",
    "print(\"\\nüìê Architecture:\")\n",
    "params = count_parameters(model)\n",
    "print(f\"   Total parameters: {params['total']:,}\")\n",
    "print(f\"   Trainable: {params['trainable']:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nüß™ Testing forward pass...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf746e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate encoder outputs\n",
    "batch_size = 4\n",
    "vision_feats = torch.randn(batch_size, 50, config.d_vision)   # CLIP: 50 patches\n",
    "audio_feats = torch.randn(batch_size, 1500, config.d_audio)   # Whisper: ~1500 frames\n",
    "text_feats = torch.randn(batch_size, 32, config.d_text)       # Text: 32 tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d5331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vision embedding: torch.Size([4, 512])\n",
      "   Audio embedding: torch.Size([4, 512])\n",
      "   Text embedding: torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "# Encode each modality\n",
    "z_vision = model.encode_vision(vision_feats)\n",
    "z_audio = model.encode_audio(audio_feats)\n",
    "z_text = model.encode_text(text_feats)\n",
    "\n",
    "print(f\"   Vision embedding: {z_vision.shape}\")  # (4, 512)\n",
    "print(f\"   Audio embedding: {z_audio.shape}\")    # (4, 512)\n",
    "print(f\"   Text embedding: {z_text.shape}\")      # (4, 512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b934327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LLM prefix: torch.Size([4, 64, 1536])\n",
      "\n",
      "üìâ Testing loss computation...\n"
     ]
    }
   ],
   "source": [
    "# Test LLM projection\n",
    "llm_prefix = model.project_to_llm(vision_feats, 'vision')\n",
    "print(f\"   LLM prefix: {llm_prefix.shape}\")      # (4, 64, 1536)\n",
    "\n",
    "# Test loss computation\n",
    "print(\"\\nüìâ Testing loss computation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1778e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CLIP loss: 1.4866\n",
      "   MRL loss: 1.5836\n",
      "\n",
      "‚úÖ All tests passed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_clip = contrastive_loss(z_vision, z_text)\n",
    "loss_mrl = matryoshka_loss(z_vision, z_text, dims=config.mrl_dims)\n",
    "\n",
    "print(f\"   CLIP loss: {loss_clip.item():.4f}\")\n",
    "print(f\"   MRL loss: {loss_mrl.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tests passed!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42a0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3bfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e8b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce4f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796824a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
