{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ea2fd3",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Multimodal Alignment (Vision, Audio, Text) + Alignment Plots (W&B)\n",
    "\n",
    "This notebook trains **Phase 1 multimodal alignment** for **vision, audio, and text** encoders,\n",
    "using two model variants:\n",
    "\n",
    "1. **MLP + MRL (no Perceiver bottleneck)**  \n",
    "2. **Perceiver + MLP + MRL** (full model from `multimodal_alignment_perceiver.py`)\n",
    "\n",
    "Both variants:\n",
    "\n",
    "- Use **Matryoshka Representation Learning (MRL)** + CLIP-style contrastive loss.\n",
    "- Log training / validation metrics to **Weights & Biases (W&B)**.\n",
    "- Support **multi-GPU training** via `torch.nn.DataParallel`.\n",
    "- Save checkpoints under a common directory for later **Phase 2** experiments.\n",
    "- Produce **alignment plots** in W&B:\n",
    "  - Cosine similarity histograms (pos vs neg pairs).\n",
    "  - Cosine similarity heatmaps (vision ‚Üî text).\n",
    "  - t-SNE 2D scatter plots of aligned embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6082a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9226126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Basic imports & environment setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Local modules\n",
    "from imports.in_memory_datasets import *\n",
    "from imports.multimodal_alignment_perceiver import (\n",
    "    MultimodalAlignmentConfig,\n",
    "    MultimodalAlignmentModel,\n",
    "    contrastive_loss,\n",
    "    matryoshka_loss,\n",
    ")\n",
    "from imports.core import set_seed\n",
    "\n",
    "# Transformers encoders\n",
    "from transformers import (\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperModel,\n",
    "    WhisperProcessor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ac2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Num GPUs: 2\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Num GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f1f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "Checkpoint root: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal\n",
      "MLP run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/mlp_mrl\n",
      "Perceiver run dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # General\n",
    "    seed: int = 42\n",
    "    num_epochs_mlp: int = 3\n",
    "    num_epochs_perceiver: int = 5\n",
    "    log_every: int = 20\n",
    "\n",
    "    # Data: if Parquet paths are None, we fall back to HF datasets\n",
    "    pixmo_parquet_dir: Path = Path(\"data/alignment_offline\")\n",
    "    clotho_parquet_dir: Path = Path.cwd() / \"data\" / \"alignment_offline\"\n",
    "    pixmo_parquet_glob: str = \"pixmocap_offline_20000*.parquet\"\n",
    "    clotho_parquet_glob: str = \"clotho_development.parquet\"\n",
    "\n",
    "    image_hf_dataset: str = \"allenai/pixmo-cap\"\n",
    "    audio_hf_dataset: str = \"clotho_v2\"\n",
    "    \n",
    "    \n",
    "    max_image_samples: Optional[int] = 10_000   # None for all\n",
    "    max_audio_samples: Optional[int] = 5_000    # None for all\n",
    "    image_size: Tuple[int, int] = (224, 224)\n",
    "    sample_val_fraction: float = 0.05  # fraction of samples for validation\n",
    "\n",
    "    # Training\n",
    "    base_batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # WandB\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"edgeglass_phase1_alignment\"\n",
    "    wandb_entity: Optional[str] = None  # set if you use a team\n",
    "    wandb_mode: str = \"online\"  # \"offline\" or \"disabled\" etc.\n",
    "    wandb_run_name: Optional[str] = \"audio_align_1\"  # if None, auto-generated\n",
    "\n",
    "    # Checkpoints (Phase 1)\n",
    "    root_dir: Path = Path(\".\").resolve()\n",
    "    ckpt_root: str = \"checkpoints/phase1_multimodal\"  # relative to root_dir\n",
    "\n",
    "    # Alignment / model (weights are mirrored into mm_cfg later)\n",
    "    mrl_weight: float = 1.0\n",
    "    clip_weight: float = 0.5\n",
    "\n",
    "cfg = ExperimentConfig()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "ROOT_DIR = cfg.root_dir\n",
    "CKPT_ROOT = ROOT_DIR / cfg.ckpt_root\n",
    "MLP_DIR = CKPT_ROOT / \"mlp_mrl\"\n",
    "PERCEIVER_DIR = CKPT_ROOT / \"perceiver_mrl\"\n",
    "\n",
    "for d in [CKPT_ROOT, MLP_DIR, PERCEIVER_DIR, cfg.pixmo_parquet_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"Checkpoint root:\", CKPT_ROOT)\n",
    "print(\"MLP run dir:\", MLP_DIR)\n",
    "print(\"Perceiver run dir:\", PERCEIVER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2aba699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases helpers\n",
    "\n",
    "def init_wandb(run_name: str, variant: str, extra_config: Dict[str, Any] = None):\n",
    "    if not cfg.use_wandb:\n",
    "        return None\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    base_config = asdict(cfg)\n",
    "    base_config[\"variant\"] = variant\n",
    "    if extra_config:\n",
    "        base_config.update(extra_config)\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=cfg.wandb_project,\n",
    "        entity=cfg.wandb_entity,\n",
    "        name=run_name,\n",
    "        mode=cfg.wandb_mode,\n",
    "        config=base_config,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "def log_alignment_histograms(\n",
    "    run,\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    "    prefix: str,\n",
    "    max_points: int = 512,\n",
    "):\n",
    "    \"\"\"Log positive vs negative cosine similarity histograms to W&B.\n",
    "\n",
    "    This is a lightweight way to 'see' alignment progress:\n",
    "    - Positive sims: cosine between matching pairs\n",
    "    - Negative sims: cosine between random mismatched pairs\n",
    "    \"\"\"\n",
    "    if run is None:\n",
    "        return\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_a = F.normalize(z_a, dim=-1)\n",
    "        z_b = F.normalize(z_b, dim=-1)\n",
    "\n",
    "        # Sample subset to keep logging light\n",
    "        n = min(z_a.size(0), max_points)\n",
    "        idx = torch.randperm(z_a.size(0))[:n]\n",
    "        za = z_a[idx]\n",
    "        zb = z_b[idx]\n",
    "\n",
    "        # Positive similarities\n",
    "        pos_sims = (za * zb).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "        # Negative similarities (shuffle)\n",
    "        shuffle_idx = torch.randperm(n)\n",
    "        neg_sims = (za * zb[shuffle_idx]).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "        run.log({\n",
    "            f\"{prefix}/pos_sim\": wandb.Histogram(pos_sims),\n",
    "            f\"{prefix}/neg_sim\": wandb.Histogram(neg_sims),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4449e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: load PixMo-Cap and MusicCaps from local Parquet (or HF fallback)\n",
    "\n",
    "import glob\n",
    "\n",
    "def get_pixmo_dataset():\n",
    "    \"\"\"Load PixMo-Cap 'train' split from Parquet if available, otherwise from HF.\"\"\"\n",
    "    parquet_pattern = cfg.pixmo_parquet_dir / cfg.pixmo_parquet_glob\n",
    "    matches = sorted(glob.glob(str(parquet_pattern)))\n",
    "    if matches:\n",
    "        print(f\"Loading PixMo-Cap from Parquet: {matches[-1]}\")\n",
    "        ds_dict = load_dataset(\"parquet\", data_files={\"train\": matches[-1]})\n",
    "        ds = ds_dict[\"train\"]\n",
    "    else:\n",
    "        print(f\"No PixMo Parquet found at pattern {parquet_pattern}, loading from HF: {cfg.image_hf_dataset}\")\n",
    "        ds = load_dataset(cfg.image_hf_dataset, split=\"train\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01120de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Image-Text Datasets (PixMo-Cap) ===\n",
      "Loading PixMo-Cap from Parquet: data/alignment_offline/pixmocap_offline_20000.parquet\n",
      "\n",
      "üì• Pre-loading 10000 images into memory...\n",
      "   Image size: (224, 224)\n",
      "   Using 32 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/10000 [00:00<?, ?it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:   0%|          | 1/10000 [00:34<96:16:55, 34.67s/it]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  26%|‚ñà‚ñà‚ñå       | 2590/10000 [00:53<01:30, 81.89it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  27%|‚ñà‚ñà‚ñã       | 2653/10000 [01:14<03:18, 36.93it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 4603/10000 [01:36<01:04, 83.71it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5695/10000 [01:49<00:51, 84.19it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Loading images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5695/10000 [02:00<00:51, 84.19it/s]/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (130382142 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "Loading images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [02:42<00:00, 61.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 10000 images into memory\n",
      "   ‚ö†Ô∏è  70 images failed to load (using fallback)\n",
      "Image train size: 9500 | val size: 500 | batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_image_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create train/val DataLoaders for image-text data using PixMo-Cap.\"\"\"\n",
    "    print(\"\\n=== Building Image-Text Datasets (PixMo-Cap) ===\")\n",
    "    hf_ds = get_pixmo_dataset()\n",
    "\n",
    "    if cfg.max_image_samples is not None:\n",
    "        hf_ds = hf_ds.select(range(min(cfg.max_image_samples, len(hf_ds))))\n",
    "\n",
    "    # Expect PixMo-Cap columns: image_url + caption\n",
    "    colnames = hf_ds.column_names\n",
    "    if \"image_url\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'image_url' column in PixMo dataset, found: {colnames}\")\n",
    "    if \"caption\" not in colnames:\n",
    "        raise ValueError(f\"Expected 'caption' column in PixMo dataset, found: {colnames}\")\n",
    "\n",
    "    ds = InMemoryImageTextDataset(\n",
    "        hf_dataset=hf_ds,\n",
    "        img_col=\"image_url\",\n",
    "        txt_col=\"caption\",\n",
    "        max_samples=None,  # already limited above\n",
    "        image_size=cfg.image_size,\n",
    "    )\n",
    "\n",
    "    # Train/val split via indices\n",
    "    n = len(ds)\n",
    "    idx = np.random.permutation(n)\n",
    "    split = int(n * (1.0 - cfg.sample_val_fraction))\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "    train_ds = Subset(ds, train_idx)\n",
    "    val_ds = Subset(ds, val_idx)\n",
    "\n",
    "    # Scale batch size with number of GPUs\n",
    "    num_gpus = max(1, torch.cuda.device_count())\n",
    "    batch_size = cfg.base_batch_size * num_gpus\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_in_memory_images,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Image train size: {len(train_ds)} | val size: {len(val_ds)} | batch size: {batch_size}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "image_train_loader, image_val_loader = build_image_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56803f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# cache resamplers per original sr\n",
    "_resamplers: Dict[int, torchaudio.transforms.Resample] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5503dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_clotho_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate function for ClothoAudioCaptionDataset.\n",
    "\n",
    "    - Takes variable-length waveforms at original sr (typically 44100)\n",
    "    - Resamples each to 16 kHz for Whisper\n",
    "    - Pads to max length in the batch\n",
    "    \"\"\"\n",
    "    audios_16k = []\n",
    "    lengths_16k = []\n",
    "    captions = []\n",
    "    file_names = []\n",
    "\n",
    "    for b in batch:\n",
    "        wav = b[\"audio\"]        # Tensor [T_orig]\n",
    "        sr_orig = int(b[\"sr\"])  # e.g. 44100\n",
    "\n",
    "        # --- resample to 16 kHz ---\n",
    "        if sr_orig != 16000:\n",
    "            if sr_orig not in _resamplers:\n",
    "                _resamplers[sr_orig] = torchaudio.transforms.Resample(\n",
    "                    orig_freq=sr_orig,\n",
    "                    new_freq=16000,\n",
    "                )\n",
    "            wav = _resamplers[sr_orig](wav)  # [T_16k]\n",
    "\n",
    "        audios_16k.append(wav)\n",
    "        lengths_16k.append(wav.shape[0])\n",
    "        captions.append(b[\"caption\"])\n",
    "        file_names.append(b[\"file_name\"])\n",
    "\n",
    "    # Pad to max length at 16 kHz\n",
    "    max_len = max(lengths_16k)\n",
    "    B = len(audios_16k)\n",
    "\n",
    "    padded = audios_16k[0].new_zeros(B, max_len)  # (B, T_max_16k)\n",
    "    for i, a in enumerate(audios_16k):\n",
    "        padded[i, : a.shape[0]] = a\n",
    "\n",
    "    return {\n",
    "        \"audio\": padded,                                    # (B, T_max_16k)\n",
    "        \"audio_lengths\": torch.tensor(lengths_16k, dtype=torch.long),\n",
    "        \"sr\": 16000,                                        # <-- now fixed to 16k\n",
    "        \"captions\": captions,                               # list[str]\n",
    "        \"file_names\": file_names,                           # list[str]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_clotho_dataset():\n",
    "    \"\"\"\n",
    "    DEPRECATED: We no longer load MusicCaps.\n",
    "    \n",
    "    This function now simply loads the local CLOTHO torch records file.\n",
    "\n",
    "    Returns:\n",
    "        records_path (str) if exists, else None\n",
    "    \"\"\"\n",
    "    # Update this to the actual location of your processed Clotho .pt file\n",
    "    clotho_records_path = cfg.pixmo_parquet_dir / \"clotho_records.pt\"\n",
    "\n",
    "    if clotho_records_path.exists():\n",
    "        print(f\"Loading CLOTHO records from: {clotho_records_path}\")\n",
    "        return str(clotho_records_path)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è Clotho records file not found: {clotho_records_path}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981c8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# === NEW AUDIO DATALOADER FOR CLOTHO AUDIO-CAPTION DATASET ===\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# IMPORTANT: Update import path to where your class is located\n",
    "class ClothoAudioCaptionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        records_path: str,\n",
    "        pick_random_caption: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.records: List[Dict[str, Any]] = torch.load(records_path)\n",
    "        self.pick_random_caption = pick_random_caption\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.records[idx]\n",
    "        waveform = item[\"waveform\"]       # torch.FloatTensor [T]\n",
    "        sr = item[\"sr\"]\n",
    "        file_name = item[\"file_name\"]\n",
    "        captions = item[\"captions\"]       # list[str]\n",
    "\n",
    "        if self.pick_random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]\n",
    "\n",
    "        return {\n",
    "            \"audio\": waveform,\n",
    "            \"sr\": sr,\n",
    "            \"caption\": caption,\n",
    "            \"file_name\": file_name,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d92fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Audio-Text Datasets (CLOTHO) ===\n",
      "CLOTHO train size: 3647 | val size: 192 | batch size: 64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "def build_audio_datasets() -> Tuple[Optional[DataLoader], Optional[DataLoader]]:\n",
    "    \"\"\"Create train/val DataLoaders for audio-text data using local Clotho records.\"\"\"\n",
    "    print(\"\\n=== Building Audio-Text Datasets (CLOTHO) ===\")\n",
    "\n",
    "    # This is the file you created with torch.save(records, OUTPUT_PARQUET_PATH)\n",
    "    clotho_records_path = Path.cwd() / \"data\" / \"alignment_offline\" / \"clotho_development.parquet\"\n",
    "\n",
    "    if not clotho_records_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Clotho records file not found: {clotho_records_path}\")\n",
    "        print(\"   Skipping audio alignment.\")\n",
    "        return None, None\n",
    "\n",
    "    full_ds = ClothoAudioCaptionDataset(\n",
    "        records_path=str(clotho_records_path),\n",
    "        pick_random_caption=True,\n",
    "    )\n",
    "\n",
    "    n = len(full_ds)\n",
    "    idx = torch.randperm(n)\n",
    "    split = int(n * (1.0 - cfg.sample_val_fraction))\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "    train_ds = Subset(full_ds, train_idx)\n",
    "    val_ds = Subset(full_ds, val_idx)\n",
    "\n",
    "    num_gpus = max(1, torch.cuda.device_count())\n",
    "    batch_size = cfg.base_batch_size * num_gpus\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_clotho_batch,   # <- use our collate\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count() or 4),\n",
    "        collate_fn=collate_clotho_batch,   # <- same collate\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(f\"CLOTHO train size: {len(train_ds)} | val size: {len(val_ds)} | batch size: {batch_size}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "audio_train_loader, audio_val_loader = build_audio_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14224b",
   "metadata": {},
   "source": [
    "## Loading Model Architectures for Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2e053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalAlignmentConfig(vision_model_name='openai/clip-vit-base-patch32', audio_model_name='openai/whisper-base', text_model_name='sentence-transformers/all-MiniLM-L6-v2', llm_model_name='Qwen/Qwen2.5-1.5B-Instruct', d_vision=768, d_audio=512, d_text=384, perceiver_dim=512, num_latents=64, num_perceiver_layers=4, num_attn_heads=8, perceiver_mlp_ratio=4.0, perceiver_dropout=0.1, d_align=512, mrl_dims=(64, 128, 256, 512), llm_hidden_size=1536, num_prefix_tokens=64, batch_size=32, learning_rate=0.0001, weight_decay=0.01, num_epochs=10, warmup_ratio=0.1, max_grad_norm=1.0, temperature=0.07, mrl_weight=1.0, clip_weight=0.5, seed=42, device='cuda:1', dtype='float32')\n"
     ]
    }
   ],
   "source": [
    "# Frozen encoders: CLIP (vision), Whisper (audio), MiniLM (text)\n",
    "\n",
    "# Multimodal alignment config (from multimodal_alignment_perceiver.py)\n",
    "\n",
    "mm_cfg = MultimodalAlignmentConfig()\n",
    "mm_cfg.device = str(device)\n",
    "mm_cfg.mrl_weight = cfg.mrl_weight\n",
    "mm_cfg.clip_weight = cfg.clip_weight\n",
    "\n",
    "print(mm_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f07bbc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIP vision encoder: openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "# Vision encoder (CLIP)\n",
    "vision_model_name = mm_cfg.vision_model_name\n",
    "print(\"\\nLoading CLIP vision encoder:\", vision_model_name)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n",
    "clip_vision = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "clip_vision.to(device)\n",
    "clip_vision.eval()\n",
    "for p in clip_vision.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e9b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading audio encoder: openai/whisper-base\n"
     ]
    }
   ],
   "source": [
    "# Text encoder (MiniLM)\n",
    "text_model_name = mm_cfg.text_model_name\n",
    "print(\"Loading text encoder:\", text_model_name)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Audio encoder (Whisper)\n",
    "audio_model_name = mm_cfg.audio_model_name\n",
    "print(\"Loading audio encoder:\", audio_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(audio_model_name)\n",
    "whisper_encoder = WhisperModel.from_pretrained(audio_model_name).get_encoder()\n",
    "whisper_encoder.to(device)\n",
    "whisper_encoder.eval()\n",
    "for p in whisper_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772e0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_images_to_features(images: List) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of PIL images to CLIP patch features (B, T, 768).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = clip_vision(**inputs)\n",
    "        feats = outputs.last_hidden_state  # (B, T, 768)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def encode_texts_to_features(texts: List[str]) -> torch.Tensor:\n",
    "    \"\"\"Encode a batch of texts to token features (B, L, 384).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tokens = text_tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        outputs = text_encoder(**tokens)\n",
    "        feats = outputs.last_hidden_state  # (B, L, 384)\n",
    "    return feats\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def encode_audio_to_features(audio_batch: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode a batch of padded 16 kHz audio waveforms using Whisper.\n",
    "\n",
    "    Args:\n",
    "        audio_batch: Tensor of shape (B, T_max_16k)\n",
    "        sr: sampling rate (should be 16000 after collate)\n",
    "\n",
    "    Returns:\n",
    "        feats: Tensor of shape (B, T_feat_max, 512)\n",
    "    \"\"\"\n",
    "    assert isinstance(audio_batch, torch.Tensor), \"audio_batch must be Tensor[B, T]\"\n",
    "    if sr != 16000:\n",
    "        # In case something upstream goes wrong, fail loudly\n",
    "        raise ValueError(f\"Expected 16 kHz audio for Whisper, got sr={sr}\")\n",
    "\n",
    "    B, T_max = audio_batch.shape\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(B):\n",
    "            # 1) Take single waveform [T]\n",
    "            wav = audio_batch[i].detach().cpu().float().numpy()  # (T,)\n",
    "\n",
    "            # 2) Run WhisperProcessor on this single example at 16k\n",
    "            inputs = whisper_processor(\n",
    "                wav,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            # 3) Encode with Whisper encoder\n",
    "            out = whisper_encoder(inputs.input_features)          # (1, T_feat_i, 512)\n",
    "            feat_i = out.last_hidden_state.squeeze(0)            # (T_feat_i, 512)\n",
    "            features.append(feat_i)\n",
    "\n",
    "    # 4) Pad along time dimension to get (B, T_feat_max, 512)\n",
    "    max_T_feat = max(f.shape[0] for f in features)\n",
    "    hidden_dim = features[0].shape[1]\n",
    "\n",
    "    feats_batch = torch.zeros(B, max_T_feat, hidden_dim, device=device)\n",
    "    for i, f in enumerate(features):\n",
    "        feats_batch[i, : f.shape[0]] = f\n",
    "\n",
    "    return feats_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "317a4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalAlignmentModel created.\n"
     ]
    }
   ],
   "source": [
    "# Multimodal alignment model (Perceiver backbone)\n",
    "\n",
    "model = MultimodalAlignmentModel(mm_cfg).to(device)\n",
    "print(\"MultimodalAlignmentModel created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e97fc206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping model in DataParallel for 2 GPUs\n",
      "Trainable parameters: 21,621,760\n"
     ]
    }
   ],
   "source": [
    "# Multi-GPU support\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Wrapping model in DataParallel for {num_gpus} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "def get_model_module(m: nn.Module) -> nn.Module:\n",
    "    \"\"\"Return underlying module if wrapped in DataParallel.\"\"\"\n",
    "    return m.module if isinstance(m, nn.DataParallel) else m\n",
    "\n",
    "\n",
    "def count_trainable_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Trainable parameters:\", f\"{count_trainable_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5079cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint utilities\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    best_metric: float,\n",
    "    out_dir: Path,\n",
    "    tag: str,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_metric\": best_metric,\n",
    "        \"model_state\": get_model_module(model).state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"mm_config\": mm_cfg.__dict__,\n",
    "        \"exp_config\": asdict(cfg),\n",
    "    }\n",
    "    ckpt_path = out_dir / f\"{tag}.pt\"\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(f\"Saved checkpoint to: {ckpt_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: Optional[torch.optim.Optimizer],\n",
    "    ckpt_path: Path,\n",
    "    strict: bool = True,\n",
    "):\n",
    "    state = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    get_model_module(model).load_state_dict(state[\"model_state\"], strict=strict)\n",
    "    if optimizer is not None and \"optimizer_state\" in state:\n",
    "        optimizer.load_state_dict(state[\"optimizer_state\"])\n",
    "    print(f\"Loaded checkpoint from epoch {state.get('epoch', 'N/A')} at {ckpt_path}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e39590ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_1261250/2129190948.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=cfg.use_amp)\n"
     ]
    }
   ],
   "source": [
    "# Training helpers ‚Äì MLP-only and Perceiver variants\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "\n",
    "def encode_modality(\n",
    "    model: MultimodalAlignmentModel,\n",
    "    feats: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor],\n",
    "    modality: str,\n",
    "    use_perceiver: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encode features into aligned space.\n",
    "\n",
    "    If use_perceiver=True, uses full Perceiver pipeline.\n",
    "    If False, bypasses Perceiver and uses adapters + alignment projector directly.\n",
    "    \"\"\"\n",
    "    m = get_model_module(model)\n",
    "\n",
    "    if modality == \"vision\":\n",
    "        adapter = m.vision_adapter\n",
    "    elif modality == \"audio\":\n",
    "        adapter = m.audio_adapter\n",
    "    elif modality == \"text\":\n",
    "        adapter = m.text_adapter\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    tokens = adapter(feats)  # (B, T, perceiver_dim)\n",
    "\n",
    "    if use_perceiver:\n",
    "        latents = m.perceiver(tokens, mask)  # (B, K, perceiver_dim)\n",
    "    else:\n",
    "        # MLP-only: treat tokens as latents and pool across sequence\n",
    "        latents = tokens  # (B, T, perceiver_dim)\n",
    "\n",
    "    z = m.alignment_projector(latents)  # (B, d_align)\n",
    "    return z\n",
    "\n",
    "\n",
    "def compute_alignment_losses(\n",
    "    z_a: torch.Tensor,\n",
    "    z_b: torch.Tensor,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Compute MRL + CLIP contrastive losses between two modalities.\"\"\"\n",
    "    loss_mrl = matryoshka_loss(\n",
    "        z_a,\n",
    "        z_b,\n",
    "        dims=mm_cfg.mrl_dims,\n",
    "        temperature=mm_cfg.temperature,\n",
    "    )\n",
    "    loss_clip = contrastive_loss(\n",
    "        z_a,\n",
    "        z_b,\n",
    "        temperature=mm_cfg.temperature,\n",
    "    )\n",
    "    loss_total = cfg.mrl_weight * loss_mrl + cfg.clip_weight * loss_clip\n",
    "    return {\n",
    "        \"loss_total\": loss_total,\n",
    "        \"loss_mrl\": loss_mrl,\n",
    "        \"loss_clip\": loss_clip,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b64ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    epoch: int,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    image_loader: DataLoader,\n",
    "    audio_loader: Optional[DataLoader],\n",
    "    use_perceiver: bool,\n",
    "    run=None,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    m = get_model_module(model)\n",
    "\n",
    "    running = {\n",
    "        \"loss\": 0.0,\n",
    "        \"loss_mrl_vt\": 0.0,\n",
    "        \"loss_clip_vt\": 0.0,\n",
    "        \"loss_mrl_at\": 0.0,\n",
    "        \"loss_clip_at\": 0.0,\n",
    "        \"steps\": 0,\n",
    "    }\n",
    "\n",
    "    audio_iter = iter(audio_loader) if audio_loader is not None else None\n",
    "    global_step = 0\n",
    "\n",
    "    for step, img_batch in enumerate(image_loader, start=1):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        images = img_batch[\"images\"]\n",
    "        img_texts = img_batch[\"captions\"]\n",
    "\n",
    "        with autocast(enabled=cfg.use_amp):\n",
    "            # Vision‚ÄìText\n",
    "            vision_feats = encode_images_to_features(images)  # (B, T, 768)\n",
    "            text_feats_img = encode_texts_to_features(img_texts)  # (B, L, 384)\n",
    "\n",
    "            z_v = encode_modality(model, vision_feats, None, \"vision\", use_perceiver)\n",
    "            z_t_img = encode_modality(model, text_feats_img, None, \"text\", use_perceiver)\n",
    "\n",
    "            losses_vt = compute_alignment_losses(z_v, z_t_img)\n",
    "            loss = losses_vt[\"loss_total\"]\n",
    "\n",
    "            losses_at = None\n",
    "            if audio_iter is not None:\n",
    "                try:\n",
    "                    aud_batch = next(audio_iter)\n",
    "                except StopIteration:\n",
    "                    audio_iter = iter(audio_loader)\n",
    "                    aud_batch = next(audio_iter)\n",
    "\n",
    "                audio = aud_batch[\"audio\"]              # Tensor [B, T_max]\n",
    "                aud_texts = aud_batch[\"captions\"]       # list[str]\n",
    "                sr = int(aud_batch.get(\"sr\", 44100))    # from collate_clotho_batch\n",
    "\n",
    "                audio_feats = encode_audio_to_features(audio, sr)  # (B, T_feat, 512)\n",
    "                text_feats_aud = encode_texts_to_features(aud_texts)\n",
    "\n",
    "                z_a = encode_modality(model, audio_feats, None, \"audio\", use_perceiver)\n",
    "                z_t_aud = encode_modality(model, text_feats_aud, None, \"text\", use_perceiver)\n",
    "\n",
    "                losses_at = compute_alignment_losses(z_a, z_t_aud)\n",
    "                loss = loss + losses_at[\"loss_total\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(m.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running[\"loss\"] += loss.item()\n",
    "        running[\"loss_mrl_vt\"] += losses_vt[\"loss_mrl\"].item()\n",
    "        running[\"loss_clip_vt\"] += losses_vt[\"loss_clip\"].item()\n",
    "        if losses_at is not None:\n",
    "            running[\"loss_mrl_at\"] += losses_at[\"loss_mrl\"].item()\n",
    "            running[\"loss_clip_at\"] += losses_at[\"loss_clip\"].item()\n",
    "        running[\"steps\"] += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if step % cfg.log_every == 0:\n",
    "            avg_loss = running[\"loss\"] / running[\"steps\"]\n",
    "            print(\n",
    "                f\"Epoch {epoch} | Step {step}/{len(image_loader)} | \"\n",
    "                f\"Loss: {avg_loss:.4f} | VT MRL: {running['loss_mrl_vt']/running['steps']:.4f}\"\n",
    "            )\n",
    "\n",
    "        if run is not None:\n",
    "            log_dict = {\n",
    "                \"train/loss_total\": loss.item(),\n",
    "                \"train/loss_mrl_vt\": losses_vt[\"loss_mrl\"].item(),\n",
    "                \"train/loss_clip_vt\": losses_vt[\"loss_clip\"].item(),\n",
    "                \"train/use_perceiver\": int(use_perceiver),\n",
    "            }\n",
    "            if losses_at is not None:\n",
    "                log_dict.update(\n",
    "                    {\n",
    "                        \"train/loss_mrl_at\": losses_at[\"loss_mrl\"].item(),\n",
    "                        \"train/loss_clip_at\": losses_at[\"loss_clip\"].item(),\n",
    "                    }\n",
    "                )\n",
    "            run.log(log_dict)\n",
    "\n",
    "    for k in running:\n",
    "        if k != \"steps\":\n",
    "            running[k] /= max(1, running[\"steps\"])\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} summary | loss={running['loss']:.4f} | \"\n",
    "        f\"VT MRL={running['loss_mrl_vt']:.4f} | AT MRL={running['loss_mrl_at']:.4f}\"\n",
    "    )\n",
    "    return running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2098a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_alignment(\n",
    "    model: nn.Module,\n",
    "    image_val_loader: DataLoader,\n",
    "    audio_val_loader: Optional[DataLoader],\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 20,\n",
    "    run=None,\n",
    "    prefix: str = \"val\",\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    all_z_v = []\n",
    "    all_z_t_img = []\n",
    "\n",
    "    for b_idx, batch in enumerate(image_val_loader):\n",
    "        if b_idx >= max_batches:\n",
    "            break\n",
    "        images = batch[\"images\"]\n",
    "        texts = batch[\"captions\"]\n",
    "\n",
    "        vision_feats = encode_images_to_features(images)\n",
    "        text_feats = encode_texts_to_features(texts)\n",
    "\n",
    "        z_v = encode_modality(model, vision_feats, None, \"vision\", use_perceiver)\n",
    "        z_t = encode_modality(model, text_feats, None, \"text\", use_perceiver)\n",
    "\n",
    "        all_z_v.append(z_v.cpu())\n",
    "        all_z_t_img.append(z_t.cpu())\n",
    "\n",
    "    if not all_z_v:\n",
    "        return {}\n",
    "\n",
    "    z_v_all = torch.cat(all_z_v, dim=0)\n",
    "    z_t_all = torch.cat(all_z_t_img, dim=0)\n",
    "\n",
    "    # Log histograms for VT\n",
    "    log_alignment_histograms(run, z_v_all.to(device), z_t_all.to(device), f\"{prefix}/vision_text\")\n",
    "\n",
    "    metrics = {\n",
    "        f\"{prefix}/num_samples_vt\": float(z_v_all.size(0)),\n",
    "    }\n",
    "\n",
    "    # Optionally log audio-text histograms\n",
    "    if audio_val_loader is not None:\n",
    "        all_z_a = []\n",
    "        all_z_t_a = []\n",
    "        for b_idx, batch in enumerate(audio_val_loader):\n",
    "            if b_idx >= max_batches:\n",
    "                break\n",
    "            audio = batch[\"audio\"]                    # (B, T_max)\n",
    "            texts = batch[\"captions\"]                 # list[str]\n",
    "            sr = int(batch.get(\"sr\", 44100))\n",
    "\n",
    "            audio_feats = encode_audio_to_features(audio, sr)\n",
    "            text_feats = encode_texts_to_features(texts)\n",
    "\n",
    "            z_a = encode_modality(model, audio_feats, None, \"audio\", use_perceiver)\n",
    "            z_t = encode_modality(model, text_feats, None, \"text\", use_perceiver)\n",
    "\n",
    "            all_z_a.append(z_a.cpu())\n",
    "            all_z_t_a.append(z_t.cpu())\n",
    "\n",
    "        if all_z_a:\n",
    "            z_a_all = torch.cat(all_z_a, dim=0)\n",
    "            z_t_a_all = torch.cat(all_z_t_a, dim=0)\n",
    "            log_alignment_histograms(run, z_a_all.to(device), z_t_a_all.to(device), f\"{prefix}/audio_text\")\n",
    "            metrics[f\"{prefix}/num_samples_at\"] = float(z_a_all.size(0))\n",
    "\n",
    "    if run is not None:\n",
    "        run.log(metrics)\n",
    "\n",
    "    print(f\"Validation ({prefix}) | VT samples: {metrics[f'{prefix}/num_samples_vt']}\")\n",
    "    if audio_val_loader is not None and f\"{prefix}/num_samples_at\" in metrics:\n",
    "        print(f\"Validation ({prefix}) | AT samples: {metrics[f'{prefix}/num_samples_at']}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37aa49de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceiver model trainable params: 21621760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251201_193557-kmrr7znk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/kmrr7znk' target=\"_blank\">02_multimodal_alignment_perceiver_mrl</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/kmrr7znk' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/kmrr7znk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Variant B: Perceiver + MLP + MRL (full model) ===\n",
    "\n",
    "# Re-instantiate model to avoid interference from MLP-only run\n",
    "model_perceiver = MultimodalAlignmentModel(mm_cfg).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_perceiver = nn.DataParallel(model_perceiver)\n",
    "\n",
    "print(\"Perceiver model trainable params:\", count_trainable_params(model_perceiver))\n",
    "\n",
    "optimizer_perceiver = torch.optim.AdamW(\n",
    "    get_model_module(model_perceiver).parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "best_metric_perceiver = -float(\"inf\")\n",
    "\n",
    "run_perceiver = init_wandb(run_name=\"02_multimodal_alignment_perceiver_mrl\", variant=\"perceiver_mrl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b9f81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_1261250/3010264231.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=cfg.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 20/148 | Loss: 12.5049 | VT MRL: 4.1982\n",
      "Epoch 1 | Step 40/148 | Loss: 11.6143 | VT MRL: 3.8075\n",
      "Epoch 1 | Step 60/148 | Loss: 10.8333 | VT MRL: 3.4829\n",
      "Epoch 1 | Step 80/148 | Loss: 10.2482 | VT MRL: 3.2439\n",
      "Epoch 1 | Step 100/148 | Loss: 9.7829 | VT MRL: 3.0474\n",
      "Epoch 1 | Step 120/148 | Loss: 9.3939 | VT MRL: 2.8840\n",
      "Epoch 1 | Step 140/148 | Loss: 9.0893 | VT MRL: 2.7555\n",
      "Epoch 1 summary | loss=8.9831 | VT MRL=2.7090 | AT MRL=3.2867\n",
      "Validation (val_perceiver) | VT samples: 500.0\n",
      "Validation (val_perceiver) | AT samples: 192.0\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Epoch 2 | Step 20/148 | Loss: 6.5770 | VT MRL: 1.7141\n",
      "Epoch 2 | Step 40/148 | Loss: 6.4771 | VT MRL: 1.6766\n",
      "Epoch 2 | Step 60/148 | Loss: 6.4041 | VT MRL: 1.6239\n",
      "Epoch 2 | Step 80/148 | Loss: 6.2889 | VT MRL: 1.5922\n",
      "Epoch 2 | Step 100/148 | Loss: 6.2748 | VT MRL: 1.5928\n",
      "Epoch 2 | Step 120/148 | Loss: 6.1959 | VT MRL: 1.5704\n",
      "Epoch 2 | Step 140/148 | Loss: 6.0971 | VT MRL: 1.5338\n",
      "Epoch 2 summary | loss=6.0752 | VT MRL=1.5238 | AT MRL=2.5303\n",
      "Validation (val_perceiver) | VT samples: 500.0\n",
      "Validation (val_perceiver) | AT samples: 192.0\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Epoch 3 | Step 20/148 | Loss: 4.8937 | VT MRL: 1.0458\n",
      "Epoch 3 | Step 40/148 | Loss: 4.9906 | VT MRL: 1.0711\n",
      "Epoch 3 | Step 60/148 | Loss: 5.0464 | VT MRL: 1.0963\n",
      "Epoch 3 | Step 80/148 | Loss: 4.9744 | VT MRL: 1.0826\n",
      "Epoch 3 | Step 100/148 | Loss: 4.9286 | VT MRL: 1.0687\n",
      "Epoch 3 | Step 120/148 | Loss: 4.8840 | VT MRL: 1.0546\n",
      "Epoch 3 | Step 140/148 | Loss: 4.8565 | VT MRL: 1.0500\n",
      "Epoch 3 summary | loss=4.8398 | VT MRL=1.0479 | AT MRL=2.1813\n",
      "Validation (val_perceiver) | VT samples: 500.0\n",
      "Validation (val_perceiver) | AT samples: 192.0\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Epoch 4 | Step 20/148 | Loss: 4.1483 | VT MRL: 0.7049\n",
      "Epoch 4 | Step 40/148 | Loss: 4.1218 | VT MRL: 0.7195\n",
      "Epoch 4 | Step 60/148 | Loss: 4.1315 | VT MRL: 0.7267\n",
      "Epoch 4 | Step 80/148 | Loss: 4.1350 | VT MRL: 0.7413\n",
      "Epoch 4 | Step 100/148 | Loss: 4.1376 | VT MRL: 0.7655\n",
      "Epoch 4 | Step 120/148 | Loss: 4.1181 | VT MRL: 0.7690\n",
      "Epoch 4 | Step 140/148 | Loss: 4.0877 | VT MRL: 0.7674\n",
      "Epoch 4 summary | loss=4.0798 | VT MRL=0.7701 | AT MRL=1.9517\n",
      "Validation (val_perceiver) | VT samples: 500.0\n",
      "Validation (val_perceiver) | AT samples: 192.0\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Epoch 5 | Step 20/148 | Loss: 3.3446 | VT MRL: 0.5168\n",
      "Epoch 5 | Step 40/148 | Loss: 3.4551 | VT MRL: 0.5341\n",
      "Epoch 5 | Step 60/148 | Loss: 3.4507 | VT MRL: 0.5474\n",
      "Epoch 5 | Step 80/148 | Loss: 3.4507 | VT MRL: 0.5535\n",
      "Epoch 5 | Step 100/148 | Loss: 3.4669 | VT MRL: 0.5651\n",
      "Epoch 5 | Step 120/148 | Loss: 3.4337 | VT MRL: 0.5711\n",
      "Epoch 5 | Step 140/148 | Loss: 3.4079 | VT MRL: 0.5713\n",
      "Epoch 5 summary | loss=3.3999 | VT MRL=0.5719 | AT MRL=1.6951\n",
      "Validation (val_perceiver) | VT samples: 500.0\n",
      "Validation (val_perceiver) | AT samples: 192.0\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Saved checkpoint to: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/final.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/loss_clip_at</td><td>‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss_clip_vt</td><td>‚ñà‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss_mrl_at</td><td>‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss_mrl_vt</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss_total</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/use_perceiver</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_perceiver/num_samples_at</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_perceiver/num_samples_vt</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/loss_clip_at</td><td>1.64251</td></tr><tr><td>train/loss_clip_vt</td><td>0.58885</td></tr><tr><td>train/loss_mrl_at</td><td>1.63587</td></tr><tr><td>train/loss_mrl_vt</td><td>0.59913</td></tr><tr><td>train/loss_total</td><td>3.35068</td></tr><tr><td>train/use_perceiver</td><td>1</td></tr><tr><td>val_perceiver/num_samples_at</td><td>192</td></tr><tr><td>val_perceiver/num_samples_vt</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">02_multimodal_alignment_perceiver_mrl</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/kmrr7znk' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/kmrr7znk</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_193557-kmrr7znk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, cfg.num_epochs_perceiver + 1):\n",
    "    stats = train_one_epoch(\n",
    "        epoch=epoch,\n",
    "        model=model_perceiver,\n",
    "        optimizer=optimizer_perceiver,\n",
    "        image_loader=image_train_loader,\n",
    "        audio_loader=audio_train_loader,\n",
    "        use_perceiver=True,\n",
    "        run=run_perceiver,\n",
    "    )\n",
    "\n",
    "    val_metrics = validate_alignment(\n",
    "        model=model_perceiver,\n",
    "        image_val_loader=image_val_loader,\n",
    "        audio_val_loader=audio_val_loader,\n",
    "        use_perceiver=True,\n",
    "        max_batches=20,\n",
    "        run=run_perceiver,\n",
    "        prefix=\"val_perceiver\",\n",
    "    )\n",
    "\n",
    "    current_metric = val_metrics.get(\"val_perceiver/num_samples_vt\", 0.0)\n",
    "    if current_metric >= best_metric_perceiver:\n",
    "        best_metric_perceiver = current_metric\n",
    "        save_checkpoint(\n",
    "            model=model_perceiver,\n",
    "            optimizer=optimizer_perceiver,\n",
    "            epoch=epoch,\n",
    "            best_metric=best_metric_perceiver,\n",
    "            out_dir=PERCEIVER_DIR,\n",
    "            tag=\"best\",\n",
    "        )\n",
    "\n",
    "# Save final checkpoint\n",
    "save_checkpoint(\n",
    "    model=model_perceiver,\n",
    "    optimizer=optimizer_perceiver,\n",
    "    epoch=cfg.num_epochs_perceiver,\n",
    "    best_metric=best_metric_perceiver,\n",
    "    out_dir=PERCEIVER_DIR,\n",
    "    tag=\"final\",\n",
    ")\n",
    "\n",
    "if run_perceiver is not None:\n",
    "    run_perceiver.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d54f382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Detailed alignment analysis for best checkpoints ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import wandb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_val_embeddings(\n",
    "    model: nn.Module,\n",
    "    image_val_loader: DataLoader,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 10,\n",
    "):\n",
    "    \"\"\"Collect aligned embeddings for a subset of the image-text val set.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_z_v = []\n",
    "    all_z_t = []\n",
    "\n",
    "    for b_idx, batch in enumerate(image_val_loader):\n",
    "        if b_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        images = batch[\"images\"]\n",
    "        texts = batch[\"captions\"]\n",
    "\n",
    "        vision_feats = encode_images_to_features(images)\n",
    "        text_feats = encode_texts_to_features(texts)\n",
    "\n",
    "        z_v = encode_modality(model, vision_feats, None, \"vision\", use_perceiver)\n",
    "        z_t = encode_modality(model, text_feats, None, \"text\", use_perceiver)\n",
    "\n",
    "        all_z_v.append(z_v.cpu())\n",
    "        all_z_t.append(z_t.cpu())\n",
    "\n",
    "    if not all_z_v:\n",
    "        raise RuntimeError(\"No validation batches collected for embeddings.\")\n",
    "\n",
    "    z_v_all = torch.cat(all_z_v, dim=0)\n",
    "    z_t_all = torch.cat(all_z_t, dim=0)\n",
    "\n",
    "    return z_v_all, z_t_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "289aa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_similarity_heatmap(z_v: torch.Tensor, z_t: torch.Tensor, title: str = \"\"):\n",
    "    \"\"\"Create a cosine similarity heatmap between image and text embeddings.\"\"\"\n",
    "    z_v_norm = F.normalize(z_v, dim=-1)\n",
    "    z_t_norm = F.normalize(z_t, dim=-1)\n",
    "\n",
    "    sim_matrix = z_v_norm @ z_t_norm.T  # (N, N)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(sim_matrix.numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "    ax.set_title(title or \"Cosine Similarity (vision ‚Üî text)\")\n",
    "    ax.set_xlabel(\"Text index\")\n",
    "    ax.set_ylabel(\"Image index\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    return fig, sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68895159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_tsne_plot(z_v: torch.Tensor, z_t: torch.Tensor, title: str = \"\"):\n",
    "    \"\"\"Create a t-SNE 2D scatter of vision and text aligned embeddings.\"\"\"\n",
    "    z_v_np = z_v.numpy()\n",
    "    z_t_np = z_t.numpy()\n",
    "\n",
    "    n_v = z_v_np.shape[0]\n",
    "    n_t = z_t_np.shape[0]\n",
    "\n",
    "    all_embeds = np.concatenate([z_v_np, z_t_np], axis=0)\n",
    "    perplexity = min(30, max(5, (n_v + n_t) // 5))\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        init=\"random\",\n",
    "        learning_rate=\"auto\",\n",
    "    )\n",
    "    all_2d = tsne.fit_transform(all_embeds)\n",
    "\n",
    "    v_2d = all_2d[:n_v]\n",
    "    t_2d = all_2d[n_v:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.scatter(v_2d[:, 0], v_2d[:, 1], label=\"vision\", alpha=0.7, s=20)\n",
    "    ax.scatter(t_2d[:, 0], t_2d[:, 1], label=\"text\", alpha=0.7, s=20, marker=\"x\")\n",
    "    ax.set_title(title or \"t-SNE of aligned embeddings (vision & text)\")\n",
    "    ax.set_xlabel(\"t-SNE dim 1\")\n",
    "    ax.set_ylabel(\"t-SNE dim 2\")\n",
    "    ax.legend()\n",
    "\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alignment analysis for variant: perceiver_mrl ===\n",
      "Loading best checkpoint from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "Loaded checkpoint from epoch 5 at /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/wandb/run-20251201_200135-ghacpglx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/ghacpglx' target=\"_blank\">02_alignment_analysis_perceiver_mrl</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/ghacpglx' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/ghacpglx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal similarity: mean=0.665, std=0.157 | Off-diagonal: mean=0.021, std=0.242\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>analysis/perceiver_mrl/d_align</td><td>‚ñÅ</td></tr><tr><td>analysis/perceiver_mrl/diag_mean</td><td>‚ñÅ</td></tr><tr><td>analysis/perceiver_mrl/diag_std</td><td>‚ñÅ</td></tr><tr><td>analysis/perceiver_mrl/num_samples</td><td>‚ñÅ</td></tr><tr><td>analysis/perceiver_mrl/offdiag_mean</td><td>‚ñÅ</td></tr><tr><td>analysis/perceiver_mrl/offdiag_std</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>analysis/perceiver_mrl/d_align</td><td>512</td></tr><tr><td>analysis/perceiver_mrl/diag_mean</td><td>0.66505</td></tr><tr><td>analysis/perceiver_mrl/diag_std</td><td>0.15684</td></tr><tr><td>analysis/perceiver_mrl/num_samples</td><td>500</td></tr><tr><td>analysis/perceiver_mrl/offdiag_mean</td><td>0.02055</td></tr><tr><td>analysis/perceiver_mrl/offdiag_std</td><td>0.2421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">02_alignment_analysis_perceiver_mrl</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/ghacpglx' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment/runs/ghacpglx</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass_phase1_alignment</a><br>Synced 4 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_200135-ghacpglx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def analyze_best_checkpoint(\n",
    "    model: nn.Module,\n",
    "    ckpt_dir: Path,\n",
    "    variant_name: str,\n",
    "    use_perceiver: bool,\n",
    "    max_batches: int = 10,\n",
    "):\n",
    "    \"\"\"Load best checkpoint, compute alignment diagnostics, and log plots to W&B.\"\"\"\n",
    "    best_ckpt_path = ckpt_dir / \"best.pt\"\n",
    "    if not best_ckpt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Best checkpoint not found at {best_ckpt_path}, skipping analysis.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Alignment analysis for variant: {variant_name} ===\")\n",
    "    print(f\"Loading best checkpoint from: {best_ckpt_path}\")\n",
    "    _ = load_checkpoint(model, optimizer=None, ckpt_path=best_ckpt_path, strict=True)\n",
    "\n",
    "    # Collect embeddings\n",
    "    z_v_all, z_t_all = collect_val_embeddings(\n",
    "        model=model,\n",
    "        image_val_loader=image_val_loader,\n",
    "        use_perceiver=use_perceiver,\n",
    "        max_batches=max_batches,\n",
    "    )\n",
    "\n",
    "    analysis_run = init_wandb(\n",
    "        run_name=f\"02_alignment_analysis_{variant_name}\",\n",
    "        variant=f\"{variant_name}_analysis\",\n",
    "        extra_config={\"checkpoint_path\": str(best_ckpt_path)},\n",
    "    )\n",
    "\n",
    "    # Histograms\n",
    "    if analysis_run is not None:\n",
    "        log_alignment_histograms(\n",
    "            analysis_run,\n",
    "            z_a=z_v_all.to(device),\n",
    "            z_b=z_t_all.to(device),\n",
    "            prefix=f\"analysis/{variant_name}/vision_text\",\n",
    "            max_points=512,\n",
    "        )\n",
    "\n",
    "    # Heatmap\n",
    "    fig_heatmap, sim_matrix = make_similarity_heatmap(\n",
    "        z_v_all,\n",
    "        z_t_all,\n",
    "        title=f\"{variant_name}: cosine similarity (vision ‚Üî text)\",\n",
    "    )\n",
    "\n",
    "    sim_np = sim_matrix.numpy()\n",
    "    diag = np.diag(sim_np)\n",
    "    off_diag = sim_np[~np.eye(sim_np.shape[0], dtype=bool)]\n",
    "\n",
    "    diag_mean = float(diag.mean())\n",
    "    diag_std = float(diag.std())\n",
    "    off_mean = float(off_diag.mean())\n",
    "    off_std = float(off_diag.std())\n",
    "\n",
    "    if analysis_run is not None:\n",
    "        analysis_run.log(\n",
    "            {\n",
    "                f\"analysis/{variant_name}/sim_heatmap\": wandb.Image(fig_heatmap),\n",
    "                f\"analysis/{variant_name}/diag_mean\": diag_mean,\n",
    "                f\"analysis/{variant_name}/diag_std\": diag_std,\n",
    "                f\"analysis/{variant_name}/offdiag_mean\": off_mean,\n",
    "                f\"analysis/{variant_name}/offdiag_std\": off_std,\n",
    "            }\n",
    "        )\n",
    "    plt.close(fig_heatmap)\n",
    "\n",
    "    print(\n",
    "        f\"Diagonal similarity: mean={diag_mean:.3f}, std={diag_std:.3f} | \"\n",
    "        f\"Off-diagonal: mean={off_mean:.3f}, std={off_std:.3f}\"\n",
    "    )\n",
    "\n",
    "    # t-SNE\n",
    "    fig_tsne = make_tsne_plot(\n",
    "        z_v_all,\n",
    "        z_t_all,\n",
    "        title=f\"{variant_name}: t-SNE of aligned embeddings\",\n",
    "    )\n",
    "    if analysis_run is not None:\n",
    "        analysis_run.log({f\"analysis/{variant_name}/tsne\": wandb.Image(fig_tsne)})\n",
    "        analysis_run.log(\n",
    "            {\n",
    "                f\"analysis/{variant_name}/num_samples\": float(z_v_all.size(0)),\n",
    "                f\"analysis/{variant_name}/d_align\": float(z_v_all.size(1)),\n",
    "            }\n",
    "        )\n",
    "        analysis_run.finish()\n",
    "    plt.close(fig_tsne)\n",
    "\n",
    "\n",
    "analyze_best_checkpoint(\n",
    "    model=model_perceiver,\n",
    "    ckpt_dir=PERCEIVER_DIR,\n",
    "    variant_name=\"perceiver_mrl\",\n",
    "    use_perceiver=True,\n",
    "    max_batches=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727c7f",
   "metadata": {},
   "source": [
    "## Outputs & Next Steps\n",
    "\n",
    "This notebook produces **Phase 1 multimodal alignment checkpoints**:\n",
    "\n",
    "- **MLP + MRL (no Perceiver)**: `checkpoints/phase1_multimodal/mlp_mrl/`\n",
    "- **Perceiver + MLP + MRL**: `checkpoints/phase1_multimodal/perceiver_mrl/`\n",
    "\n",
    "Each directory contains:\n",
    "\n",
    "- `best.pt` ‚Äì best model according to a validation proxy metric (placeholder for now)\n",
    "- `final.pt` ‚Äì final epoch checkpoint\n",
    "\n",
    "Additionally, W&B runs include:\n",
    "\n",
    "- Training curves (`train/loss_*`, `val_*/num_samples_*`).\n",
    "- Cosine similarity **histograms** (pos vs neg pairs) for vision‚Äìtext and audio‚Äìtext.\n",
    "- Cosine similarity **heatmaps** (vision ‚Üî text) for best checkpoints.\n",
    "- **t-SNE visualizations** of aligned embeddings (vision & text) for best checkpoints.\n",
    "\n",
    "These checkpoints and diagnostics are the foundation for Phase 2 experiments:\n",
    "\n",
    "- LLM decoder alignment (normal decoder).\n",
    "- TRM decoder alignment.\n",
    "- MoE decoder alignment.\n",
    "- Full retrieval-based evaluation and Matryoshka ablations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
