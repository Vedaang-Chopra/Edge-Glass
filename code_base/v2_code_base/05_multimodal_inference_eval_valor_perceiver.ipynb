{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a08ba2",
   "metadata": {},
   "source": [
    "# 05 — Multimodal LLM Inference & Evaluation (VALOR + Perceiver Alignment)\n",
    "\n",
    "This notebook performs **pure inference** with the trained multimodal LLM (image–text via Perceiver alignment, audio available in VALOR clips), using the Phase‑3 checkpoint trained on VALOR.\n",
    "\n",
    "It will:\n",
    "- Load the **Phase‑1 Perceiver alignment** checkpoint and **Phase‑3 multimodal LLM** checkpoint.\n",
    "- Build lightweight VALOR clip‑level dataset and sample random **image + caption + audio** clips.\n",
    "- Run **qualitative generations** (single‑sample debug view).\n",
    "- Run **batched evaluation** on random clips and compute simple text similarity metrics.\n",
    "- Produce a few **plots for explainability**, such as:\n",
    "  - Distribution of generated caption lengths vs ground‑truth.\n",
    "  - Simple lexical similarity scores between generated and ground‑truth captions.\n",
    "  - A small table of qualitative examples.\n",
    "  \n",
    "> **NOTE:** This notebook assumes the same project layout as the Phase‑3 training notebook\n",
    "> (`04_multimodal_llm_decoder_training_valor_perceiver.ipynb`). Adjust paths marked with `# <-- EDIT` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa9f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "from PIL import UnidentifiedImageError\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: enable inline plots if running in a notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# Project imports (must match your codebase)\n",
    "from imports.core import VisionEncoder, set_seed, count_parameters\n",
    "from imports.llm_integration import LLMConfig, MultimodalLLM\n",
    "from imports.multimodal_alignment_perceiver import MultimodalAlignmentConfig, MultimodalAlignmentModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe7b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR                     : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base\n",
      "PHASE1_PERCEIVER_CKPT_PATH   : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\n",
      "VALOR_SHARDS_DIR             : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards\n",
      "PHASE3_OUT_DIR               : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase3_llm_valor_perceiver/valor_qwen2p5_phase3_perceiver_v1\n",
      "BEST_CKPT_PATH               : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase3_llm_valor_perceiver/valor_qwen2p5_phase3_perceiver_v1/best_phase3_valor_perceiver.pt\n",
      "FINAL_CKPT_PATH              : /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase3_llm_valor_perceiver/valor_qwen2p5_phase3_perceiver_v1/final_phase3_valor_perceiver.pt\n",
      "Using device: cuda\n",
      "Using dtype : torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# === Paths & run configuration (mirror Phase-3 training) ===\n",
    "\n",
    "ROOT_DIR = Path.cwd()   # <-- EDIT if you typically run notebooks from a different root\n",
    "\n",
    "# Phase‑1 Perceiver alignment checkpoint (used to rebuild aligner)\n",
    "PHASE1_PERCEIVER_CKPT_PATH = ROOT_DIR / \"checkpoints\" / \"phase1_multimodal\" / \"perceiver_mrl\" / \"best.pt\"  # <-- EDIT\n",
    "\n",
    "# VALOR shards directory (same as training)\n",
    "VALOR_SHARDS_DIR = ROOT_DIR / \"data\" / \"alignment_subsets\" / \"valor32k_train_shards\"  # <-- EDIT\n",
    "\n",
    "PHASE3_RUN_NAME = \"valor_qwen2p5_phase3_perceiver_v1\"  # <-- EDIT if you changed run name\n",
    "PHASE3_OUT_DIR = ROOT_DIR / \"checkpoints\" / \"phase3_llm_valor_perceiver\" / PHASE3_RUN_NAME\n",
    "\n",
    "BEST_CKPT_PATH = PHASE3_OUT_DIR / \"best_phase3_valor_perceiver.pt\"   # saved during training loop\n",
    "FINAL_CKPT_PATH = PHASE3_OUT_DIR / \"final_phase3_valor_perceiver.pt\" # saved at the end\n",
    "\n",
    "print(\"ROOT_DIR                     :\", ROOT_DIR)\n",
    "print(\"PHASE1_PERCEIVER_CKPT_PATH   :\", PHASE1_PERCEIVER_CKPT_PATH)\n",
    "print(\"VALOR_SHARDS_DIR             :\", VALOR_SHARDS_DIR)\n",
    "print(\"PHASE3_OUT_DIR               :\", PHASE3_OUT_DIR)\n",
    "print(\"BEST_CKPT_PATH               :\", BEST_CKPT_PATH)\n",
    "print(\"FINAL_CKPT_PATH              :\", FINAL_CKPT_PATH)\n",
    "\n",
    "assert PHASE1_PERCEIVER_CKPT_PATH.is_file(), f\"Phase‑1 Perceiver checkpoint not found: {PHASE1_PERCEIVER_CKPT_PATH}\"\n",
    "assert VALOR_SHARDS_DIR.is_dir(), f\"VALOR shards directory not found: {VALOR_SHARDS_DIR}\"\n",
    "assert BEST_CKPT_PATH.is_file() or FINAL_CKPT_PATH.is_file(), \"No Phase‑3 checkpoint found (best or final).\"\n",
    "\n",
    "# Device / dtype (same heuristic as in training)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif torch.cuda.is_available():\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Using dtype :\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f231f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValorMultiFrameDataset(Dataset):\n",
    "    \"\"\"Clip-level VALOR dataset (multi-frame, audio + caption).\n",
    "\n",
    "    This version is identical in spirit to the training notebook, but we only\n",
    "    use it for **inference** and keep options for subsampling for speed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_all: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length: int = 96,\n",
    "        frame_limit_per_clip: Optional[int] = None,\n",
    "        max_clips: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Group by video_id just like training\n",
    "        grouped = df_all.groupby(\"video_id\", sort=False)\n",
    "        self.clips: List[Dict[str, Any]] = []\n",
    "        total_frames = 0\n",
    "\n",
    "        for vid, group in grouped:\n",
    "            frames = group[\"image_jpegs\"].iloc[0]\n",
    "            audio = group[\"audio_wav\"].iloc[0]\n",
    "            caption = group[\"caption\"].iloc[0]\n",
    "\n",
    "            if frame_limit_per_clip is not None and frames is not None:\n",
    "                frames = frames[:frame_limit_per_clip]\n",
    "\n",
    "            total_frames += len(frames) if frames is not None else 0\n",
    "\n",
    "            self.clips.append(\n",
    "                {\n",
    "                    \"video_id\": vid,\n",
    "                    \"frames\": frames,   # list of jpeg bytes (or lists)\n",
    "                    \"audio_wav\": audio,\n",
    "                    \"caption\": str(caption),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if max_clips is not None and len(self.clips) >= max_clips:\n",
    "                break\n",
    "\n",
    "        print(\"Total clips (videos):\", len(self.clips))\n",
    "        print(\"Total raw frames stored:\", total_frames)\n",
    "\n",
    "        # Pre-tokenize captions for potential text-based baselines\n",
    "        self._tok: List[Dict[str, Any]] = []\n",
    "        for clip in tqdm(self.clips, desc=\"Tokenizing captions\"):\n",
    "            text = clip[\"caption\"]\n",
    "            toks = tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self._tok.append({k: v.squeeze(0) for k, v in toks.items()})\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clips)\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_image(jpeg_bytes: bytes) -> PILImage.Image:\n",
    "        return PILImage.open(io.BytesIO(jpeg_bytes)).convert(\"RGB\")\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        clip = self.clips[idx]\n",
    "        toks = self._tok[idx]\n",
    "\n",
    "        frames: List[PILImage.Image] = []\n",
    "\n",
    "        for cell in clip[\"frames\"]:\n",
    "            if cell is None:\n",
    "                continue\n",
    "\n",
    "            # Handle possible nested lists from parquet\n",
    "            if isinstance(cell, (list, tuple)):\n",
    "                for sub in cell:\n",
    "                    if sub is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        frames.append(self._decode_image(sub))\n",
    "                    except UnidentifiedImageError:\n",
    "                        continue\n",
    "            else:\n",
    "                try:\n",
    "                    frames.append(self._decode_image(cell))\n",
    "                except UnidentifiedImageError:\n",
    "                    continue\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            # Fallback gray image if all decodes fail\n",
    "            frames = [PILImage.new(\"RGB\", (224, 224), color=(128, 128, 128))]\n",
    "\n",
    "        return {\n",
    "            \"video_id\": clip[\"video_id\"],\n",
    "            \"frames\": frames,\n",
    "            \"audio_wav\": clip[\"audio_wav\"],\n",
    "            \"caption\": clip[\"caption\"],\n",
    "            \"input_ids\": toks[\"input_ids\"],\n",
    "            \"attention_mask\": toks[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_valor(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Simple collate: keep Python lists for frames, stack token tensors.\"\"\"\n",
    "    video_ids = [b[\"video_id\"] for b in batch]\n",
    "    frames = [b[\"frames\"] for b in batch]\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "    audio_wav = [b[\"audio_wav\"] for b in batch]\n",
    "\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch], dim=0)\n",
    "\n",
    "    return {\n",
    "        \"video_id\": video_ids,\n",
    "        \"frames\": frames,\n",
    "        \"captions\": captions,\n",
    "        \"audio_wav\": audio_wav,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_single_frame_per_clip(frames_batch: List[List[PILImage.Image]]) -> List[PILImage.Image]:\n",
    "    \"\"\"Pick exactly one random frame per clip for image‑conditioned generation.\"\"\"\n",
    "    images: List[PILImage.Image] = []\n",
    "    for frames in frames_batch:\n",
    "        if len(frames) == 0:\n",
    "            images.append(PILImage.new(\"RGB\", (224, 224), color=(128, 128, 128)))\n",
    "        else:\n",
    "            images.append(random.choice(frames))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40203a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'model_state' from checkpoint as state_dict.\n",
      "Loaded Perceiver alignment checkpoint.\n",
      "Missing keys   : 0\n",
      "Unexpected keys: 0\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "perceiver_cfg = MultimodalAlignmentConfig()\n",
    "perceiver_model = MultimodalAlignmentModel(perceiver_cfg)\n",
    "\n",
    "# Attach cfg so MultimodalLLM can use aligner.cfg.*\n",
    "perceiver_cfg.device = device\n",
    "perceiver_cfg.dtype = dtype\n",
    "perceiver_model.cfg = perceiver_cfg\n",
    "\n",
    "ckpt = torch.load(PHASE1_PERCEIVER_CKPT_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "# Correct: your training script stored the real weights under \"model_state\"\n",
    "if isinstance(ckpt, dict) and \"model_state\" in ckpt and isinstance(ckpt[\"model_state\"], dict):\n",
    "    state_dict = ckpt[\"model_state\"]\n",
    "    print(\"Using 'model_state' from checkpoint as state_dict.\")\n",
    "else:\n",
    "    # Fallback heuristics if you ever change the saving format\n",
    "    state_dict = None\n",
    "    if isinstance(ckpt, dict):\n",
    "        for key in [\"model_state_dict\", \"alignment_model\", \"state_dict\", \"model\"]:\n",
    "            if key in ckpt and isinstance(ckpt[key], dict):\n",
    "                state_dict = ckpt[key]\n",
    "                print(f\"Using '{key}' from checkpoint as state_dict.\")\n",
    "                break\n",
    "        if state_dict is None:\n",
    "            print(\"No wrapper key found — treating entire checkpoint as state_dict.\")\n",
    "            state_dict = ckpt\n",
    "    else:\n",
    "        print(\"Checkpoint is not a dict with wrapper keys — treating as state_dict.\")\n",
    "        state_dict = ckpt\n",
    "\n",
    "missing, unexpected = perceiver_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Loaded Perceiver alignment checkpoint.\")\n",
    "print(\"Missing keys   :\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002f30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Perceiver alignment checkpoint.\n",
      "Missing keys   : 0\n",
      "Unexpected keys: 0\n",
      "\n",
      "=== Missing Keys ===\n",
      "\n",
      "=== Unexpected Keys ===\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = perceiver_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Loaded Perceiver alignment checkpoint.\")\n",
    "print(\"Missing keys   :\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "# Print full lists for debugging\n",
    "print(\"\\n=== Missing Keys ===\")\n",
    "for k in missing:\n",
    "    print(k)\n",
    "\n",
    "print(\"\\n=== Unexpected Keys ===\")\n",
    "for k in unexpected:\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f9772fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "Perceiver params total : {'total': 21621760, 'trainable': 21621760, 'frozen': 0}\n",
      "Vision backbone hidden : 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "perceiver_model.to(device)\n",
    "perceiver_model = perceiver_model.to(dtype=dtype)\n",
    "perceiver_model.eval()\n",
    "\n",
    "\n",
    "# Vision backbone (CLIP) – same as in Phase‑1\n",
    "vision_backbone = VisionEncoder(\n",
    "    model_name=perceiver_cfg.vision_model_name,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "vision_backbone.eval()\n",
    "for p in vision_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Perceiver params total :\", count_parameters(perceiver_model))\n",
    "print(\"Vision backbone hidden :\", vision_backbone.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e69c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMDecoder] Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833f1a32f76454090d14fa08bf0b29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMDecoder] hidden_size=2048, frozen=True\n",
      "[MultimodalLLM] Projector: 512 → 8 × 2048\n",
      "Wrapping multimodal model in DataParallel over 2 GPUs\n",
      "Loading Phase‑3 checkpoint from: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/checkpoints/phase3_llm_valor_perceiver/valor_qwen2p5_phase3_perceiver_v1/best_phase3_valor_perceiver.pt\n",
      "Loaded Phase‑3 state_dict.\n",
      "Missing keys   : 149\n",
      "Unexpected keys: 348\n",
      "Multimodal params (trainable): 8406016\n"
     ]
    }
   ],
   "source": [
    "# === Rebuild Multimodal LLM (Perceiver aligner + Qwen2.5) and load Phase‑3 checkpoint ===\n",
    "\n",
    "# Keep cfg small and focused on inference\n",
    "llm_cfg = LLMConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",   # <-- EDIT if you changed this in training\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_prefix_tokens=8,                     # must match training\n",
    "    freeze_llm=True,                         # we are only doing inference\n",
    "    \n",
    ")\n",
    "\n",
    "# Wrap the Perceiver aligner as the \"aligner\" used by MultimodalLLM\n",
    "aligner = perceiver_model\n",
    "\n",
    "mm = MultimodalLLM(\n",
    "    aligner=aligner,\n",
    "    llm_config=llm_cfg,\n",
    ")\n",
    "\n",
    "mm.to(device)\n",
    "mm = mm.to(dtype=dtype)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Wrapping multimodal model in DataParallel over {torch.cuda.device_count()} GPUs\")\n",
    "    mm = nn.DataParallel(mm)\n",
    "\n",
    "mm_module = mm.module if isinstance(mm, nn.DataParallel) else mm\n",
    "\n",
    "# Load best checkpoint if available, else fall back to final\n",
    "ckpt_path = BEST_CKPT_PATH if BEST_CKPT_PATH.is_file() else FINAL_CKPT_PATH\n",
    "print(\"Loading Phase‑3 checkpoint from:\", ckpt_path)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
    "missing, unexpected = mm_module.load_state_dict(state_dict, strict=False)\n",
    "print(\"Loaded Phase‑3 state_dict.\")\n",
    "print(\"Missing keys   :\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "mm_module.eval()\n",
    "\n",
    "def count_parameters(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "print(\"Multimodal params (trainable):\", count_parameters(mm_module, trainable_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0f7101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 VALOR shards.\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard000.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard001.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard002.parquet\n",
      "  Error loading shard /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard002.parquet: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard003.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch000_shard004.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch001_shard000.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch001_shard001.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch001_shard002.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch001_shard003.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch001_shard004.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch002_shard000.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch002_shard001.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch002_shard002.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch002_shard003.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch002_shard004.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch003_shard000.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch003_shard001.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch003_shard002.parquet\n",
      "Loading shard: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v2_code_base/data/alignment_subsets/valor32k_train_shards/valor32k_train_batch003_shard003.parquet\n",
      "Full VALOR rows: 3307\n",
      "                      video_id        yt_id  start    end  \\\n",
      "0    x-2Abohj8VY_30.000_40.000  x-2Abohj8VY   30.0   40.0   \n",
      "1    ILE12hEW5Ck_30.000_40.000  ILE12hEW5Ck   30.0   40.0   \n",
      "2    o2bqT0ZTz7E_70.000_80.000  o2bqT0ZTz7E   70.0   80.0   \n",
      "3  ItvWPMW7RWE_150.000_160.000  ItvWPMW7RWE  150.0  160.0   \n",
      "4    1zoqi-E5vFY_50.000_60.000  1zoqi-E5vFY   50.0   60.0   \n",
      "\n",
      "                                             caption  \\\n",
      "0   With the rumble, on a moving bus, a crowd spoke.   \n",
      "1  In one room, a group of women were dancing dis...   \n",
      "2  To the music, a man in a blue shirt sings whil...   \n",
      "3  A shoal of grey poultry creaking in the grass ...   \n",
      "4  A woman explained by the sewing machine and se...   \n",
      "\n",
      "                                         image_jpegs  \\\n",
      "0  [b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x0...   \n",
      "1  [b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x0...   \n",
      "2  [b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x0...   \n",
      "3  [b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x0...   \n",
      "4  [b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x0...   \n",
      "\n",
      "                                           audio_wav  \n",
      "0  b'RIFF\\xff\\xff\\xff\\xffWAVEfmt \\x10\\x00\\x00\\x00...  \n",
      "1  b'RIFF\\xff\\xff\\xff\\xffWAVEfmt \\x10\\x00\\x00\\x00...  \n",
      "2  b'RIFF\\xff\\xff\\xff\\xffWAVEfmt \\x10\\x00\\x00\\x00...  \n",
      "3  b'RIFF\\xff\\xff\\xff\\xffWAVEfmt \\x10\\x00\\x00\\x00...  \n",
      "4  b'RIFF\\xff\\xff\\xff\\xffWAVEfmt \\x10\\x00\\x00\\x00...  \n",
      "Subsampled VALOR rows: 100\n"
     ]
    }
   ],
   "source": [
    "# === Load VALOR shards into a single DataFrame (lightweight subset) ===\n",
    "\n",
    "shard_paths = sorted(VALOR_SHARDS_DIR.glob(\"*.parquet\"))\n",
    "assert len(shard_paths) > 0, f\"No parquet shards found in {VALOR_SHARDS_DIR}\"\n",
    "\n",
    "print(\"Found\", len(shard_paths), \"VALOR shards.\")\n",
    "\n",
    "dfs = []\n",
    "for p in shard_paths:\n",
    "    try:\n",
    "        print(\"Loading shard:\", p)\n",
    "        df_shard = pd.read_parquet(p)\n",
    "        dfs.append(df_shard)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading shard {p}: {e}\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Full VALOR rows:\", len(df_all))\n",
    "print(df_all.head())\n",
    "\n",
    "# Optional: subsample for faster inference\n",
    "MAX_ROWS_FOR_INFERENCE = 100  # <-- EDIT; None = all rows\n",
    "if MAX_ROWS_FOR_INFERENCE is not None and len(df_all) > MAX_ROWS_FOR_INFERENCE:\n",
    "    df_all = df_all.sample(n=MAX_ROWS_FOR_INFERENCE, random_state=42)\n",
    "    print(\"Subsampled VALOR rows:\", len(df_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e111cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips (videos): 100\n",
      "Total raw frames stored: 277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54167dfe39d442639730724810828621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing captions:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loader ready with 100 clips.\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizer via the LLM module\n",
    "tokenizer = mm_module.llm.tokenizer\n",
    "\n",
    "dataset = ValorMultiFrameDataset(\n",
    "    df_all=df_all,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=96,\n",
    "    frame_limit_per_clip=8,\n",
    "    max_clips=512,     # cap at a few hundred clips for evaluation\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_valor,\n",
    ")\n",
    "\n",
    "print(\"Eval loader ready with\", len(dataset), \"clips.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3476665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now call:\n",
      "  debug_generate_for_clip(0)\n",
      "  debug_generate_for_clip(10, prompt='Give a detailed description:')\n"
     ]
    }
   ],
   "source": [
    "def debug_generate_for_clip(idx: int = 0, prompt: str = \"Describe this video frame:\") -> Dict[str, Any]:\n",
    "    \"\"\"Qualitative inspection: pick one clip, one random frame, and generate a caption.\"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    clip = dataset.clips[idx]\n",
    "    frames = [ValorMultiFrameDataset._decode_image(b) for b in clip[\"frames\"]]\n",
    "    img = random.choice(frames) if len(frames) > 0 else PILImage.new(\"RGB\", (224, 224), color=(128, 128, 128))\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Video ID: {clip['video_id']}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Ground‑truth caption:\")\n",
    "    print(clip[\"caption\"])\n",
    "    print(\"\\nGenerated caption:\")\n",
    "\n",
    "    gen_text = mm_module.generate(\n",
    "        images=img,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=96,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(gen_text)\n",
    "\n",
    "    return {\n",
    "        \"video_id\": clip[\"video_id\"],\n",
    "        \"image\": img,\n",
    "        \"caption_gt\": clip[\"caption\"],\n",
    "        \"caption_gen\": gen_text,\n",
    "    }\n",
    "\n",
    "print(\"You can now call:\")\n",
    "print(\"  debug_generate_for_clip(0)\")\n",
    "print(\"  debug_generate_for_clip(10, prompt='Give a detailed description:')\")\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "from typing import Union\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_pil_image(\n",
    "    img: Union[PILImage.Image, str],\n",
    "    prompt: str = \"Describe this image:\",\n",
    "    max_new_tokens: int = 96,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Image-conditioned generation:\n",
    "\n",
    "    PIL image -> CLIP vision encoder -> Perceiver aligner -> projector -> LLM.\n",
    "    \"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    # 0. Load image if path\n",
    "    if isinstance(img, str):\n",
    "        img = PILImage.open(img).convert(\"RGB\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    # 1. Vision backbone: get vision features (same as in training)\n",
    "    # If your API differs, adapt this line to whatever you used during Perceiver training:\n",
    "    feats = vision_backbone.encode_images([img])   # (B, T, D) or (B, D)\n",
    "    feats = feats.to(device=device, dtype=dtype)\n",
    "\n",
    "    # 2. Perceiver: features -> aligned vision embedding\n",
    "    aligner = mm_module.aligner\n",
    "    z_align = aligner.encode_vision(feats)         # (B, d_align)\n",
    "\n",
    "    # 3. Project to LLM prefix tokens\n",
    "    projector = mm_module.projector\n",
    "    prefix = projector(z_align)                    # (B, num_tokens, d_llm)\n",
    "\n",
    "    # 4. LLM generate with prefix\n",
    "    tokenizer = mm_module.llm.tokenizer\n",
    "    llm_model = mm_module.llm.model\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        prefix_embeds=prefix,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98b0b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now call:\n",
      "  debug_generate_for_clip(0)\n",
      "  debug_generate_for_clip(10, prompt='Give a detailed description:')\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as PILImage\n",
    "from typing import Union, Optional\n",
    "from collections import Counter\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 1. Single-image generator (PIL → features → Perceiver → LLM) ----------\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_pil_image(\n",
    "    img: Union[PILImage.Image, str],\n",
    "    prompt: str = \"Describe this image:\",\n",
    "    max_new_tokens: int = 96,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Image-conditioned generation:\n",
    "\n",
    "    PIL image/path -> VisionEncoder -> Perceiver aligner -> projector -> LLM.\n",
    "    \"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    # 0. Load image if path\n",
    "    if isinstance(img, str):\n",
    "        img = PILImage.open(img).convert(\"RGB\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    # 1. Vision backbone: get vision features (same as in training)\n",
    "    # Your VisionEncoder is an nn.Module, so we just call it.\n",
    "    # It may return either a tensor or a dict — handle both.\n",
    "    vision_backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        feats = vision_backbone([img])  # (B, T, D) or (B, D) or dict\n",
    "\n",
    "    if isinstance(feats, dict):\n",
    "        # Try common keys used in encoders; adapt if your VisionEncoder uses a specific one.\n",
    "        for key in [\"image_embeds\", \"features\", \"last_hidden_state\", \"pooler_output\"]:\n",
    "            if key in feats:\n",
    "                feats = feats[key]\n",
    "                break\n",
    "\n",
    "    if not isinstance(feats, torch.Tensor):\n",
    "        raise TypeError(\n",
    "            f\"VisionEncoder returned type {type(feats)}; expected tensor or dict of tensors. \"\n",
    "            \"Please adapt generate_from_pil_image to match your VisionEncoder API.\"\n",
    "        )\n",
    "\n",
    "    feats = feats.to(device=device, dtype=dtype)\n",
    "\n",
    "    # 2. Perceiver: features -> aligned vision embedding\n",
    "    aligner = mm_module.aligner                    # MultimodalAlignmentModel\n",
    "    z_align = aligner.encode_vision(feats)         # (B, d_align)\n",
    "\n",
    "    # 3. Project to LLM prefix tokens\n",
    "    projector = mm_module.projector                # VisionToLLMProjector\n",
    "    prefix = projector(z_align)                    # (B, num_tokens, d_llm)\n",
    "\n",
    "    # 4. LLM generate with prefix\n",
    "    tokenizer = mm_module.llm.tokenizer\n",
    "    llm_model = mm_module.llm.model\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        prefix_embeds=prefix,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ---------- 2. Qualitative single-clip debug ----------\n",
    "\n",
    "def debug_generate_for_clip(idx: int = 0, prompt: str = \"Describe this video frame:\"):\n",
    "    \"\"\"Qualitative inspection: pick one clip, one random frame, and generate a caption.\"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    clip = dataset.clips[idx]\n",
    "\n",
    "    # Decode stored JPEG bytes to PIL images\n",
    "    frames = []\n",
    "    for cell in clip[\"frames\"]:\n",
    "        if cell is None:\n",
    "            continue\n",
    "        if isinstance(cell, (list, tuple)):\n",
    "            for sub in cell:\n",
    "                if sub is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    frames.append(ValorMultiFrameDataset._decode_image(sub))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        else:\n",
    "            try:\n",
    "                frames.append(ValorMultiFrameDataset._decode_image(cell))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    img = random.choice(frames) if len(frames) > 0 else PILImage.new(\n",
    "        \"RGB\", (224, 224), color=(128, 128, 128)\n",
    "    )\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Video ID: {clip['video_id']}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Ground-truth caption:\")\n",
    "    print(clip[\"caption\"])\n",
    "    print(\"\\nGenerated caption:\")\n",
    "\n",
    "    gen_text = generate_from_pil_image(\n",
    "        img,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=96,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    print(gen_text)\n",
    "\n",
    "    return {\n",
    "        \"video_id\": clip[\"video_id\"],\n",
    "        \"image\": img,\n",
    "        \"caption_gt\": clip[\"caption\"],\n",
    "        \"caption_gen\": gen_text,\n",
    "    }\n",
    "\n",
    "print(\"You can now call:\")\n",
    "print(\"  debug_generate_for_clip(0)\")\n",
    "print(\"  debug_generate_for_clip(10, prompt='Give a detailed description:')\")\n",
    "\n",
    "\n",
    "# ---------- 3. Simple lexical similarity helpers ----------\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    return \" \".join(s.lower().strip().split())\n",
    "\n",
    "def _bag_of_words_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Very simple BoW Jaccard similarity between two strings.\"\"\"\n",
    "    a_toks = _normalize_text(a).split()\n",
    "    b_toks = _normalize_text(b).split()\n",
    "    if not a_toks or not b_toks:\n",
    "        return 0.0\n",
    "    ca = Counter(a_toks)\n",
    "    cb = Counter(b_toks)\n",
    "    inter = sum((ca & cb).values())\n",
    "    union = sum((ca | cb).values())\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "# ---------- 4. Batch eval over VALOR clips using the fixed image pipeline ----------\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_batch_eval(num_batches: int = 32, max_samples: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run generation on a subset of VALOR clips and compute simple metrics.\n",
    "\n",
    "    Metrics per sample:\n",
    "    - `len_gt`, `len_gen`: character lengths\n",
    "    - `bow_jaccard`: simple bag-of-words Jaccard similarity\n",
    "    \"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    rows = []\n",
    "    n_seen = 0\n",
    "\n",
    "    for b_idx, batch in enumerate(eval_loader):\n",
    "        if b_idx >= num_batches:\n",
    "            break\n",
    "\n",
    "        images = sample_single_frame_per_clip(batch[\"frames\"])  # list[PIL.Image]\n",
    "        captions_gt = batch[\"captions\"]\n",
    "\n",
    "        batch_gen = []\n",
    "        for img in images:\n",
    "            gen = generate_from_pil_image(\n",
    "                img,\n",
    "                prompt=\"Describe this video frame:\",\n",
    "                max_new_tokens=96,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "            batch_gen.append(gen)\n",
    "\n",
    "        for vid, gt, gen, img in zip(batch[\"video_id\"], captions_gt, batch_gen, images):\n",
    "            n_seen += 1\n",
    "            len_gt = len(gt)\n",
    "            len_gen = len(gen)\n",
    "            sim = _bag_of_words_similarity(gt, gen)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"video_id\": vid,\n",
    "                    \"caption_gt\": gt,\n",
    "                    \"caption_gen\": gen,\n",
    "                    \"len_gt\": len_gt,\n",
    "                    \"len_gen\": len_gen,\n",
    "                    \"bow_jaccard\": sim,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if max_samples is not None and n_seen >= max_samples:\n",
    "                break\n",
    "\n",
    "        if max_samples is not None and n_seen >= max_samples:\n",
    "            break\n",
    "\n",
    "    df_eval = pd.DataFrame(rows)\n",
    "    print(\"Collected eval samples:\", len(df_eval))\n",
    "    return df_eval\n",
    "\n",
    "# Example call:\n",
    "# eval_results = run_batch_eval(num_batches=16, max_samples=128)\n",
    "# eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fd0db3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [VisionEncoder] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run a small eval by default (edit parameters)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m eval_results = \u001b[43mrun_batch_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m eval_results.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mrun_batch_eval\u001b[39m\u001b[34m(num_batches, max_samples)\u001b[39m\n\u001b[32m    188\u001b[39m batch_gen = []\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     gen = \u001b[43mgenerate_from_pil_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescribe this video frame:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     batch_gen.append(gen)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vid, gt, gen, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33mvideo_id\u001b[39m\u001b[33m\"\u001b[39m], captions_gt, batch_gen, images):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mgenerate_from_pil_image\u001b[39m\u001b[34m(img, prompt, max_new_tokens, temperature, top_p, do_sample)\u001b[39m\n\u001b[32m     36\u001b[39m vision_backbone.eval()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     feats = \u001b[43mvision_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, T, D) or (B, D) or dict\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feats, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Try common keys used in encoders; adapt if your VisionEncoder uses a specific one.\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpooler_output\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Module [VisionEncoder] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# Run a small eval by default (edit parameters)\n",
    "eval_results = run_batch_eval(num_batches=16, max_samples=128)\n",
    "eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30b00b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Plots: length distributions and BoW similarity ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43meval_results\u001b[49m) > \u001b[32m0\u001b[39m:\n\u001b[32m      4\u001b[39m     plt.figure()\n\u001b[32m      5\u001b[39m     plt.hist(eval_results[\u001b[33m\"\u001b[39m\u001b[33mlen_gt\u001b[39m\u001b[33m\"\u001b[39m], bins=\u001b[32m20\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mGT length\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'eval_results' is not defined"
     ]
    }
   ],
   "source": [
    "# === Plots: length distributions and BoW similarity ===\n",
    "\n",
    "if len(eval_results) > 0:\n",
    "    plt.figure()\n",
    "    plt.hist(eval_results[\"len_gt\"], bins=20, alpha=0.5, label=\"GT length\")\n",
    "    plt.hist(eval_results[\"len_gen\"], bins=20, alpha=0.5, label=\"Gen length\")\n",
    "    plt.xlabel(\"Caption length (characters)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Ground‑truth vs Generated caption lengths\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(eval_results[\"bow_jaccard\"], bins=20)\n",
    "    plt.xlabel(\"BoW Jaccard similarity\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of lexical similarity (GT vs Gen)\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Basic statistics:\")\n",
    "    print(eval_results[[\"len_gt\", \"len_gen\", \"bow_jaccard\"]].describe())\n",
    "\n",
    "    # Show a few best and worst examples by similarity\n",
    "    print(\"\\nTop 5 highest similarity examples:\")\n",
    "    display(eval_results.sort_values(\"bow_jaccard\", ascending=False).head(5))\n",
    "\n",
    "    print(\"\\nTop 5 lowest similarity examples:\")\n",
    "    display(eval_results.sort_values(\"bow_jaccard\", ascending=True).head(5))\n",
    "else:\n",
    "    print(\"No eval results available to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: inspect raw audio for a random clip ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def debug_audio_for_clip(idx: int = 0):\n",
    "    \"\"\"Visualize raw audio waveform (and optionally spectrogram) for one clip.\n",
    "\n",
    "    NOTE: The current Phase‑3 decoder uses **vision + text**; audio is part of\n",
    "    the aligned embedding space from Phase‑1, but is not directly consumed by\n",
    "    the decoder in this notebook. This cell is purely for sanity‑checking the\n",
    "    audio modality.\n",
    "    \"\"\"\n",
    "    clip = dataset.clips[idx]\n",
    "    audio_bytes = clip[\"audio_wav\"]\n",
    "    video_id = clip[\"video_id\"]\n",
    "\n",
    "    print(\"Video ID:\", video_id)\n",
    "    print(\"Caption :\", clip[\"caption\"])\n",
    "\n",
    "    # If audio is stored as raw bytes of a WAV/OGG file, users can decode it with soundfile/torchaudio.\n",
    "    # Here we just visualize the raw byte length as a placeholder; adapt decoding to your format.\n",
    "    print(\"Audio byte length:\", len(audio_bytes))\n",
    "\n",
    "    # If you know the exact format, replace this section with proper decoding + waveform plot.\n",
    "\n",
    "print(\"You can inspect audio with: debug_audio_for_clip(0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf04b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5de6a602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now call: ask_text('Explain what this model is doing.')\n"
     ]
    }
   ],
   "source": [
    "# === Text-only Q&A helper using the Phase-3 LLM decoder ===\n",
    "\n",
    "import torch\n",
    "\n",
    "def ask_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a pure text-only generation through the underlying decoder LLM.\n",
    "\n",
    "    This ignores images/audio and just uses the language model that was\n",
    "    used in Phase-3 (e.g., Qwen2.5-3B-Instruct).\n",
    "    \"\"\"\n",
    "    mm_module.eval()\n",
    "\n",
    "    tokenizer = mm_module.llm.tokenizer          # already set up in the notebook\n",
    "    llm_model = mm_module.llm.model          # HF causal LM inside LLMDecoder\n",
    "\n",
    "    # You can wrap your prompt for chat-style models if you like:\n",
    "    full_prompt = prompt\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        full_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(\"You can now call: ask_text('Explain what this model is doing.')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = ask_text(\"You are a helpful AI assistant. Explain VALOR dataset in simple terms.\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8186a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from imports.core import VisionEncoder, set_seed\n",
    "from imports.llm_integration import LLMConfig, MultimodalLLM\n",
    "from imports.multimodal_alignment_perceiver import (\n",
    "    MultimodalAlignmentConfig,\n",
    "    MultimodalAlignmentModel,\n",
    ")\n",
    "\n",
    "\n",
    "class EdgeGlassEngine:\n",
    "    \"\"\"\n",
    "    vLLM-style inference wrapper around your Perceiver + VisionEncoder + LLM stack.\n",
    "\n",
    "    Supports:\n",
    "      - text-only generation\n",
    "      - image-conditioned generation (image -> CLIP -> Perceiver -> projector -> LLM)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Union[str, Path],\n",
    "        phase1_perceiver_ckpt: Union[str, Path],\n",
    "        phase3_ckpt: Union[str, Path],\n",
    "        llm_name: str = \"Qwen/Qwen2.5-3B-Instruct\",   # match your training\n",
    "        num_prefix_tokens: int = 8,                    # match your training\n",
    "        seed: int = 42,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> None:\n",
    "        set_seed(seed)\n",
    "\n",
    "        self.root_dir = Path(root_dir)\n",
    "\n",
    "        # --- device / dtype ---\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "            self.dtype = torch.bfloat16\n",
    "        elif torch.cuda.is_available():\n",
    "            self.dtype = torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        print(f\"[Engine] device = {self.device}, dtype = {self.dtype}\")\n",
    "\n",
    "        # --- 1. Perceiver alignment model ---\n",
    "        self.align_cfg = MultimodalAlignmentConfig()\n",
    "        self.aligner = MultimodalAlignmentModel(self.align_cfg)\n",
    "\n",
    "        # attach cfg like training expects\n",
    "        self.align_cfg.device = self.device\n",
    "        self.align_cfg.dtype = self.dtype\n",
    "        self.aligner.cfg = self.align_cfg\n",
    "\n",
    "        phase1_perceiver_ckpt = Path(phase1_perceiver_ckpt)\n",
    "        ckpt1 = torch.load(phase1_perceiver_ckpt, map_location=self.device, weights_only=False)\n",
    "\n",
    "        # your tri-modal trainer uses `model_state` key\n",
    "        if isinstance(ckpt1, dict) and \"model_state\" in ckpt1:\n",
    "            state_dict = ckpt1[\"model_state\"]\n",
    "            print(\"[Engine] Using 'model_state' from Phase-1 checkpoint.\")\n",
    "        else:\n",
    "            state_dict = ckpt1\n",
    "            print(\"[Engine] Phase-1 checkpoint has no 'model_state' wrapper; using full dict.\")\n",
    "\n",
    "        missing, unexpected = self.aligner.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"[Engine] Perceiver loaded. missing={len(missing)}, unexpected={len(unexpected)}\")\n",
    "\n",
    "        self.aligner.to(self.device, dtype=self.dtype)\n",
    "        self.aligner.eval()\n",
    "\n",
    "        # --- 2. Vision backbone (CLIP / SigLIP etc.) ---\n",
    "        self.vision = VisionEncoder(\n",
    "            model_name=self.align_cfg.vision_model_name,\n",
    "            device=self.device,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        self.vision.eval()\n",
    "        for p in self.vision.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # --- 3. LLM + projector ---\n",
    "        self.llm_cfg = LLMConfig(\n",
    "            model_name=llm_name,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            num_prefix_tokens=num_prefix_tokens,\n",
    "            freeze_llm=True,\n",
    "        )\n",
    "\n",
    "        mm = MultimodalLLM(\n",
    "            aligner=self.aligner,\n",
    "            llm_config=self.llm_cfg,\n",
    "        ).to(self.device, dtype=self.dtype)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"[Engine] Using DataParallel over {torch.cuda.device_count()} GPUs\")\n",
    "            mm = nn.DataParallel(mm)\n",
    "\n",
    "        self.mm = mm\n",
    "        self.mm_module = mm.module if isinstance(mm, nn.DataParallel) else mm\n",
    "\n",
    "        # --- 4. Load Phase-3 checkpoint (decoder training) ---\n",
    "        phase3_ckpt = Path(phase3_ckpt)\n",
    "        ckpt3 = torch.load(phase3_ckpt, map_location=self.device, weights_only=False)\n",
    "\n",
    "        if isinstance(ckpt3, dict) and \"model_state_dict\" in ckpt3:\n",
    "            mm_state = ckpt3[\"model_state_dict\"]\n",
    "            print(\"[Engine] Using 'model_state_dict' from Phase-3 checkpoint.\")\n",
    "        else:\n",
    "            mm_state = ckpt3\n",
    "            print(\"[Engine] Phase-3 checkpoint has no 'model_state_dict'; using full dict.\")\n",
    "\n",
    "        missing, unexpected = self.mm_module.load_state_dict(mm_state, strict=False)\n",
    "        print(f\"[Engine] Multimodal LLM loaded. missing={len(missing)}, unexpected={len(unexpected)}\")\n",
    "\n",
    "        self.mm_module.eval()\n",
    "\n",
    "        # shortcuts\n",
    "        self.tokenizer = self.mm_module.llm.tokenizer\n",
    "        self.llm_model = self.mm_module.llm.model\n",
    "        self.projector = self.mm_module.projector  # VisionToLLMProjector\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Text-only generation (no Perceiver / vision needed)\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "    ) -> str:\n",
    "        self.mm_module.eval()\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        outputs = self.llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Image-conditioned generation: PIL / path → CLIP → Perceiver → LLM\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def generate_image(\n",
    "        self,\n",
    "        image: Union[PILImage.Image, str],\n",
    "        question: str = \"Describe this image.\",\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "    ) -> str:\n",
    "        self.mm_module.eval()\n",
    "\n",
    "        # 0. Load / normalize image\n",
    "        if isinstance(image, str):\n",
    "            img = PILImage.open(image).convert(\"RGB\")\n",
    "        else:\n",
    "            img = image.convert(\"RGB\")\n",
    "\n",
    "        # 1. Vision encoder forward (same as in training)\n",
    "        self.vision.eval()\n",
    "        feats = self.vision([img])   # may be tensor or dict\n",
    "\n",
    "        if isinstance(feats, dict):\n",
    "            # Adapt key choice to however VisionEncoder returns features\n",
    "            for key in [\"image_embeds\", \"features\", \"last_hidden_state\", \"pooler_output\"]:\n",
    "                if key in feats:\n",
    "                    feats = feats[key]\n",
    "                    break\n",
    "\n",
    "        if not isinstance(feats, torch.Tensor):\n",
    "            raise TypeError(\n",
    "                f\"VisionEncoder returned {type(feats)}; \"\n",
    "                \"update EdgeGlassEngine.generate_image to match your VisionEncoder API.\"\n",
    "            )\n",
    "\n",
    "        feats = feats.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        # 2. Perceiver aligner: features -> aligned embedding\n",
    "        z_align = self.aligner.encode_vision(feats)      # (B, d_align)\n",
    "\n",
    "        # 3. Project to LLM prefix tokens\n",
    "        prefix = self.projector(z_align)                 # (B, num_tokens, d_llm)\n",
    "\n",
    "        # 4. LLM generate with prefix\n",
    "        inputs = self.tokenizer(question, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        outputs = self.llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            prefix_embeds=prefix,\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Unified API like vLLM: choose modality by arguments\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image: Optional[Union[PILImage.Image, str]] = None,\n",
    "        # audio_features: Optional[torch.Tensor] = None,  # TODO: once you wire audio→LLM\n",
    "        **gen_kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        If only prompt is given -> text-only.\n",
    "        If image is given -> image-conditioned generation.\n",
    "        Later you can extend for audio_features as well.\n",
    "        \"\"\"\n",
    "        if image is None:\n",
    "            return self.generate_text(prompt, **gen_kwargs)\n",
    "        else:\n",
    "            return self.generate_image(image, question=prompt, **gen_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adfbb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Engine] device = cuda, dtype = torch.bfloat16\n",
      "[Engine] Using 'model_state' from Phase-1 checkpoint.\n",
      "[Engine] Perceiver loaded. missing=0, unexpected=0\n",
      "[VisionEncoder] Loaded openai/clip-vit-base-patch32, hidden_size=768\n",
      "[LLMDecoder] Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69500312919e4ba48e89a35a4e9737e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMDecoder] hidden_size=2048, frozen=True\n",
      "[MultimodalLLM] Projector: 512 → 8 × 2048\n",
      "[Engine] Using DataParallel over 2 GPUs\n",
      "[Engine] Using 'model_state_dict' from Phase-3 checkpoint.\n",
      "[Engine] Multimodal LLM loaded. missing=149, unexpected=348\n",
      "Explain what the VALOR dataset is in simple terms. The VALOR dataset is a collection of images used for training computer vision models, particularly those that can recognize and understand objects or scenes in photographs. Think of it like a big photo album with lots of pictures of different things, but these aren't just any photos - they're specifically chosen to help machines learn to identify and describe various objects or situations accurately.\n",
      "\n",
      "Imagine you have a robot that needs to recognize toys in a room. The VALOR dataset would provide it with many photos of toys in different positions and settings, helping the robot to better understand and recognize toys in real-world scenarios. This way, when it sees a toy, it can more reliably tell you what kind of toy it is and where it might be located. \n",
      "\n",
      "In essence, it's a tool designed to help improve the performance of AI systems by providing them with a large and diverse set of visual examples to learn from.\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"/home/hice1/vchopra37/scratch/projects/edge_glass/code_base/v2_code_base\"\n",
    "PHASE1 = f\"{ROOT_DIR}/checkpoints/phase1_multimodal/perceiver_mrl/best.pt\"\n",
    "PHASE3 = f\"{ROOT_DIR}/checkpoints/phase3_llm_valor_perceiver/valor_qwen2p5_phase3_perceiver_v1/best_phase3_valor_perceiver.pt\"\n",
    "\n",
    "engine = EdgeGlassEngine(\n",
    "    root_dir=ROOT_DIR,\n",
    "    phase1_perceiver_ckpt=PHASE1,\n",
    "    phase3_ckpt=PHASE3,\n",
    ")\n",
    "\n",
    "# 1) Text-only question\n",
    "print(engine.generate(\"Explain what the VALOR dataset is in simple terms.\"))\n",
    "\n",
    "# 2) Image question\n",
    "# from PIL import Image\n",
    "# img = Image.open(\"/path/to/some/frame.jpg\")\n",
    "# print(engine.generate(\"What is happening in this scene?\", image=img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e0e9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I have a question. I need to find the derivative of the function f(x) = x^2 * e^x using the product rule. How can I do that? Sure! To find the derivative of the function \\( f(x) = x^2 \\cdot e^x \\) using the product rule, you'll follow these steps:\n",
      "\n",
      "1. Identify the two functions being multiplied together. In this case, we have:\n",
      "   \\[\n",
      "   u(x) = x^2 \\quad \\text{and} \\quad v(x) = e^x\n",
      "   \\]\n",
      "\n",
      "2. Find the derivatives of these two functions:\n",
      "   \\[\n",
      "   u'(x) = \\frac{d}{dx}(x^2) = 2x\n",
      "   \\]\n",
      "   \\[\n",
      "   v'(x) = \\frac{d}{dx}(e^x) = e^x\n",
      "   \\]\n",
      "\n",
      "3. Apply the product rule, which states:\n",
      "   \\[\n",
      "   (u \\cdot v)' = u' \\cdot v + u \\cdot v'\n",
      "   \\]\n",
      "\n",
      "4. Substitute \\( u(x) \\), \\( u'(x) \\), \\( v(x) \\), and \\( v'(x) \\) into the product rule formula\n"
     ]
    }
   ],
   "source": [
    "# 1) Text-only question\n",
    "print(engine.generate(\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4cf4600",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [VisionEncoder] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m img = \u001b[33m\"\u001b[39m\u001b[33m/home/hice1/vchopra37/scratch/projects/edge_glass/temp.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is happening here?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 236\u001b[39m, in \u001b[36mEdgeGlassEngine.generate\u001b[39m\u001b[34m(self, prompt, image, **gen_kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_text(prompt, **gen_kwargs)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mEdgeGlassEngine.generate_image\u001b[39m\u001b[34m(self, image, question, max_new_tokens, temperature, top_p, do_sample)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# 1. Vision encoder forward (same as in training)\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m.vision.eval()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m feats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# may be tensor or dict\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feats, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# Adapt key choice to however VisionEncoder returns features\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpooler_output\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Module [VisionEncoder] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "img = \"/home/hice1/vchopra37/scratch/projects/edge_glass/temp.jpg\"\n",
    "print(engine.generate(\"What is happening here?\", image=img))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
