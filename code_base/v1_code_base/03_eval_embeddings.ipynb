{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d025df",
   "metadata": {},
   "source": [
    "<p>\n",
    "# ============================================================\n",
    "# 05_eval_embeddings.ipynb\n",
    "# Edge Assistant — Embedding Evaluation Notebook\n",
    "#\n",
    "# This notebook assumes:\n",
    "# - You have an \"aligned\" model (encoders + perceiver + projectors)\n",
    "# - You have a saved checkpoint: ckpt_path (see EvalConfig below)\n",
    "#\n",
    "# It evaluates:\n",
    "#   1. Text↔Image retrieval (R@1/5/10)\n",
    "#   2. Text↔Audio retrieval (R@1/5/10)\n",
    "#   3. Audio→Image zero-shot classification (ESC-50-style)\n",
    "#   4. Matryoshka compression curve (accuracy vs K dims/tokens)\n",
    "#   5. Gramian Volume (latent geometry)\n",
    "#\n",
    "# You plug in:\n",
    "#   - Your model in `load_aligned_model`\n",
    "#   - Your DataLoaders for COCO/Flickr/AudioCaps/Clotho/ESC-50/etc.\n",
    "# ============================================================\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ec0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Config & Reproducibility\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    # --- model ---\n",
    "    ckpt_path: str = \"./checkpoints/aligned_model.pt\"  # <--- EDIT THIS\n",
    "    use_bfloat16: bool = False\n",
    "\n",
    "    # --- evaluation ---\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # matryoshka / dimensionality\n",
    "    emb_dim_image: int = 1024   # set to your projector output dim\n",
    "    emb_dim_audio: int = 1024\n",
    "    emb_dim_text: int = 1024\n",
    "\n",
    "    # gramian volume\n",
    "    gram_n_samples: int = 256\n",
    "\n",
    "cfg = EvalConfig()\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model Loading & Wrapper\n",
    "# ============================================================\n",
    "\n",
    "class OmniEmbedWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper that standardizes the interface:\n",
    "        - embed_image(pixel_values)  -> (B, D)\n",
    "        - embed_audio(audio_values)  -> (B, D)\n",
    "        - embed_text(input_ids, attention_mask) -> (B, D)\n",
    "    You should adapt the internals to your model's API.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # EXPECTED: pixel_values: (B, C, H, W)\n",
    "        # Replace this with your own call:\n",
    "        if hasattr(self.backbone, \"encode_image\"):\n",
    "            z = self.backbone.encode_image(pixel_values)\n",
    "        elif hasattr(self.backbone, \"forward_image\"):\n",
    "            z = self.backbone.forward_image(pixel_values)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no image encoder method.\")\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_audio(self, audio_values: torch.Tensor) -> torch.Tensor:\n",
    "        # EXPECTED: audio_values: (B, T) or (B, C, T) depending on your pipeline\n",
    "        if hasattr(self.backbone, \"encode_audio\"):\n",
    "            z = self.backbone.encode_audio(audio_values)\n",
    "        elif hasattr(self.backbone, \"forward_audio\"):\n",
    "            z = self.backbone.forward_audio(audio_values)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no audio encoder method.\")\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        if hasattr(self.backbone, \"encode_text\"):\n",
    "            z = self.backbone.encode_text(input_ids, attention_mask)\n",
    "        elif hasattr(self.backbone, \"forward_text\"):\n",
    "            z = self.backbone.forward_text(input_ids, attention_mask)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no text encoder method.\")\n",
    "        return z\n",
    "\n",
    "\n",
    "def load_aligned_model(cfg: EvalConfig) -> OmniEmbedWrapper:\n",
    "    \"\"\"\n",
    "    EDIT THIS FUNCTION for your project.\n",
    "    It should:\n",
    "      1. Instantiate your aligned model class (Perceiver + projectors)\n",
    "      2. Load the checkpoint\n",
    "      3. Wrap it with OmniEmbedWrapper\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Example skeleton (you must replace with your own import)\n",
    "    # --------------------------------------------------------\n",
    "    #\n",
    "    # from edge_glass.models.aligned_model import AlignedModel\n",
    "    # model = AlignedModel(...)\n",
    "    #\n",
    "    # state = torch.load(cfg.ckpt_path, map_location=\"cpu\")\n",
    "    # if \"model\" in state:\n",
    "    #     model.load_state_dict(state[\"model\"])\n",
    "    # else:\n",
    "    #     model.load_state_dict(state)\n",
    "    #\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "    # return OmniEmbedWrapper(model)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    raise NotImplementedError(\n",
    "        \"Implement load_aligned_model(cfg) to return OmniEmbedWrapper(backbone).\"\n",
    "    )\n",
    "\n",
    "# Try loading (will error until you implement)\n",
    "# omni = load_aligned_model(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "omni = load_aligned_model(cfg)\n",
    "dtype = torch.bfloat16 if cfg.use_bfloat16 and torch.cuda.is_bf16_supported() else torch.float32\n",
    "omni = omni.to(device=device, dtype=dtype)\n",
    "omni.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Retrieval Metrics (R@K)\n",
    "# ============================================================\n",
    "\n",
    "def recall_at_k(sim_matrix: torch.Tensor, k: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    sim_matrix: (N_query, N_target)\n",
    "    Ground truth is assumed to be 'diagonal' (i <-> i).\n",
    "    \"\"\"\n",
    "    # indices of top-k targets for each query\n",
    "    topk = sim_matrix.topk(k, dim=-1).indices\n",
    "    correct = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "    hits = (topk == correct.unsqueeze(-1)).any(dim=-1).float().mean().item()\n",
    "    return hits\n",
    "\n",
    "\n",
    "def compute_retrieval_scores(query_emb: torch.Tensor,\n",
    "                             target_emb: torch.Tensor,\n",
    "                             prefix: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Cosine similarities + R@1/5/10 + (optional) mean/median rank.\n",
    "    \"\"\"\n",
    "    q = F.normalize(query_emb, dim=-1)\n",
    "    t = F.normalize(target_emb, dim=-1)\n",
    "    sim = q @ t.T  # (N, N)\n",
    "\n",
    "    r1 = recall_at_k(sim, 1)\n",
    "    r5 = recall_at_k(sim, 5)\n",
    "    r10 = recall_at_k(sim, 10)\n",
    "\n",
    "    # ranks for each query\n",
    "    sorted_indices = sim.argsort(dim=-1, descending=True)\n",
    "    targets = torch.arange(sim.size(0), device=sim.device)\n",
    "    ranks = (sorted_indices == targets.unsqueeze(-1)).nonzero(as_tuple=False)[:, 1]\n",
    "    mean_rank = ranks.float().mean().item()\n",
    "    median_rank = ranks.median().item()\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}R@1\": r1,\n",
    "        f\"{prefix}R@5\": r5,\n",
    "        f\"{prefix}R@10\": r10,\n",
    "        f\"{prefix}MeanRank\": mean_rank,\n",
    "        f\"{prefix}MedianRank\": median_rank,\n",
    "        \"sim_matrix\": sim.detach().cpu()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7251c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset Interfaces (YOU NEED TO PROVIDE DATALOADERS)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "The notebook expects DataLoaders of the following form:\n",
    "\n",
    "1) Image–Text retrieval (e.g., COCO/Flickr):\n",
    "   batch = {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L)\n",
    "   }\n",
    "\n",
    "2) Text–Audio retrieval (e.g., AudioCaps/Clotho):\n",
    "   batch = {\n",
    "       \"audio_values\": FloatTensor (B, T or C, T),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L)\n",
    "   }\n",
    "\n",
    "3) Audio–Image classification (ESC-50 style):\n",
    "   train batch: {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"label\": LongTensor (B,)\n",
    "   }\n",
    "   test batch: {\n",
    "       \"audio_values\": FloatTensor (B, T),\n",
    "       \"label\": LongTensor (B,)\n",
    "   }\n",
    "\n",
    "4) Tri-modal for GRAM test (e.g., VALOR):\n",
    "   batch = {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"audio_values\": FloatTensor (B, T),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L),\n",
    "   }\n",
    "\n",
    "You can build these DataLoaders in a separate cell or module.\n",
    "Below we only define EVALUATION functions that *consume* them.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d678fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text → Image Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def eval_text_to_image(omni: OmniEmbedWrapper,\n",
    "                       dataloader: DataLoader,\n",
    "                       max_batches: int | None = None) -> dict:\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Text→Image\")):\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        img_embs.append(i_emb)\n",
    "        txt_embs.append(t_emb)\n",
    "\n",
    "    img_embs = torch.cat(img_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    metrics = compute_retrieval_scores(txt_embs, img_embs, prefix=\"T2I_\")\n",
    "    print(\"Text→Image Retrieval:\")\n",
    "    for k, v in metrics.items():\n",
    "        if k.endswith(\"sim_matrix\"):\n",
    "            continue\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text → Audio Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def eval_text_to_audio(omni: OmniEmbedWrapper,\n",
    "                       dataloader: DataLoader,\n",
    "                       max_batches: int | None = None) -> dict:\n",
    "    aud_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Text→Audio\")):\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_emb = omni.embed_audio(audio)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        aud_embs.append(a_emb)\n",
    "        txt_embs.append(t_emb)\n",
    "\n",
    "    aud_embs = torch.cat(aud_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    metrics = compute_retrieval_scores(txt_embs, aud_embs, prefix=\"T2A_\")\n",
    "    print(\"Text→Audio Retrieval:\")\n",
    "    for k, v in metrics.items():\n",
    "        if k.endswith(\"sim_matrix\"):\n",
    "            continue\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Audio → Image Zero-shot Classification (ESC-50 style)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_image_class_prototypes(omni: OmniEmbedWrapper,\n",
    "                                 train_loader: DataLoader,\n",
    "                                 n_classes: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build class prototypes by averaging image embeddings per class.\n",
    "    Returns Tensor of shape (n_classes, D)\n",
    "    \"\"\"\n",
    "    protos = [ [] for _ in range(n_classes) ]\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Building image prototypes\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"]  # (B,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_emb = omni.embed_image(pixels)  # (B, D)\n",
    "            img_emb = img_emb.cpu()\n",
    "\n",
    "        for emb, lab in zip(img_emb, labels):\n",
    "            protos[lab.item()].append(emb)\n",
    "\n",
    "    # average\n",
    "    proto_tensors = []\n",
    "    for cls_idx, vecs in enumerate(protos):\n",
    "        if len(vecs) == 0:\n",
    "            raise ValueError(f\"No samples found for class {cls_idx}.\")\n",
    "        proto_tensors.append(torch.stack(vecs, dim=0).mean(dim=0))\n",
    "\n",
    "    proto_mat = torch.stack(proto_tensors, dim=0)  # (C, D)\n",
    "    return proto_mat.to(device)\n",
    "\n",
    "\n",
    "def eval_audio_zero_shot_classification(omni: OmniEmbedWrapper,\n",
    "                                        proto_mat: torch.Tensor,\n",
    "                                        test_loader: DataLoader) -> float:\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    proto_norm = F.normalize(proto_mat, dim=-1)\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Audio→Image Zero-shot\"):\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        labels = batch[\"label\"].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_emb = omni.embed_audio(audio)  # (B, D)\n",
    "            a_emb = F.normalize(a_emb, dim=-1)\n",
    "\n",
    "            sim = a_emb @ proto_norm.T  # (B, C)\n",
    "            pred = sim.argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "        preds.extend(pred)\n",
    "        gts.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    print(f\"Audio→Image zero-shot Top-1 accuracy: {acc:.4f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Matryoshka Compression Curve\n",
    "# ============================================================\n",
    "\n",
    "def matryoshka_curve_image(omni: OmniEmbedWrapper,\n",
    "                           dataloader: DataLoader,\n",
    "                           ks: list[int],\n",
    "                           full_dim: int) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate Text→Image R@1 while truncating image embeddings\n",
    "    to first K dimensions (Matryoshka-style).\n",
    "    \"\"\"\n",
    "    # First, cache FULL embeddings once\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Caching full embeddings\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)   # (B, D)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        img_embs.append(i_emb.cpu())\n",
    "        txt_embs.append(t_emb.cpu())\n",
    "\n",
    "    img_embs = torch.cat(img_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for k in ks:\n",
    "        assert k <= full_dim, f\"K={k} > full_dim={full_dim}\"\n",
    "        print(f\"\\nEvaluating Matryoshka at K={k} dims\")\n",
    "\n",
    "        i_k = img_embs[:, :k].to(device)\n",
    "        t_k = txt_embs[:, :k].to(device)\n",
    "\n",
    "        metrics = compute_retrieval_scores(t_k, i_k, prefix=f\"K{k}_\")\n",
    "        r1 = metrics[f\"K{k}_R@1\"]\n",
    "        results[k] = r1\n",
    "        print(f\"  R@1 (K={k}): {r1:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gramian Volume (GRAM) — Latent Geometry\n",
    "# ============================================================\n",
    "\n",
    "def gramian_volume_triplet(a: torch.Tensor,\n",
    "                           b: torch.Tensor,\n",
    "                           c: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    a, b, c: (D,) vectors.\n",
    "    Compute det(G) where G_ij = <v_i, v_j> for normalized vectors.\n",
    "    \"\"\"\n",
    "    A = F.normalize(a, dim=-1)\n",
    "    B = F.normalize(b, dim=-1)\n",
    "    C = F.normalize(c, dim=-1)\n",
    "\n",
    "    G = torch.stack([\n",
    "        torch.stack([A @ A, A @ B, A @ C]),\n",
    "        torch.stack([B @ A, B @ B, B @ C]),\n",
    "        torch.stack([C @ A, C @ B, C @ C]),\n",
    "    ])\n",
    "    return torch.det(G).item()\n",
    "\n",
    "\n",
    "def eval_gramian_volume(omni: OmniEmbedWrapper,\n",
    "                        dataloader: DataLoader,\n",
    "                        n_samples: int) -> list[float]:\n",
    "    \"\"\"\n",
    "    Expects tri-modal batches with keys:\n",
    "      - \"pixel_values\"\n",
    "      - \"audio_values\"\n",
    "      - \"input_ids\"\n",
    "      - \"attention_mask\"\n",
    "    \"\"\"\n",
    "    vols = []\n",
    "    for batch in tqdm(dataloader, desc=\"GRAM volume\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)       # (B, D)\n",
    "            a_emb = omni.embed_audio(audio)        # (B, D)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        for j in range(i_emb.size(0)):\n",
    "            v = gramian_volume_triplet(\n",
    "                i_emb[j].cpu(), a_emb[j].cpu(), t_emb[j].cpu()\n",
    "            )\n",
    "            vols.append(v)\n",
    "            if len(vols) >= n_samples:\n",
    "                break\n",
    "\n",
    "        if len(vols) >= n_samples:\n",
    "            break\n",
    "\n",
    "    vols = vols[:n_samples]\n",
    "    print(f\"Collected {len(vols)} Gramian volumes.\")\n",
    "    print(f\"Mean: {np.mean(vols):.6f}, Median: {np.median(vols):.6f}\")\n",
    "    return vols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fddeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN DRIVER — Plug in your loaders here\n",
    "# ============================================================\n",
    "\n",
    "# These should be real DataLoaders from your code.\n",
    "# For now we keep them as placeholders.\n",
    "coco_val_loader = None          # -> image-text (COCO/Flickr)\n",
    "audiocaps_val_loader = None     # -> text-audio (AudioCaps/Clotho)\n",
    "esc50_train_loader = None       # -> image-labels for prototypes\n",
    "esc50_test_loader = None        # -> audio-labels for classification\n",
    "tri_modal_loader = None         # -> VALOR/VGGSound-style (I+A+T)\n",
    "\n",
    "\n",
    "def run_all_evals(omni: OmniEmbedWrapper, cfg: EvalConfig):\n",
    "    results = {}\n",
    "\n",
    "    # 1) Text→Image retrieval (COCO/Flickr)\n",
    "    if coco_val_loader is not None:\n",
    "        t2i_metrics = eval_text_to_image(omni, coco_val_loader)\n",
    "        results.update(t2i_metrics)\n",
    "\n",
    "        # Matryoshka curve on same loader (optional)\n",
    "        ks = [8, 16, 32, 64, 128, 256, cfg.emb_dim_image]\n",
    "        ks = [k for k in ks if k <= cfg.emb_dim_image]\n",
    "        mat_res = matryoshka_curve_image(\n",
    "            omni, coco_val_loader, ks=ks, full_dim=cfg.emb_dim_image\n",
    "        )\n",
    "        results[\"matryoshka\"] = mat_res\n",
    "        plot_matryoshka_curve(mat_res)\n",
    "\n",
    "        # Heatmap for a small subset\n",
    "        sim_matrix = t2i_metrics[\"sim_matrix\"]\n",
    "        plot_similarity_heatmap(sim_matrix[:32, :32], title=\"T→I Similarity (First 32 examples)\")\n",
    "\n",
    "    # 2) Text→Audio retrieval (AudioCaps/Clotho)\n",
    "    if audiocaps_val_loader is not None:\n",
    "        t2a_metrics = eval_text_to_audio(omni, audiocaps_val_loader)\n",
    "        results.update(t2a_metrics)\n",
    "\n",
    "    # 3) Audio→Image zero-shot classification (ESC-50)\n",
    "    if esc50_train_loader is not None and esc50_test_loader is not None:\n",
    "        # You must pass correct n_classes (e.g., 50 for ESC-50)\n",
    "        n_classes = 50\n",
    "        proto_mat = build_image_class_prototypes(omni, esc50_train_loader, n_classes)\n",
    "        esc_acc = eval_audio_zero_shot_classification(omni, proto_mat, esc50_test_loader)\n",
    "        results[\"ESC50_zeroshot_acc\"] = esc_acc\n",
    "\n",
    "    # 4) Gramian Volume (tri-modal binding)\n",
    "    if tri_modal_loader is not None:\n",
    "        vols = eval_gramian_volume(omni, tri_modal_loader, n_samples=cfg.gram_n_samples)\n",
    "        results[\"gram_volumes\"] = vols\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(vols, bins=30)\n",
    "        plt.title(\"Distribution of Gramian Volumes (Tri-modal)\")\n",
    "        plt.xlabel(\"Volume\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (after you've constructed all loaders):\n",
    "\n",
    "# omni = load_aligned_model(cfg).to(device)\n",
    "# omni.eval()\n",
    "# all_results = run_all_evals(omni, cfg)\n",
    "# all_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
