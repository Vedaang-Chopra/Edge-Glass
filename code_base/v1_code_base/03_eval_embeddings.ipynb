{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d025df",
   "metadata": {},
   "source": [
    "<p>\n",
    "# ============================================================\n",
    "# 05_eval_embeddings.ipynb\n",
    "# Edge Assistant — Embedding Evaluation Notebook\n",
    "#\n",
    "# This notebook assumes:\n",
    "# - You have an \"aligned\" model (encoders + perceiver + projectors)\n",
    "# - You have a saved checkpoint: ckpt_path (see EvalConfig below)\n",
    "#\n",
    "# It evaluates:\n",
    "#   1. Text↔Image retrieval (R@1/5/10)\n",
    "#   2. Text↔Audio retrieval (R@1/5/10)\n",
    "#   3. Audio→Image zero-shot classification (ESC-50-style)\n",
    "#   4. Matryoshka compression curve (accuracy vs K dims/tokens)\n",
    "#   5. Gramian Volume (latent geometry)\n",
    "#\n",
    "# You plug in:\n",
    "#   - Your model in `load_aligned_model`\n",
    "#   - Your DataLoaders for COCO/Flickr/AudioCaps/Clotho/ESC-50/etc.\n",
    "# ============================================================\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8ec0ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     16\u001b[39m sns.set_theme()\n\u001b[32m     17\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/seaborn/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import seaborn objects\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/seaborn/rcmod.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcycler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m palettes\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mset_theme\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreset_defaults\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreset_orig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33maxes_style\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mset_style\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mplotting_context\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mset_context\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mset_palette\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m _style_keys = [\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maxes.facecolor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/seaborn/palettes.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m husl\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m desaturate, get_color_cycle\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m xkcd_rgb, crayons\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_colormap\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/seaborn/utils.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModuleType\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_rgb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     51\u001b[39m     ArrowDtype,\n\u001b[32m     52\u001b[39m     Int8Dtype,\n\u001b[32m     53\u001b[39m     Int16Dtype,\n\u001b[32m     54\u001b[39m     Int32Dtype,\n\u001b[32m     55\u001b[39m     Int64Dtype,\n\u001b[32m     56\u001b[39m     UInt8Dtype,\n\u001b[32m     57\u001b[39m     UInt16Dtype,\n\u001b[32m     58\u001b[39m     UInt32Dtype,\n\u001b[32m     59\u001b[39m     UInt64Dtype,\n\u001b[32m     60\u001b[39m     Float32Dtype,\n\u001b[32m     61\u001b[39m     Float64Dtype,\n\u001b[32m     62\u001b[39m     CategoricalDtype,\n\u001b[32m     63\u001b[39m     PeriodDtype,\n\u001b[32m     64\u001b[39m     IntervalDtype,\n\u001b[32m     65\u001b[39m     DatetimeTZDtype,\n\u001b[32m     66\u001b[39m     StringDtype,\n\u001b[32m     67\u001b[39m     BooleanDtype,\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     69\u001b[39m     NA,\n\u001b[32m     70\u001b[39m     isna,\n\u001b[32m     71\u001b[39m     isnull,\n\u001b[32m     72\u001b[39m     notna,\n\u001b[32m     73\u001b[39m     notnull,\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     75\u001b[39m     Index,\n\u001b[32m     76\u001b[39m     CategoricalIndex,\n\u001b[32m     77\u001b[39m     RangeIndex,\n\u001b[32m     78\u001b[39m     MultiIndex,\n\u001b[32m     79\u001b[39m     IntervalIndex,\n\u001b[32m     80\u001b[39m     TimedeltaIndex,\n\u001b[32m     81\u001b[39m     DatetimeIndex,\n\u001b[32m     82\u001b[39m     PeriodIndex,\n\u001b[32m     83\u001b[39m     IndexSlice,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     85\u001b[39m     NaT,\n\u001b[32m     86\u001b[39m     Period,\n\u001b[32m     87\u001b[39m     period_range,\n\u001b[32m     88\u001b[39m     Timedelta,\n\u001b[32m     89\u001b[39m     timedelta_range,\n\u001b[32m     90\u001b[39m     Timestamp,\n\u001b[32m     91\u001b[39m     date_range,\n\u001b[32m     92\u001b[39m     bdate_range,\n\u001b[32m     93\u001b[39m     Interval,\n\u001b[32m     94\u001b[39m     interval_range,\n\u001b[32m     95\u001b[39m     DateOffset,\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m     97\u001b[39m     to_numeric,\n\u001b[32m     98\u001b[39m     to_datetime,\n\u001b[32m     99\u001b[39m     to_timedelta,\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    101\u001b[39m     Flags,\n\u001b[32m    102\u001b[39m     Grouper,\n\u001b[32m    103\u001b[39m     factorize,\n\u001b[32m    104\u001b[39m     unique,\n\u001b[32m    105\u001b[39m     value_counts,\n\u001b[32m    106\u001b[39m     NamedAgg,\n\u001b[32m    107\u001b[39m     array,\n\u001b[32m    108\u001b[39m     Categorical,\n\u001b[32m    109\u001b[39m     set_eng_float_format,\n\u001b[32m    110\u001b[39m     Series,\n\u001b[32m    111\u001b[39m     DataFrame,\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/pandas/core/api.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     Grouper,\n\u001b[32m     49\u001b[39m     NamedAgg,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     CategoricalIndex,\n\u001b[32m     53\u001b[39m     DatetimeIndex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     TimedeltaIndex,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     bdate_range,\n\u001b[32m     63\u001b[39m     date_range,\n\u001b[32m     64\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/pandas/core/groupby/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     DataFrameGroupBy,\n\u001b[32m      3\u001b[39m     NamedAgg,\n\u001b[32m      4\u001b[39m     SeriesGroupBy,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/pandas/core/groupby/generic.py:60\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     55\u001b[39m     isna,\n\u001b[32m     56\u001b[39m     notna,\n\u001b[32m     57\u001b[39m )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     61\u001b[39m     GroupByApply,\n\u001b[32m     62\u001b[39m     maybe_mangle_lambdas,\n\u001b[32m     63\u001b[39m     reconstruct_func,\n\u001b[32m     64\u001b[39m     validate_func_kwargs,\n\u001b[32m     65\u001b[39m     warn_alias_replacement,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcom\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/pandas/core/apply.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m option_context\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockValuesRefs\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     AggFuncType,\n\u001b[32m     25\u001b[39m     AggFuncTypeBase,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     npt,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_optional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m import_optional_dependency\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Config & Reproducibility\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    # --- model ---\n",
    "    ckpt_path: str = \"./checkpoints/aligned_model.pt\"  # <--- EDIT THIS\n",
    "    use_bfloat16: bool = False\n",
    "\n",
    "    # --- evaluation ---\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # matryoshka / dimensionality\n",
    "    emb_dim_image: int = 1024   # set to your projector output dim\n",
    "    emb_dim_audio: int = 1024\n",
    "    emb_dim_text: int = 1024\n",
    "\n",
    "    # gramian volume\n",
    "    gram_n_samples: int = 256\n",
    "\n",
    "cfg = EvalConfig()\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model Loading & Wrapper\n",
    "# ============================================================\n",
    "\n",
    "class OmniEmbedWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Thin wrapper that standardizes the interface:\n",
    "        - embed_image(pixel_values)  -> (B, D)\n",
    "        - embed_audio(audio_values)  -> (B, D)\n",
    "        - embed_text(input_ids, attention_mask) -> (B, D)\n",
    "    You should adapt the internals to your model's API.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # EXPECTED: pixel_values: (B, C, H, W)\n",
    "        # Replace this with your own call:\n",
    "        if hasattr(self.backbone, \"encode_image\"):\n",
    "            z = self.backbone.encode_image(pixel_values)\n",
    "        elif hasattr(self.backbone, \"forward_image\"):\n",
    "            z = self.backbone.forward_image(pixel_values)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no image encoder method.\")\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_audio(self, audio_values: torch.Tensor) -> torch.Tensor:\n",
    "        # EXPECTED: audio_values: (B, T) or (B, C, T) depending on your pipeline\n",
    "        if hasattr(self.backbone, \"encode_audio\"):\n",
    "            z = self.backbone.encode_audio(audio_values)\n",
    "        elif hasattr(self.backbone, \"forward_audio\"):\n",
    "            z = self.backbone.forward_audio(audio_values)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no audio encoder method.\")\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        if hasattr(self.backbone, \"encode_text\"):\n",
    "            z = self.backbone.encode_text(input_ids, attention_mask)\n",
    "        elif hasattr(self.backbone, \"forward_text\"):\n",
    "            z = self.backbone.forward_text(input_ids, attention_mask)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone has no text encoder method.\")\n",
    "        return z\n",
    "\n",
    "\n",
    "def load_aligned_model(cfg: EvalConfig) -> OmniEmbedWrapper:\n",
    "    \"\"\"\n",
    "    EDIT THIS FUNCTION for your project.\n",
    "    It should:\n",
    "      1. Instantiate your aligned model class (Perceiver + projectors)\n",
    "      2. Load the checkpoint\n",
    "      3. Wrap it with OmniEmbedWrapper\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Example skeleton (you must replace with your own import)\n",
    "    # --------------------------------------------------------\n",
    "    #\n",
    "    # from edge_glass.models.aligned_model import AlignedModel\n",
    "    # model = AlignedModel(...)\n",
    "    #\n",
    "    # state = torch.load(cfg.ckpt_path, map_location=\"cpu\")\n",
    "    # if \"model\" in state:\n",
    "    #     model.load_state_dict(state[\"model\"])\n",
    "    # else:\n",
    "    #     model.load_state_dict(state)\n",
    "    #\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "    # return OmniEmbedWrapper(model)\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    raise NotImplementedError(\n",
    "        \"Implement load_aligned_model(cfg) to return OmniEmbedWrapper(backbone).\"\n",
    "    )\n",
    "\n",
    "# Try loading (will error until you implement)\n",
    "# omni = load_aligned_model(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "omni = load_aligned_model(cfg)\n",
    "dtype = torch.bfloat16 if cfg.use_bfloat16 and torch.cuda.is_bf16_supported() else torch.float32\n",
    "omni = omni.to(device=device, dtype=dtype)\n",
    "omni.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Retrieval Metrics (R@K)\n",
    "# ============================================================\n",
    "\n",
    "def recall_at_k(sim_matrix: torch.Tensor, k: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    sim_matrix: (N_query, N_target)\n",
    "    Ground truth is assumed to be 'diagonal' (i <-> i).\n",
    "    \"\"\"\n",
    "    # indices of top-k targets for each query\n",
    "    topk = sim_matrix.topk(k, dim=-1).indices\n",
    "    correct = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "    hits = (topk == correct.unsqueeze(-1)).any(dim=-1).float().mean().item()\n",
    "    return hits\n",
    "\n",
    "\n",
    "def compute_retrieval_scores(query_emb: torch.Tensor,\n",
    "                             target_emb: torch.Tensor,\n",
    "                             prefix: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Cosine similarities + R@1/5/10 + (optional) mean/median rank.\n",
    "    \"\"\"\n",
    "    q = F.normalize(query_emb, dim=-1)\n",
    "    t = F.normalize(target_emb, dim=-1)\n",
    "    sim = q @ t.T  # (N, N)\n",
    "\n",
    "    r1 = recall_at_k(sim, 1)\n",
    "    r5 = recall_at_k(sim, 5)\n",
    "    r10 = recall_at_k(sim, 10)\n",
    "\n",
    "    # ranks for each query\n",
    "    sorted_indices = sim.argsort(dim=-1, descending=True)\n",
    "    targets = torch.arange(sim.size(0), device=sim.device)\n",
    "    ranks = (sorted_indices == targets.unsqueeze(-1)).nonzero(as_tuple=False)[:, 1]\n",
    "    mean_rank = ranks.float().mean().item()\n",
    "    median_rank = ranks.median().item()\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}R@1\": r1,\n",
    "        f\"{prefix}R@5\": r5,\n",
    "        f\"{prefix}R@10\": r10,\n",
    "        f\"{prefix}MeanRank\": mean_rank,\n",
    "        f\"{prefix}MedianRank\": median_rank,\n",
    "        \"sim_matrix\": sim.detach().cpu()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7251c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset Interfaces (YOU NEED TO PROVIDE DATALOADERS)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "The notebook expects DataLoaders of the following form:\n",
    "\n",
    "1) Image–Text retrieval (e.g., COCO/Flickr):\n",
    "   batch = {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L)\n",
    "   }\n",
    "\n",
    "2) Text–Audio retrieval (e.g., AudioCaps/Clotho):\n",
    "   batch = {\n",
    "       \"audio_values\": FloatTensor (B, T or C, T),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L)\n",
    "   }\n",
    "\n",
    "3) Audio–Image classification (ESC-50 style):\n",
    "   train batch: {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"label\": LongTensor (B,)\n",
    "   }\n",
    "   test batch: {\n",
    "       \"audio_values\": FloatTensor (B, T),\n",
    "       \"label\": LongTensor (B,)\n",
    "   }\n",
    "\n",
    "4) Tri-modal for GRAM test (e.g., VALOR):\n",
    "   batch = {\n",
    "       \"pixel_values\": FloatTensor (B, C, H, W),\n",
    "       \"audio_values\": FloatTensor (B, T),\n",
    "       \"input_ids\": LongTensor (B, L),\n",
    "       \"attention_mask\": LongTensor (B, L),\n",
    "   }\n",
    "\n",
    "You can build these DataLoaders in a separate cell or module.\n",
    "Below we only define EVALUATION functions that *consume* them.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d678fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text → Image Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def eval_text_to_image(omni: OmniEmbedWrapper,\n",
    "                       dataloader: DataLoader,\n",
    "                       max_batches: int | None = None) -> dict:\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Text→Image\")):\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        img_embs.append(i_emb)\n",
    "        txt_embs.append(t_emb)\n",
    "\n",
    "    img_embs = torch.cat(img_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    metrics = compute_retrieval_scores(txt_embs, img_embs, prefix=\"T2I_\")\n",
    "    print(\"Text→Image Retrieval:\")\n",
    "    for k, v in metrics.items():\n",
    "        if k.endswith(\"sim_matrix\"):\n",
    "            continue\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text → Audio Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def eval_text_to_audio(omni: OmniEmbedWrapper,\n",
    "                       dataloader: DataLoader,\n",
    "                       max_batches: int | None = None) -> dict:\n",
    "    aud_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Text→Audio\")):\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_emb = omni.embed_audio(audio)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        aud_embs.append(a_emb)\n",
    "        txt_embs.append(t_emb)\n",
    "\n",
    "    aud_embs = torch.cat(aud_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    metrics = compute_retrieval_scores(txt_embs, aud_embs, prefix=\"T2A_\")\n",
    "    print(\"Text→Audio Retrieval:\")\n",
    "    for k, v in metrics.items():\n",
    "        if k.endswith(\"sim_matrix\"):\n",
    "            continue\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Audio → Image Zero-shot Classification (ESC-50 style)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_image_class_prototypes(omni: OmniEmbedWrapper,\n",
    "                                 train_loader: DataLoader,\n",
    "                                 n_classes: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build class prototypes by averaging image embeddings per class.\n",
    "    Returns Tensor of shape (n_classes, D)\n",
    "    \"\"\"\n",
    "    protos = [ [] for _ in range(n_classes) ]\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Building image prototypes\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"]  # (B,)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_emb = omni.embed_image(pixels)  # (B, D)\n",
    "            img_emb = img_emb.cpu()\n",
    "\n",
    "        for emb, lab in zip(img_emb, labels):\n",
    "            protos[lab.item()].append(emb)\n",
    "\n",
    "    # average\n",
    "    proto_tensors = []\n",
    "    for cls_idx, vecs in enumerate(protos):\n",
    "        if len(vecs) == 0:\n",
    "            raise ValueError(f\"No samples found for class {cls_idx}.\")\n",
    "        proto_tensors.append(torch.stack(vecs, dim=0).mean(dim=0))\n",
    "\n",
    "    proto_mat = torch.stack(proto_tensors, dim=0)  # (C, D)\n",
    "    return proto_mat.to(device)\n",
    "\n",
    "\n",
    "def eval_audio_zero_shot_classification(omni: OmniEmbedWrapper,\n",
    "                                        proto_mat: torch.Tensor,\n",
    "                                        test_loader: DataLoader) -> float:\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    proto_norm = F.normalize(proto_mat, dim=-1)\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Audio→Image Zero-shot\"):\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        labels = batch[\"label\"].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_emb = omni.embed_audio(audio)  # (B, D)\n",
    "            a_emb = F.normalize(a_emb, dim=-1)\n",
    "\n",
    "            sim = a_emb @ proto_norm.T  # (B, C)\n",
    "            pred = sim.argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "        preds.extend(pred)\n",
    "        gts.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    print(f\"Audio→Image zero-shot Top-1 accuracy: {acc:.4f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Matryoshka Compression Curve\n",
    "# ============================================================\n",
    "\n",
    "def matryoshka_curve_image(omni: OmniEmbedWrapper,\n",
    "                           dataloader: DataLoader,\n",
    "                           ks: list[int],\n",
    "                           full_dim: int) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate Text→Image R@1 while truncating image embeddings\n",
    "    to first K dimensions (Matryoshka-style).\n",
    "    \"\"\"\n",
    "    # First, cache FULL embeddings once\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Caching full embeddings\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)   # (B, D)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        img_embs.append(i_emb.cpu())\n",
    "        txt_embs.append(t_emb.cpu())\n",
    "\n",
    "    img_embs = torch.cat(img_embs, dim=0)\n",
    "    txt_embs = torch.cat(txt_embs, dim=0)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for k in ks:\n",
    "        assert k <= full_dim, f\"K={k} > full_dim={full_dim}\"\n",
    "        print(f\"\\nEvaluating Matryoshka at K={k} dims\")\n",
    "\n",
    "        i_k = img_embs[:, :k].to(device)\n",
    "        t_k = txt_embs[:, :k].to(device)\n",
    "\n",
    "        metrics = compute_retrieval_scores(t_k, i_k, prefix=f\"K{k}_\")\n",
    "        r1 = metrics[f\"K{k}_R@1\"]\n",
    "        results[k] = r1\n",
    "        print(f\"  R@1 (K={k}): {r1:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gramian Volume (GRAM) — Latent Geometry\n",
    "# ============================================================\n",
    "\n",
    "def gramian_volume_triplet(a: torch.Tensor,\n",
    "                           b: torch.Tensor,\n",
    "                           c: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    a, b, c: (D,) vectors.\n",
    "    Compute det(G) where G_ij = <v_i, v_j> for normalized vectors.\n",
    "    \"\"\"\n",
    "    A = F.normalize(a, dim=-1)\n",
    "    B = F.normalize(b, dim=-1)\n",
    "    C = F.normalize(c, dim=-1)\n",
    "\n",
    "    G = torch.stack([\n",
    "        torch.stack([A @ A, A @ B, A @ C]),\n",
    "        torch.stack([B @ A, B @ B, B @ C]),\n",
    "        torch.stack([C @ A, C @ B, C @ C]),\n",
    "    ])\n",
    "    return torch.det(G).item()\n",
    "\n",
    "\n",
    "def eval_gramian_volume(omni: OmniEmbedWrapper,\n",
    "                        dataloader: DataLoader,\n",
    "                        n_samples: int) -> list[float]:\n",
    "    \"\"\"\n",
    "    Expects tri-modal batches with keys:\n",
    "      - \"pixel_values\"\n",
    "      - \"audio_values\"\n",
    "      - \"input_ids\"\n",
    "      - \"attention_mask\"\n",
    "    \"\"\"\n",
    "    vols = []\n",
    "    for batch in tqdm(dataloader, desc=\"GRAM volume\"):\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        audio = batch[\"audio_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i_emb = omni.embed_image(pixels)       # (B, D)\n",
    "            a_emb = omni.embed_audio(audio)        # (B, D)\n",
    "            t_emb = omni.embed_text(input_ids, mask)\n",
    "\n",
    "        for j in range(i_emb.size(0)):\n",
    "            v = gramian_volume_triplet(\n",
    "                i_emb[j].cpu(), a_emb[j].cpu(), t_emb[j].cpu()\n",
    "            )\n",
    "            vols.append(v)\n",
    "            if len(vols) >= n_samples:\n",
    "                break\n",
    "\n",
    "        if len(vols) >= n_samples:\n",
    "            break\n",
    "\n",
    "    vols = vols[:n_samples]\n",
    "    print(f\"Collected {len(vols)} Gramian volumes.\")\n",
    "    print(f\"Mean: {np.mean(vols):.6f}, Median: {np.median(vols):.6f}\")\n",
    "    return vols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fddeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN DRIVER — Plug in your loaders here\n",
    "# ============================================================\n",
    "\n",
    "# These should be real DataLoaders from your code.\n",
    "# For now we keep them as placeholders.\n",
    "coco_val_loader = None          # -> image-text (COCO/Flickr)\n",
    "audiocaps_val_loader = None     # -> text-audio (AudioCaps/Clotho)\n",
    "esc50_train_loader = None       # -> image-labels for prototypes\n",
    "esc50_test_loader = None        # -> audio-labels for classification\n",
    "tri_modal_loader = None         # -> VALOR/VGGSound-style (I+A+T)\n",
    "\n",
    "\n",
    "def run_all_evals(omni: OmniEmbedWrapper, cfg: EvalConfig):\n",
    "    results = {}\n",
    "\n",
    "    # 1) Text→Image retrieval (COCO/Flickr)\n",
    "    if coco_val_loader is not None:\n",
    "        t2i_metrics = eval_text_to_image(omni, coco_val_loader)\n",
    "        results.update(t2i_metrics)\n",
    "\n",
    "        # Matryoshka curve on same loader (optional)\n",
    "        ks = [8, 16, 32, 64, 128, 256, cfg.emb_dim_image]\n",
    "        ks = [k for k in ks if k <= cfg.emb_dim_image]\n",
    "        mat_res = matryoshka_curve_image(\n",
    "            omni, coco_val_loader, ks=ks, full_dim=cfg.emb_dim_image\n",
    "        )\n",
    "        results[\"matryoshka\"] = mat_res\n",
    "        plot_matryoshka_curve(mat_res)\n",
    "\n",
    "        # Heatmap for a small subset\n",
    "        sim_matrix = t2i_metrics[\"sim_matrix\"]\n",
    "        plot_similarity_heatmap(sim_matrix[:32, :32], title=\"T→I Similarity (First 32 examples)\")\n",
    "\n",
    "    # 2) Text→Audio retrieval (AudioCaps/Clotho)\n",
    "    if audiocaps_val_loader is not None:\n",
    "        t2a_metrics = eval_text_to_audio(omni, audiocaps_val_loader)\n",
    "        results.update(t2a_metrics)\n",
    "\n",
    "    # 3) Audio→Image zero-shot classification (ESC-50)\n",
    "    if esc50_train_loader is not None and esc50_test_loader is not None:\n",
    "        # You must pass correct n_classes (e.g., 50 for ESC-50)\n",
    "        n_classes = 50\n",
    "        proto_mat = build_image_class_prototypes(omni, esc50_train_loader, n_classes)\n",
    "        esc_acc = eval_audio_zero_shot_classification(omni, proto_mat, esc50_test_loader)\n",
    "        results[\"ESC50_zeroshot_acc\"] = esc_acc\n",
    "\n",
    "    # 4) Gramian Volume (tri-modal binding)\n",
    "    if tri_modal_loader is not None:\n",
    "        vols = eval_gramian_volume(omni, tri_modal_loader, n_samples=cfg.gram_n_samples)\n",
    "        results[\"gram_volumes\"] = vols\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(vols, bins=30)\n",
    "        plt.title(\"Distribution of Gramian Volumes (Tri-modal)\")\n",
    "        plt.xlabel(\"Volume\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (after you've constructed all loaders):\n",
    "\n",
    "# omni = load_aligned_model(cfg).to(device)\n",
    "# omni.eval()\n",
    "# all_results = run_all_evals(omni, cfg)\n",
    "# all_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
