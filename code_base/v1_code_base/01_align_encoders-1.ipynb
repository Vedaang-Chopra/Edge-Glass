{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb558b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from imports.configs.config import setup_from_yaml\n",
    "from imports.dataset import PixmoFeatureDataset, LibriSpeechFeatureDataset, collate_alignment\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "from imports.align_training.text_encoder import HFTextEncoderConfig, HFTextEncoder\n",
    "from imports.align_training.steps import AlignmentModules, AlignmentConfig\n",
    "from imports.align_training.training import build_alignment_optimizer, train_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e093470b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phase0_global_setup</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/owxgg13f' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/owxgg13f</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251124_114106-owxgg13f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/wandb/run-20251124_114326-jccwez79</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jccwez79' target=\"_blank\">phase0_global_setup</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jccwez79' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jccwez79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: cuda, dtype: torch.float16\n",
      "[Config] root_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass\n",
      "[Config] features_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass/features\n",
      "Device: cuda dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "cfg = setup_from_yaml(\"imports/configs/config.yaml\")  # uses your Config + YAML loader\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "print(\"Device:\", device, \"dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530dd21e",
   "metadata": {},
   "source": [
    "### 6.2 Build datasets & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "392b7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/train_feat_769.pt\n",
      "[PixmoFeatureDataset] Loaded 872 valid entries from 873 total.\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_97.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_98.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_8.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_10.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_19.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_21.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_55.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_57.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_85.pt\n",
      "[PixmoFeatureDataset] Loaded 80 valid entries from 89 total.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.models.llm_model_name)\n",
    "\n",
    "# Vision (PixMo) features\n",
    "pixmo_train = PixmoFeatureDataset(cfg.datasets.pixmo_train_index)\n",
    "pixmo_val   = PixmoFeatureDataset(cfg.datasets.pixmo_val_index)\n",
    "\n",
    "# Optional small subsets for quick testing\n",
    "if cfg.training.train_subset_size and cfg.training.train_subset_size < len(pixmo_train):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_train = Subset(pixmo_train, range(cfg.training.train_subset_size))\n",
    "\n",
    "if cfg.training.val_subset_size and cfg.training.val_subset_size < len(pixmo_val):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_val = Subset(pixmo_val, range(cfg.training.val_subset_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f65b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/train_feat_769.pt\n",
      "[PixmoFeatureDataset] Loaded 872 valid entries from 873 total.\n",
      "0 data/data/pixmo/features/train_feat_64.pt\n",
      "10 data/data/pixmo/features/train_feat_77.pt\n",
      "100 data/data/pixmo/features/train_feat_20.pt\n"
     ]
    }
   ],
   "source": [
    "from imports.dataset import PixmoFeatureDataset\n",
    "\n",
    "ds_test = PixmoFeatureDataset(cfg.datasets.pixmo_train_index)\n",
    "len(ds_test)  # should be <= original, but all entries valid now\n",
    "\n",
    "# sanity check a few random samples\n",
    "for i in [0, 10, 100]:\n",
    "    if i >= len(ds_test):\n",
    "        break\n",
    "    rec = ds_test.index[i]\n",
    "    print(i, rec[\"resolved_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d95e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet you provided is setting up data loaders for training and validation data using the `DataLoader` class. These data loaders are used to load batches of data for training a machine learning model.\n",
    "# Nvidia\n",
    "\n",
    "vision_train_loader = DataLoader(\n",
    "    pixmo_train,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n",
    "vision_val_loader = DataLoader(\n",
    "    pixmo_val,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e809a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# collate_fn = partial(collate_alignment, tokenizer=tokenizer)\n",
    "\n",
    "# vision_train_loader = DataLoader(\n",
    "#     pixmo_train,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,  # <-- IMPORTANT for now on macOS\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# vision_val_loader = DataLoader(\n",
    "#     pixmo_val,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,  # <-- same here\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d28212cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = {\n",
    "    \"vision\": vision_train_loader,\n",
    "    # \"audio\": audio_train_loader,  # add later if you want\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ddd7",
   "metadata": {},
   "source": [
    "### 6.3 Build text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9cba83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0b261db9f34ce68735a18eac5f1829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text hidden size: 2048\n"
     ]
    }
   ],
   "source": [
    "txt_cfg = HFTextEncoderConfig(\n",
    "    model_name=cfg.models.llm_model_name,\n",
    "    max_length=128,\n",
    "    trainable=False,  # Stage-1: keep frozen\n",
    ")\n",
    "\n",
    "text_encoder = HFTextEncoder(\n",
    "    cfg=txt_cfg,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "d_text = text_encoder.hidden_size\n",
    "print(\"Text hidden size:\", d_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa06dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embed_fn(texts: list[str], max_length: int) -> torch.Tensor:\n",
    "    # We could override max_length by rebuilding text_encoder, but usually\n",
    "    # HFTextEncoderConfig.max_length is enough, so we ignore this arg.\n",
    "    with torch.no_grad():\n",
    "        return text_encoder.encode(texts).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12937dcf",
   "metadata": {},
   "source": [
    "### 6.4 Build adapters, Perceiver, projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c1583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision feature dim: 1536\n"
     ]
    }
   ],
   "source": [
    "# Peek one example to get feature dim\n",
    "sample = pixmo_train[0] if not isinstance(pixmo_train, torch.utils.data.Subset) else pixmo_train.dataset[pixmo_train.indices[0]]\n",
    "d_feat_v = sample[\"features\"].shape[-1]\n",
    "print(\"Vision feature dim:\", d_feat_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f53a4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceiver dim: 1536\n"
     ]
    }
   ],
   "source": [
    "d_perceiver = cfg.architecture.perceiver_dim or d_feat_v\n",
    "print(\"Perceiver dim:\", d_perceiver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ce0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_adapter = nn.Linear(d_feat_v, d_perceiver).to(device=device, dtype=dtype)\n",
    "\n",
    "perceiver = PerceiverLatentEncoder(\n",
    "    num_latents=cfg.architecture.num_latents,\n",
    "    d_latent=d_perceiver,\n",
    "    d_input=d_perceiver,\n",
    "    num_layers=cfg.architecture.num_perceiver_layers,\n",
    "    num_heads=cfg.architecture.num_attn_heads,\n",
    "    mlp_ratio=cfg.architecture.mlp_ratio,\n",
    "    dropout=0.1,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "projector = ProjectorMLP(\n",
    "    d_in=d_perceiver,\n",
    "    d_out=d_text,\n",
    "    hidden_factor=2.0,\n",
    "    dropout=0.1,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "modules = AlignmentModules(\n",
    "    vision_adapter=vision_adapter,\n",
    "    audio_adapter=None,  # add audio later if needed\n",
    "    perceiver=perceiver,\n",
    "    projector=projector,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d387c15",
   "metadata": {},
   "source": [
    "### 6.5 Alignment config + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f83ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdb622e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 174032384\n"
     ]
    }
   ],
   "source": [
    "# MRL radii from config; default to full dim if None\n",
    "mrl_dims = tuple(cfg.mrl.mrl_dims) if cfg.mrl.mrl_dims is not None else (d_text,)\n",
    "\n",
    "align_cfg = AlignmentConfig(\n",
    "    mrl_dims=mrl_dims,\n",
    "    mrl_temperature=cfg.mrl.mrl_temp,\n",
    "    max_text_length=64,  # arbitrary; text_encoder already truncates internally\n",
    ")\n",
    "\n",
    "optimizer = build_alignment_optimizer(\n",
    "    modules=modules,\n",
    "    learning_rate=float(cfg.training.learning_rate),\n",
    "    weight_decay=float(cfg.training.weight_decay),\n",
    ")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in optimizer.param_groups[0][\"params\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bf941",
   "metadata": {},
   "source": [
    "### 6.6 Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d12143a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 | vision] step 0050 | loss nan\n",
      "[Epoch 0] modality=vision | mean_loss=nan\n",
      "[Epoch 1 | vision] step 0050 | loss nan\n",
      "[Epoch 1] modality=vision | mean_loss=nan\n",
      "[Epoch 2 | vision] step 0050 | loss nan\n",
      "[Epoch 2] modality=vision | mean_loss=nan\n",
      "[Epoch 3 | vision] step 0050 | loss nan\n",
      "[Epoch 3] modality=vision | mean_loss=nan\n",
      "[Epoch 4 | vision] step 0050 | loss nan\n",
      "[Epoch 4] modality=vision | mean_loss=nan\n"
     ]
    }
   ],
   "source": [
    "mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "train_alignment(\n",
    "    train_loaders=train_loaders,\n",
    "    modules=modules,\n",
    "    cfg=align_cfg,\n",
    "    text_embed_fn=text_embed_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=mean_epochs,\n",
    "    log_every=cfg.training.log_every_steps,\n",
    "    log_fn=None,   # or pass wandb.log\n",
    "    modalities=(\"vision\",),  # add \"audio\" when you wire audio\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb47eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "# train_alignment(\n",
    "#     train_loaders=train_loaders,\n",
    "#     modules=modules,\n",
    "#     cfg=align_cfg,\n",
    "#     text_embed_fn=text_embed_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     num_epochs=mean_epochs,\n",
    "#     log_every=cfg.training.log_every_steps,\n",
    "#     log_fn=None,\n",
    "#     modalities=(\"vision\",),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0d323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb09fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
