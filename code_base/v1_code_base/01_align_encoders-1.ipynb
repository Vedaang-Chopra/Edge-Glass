{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb558b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from imports.configs.config import setup_from_yaml\n",
    "from imports.dataset import PixmoFeatureDataset, LibriSpeechFeatureDataset, collate_alignment\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "from imports.align_training.text_encoder import HFTextEncoderConfig, HFTextEncoder\n",
    "from imports.align_training.steps import AlignmentModules, AlignmentConfig\n",
    "from imports.align_training.training import build_alignment_optimizer, train_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e093470b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phase0_global_setup</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/p7ffenlv' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/p7ffenlv</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251124_121658-p7ffenlv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/wandb/run-20251124_122809-0aqowyve</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/0aqowyve' target=\"_blank\">phase0_global_setup</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/0aqowyve' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/0aqowyve</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: cuda, dtype: torch.float32\n",
      "[Config] root_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass\n",
      "[Config] features_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass/features\n",
      "Device: cuda dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "cfg = setup_from_yaml(\"imports/configs/config.yaml\")  # uses your Config + YAML loader\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "print(\"Device:\", device, \"dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530dd21e",
   "metadata": {},
   "source": [
    "### 6.2 Build datasets & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "392b7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/train_feat_769.pt\n",
      "[PixmoFeatureDataset] Loaded 872 valid entries from 873 total.\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_97.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_98.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_8.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_10.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_19.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_21.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_55.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_57.pt\n",
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/val_feat_85.pt\n",
      "[PixmoFeatureDataset] Loaded 80 valid entries from 89 total.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.models.llm_model_name)\n",
    "\n",
    "# Vision (PixMo) features\n",
    "pixmo_train = PixmoFeatureDataset(cfg.datasets.pixmo_train_index)\n",
    "pixmo_val   = PixmoFeatureDataset(cfg.datasets.pixmo_val_index)\n",
    "\n",
    "# Optional small subsets for quick testing\n",
    "if cfg.training.train_subset_size and cfg.training.train_subset_size < len(pixmo_train):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_train = Subset(pixmo_train, range(cfg.training.train_subset_size))\n",
    "\n",
    "if cfg.training.val_subset_size and cfg.training.val_subset_size < len(pixmo_val):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_val = Subset(pixmo_val, range(cfg.training.val_subset_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f65b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PixmoFeatureDataset] WARNING: skipping missing feature file: data/pixmo/features/train_feat_769.pt\n",
      "[PixmoFeatureDataset] Loaded 872 valid entries from 873 total.\n",
      "0 data/data/pixmo/features/train_feat_64.pt\n",
      "10 data/data/pixmo/features/train_feat_77.pt\n",
      "100 data/data/pixmo/features/train_feat_20.pt\n"
     ]
    }
   ],
   "source": [
    "from imports.dataset import PixmoFeatureDataset\n",
    "\n",
    "ds_test = PixmoFeatureDataset(cfg.datasets.pixmo_train_index)\n",
    "len(ds_test)  # should be <= original, but all entries valid now\n",
    "\n",
    "# sanity check a few random samples\n",
    "for i in [0, 10, 100]:\n",
    "    if i >= len(ds_test):\n",
    "        break\n",
    "    rec = ds_test.index[i]\n",
    "    print(i, rec[\"resolved_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784744f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d95e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet you provided is setting up data loaders for training and validation data using the `DataLoader` class. These data loaders are used to load batches of data for training a machine learning model.\n",
    "# Nvidia\n",
    "\n",
    "vision_train_loader = DataLoader(\n",
    "    pixmo_train,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n",
    "vision_val_loader = DataLoader(\n",
    "    pixmo_val,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e809a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# collate_fn = partial(collate_alignment, tokenizer=tokenizer)\n",
    "\n",
    "# vision_train_loader = DataLoader(\n",
    "#     pixmo_train,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,  # <-- IMPORTANT for now on macOS\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# vision_val_loader = DataLoader(\n",
    "#     pixmo_val,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,  # <-- same here\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d28212cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = {\n",
    "    \"vision\": vision_train_loader,\n",
    "    # \"audio\": audio_train_loader,  # add later if you want\n",
    "}\n",
    "val_loaders = {\n",
    "    \"vision\": vision_val_loader,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ddd7",
   "metadata": {},
   "source": [
    "### 6.3 Build text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9cba83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dfab82458f45318d7df66102cd1eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text hidden size: 2048\n"
     ]
    }
   ],
   "source": [
    "txt_cfg = HFTextEncoderConfig(\n",
    "    model_name=cfg.models.llm_model_name,\n",
    "    max_length=128,\n",
    "    trainable=False,  # Stage-1: keep frozen\n",
    ")\n",
    "\n",
    "text_encoder = HFTextEncoder(\n",
    "    cfg=txt_cfg,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "d_text = text_encoder.hidden_size\n",
    "print(\"Text hidden size:\", d_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa06dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embed_fn(texts: list[str], max_length: int) -> torch.Tensor:\n",
    "    # We could override max_length by rebuilding text_encoder, but usually\n",
    "    # HFTextEncoderConfig.max_length is enough, so we ignore this arg.\n",
    "    with torch.no_grad():\n",
    "        return text_encoder.encode(texts).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12937dcf",
   "metadata": {},
   "source": [
    "### 6.4 Build adapters, Perceiver, projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c1583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision feature dim: 1536\n"
     ]
    }
   ],
   "source": [
    "# Peek one example to get feature dim\n",
    "sample = pixmo_train[0] if not isinstance(pixmo_train, torch.utils.data.Subset) else pixmo_train.dataset[pixmo_train.indices[0]]\n",
    "d_feat_v = sample[\"features\"].shape[-1]\n",
    "print(\"Vision feature dim:\", d_feat_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f53a4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceiver dim: 1536\n"
     ]
    }
   ],
   "source": [
    "d_perceiver = cfg.architecture.perceiver_dim or d_feat_v\n",
    "print(\"Perceiver dim:\", d_perceiver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c0c2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.model import MultiModalAlignmentModel  # or from current cell\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b3f3c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalAlignmentModel(\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (model): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): AudioEncoder(\n",
       "    (model): WhisperModel(\n",
       "      (encoder): WhisperEncoder(\n",
       "        (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (embed_positions): Embedding(1500, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperEncoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): WhisperDecoder(\n",
       "        (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
       "        (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperDecoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (perceiver): PerceiverLatentEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x PerceiverBlock(\n",
       "        (enc_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (ln_latents_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): ProjectorMLP(\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiModalAlignmentModel(\n",
    "    d_shared=512,\n",
    "    d_latent=512,\n",
    "    d_align=1024,\n",
    "    num_latents=32,   # smaller for viz\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    use_perceiver=True,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_adapter = nn.Linear(d_feat_v, d_perceiver).to(device=device, dtype=dtype)\n",
    "\n",
    "# perceiver = PerceiverLatentEncoder(\n",
    "#     num_latents=cfg.architecture.num_latents,\n",
    "#     d_latent=d_perceiver,\n",
    "#     d_input=d_perceiver,\n",
    "#     num_layers=cfg.architecture.num_perceiver_layers,\n",
    "#     num_heads=cfg.architecture.num_attn_heads,\n",
    "#     mlp_ratio=cfg.architecture.mlp_ratio,\n",
    "#     dropout=0.1,\n",
    "# ).to(device=device, dtype=dtype)\n",
    "\n",
    "# projector = ProjectorMLP(\n",
    "#     d_in=d_perceiver,\n",
    "#     d_out=d_text,\n",
    "#     hidden_factor=2.0,\n",
    "#     dropout=0.1,\n",
    "# ).to(device=device, dtype=dtype)\n",
    "\n",
    "# modules = AlignmentModules(\n",
    "#     vision_adapter=vision_adapter,\n",
    "#     audio_adapter=None,  # add audio later if needed\n",
    "#     perceiver=perceiver,\n",
    "#     projector=projector,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d387c15",
   "metadata": {},
   "source": [
    "### 6.5 Alignment config + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb622e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRL radii from config; default to full dim if None\n",
    "mrl_dims = tuple(cfg.mrl.mrl_dims) if cfg.mrl.mrl_dims is not None else (d_text,)\n",
    "\n",
    "align_cfg = AlignmentConfig(\n",
    "    mrl_dims=mrl_dims,\n",
    "    mrl_temperature=cfg.mrl.mrl_temp,\n",
    "    max_text_length=64,  # arbitrary; text_encoder already truncates internally\n",
    ")\n",
    "\n",
    "optimizer = build_alignment_optimizer(\n",
    "    modules=modules,\n",
    "    learning_rate=float(cfg.training.learning_rate),\n",
    "    weight_decay=float(cfg.training.weight_decay),\n",
    ")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in optimizer.param_groups[0][\"params\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bf941",
   "metadata": {},
   "source": [
    "### 6.6 Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from dataclasses import asdict\n",
    "from typing import Dict\n",
    "\n",
    "if cfg.misc.use_wandb:\n",
    "    wandb.init(\n",
    "        project=cfg.misc.wandb_project,\n",
    "        name=cfg.misc.wandb_run_name,\n",
    "        config=asdict(cfg),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRL dims:\", cfg.mrl.mrl_dims)\n",
    "print(\"Text dim:\", d_text)\n",
    "\n",
    "mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "def wb_log_fn(stats: Dict[str, float]):\n",
    "    wandb.log(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alignment(\n",
    "    train_loaders=train_loaders,\n",
    "    val_loaders=val_loaders,\n",
    "    modules=modules,\n",
    "    cfg=align_cfg,\n",
    "    text_embed_fn=text_embed_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=mean_epochs,\n",
    "    log_every=cfg.training.log_every_steps,\n",
    "    log_fn=wb_log_fn,\n",
    "    modalities=(\"vision\",),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "# train_alignment(\n",
    "#     train_loaders=train_loaders,\n",
    "#     modules=modules,\n",
    "#     cfg=align_cfg,\n",
    "#     text_embed_fn=text_embed_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     num_epochs=mean_epochs,\n",
    "#     log_every=cfg.training.log_every_steps,\n",
    "#     log_fn=None,\n",
    "#     modalities=(\"vision\",),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0d323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ba633e5",
   "metadata": {},
   "source": [
    "### Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e66140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imports.dataset import PixmoFeatureDataset\n",
    "# import torch\n",
    "\n",
    "# def test_pixmo_dataset(index_path, num_samples=32):\n",
    "#     print(\"=== Testing PixMo Dataset ===\")\n",
    "#     ds = PixmoFeatureDataset(index_path)\n",
    "    \n",
    "#     print(f\"Total valid samples: {len(ds)}\\n\")\n",
    "    \n",
    "#     num_checked = min(num_samples, len(ds))\n",
    "#     print(f\"Checking first {num_checked} samples...\\n\")\n",
    "\n",
    "#     for i in range(num_checked):\n",
    "#         rec = ds[i]\n",
    "#         feats = rec[\"features\"]          # Tensor (T, D_feat)\n",
    "#         caption = rec[\"text\"]\n",
    "#         mod = rec[\"modality\"]\n",
    "#         file = rec[\"file\"]\n",
    "\n",
    "#         print(f\"--- Sample {i} ---\")\n",
    "#         print(\"file:\", file)\n",
    "#         print(\"caption:\", caption)\n",
    "#         print(\"modality:\", mod)\n",
    "#         print(\"features shape:\", tuple(feats.shape))\n",
    "\n",
    "#         # NaN / Inf checks\n",
    "#         if not torch.isfinite(feats).all():\n",
    "#             print(\"❌ NON-FINITE VALUES FOUND in features! NaNs:\", \n",
    "#                   torch.isnan(feats).sum().item(),\n",
    "#                   \"Infs:\", torch.isinf(feats).sum().item())\n",
    "#         else:\n",
    "#             print(\"✓ features finite\")\n",
    "\n",
    "#         # Basic statistics\n",
    "#         print(\"feature mean/std:\", feats.mean().item(), feats.std().item())\n",
    "        \n",
    "#         # Check if caption is empty\n",
    "#         if caption is None or caption.strip() == \"\":\n",
    "#             print(\"⚠️ Empty caption!\")\n",
    "#         else:\n",
    "#             print(\"✓ caption OK\")\n",
    "        \n",
    "#         print(\"----------------------------------\")\n",
    "\n",
    "#     print(\"\\nDataset test completed.\\n\")\n",
    "\n",
    "\n",
    "# # Run test\n",
    "# test_pixmo_dataset(cfg.datasets.pixmo_train_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scan_entire_dataset(index_path):\n",
    "#     ds = PixmoFeatureDataset(index_path)\n",
    "    \n",
    "#     nan_files = []\n",
    "#     inf_files = []\n",
    "    \n",
    "#     for i in range(len(ds)):\n",
    "#         rec = ds.index[i]\n",
    "#         path = rec[\"resolved_path\"]\n",
    "#         blob = torch.load(path, map_location=\"cpu\")\n",
    "#         feats = blob[\"features\"]\n",
    "        \n",
    "#         if torch.isnan(feats).any():\n",
    "#             nan_files.append(path)\n",
    "#         if torch.isinf(feats).any():\n",
    "#             inf_files.append(path)\n",
    "\n",
    "#     print(\"=== FULL SCAN RESULTS ===\")\n",
    "#     print(\"Files with NaNs:\", len(nan_files))\n",
    "#     print(\"Files with Infs:\", len(inf_files))\n",
    "\n",
    "#     if nan_files:\n",
    "#         print(\"NaN-containing files:\")\n",
    "#         for f in nan_files[:15]:\n",
    "#             print(\"  \", f)\n",
    "#     if inf_files:\n",
    "#         print(\"Inf-containing files:\")\n",
    "#         for f in inf_files[:15]:\n",
    "#             print(\"  \", f)\n",
    "\n",
    "#     return nan_files, inf_files\n",
    "\n",
    "# nan_list, inf_list = scan_entire_dataset(cfg.datasets.pixmo_train_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f36115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.dataset import PixmoFeatureDataset\n",
    "import torch\n",
    "\n",
    "def test_pixmo_dataset(index_path, num_samples=32):\n",
    "    print(\"=== Testing PixMo Dataset ===\")\n",
    "    ds = PixmoFeatureDataset(index_path)\n",
    "    \n",
    "    print(f\"Total valid samples (after path filtering): {len(ds)}\\n\")\n",
    "    \n",
    "    num_checked = min(num_samples, len(ds))\n",
    "    print(f\"Checking first {num_checked} samples...\\n\")\n",
    "\n",
    "    for i in range(num_checked):\n",
    "        ex = ds[i]\n",
    "        feats = ex[\"features\"]          # Tensor (T, D_feat)\n",
    "        text  = ex[\"text\"]\n",
    "        file  = ex[\"file\"]\n",
    "\n",
    "        print(f\"--- Sample {i} ---\")\n",
    "        print(\"file:\", file)\n",
    "        print(\"caption:\", repr(text))\n",
    "        print(\"features shape:\", tuple(feats.shape))\n",
    "\n",
    "        # 1) zero-length check\n",
    "        if feats.shape[0] == 0:\n",
    "            print(\"❌ zero-length features!\")\n",
    "\n",
    "        # 2) finite check\n",
    "        finite = torch.isfinite(feats).all()\n",
    "        print(\"finite:\", bool(finite))\n",
    "\n",
    "        # 3) basic stats\n",
    "        print(\"mean/std:\", feats.mean().item(), feats.std().item())\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    print(\"\\nDataset test completed.\\n\")\n",
    "\n",
    "test_pixmo_dataset(cfg.datasets.pixmo_train_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch = next(iter(vision_train_loader))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "\n",
    "# Move tensors to device\n",
    "features = batch[\"features\"].to(device)        # (B, T, D_feat)\n",
    "feat_mask = batch[\"feature_mask\"].to(device)   # (B, T)\n",
    "texts    = batch[\"raw_text\"]\n",
    "\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"mask shape:\", feat_mask.shape)\n",
    "print(\"any zero-length seqs? lengths:\", feat_mask.sum(dim=1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 1) Adapter\n",
    "    tokens = modules.vision_adapter(features)   # (B, T, d_perceiver)\n",
    "    print(\"tokens finite:\", bool(torch.isfinite(tokens).all()))\n",
    "    print(\"tokens mean/std:\",\n",
    "          tokens.mean().item(), tokens.std().item())\n",
    "\n",
    "    # 2) Perceiver\n",
    "    latents = modules.perceiver(tokens, encoder_mask=feat_mask)  # (B, L, d_perceiver)\n",
    "    print(\"latents finite:\", bool(torch.isfinite(latents).all()))\n",
    "    print(\"latents mean/std:\",\n",
    "          latents.mean().item(), latents.std().item())\n",
    "\n",
    "    # 3) Projector\n",
    "    lat_llm = modules.projector(latents)        # (B, L, d_text)\n",
    "    print(\"lat_llm finite:\", bool(torch.isfinite(lat_llm).all()))\n",
    "    print(\"lat_llm mean/std:\",\n",
    "          lat_llm.mean().item(), lat_llm.std().item())\n",
    "\n",
    "    # 4) Pooled vision embedding\n",
    "    h_mod = lat_llm.mean(dim=1)                 # (B, d_text)\n",
    "    print(\"h_mod finite:\", bool(torch.isfinite(h_mod).all()))\n",
    "    print(\"h_mod mean/std:\",\n",
    "          h_mod.mean().item(), h_mod.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce596519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Grab one batch from the train loader\n",
    "batch = next(iter(vision_train_loader))\n",
    "features = batch[\"features\"].to(device)        # (B, 256, 1536)\n",
    "feat_mask = batch[\"feature_mask\"].to(device)   # (B, 256)\n",
    "texts    = batch[\"raw_text\"]\n",
    "\n",
    "print(\"=== Batch Sanity ===\")\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"mask shape:\", feat_mask.shape)\n",
    "print(\"seq lengths:\", feat_mask.sum(dim=1))\n",
    "\n",
    "print(\"features finite:\", bool(torch.isfinite(features).all()))\n",
    "print(\"features mean/std:\",\n",
    "      features.mean().item(), features.std().item())\n",
    "\n",
    "# Now just test the adapter alone\n",
    "with torch.no_grad():\n",
    "    va = modules.vision_adapter\n",
    "    print(\"\\n=== Vision Adapter Params ===\")\n",
    "    for name, p in va.named_parameters():\n",
    "        print(\n",
    "            name, p.shape,\n",
    "            \"finite:\", bool(torch.isfinite(p).all()),\n",
    "            \"mean/std:\", p.mean().item(), p.std().item(),\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== Forward through vision_adapter ===\")\n",
    "    tokens = va(features)        # (B, 256, d_perceiver)\n",
    "    print(\"tokens shape:\", tokens.shape)\n",
    "    print(\"tokens finite:\", bool(torch.isfinite(tokens).all()))\n",
    "    print(\"tokens mean/std:\",\n",
    "          tokens.mean().item(), tokens.std().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
