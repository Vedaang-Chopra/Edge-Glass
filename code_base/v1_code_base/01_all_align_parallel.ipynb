{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multi-GPU Alignment Training (Jupyter Optimized)\n",
    "\n",
    "This notebook implements a strictly isolated multi-GPU training loop compatible with `accelerate.notebook_launcher`. \n",
    "\n",
    "**RULE:** Do not move model loading or CUDA operations to the global scope. Everything must remain inside `training_function` to avoid `RuntimeError: Cannot re-initialize CUDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import io\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torchaudio import transforms as T_audio\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPVisionModel,\n",
    "    CLIPImageProcessor,\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Configuration (Pure Python, No CUDA init)\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- Model names ---\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "    # --- Dimensions ---\n",
    "    encoder_dim_vision: int = 768     # CLIP-base dim\n",
    "    encoder_dim_audio: int = 512      # Whisper-base dim\n",
    "    llm_hidden_size: int = 3584       # Qwen 7B dim\n",
    "    \n",
    "    # --- Model Capacity ---\n",
    "    perceiver_dim: int = 768\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 6\n",
    "    num_attn_heads: int = 8\n",
    "    mlp_ratio: float = 4.0\n",
    "\n",
    "    # --- Matryoshka loss ---\n",
    "    mrl_dims: Tuple[int, ...] = (128, 256, 512, 768, 3584)\n",
    "    mrl_temperature: float = 0.07\n",
    "\n",
    "    # --- Training Dynamics ---\n",
    "    batch_size_vision: int = 32 # Adjust based on GPU VRAM\n",
    "    batch_size_audio: int = 32\n",
    "    max_train_steps_vision: int = 200 # Short for demo\n",
    "    max_train_steps_audio: int = 200\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_rounds: int = 1\n",
    "    seed: int = 42\n",
    "    \n",
    "    # --- Data (Subsets for Demo) ---\n",
    "    audio_samples: int = 200\n",
    "    vision_samples: int = 200\n",
    "\n",
    "cfg = Config()\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Module Definitions (Adapters & Perceiver)\n",
    "# ============================================\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens    = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "        self.mlp = FeedForward(dim, mlp_ratio=mlp_ratio)\n",
    "\n",
    "    def forward(self, latents, tokens, token_mask=None):\n",
    "        q = self.ln_latents_1(latents)\n",
    "        kv = self.ln_tokens(tokens)\n",
    "        key_padding_mask = ~token_mask.bool() if token_mask is not None else None\n",
    "\n",
    "        attn_out, _ = self.cross_attn(q, kv, kv, key_padding_mask=key_padding_mask, need_weights=False)\n",
    "        latents = latents + attn_out\n",
    "        \n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_attn_out, _ = self.self_attn(q2, q2, q2, need_weights=False)\n",
    "        latents = latents + self_attn_out\n",
    "        \n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "        return latents\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, dim, num_latents, num_layers, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads, mlp_ratio) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, tokens, token_mask=None):\n",
    "        B = tokens.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "datasets_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Dataset Classes (Self-contained)\n",
    "# ============================================\n",
    "\n",
    "class PixmoVisionDataset(Dataset):\n",
    "    def __init__(self, data_list, vision_model, vision_processor, device):\n",
    "        self.data = data_list\n",
    "        self.vision_model = vision_model\n",
    "        self.vision_processor = vision_processor\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        # Handle image loading\n",
    "        img = ex.get(\"image\")\n",
    "        if img is None:\n",
    "            img = Image.open(Requests.get(ex[\"image_url\"], stream=True).raw).convert(\"RGB\")\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "            \n",
    "        # Process\n",
    "        # NOTE: In a real large-scale scenario, you might want to pre-compute features.\n",
    "        # Here we compute on-the-fly, which is slower but simpler for the script.\n",
    "        inputs = self.vision_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.vision_model(pixel_values=pixel_values)\n",
    "            feats = out.last_hidden_state.squeeze(0).to(\"cpu\") # Move to CPU to save GPU RAM in queue\n",
    "            \n",
    "        return {\"features\": feats, \"text\": ex[\"caption\"]}\n",
    "\n",
    "class SimpleAudioDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list # Expects pre-computed feature dicts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_features_with_text(batch):\n",
    "    feats = [b[\"features\"] for b in batch]\n",
    "    encoder_feats = pad_sequence(feats, batch_first=True)\n",
    "    \n",
    "    B, T_max, _ = encoder_feats.shape\n",
    "    encoder_mask = torch.zeros(B, T_max, dtype=torch.bool)\n",
    "    for i, f in enumerate(feats):\n",
    "        encoder_mask[i, :f.shape[0]] = True\n",
    "        \n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    return {\n",
    "        \"encoder_feats\": encoder_feats, \n",
    "        \"encoder_mask\": encoder_mask, \n",
    "        \"texts\": texts\n",
    "    }\n",
    "\n",
    "def matryoshka_contrastive_loss(z_mod, z_txt, trunc_dims):\n",
    "    losses = []\n",
    "    targets = torch.arange(z_mod.size(0), device=z_mod.device)\n",
    "    for d in trunc_dims:\n",
    "        zm = F.normalize(z_mod[:, :d], dim=-1)\n",
    "        zt = F.normalize(z_txt[:, :d], dim=-1)\n",
    "        logits = zm @ zt.T / cfg.mrl_temperature\n",
    "        loss = 0.5 * (F.cross_entropy(logits, targets) + F.cross_entropy(logits.T, targets))\n",
    "        losses.append(loss)\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "training_fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. THE TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def training_function():\n",
    "    # --- A. Initialization ---\n",
    "    accelerator = Accelerator(mixed_precision=\"bf16\", log_with=\"wandb\")\n",
    "    device = accelerator.device\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Process {accelerator.process_index}: Initialized. Device: {device}\")\n",
    "\n",
    "    # --- B. Load Models (Local to Process) ---\n",
    "    # 1. Vision\n",
    "    vision_processor = CLIPImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "    vision_model = CLIPVisionModel.from_pretrained(cfg.vision_model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "    vision_model.eval()\n",
    "\n",
    "    # 2. Audio\n",
    "    audio_processor = WhisperProcessor.from_pretrained(cfg.audio_model_name)\n",
    "    audio_model = WhisperModel.from_pretrained(cfg.audio_model_name, torch_dtype=torch.float32).to(device)\n",
    "    audio_model.eval()\n",
    "\n",
    "    # 3. LLM (Qwen)\n",
    "    qwen_tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model_name, use_fast=True)\n",
    "    if qwen_tokenizer.pad_token is None:\n",
    "        qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n",
    "    qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.llm_model_name, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    ).to(device)\n",
    "    qwen_model.eval()\n",
    "\n",
    "    # 4. Trainable Modules\n",
    "    vision_adapter = ModalityAdapter(cfg.encoder_dim_vision, cfg.perceiver_dim).to(device)\n",
    "    audio_adapter = ModalityAdapter(cfg.encoder_dim_audio, cfg.perceiver_dim).to(device)\n",
    "    perceiver = PerceiverResampler(\n",
    "        cfg.perceiver_dim, cfg.num_latents, cfg.num_perceiver_layers, cfg.num_attn_heads\n",
    "    ).to(device)\n",
    "    projector = nn.Linear(cfg.perceiver_dim, cfg.llm_hidden_size).to(device)\n",
    "\n",
    "    # --- C. Data Prep (Robust Loading) ---\n",
    "    # To avoid API limits, main process loads list, broadcast isn't easy with lists in accel,\n",
    "    # so we'll just load a small slice independently on each for this demo.\n",
    "    \n",
    "    # Vision Data\n",
    "    if accelerator.is_main_process: print(\"Loading Vision Data...\")\n",
    "    pixmo_ds = load_dataset(\"allenai/pixmo-cap\", split=\"train\", streaming=True)\n",
    "    pixmo_data = list(pixmo_ds.take(cfg.vision_samples))\n",
    "    train_v_ds = PixmoVisionDataset(pixmo_data, vision_model, vision_processor, device)\n",
    "    train_v_loader = DataLoader(train_v_ds, batch_size=cfg.batch_size_vision, collate_fn=collate_features_with_text)\n",
    "\n",
    "    # Audio Data (Pre-process audio to features to save VRAM/Compute during loop)\n",
    "    if accelerator.is_main_process: print(\"Loading Audio Data...\")\n",
    "    libri_ds = load_dataset(\"openslr/librispeech_asr\", \"all\", split=\"train.clean.100\", streaming=True)\n",
    "    processed_audio = []\n",
    "    \n",
    "    # Helper for local processing\n",
    "    def process_audio_entry(ex):\n",
    "        wav_bytes = ex[\"audio\"][\"bytes\"]\n",
    "        wav, sr = librosa.load(io.BytesIO(wav_bytes), sr=16000)\n",
    "        dur = len(wav)/sr\n",
    "        if dur > 12.0: return None\n",
    "        \n",
    "        inputs = audio_processor(wav, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        input_features = inputs[\"input_features\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            enc_out = audio_model.encoder(input_features)\n",
    "            hidden = enc_out.last_hidden_state.squeeze(0)\n",
    "            \n",
    "        # Slice padding\n",
    "        valid_frames = min(int(dur * 50), 1500)\n",
    "        feats = hidden[:valid_frames, :].to(\"cpu\")\n",
    "        return {\"features\": feats, \"text\": ex[\"text\"].lower().capitalize(), \"duration\": dur}\n",
    "\n",
    "    count = 0\n",
    "    for ex in libri_ds:\n",
    "        if count >= cfg.audio_samples: break\n",
    "        p = process_audio_entry(ex)\n",
    "        if p: \n",
    "            processed_audio.append(p)\n",
    "            count += 1\n",
    "            \n",
    "    train_a_ds = SimpleAudioDataset(processed_audio)\n",
    "    train_a_loader = DataLoader(train_a_ds, batch_size=cfg.batch_size_audio, collate_fn=collate_features_with_text)\n",
    "\n",
    "    # --- D. Optimizer & Prepare ---\n",
    "    params = list(vision_adapter.parameters()) + list(audio_adapter.parameters()) + \\\n",
    "             list(perceiver.parameters()) + list(projector.parameters())\n",
    "    \n",
    "    optimizer = AdamW(params, lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "    \n",
    "    total_steps = (len(train_v_loader) + len(train_a_loader)) * cfg.num_rounds\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, int(0.1*total_steps), total_steps)\n",
    "\n",
    "    (vision_adapter, audio_adapter, perceiver, projector, optimizer, train_v_loader, train_a_loader, scheduler) = \\\n",
    "        accelerator.prepare(vision_adapter, audio_adapter, perceiver, projector, optimizer, train_v_loader, train_a_loader, scheduler)\n",
    "\n",
    "    # --- E. Helper for Text Encoding (Local Qwen) ---\n",
    "    def encode_text_local(texts):\n",
    "        enc = qwen_tokenizer(texts, padding=True, truncation=True, max_length=64, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = qwen_model.get_input_embeddings()(enc.input_ids)\n",
    "        mask = enc.attention_mask.unsqueeze(-1)\n",
    "        # Mean pool\n",
    "        pooled = (emb * mask).sum(dim=1) / mask.sum(dim=1).clamp_min(1)\n",
    "        return pooled\n",
    "\n",
    "    # --- F. Training Loop ---\n",
    "    for round_idx in range(cfg.num_rounds):\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"\\n=== Round {round_idx+1} ===\")\n",
    "\n",
    "        # 1. Vision Loop\n",
    "        for batch in tqdm(train_v_loader, disable=not accelerator.is_main_process, desc=\"Vision\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            tokens = vision_adapter(batch[\"encoder_feats\"])\n",
    "            latents = perceiver(tokens, batch[\"encoder_mask\"])\n",
    "            z_llm = projector(latents)\n",
    "            h_mod = z_llm.mean(dim=1)\n",
    "            \n",
    "            h_txt = encode_text_local(batch[\"texts\"])\n",
    "            \n",
    "            # Gather for Global Loss calculation\n",
    "            h_mod_g = accelerator.gather(h_mod)\n",
    "            h_txt_g = accelerator.gather(h_txt)\n",
    "            \n",
    "            loss = matryoshka_contrastive_loss(h_mod_g, h_txt_g, cfg.mrl_dims)\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(params, 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if accelerator.is_main_process:\n",
    "                wandb.log({\"vision_loss\": loss.item()})\n",
    "\n",
    "        # 2. Audio Loop\n",
    "        for batch in tqdm(train_a_loader, disable=not accelerator.is_main_process, desc=\"Audio\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            tokens = audio_adapter(batch[\"encoder_feats\"])\n",
    "            latents = perceiver(tokens, batch[\"encoder_mask\"])\n",
    "            z_llm = projector(latents)\n",
    "            h_mod = z_llm.mean(dim=1)\n",
    "            \n",
    "            h_txt = encode_text_local(batch[\"texts\"])\n",
    "            \n",
    "            h_mod_g = accelerator.gather(h_mod)\n",
    "            h_txt_g = accelerator.gather(h_txt)\n",
    "            \n",
    "            loss = matryoshka_contrastive_loss(h_mod_g, h_txt_g, cfg.mrl_dims)\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(params, 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if accelerator.is_main_process:\n",
    "                wandb.log({\"audio_loss\": loss.item()})\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training Complete. Saving models...\")\n",
    "        # Save logic here if needed (unwrapped models)\n",
    "        # accelerator.save_state(cfg.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "launcher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 CUDAs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] failed (exitcode: 1) local_rank: 0 (pid: 453380) of fn: training_function (start_method: fork)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] Traceback (most recent call last):\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 697, in _poll\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     self._pc.join(-1)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 220, in join\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] \n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] -- Process 0 terminated with the following error:\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] Traceback (most recent call last):\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 95, in _wrap\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     fn(i, *args)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 617, in _wrap\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     ret = record(fn)(*args_)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]           ^^^^^^^^^^^^^^^^^^\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     return f(*args, **kwargs)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]            ^^^^^^^^^^^^^^^^^^\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/models/tmp/ipykernel_451342/632741400.py\", line 7, in training_function\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     accelerator = Accelerator(mixed_precision=\"bf16\", log_with=\"wandb\")\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/accelerator.py\", line 461, in __init__\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     self.state = AcceleratorState(\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]                  ^^^^^^^^^^^^^^^^^\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 912, in __init__\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     PartialState(cpu, **kwargs)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 301, in __init__\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     self.set_device()\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 838, in set_device\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     device_module.set_device(self.device)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     torch._C._cuda_setDevice(device)\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]   File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742]     raise RuntimeError(\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "E1124 16:34:58.556000 451342 /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:742] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\ntraining_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-11-24_16:34:58\n  host      : atl1-1-03-017-23-0.pace.gatech.edu\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 453380)\n  error_file: /home/hice1/vchopra37/scratch/models/tmp/torchelastic_1sp9p812/none_bc6ujcpc/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/models/tmp/ipykernel_451342/632741400.py\", line 7, in training_function\n      accelerator = Accelerator(mixed_precision=\"bf16\", log_with=\"wandb\")\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/accelerator.py\", line 461, in __init__\n      self.state = AcceleratorState(\n                   ^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 912, in __init__\n      PartialState(cpu, **kwargs)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 301, in __init__\n      self.set_device()\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 838, in set_device\n      device_module.set_device(self.device)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n      torch._C._cuda_setDevice(device)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n      raise RuntimeError(\n  RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mChildFailedError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 5. LAUNCH\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/launchers.py:247\u001b[39m, in \u001b[36mnotebook_launcher\u001b[39m\u001b[34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[33m\"\u001b[39m\u001b[33m>=\u001b[39m\u001b[33m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[32m    246\u001b[39m         launch_config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlog_line_prefix_template\u001b[39m\u001b[33m\"\u001b[39m] = log_line_prefix_template\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_type.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in forked subprocess\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e.args[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/launcher/api.py:156\u001b[39m, in \u001b[36melastic_launch.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/launcher/api.py:293\u001b[39m, in \u001b[36mlaunch_agent\u001b[39m\u001b[34m(config, entrypoint, args)\u001b[39m\n\u001b[32m    286\u001b[39m     events.record(agent.get_event_succeeded(), config.event_log_handler)\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.is_failed():\n\u001b[32m    289\u001b[39m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[32m    290\u001b[39m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[32m    291\u001b[39m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[32m    292\u001b[39m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[32m    294\u001b[39m             name=entrypoint_name,\n\u001b[32m    295\u001b[39m             failures=result.failures,\n\u001b[32m    296\u001b[39m         )\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.return_values\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[31mChildFailedError\u001b[39m: \n============================================================\ntraining_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-11-24_16:34:58\n  host      : atl1-1-03-017-23-0.pace.gatech.edu\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 453380)\n  error_file: /home/hice1/vchopra37/scratch/models/tmp/torchelastic_1sp9p812/none_bc6ujcpc/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/models/tmp/ipykernel_451342/632741400.py\", line 7, in training_function\n      accelerator = Accelerator(mixed_precision=\"bf16\", log_with=\"wandb\")\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/accelerator.py\", line 461, in __init__\n      self.state = AcceleratorState(\n                   ^^^^^^^^^^^^^^^^^\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 912, in __init__\n      PartialState(cpu, **kwargs)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 301, in __init__\n      self.set_device()\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/accelerate/state.py\", line 838, in set_device\n      device_module.set_device(self.device)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n      torch._C._cuda_setDevice(device)\n    File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n      raise RuntimeError(\n  RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. LAUNCH\n",
    "# ============================================\n",
    "\n",
    "notebook_launcher(training_function, num_processes=torch.cuda.device_count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
