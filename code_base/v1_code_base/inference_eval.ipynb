{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed768350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Imports for models\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    CLIPVisionModel, \n",
    "    CLIPImageProcessor, \n",
    "    WhisperModel, \n",
    "    WhisperProcessor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ecd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. Configuration\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Checkpoint path (Ensure this matches where you saved it!)\n",
    "    ckpt_path: str = \"./runs_perceiver_mrl_qwen/multimodal_adapter_poc.pt\"\n",
    "    \n",
    "    # Models\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    \n",
    "    # Architecture Dims (Must match training!)\n",
    "    perceiver_dim: int = 512\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 2\n",
    "    num_attn_heads: int = 8\n",
    "    mlp_ratio: float = 4.0\n",
    "    \n",
    "    # Filled dynamically\n",
    "    encoder_dim_vision: int = 768 \n",
    "    encoder_dim_audio: int = 512 \n",
    "    llm_hidden_size: int = 3584\n",
    "    \n",
    "    # Data\n",
    "    batch_size: int = 8\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bec203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 2. Architecture Definitions (Must match training code)\n",
    "# ============================================================\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "        self.mlp = FeedForward(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, latents, tokens, token_mask=None):\n",
    "        # Cross-Attention\n",
    "        q = self.ln_latents_1(latents)\n",
    "        kv = self.ln_tokens(tokens)\n",
    "        key_padding_mask = ~token_mask.bool() if token_mask is not None else None\n",
    "        attn_out, _ = self.cross_attn(q, kv, kv, key_padding_mask=key_padding_mask, need_weights=False)\n",
    "        latents = latents + attn_out\n",
    "        \n",
    "        # Self-Attention\n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_out, _ = self.self_attn(q2, q2, q2, need_weights=False)\n",
    "        latents = latents + self_out\n",
    "        \n",
    "        # MLP\n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "        return latents\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, dim, num_latents, num_layers, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads, mlp_ratio) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, tokens, token_mask=None):\n",
    "        B = tokens.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd033497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. The Aligned Model Wrapper\n",
    "# ============================================================\n",
    "\n",
    "class AlignedModel(nn.Module):\n",
    "    def __init__(self, vision_adapter, audio_adapter, perceiver, projector, qwen_model, qwen_tokenizer):\n",
    "        super().__init__()\n",
    "        self.vision_adapter = vision_adapter\n",
    "        self.audio_adapter = audio_adapter\n",
    "        self.perceiver = perceiver\n",
    "        self.projector = projector\n",
    "        self.qwen_model = qwen_model\n",
    "        self.qwen_tokenizer = qwen_tokenizer\n",
    "\n",
    "    def encode_image_features(self, features, mask):\n",
    "        # Move inputs to the same device as the adapter\n",
    "        device = next(self.vision_adapter.parameters()).device\n",
    "        features = features.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        tokens = self.vision_adapter(features)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        z_llm = self.projector(latents)\n",
    "        return z_llm.mean(dim=1) # Pooling for retrieval\n",
    "\n",
    "    def encode_audio_features(self, features, mask):\n",
    "        device = next(self.audio_adapter.parameters()).device\n",
    "        features = features.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        tokens = self.audio_adapter(features)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        z_llm = self.projector(latents)\n",
    "        return z_llm.mean(dim=1)\n",
    "\n",
    "    def encode_text_raw(self, texts: list[str]):\n",
    "        # Find Qwen's device (it might be spread across GPUs, get input embedding device)\n",
    "        qwen_device = self.qwen_model.model.embed_tokens.weight.device\n",
    "        \n",
    "        enc = self.qwen_tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=64, return_tensors=\"pt\"\n",
    "        ).to(qwen_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embs = self.qwen_model.get_input_embeddings()(enc.input_ids)\n",
    "            \n",
    "        mask = enc.attention_mask.unsqueeze(-1)\n",
    "        sum_embs = (token_embs * mask).sum(dim=1)\n",
    "        count = mask.sum(dim=1).clamp(min=1)\n",
    "        return sum_embs / count\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, modality_feats, modality_type, prompt_text, max_new_tokens=50):\n",
    "        \"\"\"\n",
    "        Generates text from an image or audio feature input.\n",
    "        \"\"\"\n",
    "        # 1. Project Modality\n",
    "        if modality_type == \"vision\":\n",
    "            adapter = self.vision_adapter\n",
    "        elif modality_type == \"audio\":\n",
    "            adapter = self.audio_adapter\n",
    "            \n",
    "        device = next(adapter.parameters()).device\n",
    "        modality_feats = modality_feats.to(device)\n",
    "        \n",
    "        # Create mask (B=1)\n",
    "        mask = torch.ones(1, modality_feats.shape[1], dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Forward pass through adapters\n",
    "        tokens = adapter(modality_feats)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        inputs_embeds_modality = self.projector(latents) # (1, 64, 3584)\n",
    "\n",
    "        # 2. Embed Text\n",
    "        qwen_device = self.qwen_model.model.embed_tokens.weight.device\n",
    "        \n",
    "        # Move modality embeds to Qwen device for concatenation\n",
    "        inputs_embeds_modality = inputs_embeds_modality.to(qwen_device)\n",
    "        \n",
    "        text_inputs = self.qwen_tokenizer(prompt_text, return_tensors=\"pt\").to(qwen_device)\n",
    "        inputs_embeds_text = self.qwen_model.get_input_embeddings()(text_inputs.input_ids)\n",
    "\n",
    "        # 3. Concatenate\n",
    "        final_embeds = torch.cat([inputs_embeds_modality, inputs_embeds_text], dim=1)\n",
    "        \n",
    "        # 4. Generate\n",
    "        out = self.qwen_model.generate(\n",
    "            inputs_embeds=final_embeds,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        return self.qwen_tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80292d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 4. Data Loading Logic (Placeholder / Re-used from your nb)\n",
    "# ============================================================\n",
    "\n",
    "# Assuming 'vision_loader' and 'audio_loader' exist from your previous cells.\n",
    "# If running this as a standalone script, you would need to recreate the datasets here.\n",
    "# For the purpose of this \"Eval Only\" block, we will assume variables exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55114b01",
   "metadata": {},
   "source": [
    "### Phase-3: - Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a4c84",
   "metadata": {},
   "source": [
    "#### Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90db6f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LibriSpeech ASR (streaming mode)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b615b87a5f0342aa8b3b783e0fa25ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda40a5ca6b847e58c77eb7e3677382b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee4574f6b464b35bae249f8420fd2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded streaming dataset: IterableDataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_shards: 14\n",
      "})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'librispeech_max_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m audio_stream = librispeech_raw.decode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# We will collect up to cfg.librispeech_max_samples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m max_samples = \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrispeech_max_samples\u001b[49m  \u001b[38;5;66;03m# rename in your config if needed\u001b[39;00m\n\u001b[32m     27\u001b[39m subset = []\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTaking up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples in streaming mode...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Config' object has no attribute 'librispeech_max_samples'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Part 3 – LibriSpeech (Streaming) Audio–Text Dataset\n",
    "# ============================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nLoading LibriSpeech ASR (streaming mode)...\")\n",
    "\n",
    "# Load only train.clean.100 from the giant 124GB dataset\n",
    "librispeech_raw = load_dataset(\n",
    "    \"openslr/librispeech_asr\",\n",
    "    \"all\",\n",
    "    streaming=True,\n",
    "    split=\"train.clean.100\"\n",
    ")\n",
    "\n",
    "print(\"Loaded streaming dataset:\", librispeech_raw)\n",
    "\n",
    "# Disable automatic decoding → we want raw bytes for librosa\n",
    "audio_stream = librispeech_raw.decode(False)\n",
    "\n",
    "# We will collect up to cfg.librispeech_max_samples\n",
    "max_samples = cfg.librispeech_max_samples  # rename in your config if needed\n",
    "subset = []\n",
    "\n",
    "print(f\"\\nTaking up to {max_samples} examples in streaming mode...\")\n",
    "\n",
    "for ex in audio_stream:\n",
    "    subset.append(ex)\n",
    "    if len(subset) >= max_samples:\n",
    "        break\n",
    "\n",
    "print(\"\\nSubset collected:\", len(subset))\n",
    "print(\"Keys:\", subset[0].keys())\n",
    "print(\"Example 0:\", subset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: convert LibriSpeech streaming example → waveform\n",
    "def load_waveform_from_streaming_example(example, target_sr=16000):\n",
    "    audio_info = example[\"audio\"]\n",
    "\n",
    "    audio_bytes = audio_info[\"bytes\"]\n",
    "    if audio_bytes is None:\n",
    "        raise ValueError(\"No audio bytes in example.\")\n",
    "\n",
    "    # Convert raw bytes → file-like object\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "\n",
    "    # librosa loads PCM data and resamples to target_sr\n",
    "    wav, sr = librosa.load(audio_file, sr=target_sr)\n",
    "\n",
    "    return wav, sr\n",
    "\n",
    "\n",
    "# Helper: compute duration in seconds\n",
    "def compute_duration(wav, sr):\n",
    "    return len(wav) / float(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b19416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering by duration ≤ 12.0 seconds...\n",
      "After duration filtering: 726 examples\n"
     ]
    }
   ],
   "source": [
    "# We'll filter to keep only clips <= cfg.max_audio_duration_s\n",
    "filtered = []\n",
    "\n",
    "print(\"\\nFiltering by duration ≤\", cfg.max_audio_duration_s, \"seconds...\")\n",
    "\n",
    "for ex in subset:\n",
    "    wav, sr = load_waveform_from_streaming_example(ex, cfg.audio_sample_rate)\n",
    "    dur = compute_duration(wav, sr)\n",
    "\n",
    "    if dur <= cfg.max_audio_duration_s:\n",
    "        filtered.append({\n",
    "            \"waveform\": wav,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"duration\": dur,\n",
    "            \"text\": ex[\"text\"]\n",
    "        })\n",
    "\n",
    "print(\"After duration filtering:\", len(filtered), \"examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing a few filtered samples...\n",
      "\n",
      "Sample 0:\n",
      "  Duration: 11.12 s\n",
      "  Transcript: ASSUMED ALL AT ONCE AN APPEARANCE OF NOISE AND DISORDER NEVER BELIEVE HOWEVER DISINTERESTED THE LOVE OF A KEPT WOMAN MAY BE THAT IT WILL COST ONE NOTHING\n",
      "  Waveform shape: (178000,)\n",
      "\n",
      "Sample 1:\n",
      "  Duration: 8.3 s\n",
      "  Transcript: WHOSE ONLY DEFECT IS THAT THEY HAVE NOT TWO HUNDRED THOUSAND FRANCS A YEAR I NEED NOT TELL YOU OF THOSE WHO CHEAT AT PLAY\n",
      "  Waveform shape: (132880,)\n",
      "\n",
      "Sample 2:\n",
      "  Duration: 2.96 s\n",
      "  Transcript: IT WAS IMPOSSIBLE TO RESIST AN EXISTENCE\n",
      "  Waveform shape: (47360,)\n",
      "\n",
      "Sample 3:\n",
      "  Duration: 8.61 s\n",
      "  Transcript: WHEN AN ADROIT GAMBLER WOULD HAVE LEFT IT SETTLING ONE THING AGAINST ANOTHER I FOUND MYSELF IN POSSESSION OF SOME TEN THOUSAND FRANCS\n",
      "  Waveform shape: (137680,)\n",
      "\n",
      "Sample 4:\n",
      "  Duration: 9.14 s\n",
      "  Transcript: MARGUERITE WAS AWAKENED BY THE SUNLIGHT POURING INTO HER ROOM AND JUMPING OUT OF BED ASKED ME IF I WOULD TAKE HER INTO THE COUNTRY FOR THE WHOLE DAY\n",
      "  Waveform shape: (146320,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShowing a few filtered samples...\")\n",
    "\n",
    "for i in range(min(5, len(filtered))):\n",
    "    ex = filtered[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(\"  Duration:\", round(ex[\"duration\"], 2), \"s\")\n",
    "    print(\"  Transcript:\", ex[\"text\"])\n",
    "    print(\"  Waveform shape:\", ex[\"waveform\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489498fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PixMo-Cap vision–text dataset (allenai/pixmo-cap)...\n",
      "PixMo-Cap split size: 717042\n",
      "PixMo columns: ['image_url', 'caption', 'transcripts']\n",
      "PixMo subset size: 2048\n",
      "Using image column: image_url\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# New PixmoVisionDataset (uses HF 'image' column if available)\n",
    "# ============================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "print(\"\\nLoading PixMo-Cap vision–text dataset (allenai/pixmo-cap)...\")\n",
    "\n",
    "pixmo_raw = load_dataset(\"allenai/pixmo-cap\", split=\"train\")\n",
    "print(\"PixMo-Cap split size:\", len(pixmo_raw))\n",
    "print(\"PixMo columns:\", pixmo_raw.column_names)\n",
    "\n",
    "# We only need a small subset for the POC\n",
    "vision_max = getattr(cfg, \"vision_max_samples\", 2048)\n",
    "if len(pixmo_raw) > vision_max:\n",
    "    pixmo_subset = pixmo_raw.shuffle(seed=cfg.seed).select(range(vision_max))\n",
    "else:\n",
    "    pixmo_subset = pixmo_raw\n",
    "\n",
    "print(\"PixMo subset size:\", len(pixmo_subset))\n",
    "\n",
    "# Fields from the dataset card:\n",
    "#  - \"image_url\": URL to the image\n",
    "#  - \"caption\": long caption text\n",
    "img_col = \"image_url\"\n",
    "txt_col = \"caption\"\n",
    "\n",
    "cols = pixmo_raw.column_names\n",
    "HAS_IMAGE_COL = \"image\" in cols\n",
    "\n",
    "if HAS_IMAGE_COL:\n",
    "    img_col = \"image\"\n",
    "else:\n",
    "    img_col = \"image_url\"\n",
    "\n",
    "txt_col = \"caption\"\n",
    "\n",
    "print(f\"Using image column: {img_col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6500b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PixmoVisionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    On-the-fly image loading + CLIP feature extraction.\n",
    "\n",
    "    If 'image' column exists: uses HF-managed images (no manual HTTP).\n",
    "    Else: falls back to 'image_url' with robust skipping of bad URLs.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"features\": Tensor(T, d_vision),\n",
    "          \"text\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, vision_model, vision_processor, max_retries: int = 5):\n",
    "        self.ds = hf_dataset\n",
    "        self.vision_model = vision_model\n",
    "        self.vision_processor = vision_processor\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def _load_image_from_url(self, url: str) -> Image.Image:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        # do NOT let this propagate; we'll catch in __getitem__\n",
    "        resp.raise_for_status()\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _encode_image(self, img: Image.Image):\n",
    "        proc = self.vision_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = proc[\"pixel_values\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.vision_model(pixel_values=pixel_values)\n",
    "            # (1, T, d_vision)\n",
    "            feats = out.last_hidden_state.squeeze(0).to(\"cpu\")  # (T, d_vision)\n",
    "        return feats\n",
    "\n",
    "    def _get_example(self, idx: int):\n",
    "        ex = self.ds[idx]\n",
    "        caption = ex[txt_col]\n",
    "\n",
    "        if HAS_IMAGE_COL:\n",
    "            # HF has already downloaded/cached images; this is usually a PIL.Image\n",
    "            img = ex[img_col]\n",
    "            if not isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "        else:\n",
    "            url = ex[img_col]\n",
    "            img = self._load_image_from_url(url)\n",
    "\n",
    "        feats = self._encode_image(img)\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": caption,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Try up to max_retries times with different indices if something fails\n",
    "        (HTTP error, decoding error, etc).\n",
    "        \"\"\"\n",
    "        n = len(self.ds)\n",
    "        attempt = 0\n",
    "        cur_idx = idx\n",
    "\n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                return self._get_example(cur_idx)\n",
    "            except Exception as e:\n",
    "                # print(f\"[PixmoVisionDataset] Failed idx={cur_idx}, attempt={attempt+1}, err={e}\")\n",
    "                attempt += 1\n",
    "                cur_idx = (cur_idx + 1) % n\n",
    "\n",
    "        # Final fallback: try random indices\n",
    "        for _ in range(self.max_retries):\n",
    "            j = random.randint(0, n - 1)\n",
    "            try:\n",
    "                return self._get_example(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        raise RuntimeError(\"PixmoVisionDataset: could not load any valid images after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f27da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf2e697",
   "metadata": {},
   "source": [
    "### Part-4:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73972148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building LibriSpeech audio–text dataset from filtered streaming subset...\n",
      "Filtered LibriSpeech examples: 726\n",
      "Audio dataset ready. Example:\n",
      "  features shape: torch.Size([1500, 512])\n",
      "  duration: 11.12 s\n",
      "  text: ASSUMED ALL AT ONCE AN APPEARANCE OF NOISE AND DISORDER NEVER BELIEVE HOWEVER DISINTERESTED THE LOVE OF A KEPT WOMAN MAY BE THAT IT WILL COST ONE NOTHING\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Part 4 – Audio features dataset (LibriSpeech + Whisper)\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# We assume:\n",
    "#  - `filtered` has been built in Part 3 (streaming LibriSpeech)\n",
    "#  - Each entry: {\"waveform\": np.ndarray, \"sampling_rate\": int, \"duration\": float, \"text\": str}\n",
    "print(\"\\nBuilding LibriSpeech audio–text dataset from filtered streaming subset...\")\n",
    "print(\"Filtered LibriSpeech examples:\", len(filtered))\n",
    "\n",
    "\n",
    "def whisper_encode_sequence(wav: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    wav: 1D numpy array (time,)\n",
    "    sr:  sampling rate (expected 16k)\n",
    "    Returns:\n",
    "        feats: Tensor(T_enc, d_audio) on CPU (float16)\n",
    "    \"\"\"\n",
    "    # WhisperProcessor: raw waveform -> log-Mel spectrogram features\n",
    "    inputs = audio_processor(\n",
    "        wav,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_features = inputs[\"input_features\"].to(device)  # (1, T_mel, 80)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out = audio_model.encoder(input_features)\n",
    "        hidden = enc_out.last_hidden_state  # (1, T_enc, d_audio)\n",
    "\n",
    "    feats = hidden.squeeze(0).to(torch.float16).cpu()  # (T_enc, d_audio)\n",
    "    return feats\n",
    "\n",
    "\n",
    "class LibriSpeechAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset over the in-memory filtered LibriSpeech examples.\n",
    "    Returns:\n",
    "        {\n",
    "          \"features\": Tensor(T_enc, d_audio),\n",
    "          \"text\": str,\n",
    "          \"duration\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, examples, max_len: int | None = None):\n",
    "        self.examples = examples\n",
    "        if max_len is not None and max_len < len(examples):\n",
    "            # Optionally cut down further for faster experiments\n",
    "            self.examples = examples[:max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ex = self.examples[idx]\n",
    "        wav = ex[\"waveform\"]\n",
    "        sr = ex[\"sampling_rate\"]\n",
    "        text = ex[\"text\"]\n",
    "        dur = ex[\"duration\"]\n",
    "\n",
    "        feats = whisper_encode_sequence(wav, sr)  # (T_enc, d_audio)\n",
    "\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": text,\n",
    "            \"duration\": dur,\n",
    "        }\n",
    "\n",
    "\n",
    "audio_max = getattr(cfg, \"librispeech_max_samples\", len(filtered))\n",
    "audio_dataset = LibriSpeechAudioDataset(filtered, max_len=audio_max)\n",
    "\n",
    "print(\"Audio dataset ready. Example:\")\n",
    "sample_a = audio_dataset[0]\n",
    "print(\"  features shape:\", sample_a[\"features\"].shape)\n",
    "print(\"  duration:\", round(sample_a[\"duration\"], 2), \"s\")\n",
    "print(\"  text:\", sample_a[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd60328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d55831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Load Model Function (THE FIX IS HERE)\n",
    "# ============================================================\n",
    "\n",
    "def load_full_model(cfg_obj):\n",
    "    print(\"Loading Frozen Qwen...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg_obj.llm_model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # Load Qwen with device_map=\"auto\"\n",
    "    qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg_obj.llm_model_name, \n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    print(\"Initializing Adapters...\")\n",
    "    v_adapt = ModalityAdapter(cfg_obj.encoder_dim_vision, cfg_obj.perceiver_dim)\n",
    "    a_adapt = ModalityAdapter(cfg_obj.encoder_dim_audio, cfg_obj.perceiver_dim)\n",
    "    \n",
    "    perc = PerceiverResampler(\n",
    "        dim=cfg_obj.perceiver_dim, \n",
    "        num_latents=cfg_obj.num_latents,\n",
    "        num_layers=cfg_obj.num_perceiver_layers,\n",
    "        num_heads=cfg_obj.num_attn_heads,\n",
    "        mlp_ratio=cfg_obj.mlp_ratio\n",
    "    )\n",
    "    \n",
    "    proj = nn.Linear(cfg_obj.perceiver_dim, cfg_obj.llm_hidden_size)\n",
    "\n",
    "    # Load Weights\n",
    "    print(f\"Loading weights from {cfg_obj.ckpt_path}...\")\n",
    "    try:\n",
    "        ckpt = torch.load(cfg_obj.ckpt_path, map_location=\"cpu\")\n",
    "        v_adapt.load_state_dict(ckpt[\"vision_adapter\"])\n",
    "        a_adapt.load_state_dict(ckpt[\"audio_adapter\"])\n",
    "        perc.load_state_dict(ckpt[\"perceiver\"])\n",
    "        proj.load_state_dict(ckpt[\"projector\"])\n",
    "        print(\"✅ Weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Checkpoint not found! Using random init for testing.\")\n",
    "\n",
    "    # MOVE ADAPTERS TO DEVICE (Manual)\n",
    "    # We assume standard single-GPU inference for the adapters for simplicity\n",
    "    # Qwen handles its own split via device_map=\"auto\"\n",
    "    adapter_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    v_adapt.to(adapter_device).eval()\n",
    "    a_adapt.to(adapter_device).eval()\n",
    "    perc.to(adapter_device).eval()\n",
    "    proj.to(adapter_device).eval()\n",
    "\n",
    "    # Create Wrapper (Do NOT call .to() on this wrapper!)\n",
    "    model = AlignedModel(v_adapt, a_adapt, perc, proj, qwen, tokenizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6. Evaluation Functions\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_retrieval(model, dataloader, modality=\"vision\", num_batches=10):\n",
    "    model.eval()\n",
    "    all_mod_embs = []\n",
    "    all_txt_embs = []\n",
    "    \n",
    "    print(f\"\\nStarting {modality.upper()} retrieval eval...\")\n",
    "    adapter_device = next(model.vision_adapter.parameters()).device\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader, total=num_batches)):\n",
    "        if i >= num_batches: break\n",
    "        \n",
    "        feats = batch[\"encoder_feats\"].to(adapter_device)\n",
    "        mask  = batch[\"encoder_mask\"].to(adapter_device)\n",
    "        texts = batch[\"texts\"]\n",
    "        \n",
    "        if modality == \"vision\":\n",
    "            z_mod = model.encode_image_features(feats, mask)\n",
    "        else:\n",
    "            z_mod = model.encode_audio_features(feats, mask)\n",
    "            \n",
    "        z_txt = model.encode_text_raw(texts)\n",
    "        \n",
    "        all_mod_embs.append(z_mod.cpu().float()) # Ensure float32 for matmul\n",
    "        all_txt_embs.append(z_txt.cpu().float())\n",
    "\n",
    "    z_mod_all = torch.cat(all_mod_embs, dim=0)\n",
    "    z_txt_all = torch.cat(all_txt_embs, dim=0)\n",
    "    \n",
    "    z_mod_all = F.normalize(z_mod_all, dim=-1)\n",
    "    z_txt_all = F.normalize(z_txt_all, dim=-1)\n",
    "    \n",
    "    sim_matrix = z_mod_all @ z_txt_all.T\n",
    "    n = sim_matrix.shape[0]\n",
    "    targets = torch.arange(n)\n",
    "    \n",
    "    preds = sim_matrix.argsort(dim=1, descending=True)\n",
    "    r1 = (preds[:, 0] == targets).float().mean().item()\n",
    "    r5 = (preds[:, :5] == targets.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "    \n",
    "    print(f\"[{modality.upper()}] R@1: {r1:.4f} | R@5: {r5:.4f}\")\n",
    "\n",
    "def visualize_inference(model, dataset, raw_dataset, idx=None):\n",
    "    if idx is None: idx = random.randint(0, len(dataset) - 1)\n",
    "    print(f\"\\n--- Qualitative Inference Sample {idx} ---\")\n",
    "    \n",
    "    # 1. Get Image\n",
    "    raw_sample = raw_dataset[idx]\n",
    "    img = None\n",
    "    if \"image\" in raw_sample and raw_sample[\"image\"]:\n",
    "        img = raw_sample[\"image\"]\n",
    "    elif \"image_url\" in raw_sample:\n",
    "        try:\n",
    "            resp = requests.get(raw_sample[\"image_url\"], timeout=2)\n",
    "            img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        except: pass\n",
    "            \n",
    "    # 2. Get Features\n",
    "    model_input = dataset[idx]\n",
    "    feats = model_input[\"features\"].unsqueeze(0) # (1, T, D)\n",
    "    \n",
    "    # 3. Generate\n",
    "    prompt = \"User: Describe this image.\\nAssistant:\"\n",
    "    gen_text = model.generate(feats, \"vision\", prompt)\n",
    "    \n",
    "    # 4. Show\n",
    "    if img:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Pred: {gen_text[:50]}...\", fontsize=10)\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"GT:   {model_input['text']}\")\n",
    "    print(f\"Pred: {gen_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7. Main Execution\n",
    "# ============================================================\n",
    "\n",
    "# 1. Load the system\n",
    "eval_model = load_full_model(cfg)\n",
    "\n",
    "# 2. Run Qualitative Check (Vision)\n",
    "# Requires 'vision_dataset' and 'pixmo_subset' from previous cells\n",
    "if 'vision_dataset' in globals():\n",
    "    visualize_inference(eval_model, vision_dataset, pixmo_subset)\n",
    "\n",
    "# 3. Run Metrics\n",
    "# Requires 'vision_loader' and 'audio_loader'\n",
    "if 'vision_loader' in globals():\n",
    "    evaluate_retrieval(eval_model, vision_loader, modality=\"vision\", num_batches=20)\n",
    "if 'audio_loader' in globals():\n",
    "    evaluate_retrieval(eval_model, audio_loader, modality=\"audio\", num_batches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430ba30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
