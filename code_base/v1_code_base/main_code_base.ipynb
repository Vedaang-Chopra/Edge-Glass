{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"./data\")\n",
    "PIXMO_SAVE_DIR = DATA_ROOT / \"pixmo_cap_clean\"\n",
    "LIBRI_SAVE_DIR = DATA_ROOT / \"librispeech_clean\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixmo_ds = DatasetDict.load_from_disk(str(PIXMO_SAVE_DIR))\n",
    "pixmo_train = pixmo_ds[\"train\"]\n",
    "pixmo_val   = pixmo_ds[\"validation\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ca4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_ds = load_from_disk(str(LIBRI_SAVE_DIR))\n",
    "\n",
    "print(len(pixmo_train), len(pixmo_val), len(librispeech_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743f80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.encoders import VisionEncoder, AudioEncoder\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vision_enc = VisionEncoder(\n",
    "    model_name=\"facebook/dinov2-base\",\n",
    "    device=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "audio_enc = AudioEncoder(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    device=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: encode a batch of PIL images\n",
    "from PIL import Image\n",
    "\n",
    "imgs = [Image.open(\"/path/to/img1.jpg\"), Image.open(\"/path/to/img2.jpg\")]\n",
    "vision_out = vision_enc.encode_images(imgs)\n",
    "vision_feats, vision_mask = vision_out[\"feats\"], vision_out[\"mask\"]\n",
    "\n",
    "# Example: encode a batch of audio waveforms (Tensor)\n",
    "wav_batch = torch.randn(2, 16000 * 5)  # (B, T) fake data at 16kHz\n",
    "audio_out = audio_enc.encode_waveforms(wav_batch, sample_rates=16000)\n",
    "audio_feats, audio_mask = audio_out[\"feats\"], audio_out[\"mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Perciever import PerceiverLatentEncoder, ProjectorMLP\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Run once to set feat_dim\n",
    "encoder_out = vision_enc.encode_images(list_of_PIL_images)\n",
    "feats, mask = encoder_out[\"feats\"], encoder_out[\"mask\"]   # (B, T, D_in), (B, T)\n",
    "\n",
    "perceiver = PerceiverLatentEncoder(\n",
    "    num_latents=64,\n",
    "    d_latent=512,\n",
    "    d_input=vision_enc.feat_dim,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "proj_align = ProjectorMLP(\n",
    "    d_in=512,\n",
    "    d_out=1024,      # your alignment / MRL dim\n",
    "    hidden_factor=2.0,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "latents = perceiver(feats, encoder_mask=mask)     # (B, L, 512)\n",
    "z = proj_align(latents)                           # (B, L, 1024)\n",
    "z_pooled = z.mean(dim=1)                          # (B, 1024) if you want pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360170e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05170d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867943d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf96600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d9cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f381e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8227efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f39f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12526137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4de35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2c739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dd8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc57206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ae316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ae36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33a485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
