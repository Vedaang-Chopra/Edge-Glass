{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79cbcb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Project modules (adjust import paths if needed in your repo)\n",
    "from imports.configs.config import load_config, set_global_seed\n",
    "from imports.encoders import VisionEncoder, AudioEncoder\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "from imports.align_training.text_encoder import HFTextEncoder, HFTextEncoderConfig\n",
    "from imports.align_training.losses import matryoshka_contrastive_loss, clip_contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60390caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"imports/configs/config.yaml\")\n",
    "cfg.resolve_device_and_dtype()\n",
    "\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Dtype:\", dtype)\n",
    "\n",
    "set_global_seed(cfg.misc.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e852b24",
   "metadata": {},
   "source": [
    "ðŸ–¼ï¸ 1. Hugging Face Datasets (PixMo-Cap & LibriSpeech)\n",
    "ðŸ§± Cell 1.1 â€“ Helper: load image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55071730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import TooManyRedirects, RequestException\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def load_image_from_url(url: str, timeout: float = 10.0) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Robust image loader:\n",
    "      - Tries to fetch the URL.\n",
    "      - On TooManyRedirects / HTTP error, returns a dummy image instead\n",
    "        of crashing the DataLoader.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    except TooManyRedirects:\n",
    "        print(f\"[WARN] Too many redirects for URL, using dummy image:\\n  {url}\")\n",
    "    except RequestException as e:\n",
    "        print(f\"[WARN] Request failed for URL ({e}), using dummy image:\\n  {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Unexpected error loading URL ({e}), using dummy image:\\n  {url}\")\n",
    "\n",
    "    # Fallback: 224x224 gray image (CLIP-friendly size)\n",
    "    return Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c28c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image_url', 'caption', 'transcripts'],\n",
      "    num_rows: 717042\n",
      "})\n",
      "Sample: {'image_url': 'https://pixmo.s3.us-west-2.amazonaws.com/birds/1491.png', 'caption': \"This photograph depicts a striking black bird, possibly a grackle or similar species, perched on a white cement wall with red stains. The bird's sleek, elongated body is adorned with iridescent feathers that shimmer with shades of blue, purple, and green, most prominently on its wings and back. Its piercing yellow eye and long, sharp beak lend it a fierce, almost aerodynamic appearance. The bird's dark, slender legs and short talons grip the edge of the cement structure, which resembles a divider or barrier, possibly in an outdoor setting like a park or building patio. The background is a blur of green and white hues, hinting at lush tropical plants and tall trees, setting a serene, natural scene. The bird is poised, looking towards the upper right-hand corner of the frame, with its long tail feathers trailing elegantly to the left.\"}\n"
     ]
    }
   ],
   "source": [
    "# allenai/pixmo-cap\n",
    "pixmo_raw = load_dataset(\"allenai/pixmo-cap\", split=\"train\")\n",
    "print(pixmo_raw)\n",
    "print(\"Sample:\", {k: pixmo_raw[0][k] for k in [\"image_url\", \"caption\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f377038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixmoHFAlignmentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps allenai/pixmo-cap into:\n",
    "        {\n",
    "            \"image\": PIL.Image,\n",
    "            \"text\": str,\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_ds, max_samples: int | None = None):\n",
    "        if max_samples is not None:\n",
    "            max_samples = min(max_samples, len(hf_ds))\n",
    "            hf_ds = hf_ds.select(range(max_samples))\n",
    "        self.hf_ds = hf_ds\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf_ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.hf_ds[idx]\n",
    "        img = load_image_from_url(row[\"image_url\"])\n",
    "        caption = row[\"caption\"]\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"text\": caption,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16a81580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LibriSpeech ASR in streaming mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5fa4ea2e364374aea82cc4cab61b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d044f5f2bd42e9869bee48c0f7a17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92aee401d9240ba890fcc4fde4a460f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Any, List\n",
    "import io\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nLoading LibriSpeech ASR in streaming mode...\")\n",
    "\n",
    "librispeech_raw = load_dataset(\n",
    "    \"librispeech_asr\",\n",
    "    \"all\",\n",
    "    streaming=True,\n",
    "    split=\"train.other.500\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LibriSpeech ASR in streaming mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad3a737734b470f9572f749f5cd092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f9d0b959d54bfcad3be98f60e4dcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10977f1d1df244f59bd6c058e8c24752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded streaming dataset: IterableDataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_shards: 64\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loaded streaming dataset:\", librispeech_raw)\n",
    "\n",
    "\n",
    "class LibriSpeechHFAlignmentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a list of preprocessed LibriSpeech examples into:\n",
    "        {\n",
    "            \"waveform\": 1D float32 numpy array,\n",
    "            \"sampling_rate\": int,\n",
    "            \"text\": str,\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples: List[Dict[str, Any]]):\n",
    "        # 'samples' is a plain Python list of dicts\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return self.samples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc41cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeechHFAlignmentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps openslr/librispeech_asr into:\n",
    "        {\n",
    "            \"waveform\": 1D float32 numpy array,\n",
    "            \"sampling_rate\": int,\n",
    "            \"text\": str,\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_ds, max_samples: int | None = None):\n",
    "        if max_samples is not None:\n",
    "            max_samples = min(max_samples, len(hf_ds))\n",
    "            hf_ds = hf_ds.select(range(max_samples))\n",
    "        self.hf_ds = hf_ds\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf_ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.hf_ds[idx]\n",
    "        audio = row[\"audio\"]  # dict: {\"array\": np.ndarray, \"sampling_rate\": int}\n",
    "        wav = audio[\"array\"].astype(\"float32\")\n",
    "        sr = int(audio[\"sampling_rate\"])\n",
    "        text = row[\"text\"]\n",
    "        return {\n",
    "            \"waveform\": wav,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"text\": text,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "281aeb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking up to 5000 examples in streaming mode...\n",
      "\n",
      "Subset collected: 5000\n",
      "Keys: dict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\n",
      "Example 0 (truncated): {'file': '/home/albert/.cache/huggingface/datasets/downloads/extracted/52b35db43bb19b11fc0e8d082c86dd8d065bab1bf6605e6bb5b44b233b44cb51/8296-266250-0000.flac', 'text': 'THE ORDINARY DUTIES OF LIFE MISTER DALY ANXIOUS TO MAKE SOME RETURN FOR THE KINDNESS SHOWN HIM OFFERED TO ACT AS TUTOR TO ALL THE CHILDREN WHO WERE OLD ENOUGH FOR SCHOOL DUTIES', 'speaker_id': 8296, 'chapter_id': 266250, 'id': '8296-266250-0000'}\n",
      "Filtered valid Libri examples: 4999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Disable automatic decoding â†’ we want raw bytes\n",
    "audio_stream = librispeech_raw.decode(False)\n",
    "\n",
    "max_samples = 5000\n",
    "subset = []\n",
    "\n",
    "print(f\"\\nTaking up to {max_samples} examples in streaming mode...\")\n",
    "\n",
    "for ex in audio_stream:\n",
    "    subset.append(ex)\n",
    "    if len(subset) >= max_samples:\n",
    "        break\n",
    "\n",
    "print(\"\\nSubset collected:\", len(subset))\n",
    "if len(subset) > 0:\n",
    "    print(\"Keys:\", subset[0].keys())\n",
    "    print(\"Example 0 (truncated):\", {k: subset[0][k] for k in subset[0].keys() if k != \"audio\"})\n",
    "\n",
    "# Helper ----------------------------------------------------------\n",
    "\n",
    "def load_waveform_from_streaming_example(example, target_sr=16_000):\n",
    "    \"\"\"\n",
    "    Convert LibriSpeech streaming example -> mono waveform @ target_sr.\n",
    "    Expects example[\"audio\"][\"bytes\"] to be present.\n",
    "    \"\"\"\n",
    "    audio_info = example[\"audio\"]\n",
    "    audio_bytes = audio_info[\"bytes\"]\n",
    "    if audio_bytes is None:\n",
    "        raise ValueError(\"No audio bytes in example.\")\n",
    "\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "    wav, sr = librosa.load(audio_file, sr=target_sr, mono=True)\n",
    "    return wav, sr\n",
    "\n",
    "def compute_duration(wav, sr):\n",
    "    return len(wav) / float(sr)\n",
    "\n",
    "\n",
    "# Preprocess into alignment-friendly samples ----------------------\n",
    "\n",
    "filtered = []\n",
    "\n",
    "target_sr = 16_000  # assuming cfg is defined earlier\n",
    "\n",
    "for ex in subset:\n",
    "    try:\n",
    "        wav, sr = load_waveform_from_streaming_example(ex, target_sr=target_sr)\n",
    "    except Exception as e:\n",
    "        # Optionally log and skip problematic examples\n",
    "        # log_path = cfg.root_dir / \"decode_skipped.log\"\n",
    "        # with open(log_path, \"a\") as f:\n",
    "        #     f.write(f\"decode_error\\ttext={ex.get('text', '')[:80]!r}\\terror={repr(e)}\\n\")\n",
    "        continue\n",
    "\n",
    "    # (Optional) filter by duration, e.g., 1sâ€“20s\n",
    "    dur = compute_duration(wav, sr)\n",
    "    if not (1.0 <= dur <= 20.0):\n",
    "        continue\n",
    "\n",
    "    filtered.append(\n",
    "        {\n",
    "            \"waveform\": wav.astype(\"float32\"),\n",
    "            \"sampling_rate\": int(sr),\n",
    "            \"text\": ex[\"text\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Filtered valid Libri examples:\", len(filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a554915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixMo samples: 5000\n",
      "LibriSpeech samples: 4999\n"
     ]
    }
   ],
   "source": [
    "# Limits from cfg -------------------------------------------------\n",
    "\n",
    "MAX_PIXMO_SAMPLES = getattr(cfg.datasets, \"pixmo_max_samples\", 5000)\n",
    "MAX_LIBRI_SAMPLES = getattr(cfg.datasets, \"librispeech_max_samples\", 5000)\n",
    "\n",
    "# Pixmo still uses the HF non-streaming dataset\n",
    "pixmo_ds = PixmoHFAlignmentDataset(pixmo_raw, max_samples=MAX_PIXMO_SAMPLES)\n",
    "\n",
    "# For Libri, we now pass the filtered list and slice it in Python (no .select)\n",
    "librispeech_ds = LibriSpeechHFAlignmentDataset(filtered[:MAX_LIBRI_SAMPLES])\n",
    "\n",
    "print(\"PixMo samples:\", len(pixmo_ds))\n",
    "print(\"LibriSpeech samples:\", len(librispeech_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f18f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_vision(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    texts  = [b[\"text\"] for b in batch]\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"texts\":  texts,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_audio(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    waveforms      = [b[\"waveform\"] for b in batch]\n",
    "    sampling_rates = [b[\"sampling_rate\"] for b in batch]\n",
    "    texts          = [b[\"text\"] for b in batch]\n",
    "    return {\n",
    "        \"waveforms\":      waveforms,\n",
    "        \"sampling_rates\": sampling_rates,\n",
    "        \"texts\":          texts,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6381f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 313)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = cfg.training.batch_size\n",
    "\n",
    "pixmo_loader = DataLoader(\n",
    "    pixmo_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_vision,\n",
    ")\n",
    "\n",
    "librispeech_loader = DataLoader(\n",
    "    librispeech_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_audio,\n",
    ")\n",
    "\n",
    "len(pixmo_loader), len(librispeech_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8627980",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0213c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision encoder: openai/clip-vit-base-patch32\n",
      "Audio encoder : openai/whisper-base\n"
     ]
    }
   ],
   "source": [
    "vision_enc = VisionEncoder(\n",
    "    model_name=cfg.models.vision_model_name,  # \"openai/clip-vit-base-patch32\"\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "audio_enc = AudioEncoder(\n",
    "    model_name=cfg.models.audio_model_name,   # \"openai/whisper-base\"\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    target_sampling_rate=16_000,\n",
    ")\n",
    "\n",
    "for p in vision_enc.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in audio_enc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "vision_enc.eval()\n",
    "audio_enc.eval()\n",
    "\n",
    "print(\"Vision encoder:\", cfg.models.vision_model_name)\n",
    "print(\"Audio encoder :\", cfg.models.audio_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4a27dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Text hidden size: 384\n"
     ]
    }
   ],
   "source": [
    "TEXT_ENCODER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # or add to config\n",
    "\n",
    "text_cfg = HFTextEncoderConfig(\n",
    "    model_name=TEXT_ENCODER_NAME,\n",
    "    max_length=128,\n",
    "    trainable=False,\n",
    ")\n",
    "text_enc = HFTextEncoder(text_cfg, device=device)\n",
    "d_text = text_enc.hidden_size\n",
    "print(\"Text encoder:\", TEXT_ENCODER_NAME)\n",
    "print(\"Text hidden size:\", d_text)\n",
    "\n",
    "def text_embed_fn(texts: List[str], max_length: int = 128) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        return text_enc.encode(texts)  # (B, d_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb683d",
   "metadata": {},
   "source": [
    "### Alignment Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d32b9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (B, T, D), mask: (B, T) bool\n",
    "    returns: (B, D)\n",
    "    \"\"\"\n",
    "    mask = mask.to(x.dtype)\n",
    "    denom = mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=1) / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754ac4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc4bf4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/ZWBu0oI.jpg), using dummy image:\n",
      "  https://i.imgur.com/ZWBu0oI.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://www.libertyflagpoles.com/cdn/shop/articles/MinutementFlagpolesInc.DBALIbertyFlagpoles-237633-Flagpole-Inclement-Weather-Blogbanner1_600x600_crop_center.jpg?v=1691778269), using dummy image:\n",
      "  https://www.libertyflagpoles.com/cdn/shop/articles/MinutementFlagpolesInc.DBALIbertyFlagpoles-237633-Flagpole-Inclement-Weather-Blogbanner1_600x600_crop_center.jpg?v=1691778269\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/Kedmm.jpg), using dummy image:\n",
      "  http://i.imgur.com/Kedmm.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/XTYFe.jpg), using dummy image:\n",
      "  http://i.imgur.com/XTYFe.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/nWUmS5h.jpg), using dummy image:\n",
      "  https://i.imgur.com/nWUmS5h.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/wDlfHRt.jpg), using dummy image:\n",
      "  http://i.imgur.com/wDlfHRt.jpg\n",
      "[WARN] Request failed for URL (403 Client Error: Forbidden for url: https://www.mdpi.com/micromachines/micromachines-12-00287/article_deploy/html/images/micromachines-12-00287-g001-550.jpg), using dummy image:\n",
      "  https://www.mdpi.com/micromachines/micromachines-12-00287/article_deploy/html/images/micromachines-12-00287-g001-550.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/sS7h240.jpg), using dummy image:\n",
      "  http://i.imgur.com/sS7h240.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://imgur.com/dl2PWlM.png), using dummy image:\n",
      "  https://imgur.com/dl2PWlM.png\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://i.redd.it/e7pn4zouvhs91.jpg), using dummy image:\n",
      "  https://i.redd.it/e7pn4zouvhs91.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/cNvzPEW.jpg), using dummy image:\n",
      "  https://i.imgur.com/cNvzPEW.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/7aWaZ3u.jpg), using dummy image:\n",
      "  https://i.imgur.com/7aWaZ3u.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://ak2.rmbl.ws/s8/1/N/L/c/i/NLcim.qR4e-small-Baldurs-Gate-3-How-To-Save-.jpg), using dummy image:\n",
      "  https://ak2.rmbl.ws/s8/1/N/L/c/i/NLcim.qR4e-small-Baldurs-Gate-3-How-To-Save-.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/Qs5LFB4.jpg), using dummy image:\n",
      "  http://i.imgur.com/Qs5LFB4.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: http://www.classicvacationsdesign.com/layout/support-bx/request/flight-info-entered.png), using dummy image:\n",
      "  http://www.classicvacationsdesign.com/layout/support-bx/request/flight-info-entered.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify input_ids",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sample_vision_batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(pixmo_loader))\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     v_out = \u001b[43mvision_enc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_vision_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     d_vision = vision_enc.feat_dim\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVision feat dim:\u001b[39m\u001b[33m\"\u001b[39m, d_vision)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/imports/encoders.py:222\u001b[39m, in \u001b[36mVisionEncoder.encode_images\u001b[39m\u001b[34m(self, images, return_mask)\u001b[39m\n\u001b[32m    219\u001b[39m             \u001b[38;5;28mself\u001b[39m.cfg.dtype = torch.float32\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Forward through the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m hidden_states = outputs.hidden_states  \u001b[38;5;66;03m# tuple of (B, T, D)\u001b[39;00m\n\u001b[32m    225\u001b[39m h_idx1, h_idx2 = \u001b[38;5;28mself\u001b[39m.cfg.use_hidden_layers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:989\u001b[39m, in \u001b[36mCLIPModel.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    978\u001b[39m output_hidden_states = (\n\u001b[32m    979\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28mself\u001b[39m.vision_model(\n\u001b[32m    983\u001b[39m     pixel_values=pixel_values,\n\u001b[32m    984\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    985\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    986\u001b[39m     interpolate_pos_encoding=interpolate_pos_encoding,\n\u001b[32m    987\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m text_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m image_embeds = vision_outputs.pooler_output\n\u001b[32m    998\u001b[39m image_embeds = \u001b[38;5;28mself\u001b[39m.visual_projection(image_embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:598\u001b[39m, in \u001b[36mCLIPTextTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    593\u001b[39m output_hidden_states = (\n\u001b[32m    594\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    595\u001b[39m )\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify input_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    600\u001b[39m input_shape = input_ids.size()\n\u001b[32m    601\u001b[39m input_ids = input_ids.view(-\u001b[32m1\u001b[39m, input_shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: You have to specify input_ids"
     ]
    }
   ],
   "source": [
    "# Probe CLIP feature dim\n",
    "sample_vision_batch = next(iter(pixmo_loader))\n",
    "with torch.no_grad():\n",
    "    v_out = vision_enc.encode_images(sample_vision_batch[\"images\"])\n",
    "    d_vision = vision_enc.feat_dim\n",
    "print(\"Vision feat dim:\", d_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe Whisper feature dim\n",
    "sample_audio_batch = next(iter(librispeech_loader))\n",
    "with torch.no_grad():\n",
    "    a_out = audio_enc.encode_waveforms(\n",
    "        sample_audio_batch[\"waveforms\"],\n",
    "        sample_audio_batch[\"sampling_rates\"],\n",
    "    )\n",
    "    d_audio = audio_enc.feat_dim\n",
    "print(\"Audio feat dim:\", d_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffb87835",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_vision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Shared Perceiver dim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m d_shared = cfg.architecture.perceiver_dim \u001b[38;5;129;01mor\u001b[39;00m \u001b[43md_vision\u001b[49m\n\u001b[32m      3\u001b[39m d_align = d_text\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShared (Perceiver) dim:\u001b[39m\u001b[33m\"\u001b[39m, d_shared)\n",
      "\u001b[31mNameError\u001b[39m: name 'd_vision' is not defined"
     ]
    }
   ],
   "source": [
    "# Shared Perceiver dim\n",
    "d_shared = cfg.architecture.perceiver_dim or d_vision\n",
    "d_align = d_text\n",
    "\n",
    "print(\"Shared (Perceiver) dim:\", d_shared)\n",
    "print(\"Aligned dim:\", d_align)\n",
    "\n",
    "# Adapters\n",
    "vision_adapter = nn.Linear(d_vision, d_shared)\n",
    "audio_adapter  = nn.Linear(d_audio,  d_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadd232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/DpNz9o3.jpg), using dummy image:\n",
      "  https://i.imgur.com/DpNz9o3.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/zm4sBfg.jpg), using dummy image:\n",
      "  https://i.imgur.com/zm4sBfg.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/94MhWU6.jpg), using dummy image:\n",
      "  http://i.imgur.com/94MhWU6.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/00A6s2u.jpg), using dummy image:\n",
      "  https://i.imgur.com/00A6s2u.jpg\n",
      "[WARN] Request failed for URL (403 Client Error: Forbidden for url: https://ih1.redbubble.net/image.662143812.9372/mwo,x1000,ipad_2_skin-pad,750x1000,f8f8f8.u10.jpg), using dummy image:\n",
      "  https://ih1.redbubble.net/image.662143812.9372/mwo,x1000,ipad_2_skin-pad,750x1000,f8f8f8.u10.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/uxQUC73.jpg), using dummy image:\n",
      "  http://i.imgur.com/uxQUC73.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://m.media-amazon.com/images/M/MV5BNzhkYzhiMjItN2JmMS00YWZiLTkyOTYtYTkyYzI1OGUzYTA4XkEyXkFqcGdeQXVyODk4OTc3MTY@._V1_.jpg), using dummy image:\n",
      "  https://m.media-amazon.com/images/M/MV5BNzhkYzhiMjItN2JmMS00YWZiLTkyOTYtYTkyYzI1OGUzYTA4XkEyXkFqcGdeQXVyODk4OTc3MTY@._V1_.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://i.redd.it/q962ceu0dbp51.jpg), using dummy image:\n",
      "  https://i.redd.it/q962ceu0dbp51.jpg\n",
      "[WARN] Request failed for URL (403 Client Error: Forbidden for url: https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2013/10/resource-7-730x555.jpg), using dummy image:\n",
      "  https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2013/10/resource-7-730x555.jpg\n",
      "[WARN] Request failed for URL (403 Client Error: Forbidden for url: https://d26eb5y2jukpbz.cloudfront.net/ebs/archive/2018/large/OS_PR18034B_10.jpg), using dummy image:\n",
      "  https://d26eb5y2jukpbz.cloudfront.net/ebs/archive/2018/large/OS_PR18034B_10.jpg\n",
      "[WARN] Request failed for URL (HTTPSConnectionPool(host='support-cdn.infrasightlabs.com', port=443): Max retries exceeded with url: /wp-content/uploads/2023/05/Microsoft-SQL-Database-System.png (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7ffd93e8bbc0>: Failed to resolve 'support-cdn.infrasightlabs.com' ([Errno -2] Name or service not known)\"))), using dummy image:\n",
      "  https://support-cdn.infrasightlabs.com/wp-content/uploads/2023/05/Microsoft-SQL-Database-System.png\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: https://i.imgur.com/bQWf0a9.jpg), using dummy image:\n",
      "  https://i.imgur.com/bQWf0a9.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://m.media-amazon.com/images/M/MV5BYTZkMDNmYmYtOTc5OC00NjVkLWFjYmEtZjQwNWIxZTA5MTQ3XkEyXkFqcGdeQXVyMjUyNDk2ODc@._V1_UY1200_CR83,0,630,1200_AL_.jpg), using dummy image:\n",
      "  https://m.media-amazon.com/images/M/MV5BYTZkMDNmYmYtOTc5OC00NjVkLWFjYmEtZjQwNWIxZTA5MTQ3XkEyXkFqcGdeQXVyMjUyNDk2ODc@._V1_UY1200_CR83,0,630,1200_AL_.jpg\n",
      "[WARN] Request failed for URL (HTTPSConnectionPool(host='ogden_images.s3.amazonaws.com', port=443): Max retries exceeded with url: /www.newsandsentinel.com/images/2023/03/21001425/Image-Blood-2-630x840.jpg (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'ogden_images.s3.amazonaws.com'. (_ssl.c:1000)\")))), using dummy image:\n",
      "  https://ogden_images.s3.amazonaws.com/www.newsandsentinel.com/images/2023/03/21001425/Image-Blood-2-630x840.jpg\n",
      "[WARN] Request failed for URL (404 Client Error: Not Found for url: https://n6cloud.com/blog/wp-content/uploads/2020/12/hostinger-webhosting.jpg), using dummy image:\n",
      "  https://n6cloud.com/blog/wp-content/uploads/2020/12/hostinger-webhosting.jpg\n",
      "[WARN] Request failed for URL (429 Client Error: Unknown Error for url: http://i.imgur.com/A8QMgT9.jpg), using dummy image:\n",
      "  http://i.imgur.com/A8QMgT9.jpg\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify input_ids",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sample_vision_batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(pixmo_loader))\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     v_out = \u001b[43mvision_enc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_vision_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     d_vision = vision_enc.feat_dim\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVision feat dim:\u001b[39m\u001b[33m\"\u001b[39m, d_vision)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/imports/encoders.py:222\u001b[39m, in \u001b[36mVisionEncoder.encode_images\u001b[39m\u001b[34m(self, images, return_mask)\u001b[39m\n\u001b[32m    219\u001b[39m             \u001b[38;5;28mself\u001b[39m.cfg.dtype = torch.float32\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Forward through the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m hidden_states = outputs.hidden_states  \u001b[38;5;66;03m# tuple of (B, T, D)\u001b[39;00m\n\u001b[32m    225\u001b[39m h_idx1, h_idx2 = \u001b[38;5;28mself\u001b[39m.cfg.use_hidden_layers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:989\u001b[39m, in \u001b[36mCLIPModel.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    978\u001b[39m output_hidden_states = (\n\u001b[32m    979\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28mself\u001b[39m.vision_model(\n\u001b[32m    983\u001b[39m     pixel_values=pixel_values,\n\u001b[32m    984\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    985\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    986\u001b[39m     interpolate_pos_encoding=interpolate_pos_encoding,\n\u001b[32m    987\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m text_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m image_embeds = vision_outputs.pooler_output\n\u001b[32m    998\u001b[39m image_embeds = \u001b[38;5;28mself\u001b[39m.visual_projection(image_embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:598\u001b[39m, in \u001b[36mCLIPTextTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    593\u001b[39m output_hidden_states = (\n\u001b[32m    594\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    595\u001b[39m )\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify input_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    600\u001b[39m input_shape = input_ids.size()\n\u001b[32m    601\u001b[39m input_ids = input_ids.view(-\u001b[32m1\u001b[39m, input_shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: You have to specify input_ids"
     ]
    }
   ],
   "source": [
    "# Perceiver\n",
    "perceiver = PerceiverLatentEncoder(\n",
    "    d_input=d_shared,\n",
    "    d_latent=d_shared,\n",
    "    num_latents=cfg.architecture.num_latents,\n",
    "    num_layers=cfg.architecture.num_perceiver_layers,\n",
    "    num_heads=cfg.architecture.num_attn_heads,\n",
    "    mlp_ratio=cfg.architecture.mlp_ratio,\n",
    ")\n",
    "\n",
    "# Projector\n",
    "projector = ProjectorMLP(\n",
    "    d_in=d_shared,\n",
    "    d_hidden=int(cfg.architecture.mlp_ratio * d_shared),\n",
    "    d_out=d_align,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "modules = nn.ModuleDict(\n",
    "    dict(\n",
    "        vision_adapter=vision_adapter,\n",
    "        audio_adapter=audio_adapter,\n",
    "        perceiver=perceiver,\n",
    "        projector=projector,\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "sum(p.numel() for p in modules.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e9cab",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ 5. Forward passes & Losses (MRL + CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_modality_batch(\n",
    "    modality: str,\n",
    "    batch: Dict[str, Any],\n",
    ") -> Tuple[torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        z_mod: (B, d_align)\n",
    "        texts: list[str]\n",
    "    \"\"\"\n",
    "    if modality == \"vision\":\n",
    "        images = batch[\"images\"]\n",
    "        texts  = batch[\"texts\"]\n",
    "\n",
    "        enc_out = vision_enc.encode_images(images)\n",
    "        feats = enc_out[\"feats\"]       # (B, T_v, d_vision)\n",
    "        mask  = enc_out[\"mask\"]        # (B, T_v)\n",
    "\n",
    "        feats = feats.to(device)\n",
    "        mask  = mask.to(device)\n",
    "        h_shared = modules[\"vision_adapter\"](feats)  # (B, T_v, d_shared)\n",
    "\n",
    "    elif modality == \"audio\":\n",
    "        wavs = batch[\"waveforms\"]\n",
    "        srs  = batch[\"sampling_rates\"]\n",
    "        texts = batch[\"texts\"]\n",
    "\n",
    "        enc_out = audio_enc.encode_waveforms(wavs, srs)\n",
    "        feats = enc_out[\"feats\"]       # (B, T_a, d_audio)\n",
    "        mask  = enc_out[\"mask\"]        # (B, T_a)\n",
    "\n",
    "        feats = feats.to(device)\n",
    "        mask  = mask.to(device)\n",
    "        h_shared = modules[\"audio_adapter\"](feats)   # (B, T_a, d_shared)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    latents = modules[\"perceiver\"](h_shared, mask)      # (B, L, d_shared)\n",
    "    z_tokens = modules[\"projector\"](latents)            # (B, L, d_align)\n",
    "    z_mod = z_tokens.mean(dim=1)                        # (B, d_align)\n",
    "\n",
    "    return z_mod, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221fdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlignLossConfig(mrl_dims=(1024, 512, 256), mrl_temp=0.07, mrl_weight=1.0, clip_temp=0.07, clip_weight=1.0, max_text_length=128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class AlignLossConfig:\n",
    "    mrl_dims: Tuple[int, ...]\n",
    "    mrl_temp: float = cfg.mrl.mrl_temp\n",
    "    mrl_weight: float = cfg.mrl.mrl_weight\n",
    "\n",
    "    clip_temp: float = 0.07\n",
    "    clip_weight: float = 1.0\n",
    "\n",
    "    max_text_length: int = 128\n",
    "\n",
    "align_cfg = AlignLossConfig(\n",
    "    mrl_dims=tuple(cfg.mrl.mrl_dims),\n",
    ")\n",
    "align_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_align_step(\n",
    "    modality: str,\n",
    "    batch: Dict[str, Any],\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute total loss + metrics for one mini-batch of a given modality.\n",
    "    \"\"\"\n",
    "    modules.train()  # we only train adapters + perceiver + projector\n",
    "\n",
    "    # 1) Modality embeddings\n",
    "    with torch.no_grad():\n",
    "        z_mod, texts = encode_modality_batch(modality, batch)  # (B, d_align), list[str]\n",
    "\n",
    "    # 2) Text embeddings\n",
    "    z_txt = text_embed_fn(texts, max_length=align_cfg.max_text_length)  # (B, d_align)\n",
    "\n",
    "    # float32 for stability in loss\n",
    "    z_mod = z_mod.to(torch.float32)\n",
    "    z_txt = z_txt.to(torch.float32)\n",
    "\n",
    "    # 3) MRL loss\n",
    "    loss_mrl = matryoshka_contrastive_loss(\n",
    "        z_mod,\n",
    "        z_txt,\n",
    "        radii=align_cfg.mrl_dims,\n",
    "        temperature=align_cfg.mrl_temp,\n",
    "        symmetric=True,\n",
    "    )\n",
    "\n",
    "    # 4) CLIP loss\n",
    "    loss_clip = clip_contrastive_loss(\n",
    "        z_mod,\n",
    "        z_txt,\n",
    "        temperature=align_cfg.clip_temp,\n",
    "        symmetric=True,\n",
    "    )\n",
    "\n",
    "    loss = align_cfg.mrl_weight * loss_mrl + align_cfg.clip_weight * loss_clip\n",
    "\n",
    "    metrics = {\n",
    "        f\"{modality}/mrl_loss\": float(loss_mrl.detach().cpu().item()),\n",
    "        f\"{modality}/clip_loss\": float(loss_clip.detach().cpu().item()),\n",
    "        f\"{modality}/total_loss\": float(loss.detach().cpu().item()),\n",
    "    }\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33aa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(modules.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=float(cfg.training.learning_rate),   # \"3e-4\" in YAML -> cast to float\n",
    "    weight_decay=cfg.training.weight_decay,\n",
    ")\n",
    "\n",
    "print(\"Trainable params:\",\n",
    "      sum(p.numel() for p in params if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = getattr(cfg.misc, \"use_wandb\", False)\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=cfg.misc.wandb_project,\n",
    "        name=cfg.misc.wandb_run_name,\n",
    "        config={\"align_cfg\": align_cfg.__dict__},\n",
    "    )\n",
    "else:\n",
    "    wandb = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72602cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = cfg.training.num_epochs\n",
    "LOG_EVERY = cfg.training.log_every_steps\n",
    "MAX_GRAD_NORM = cfg.training.max_grad_norm\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n===== Epoch {epoch}/{NUM_EPOCHS} =====\")\n",
    "\n",
    "    # ---- Vision (PixMo) ----\n",
    "    for step, batch in enumerate(pixmo_loader, start=1):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = forward_align_step(\"vision\", batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modules.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % LOG_EVERY == 0:\n",
    "            msg = f\"[Vision] step {step:05d} | loss={metrics['vision/total_loss']:.4f}\"\n",
    "            print(msg)\n",
    "            if wandb is not None:\n",
    "                wandb.log({\"epoch\": epoch, **metrics})\n",
    "\n",
    "    # ---- Audio (LibriSpeech) ----\n",
    "    for step, batch in enumerate(librispeech_loader, start=1):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = forward_align_step(\"audio\", batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modules.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % LOG_EVERY == 0:\n",
    "            msg = f\"[Audio ] step {step:05d} | loss={metrics['audio/total_loss']:.4f}\"\n",
    "            print(msg)\n",
    "            if wandb is not None:\n",
    "                wandb.log({\"epoch\": epoch, **metrics})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85109b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ckpt_path = \"checkpoints/alignment_clip_whisper_live.pt\"\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"modules\": modules.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"d_shared\": d_shared,\n",
    "            \"d_align\": d_align,\n",
    "            \"vision_model_name\": cfg.models.vision_model_name,\n",
    "            \"audio_model_name\": cfg.models.audio_model_name,\n",
    "            \"text_model_name\": TEXT_ENCODER_NAME,\n",
    "            \"mrl_dims\": list(align_cfg.mrl_dims),\n",
    "        },\n",
    "    },\n",
    "    ckpt_path,\n",
    ")\n",
    "print(\"Saved alignment checkpoint to:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules.eval()\n",
    "\n",
    "# Image example\n",
    "ex_pix = pixmo_ds[0]\n",
    "with torch.no_grad():\n",
    "    z_img, _ = encode_modality_batch(\n",
    "        \"vision\",\n",
    "        {\"images\": [ex_pix[\"image\"]], \"texts\": [ex_pix[\"text\"]]},\n",
    "    )\n",
    "print(\"Image embedding shape:\", z_img.shape)\n",
    "\n",
    "# Audio example\n",
    "ex_aud = librispeech_ds[0]\n",
    "with torch.no_grad():\n",
    "    z_aud, _ = encode_modality_batch(\n",
    "        \"audio\",\n",
    "        {\n",
    "            \"waveforms\":      [ex_aud[\"waveform\"]],\n",
    "            \"sampling_rates\": [ex_aud[\"sampling_rate\"]],\n",
    "            \"texts\":          [ex_aud[\"text\"]],\n",
    "        },\n",
    "    )\n",
    "print(\"Audio embedding shape:\", z_aud.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd53b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68831564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
