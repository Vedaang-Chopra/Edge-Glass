{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cf58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bcf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80b76e",
   "metadata": {},
   "source": [
    "### Phase-0: - Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c0d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import setup_from_yaml, ModelsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/wandb/run-20251122_225051-lnzy60r8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/lnzy60r8' target=\"_blank\">phase0_global_setup</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/lnzy60r8' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/lnzy60r8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: cuda, dtype: torch.float16\n",
      "[Config] root_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/edge_glass\n",
      "[Config] features_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/edge_glass/features\n",
      "Using device: cuda\n",
      "Using dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ”¹ Phase 0 â€“ Global setup\n",
    "cfg = setup_from_yaml(\"configs/config.yaml\")\n",
    "\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Using dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebff6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, when you build encoders / perceiver:\n",
    "# from models_oop import ModelConfig, ImageEncoder, AudioEncoder, DecoderLLM\n",
    "\n",
    "# vision_model_cfg = ModelConfig(\n",
    "#     model_name=cfg.models.vision_model_name,\n",
    "#     device=str(cfg.torch_device),\n",
    "#     dtype=cfg.torch_dtype,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f753c",
   "metadata": {},
   "source": [
    "### Phase-1: - Loading the Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6a0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from architecture.audio_encoder import load_audio_encoder\n",
    "from architecture.image_encoder import load_image_encoder\n",
    "from architecture.text_decoder import (\n",
    "    load_decoder_llm,\n",
    "    TextDecoder,\n",
    "    AudioDecoder,\n",
    "    VideoDecoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489e89d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision encoder loaded: openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "# Image Encoders\n",
    "# Phase-1: Vision encoder\n",
    "vision_model_name = cfg.models.vision_model_name\n",
    "\n",
    "vision_encoder = load_image_encoder(\n",
    "    model_name=vision_model_name,\n",
    "    device=str(device),\n",
    "    dtype=dtype,\n",
    "    feature_strategy=\"layers_concat\",  # or \"auto\"/\"last_hidden\"\n",
    "    layer_indices=[2, -2],            # 2nd & 2nd-to-last layer like your spec\n",
    "    pool=\"mean\",                      # mean over patches\n",
    ")\n",
    "\n",
    "print(\"Vision encoder loaded:\", vision_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade67828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2081cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio encoder loaded: openai/whisper-base\n"
     ]
    }
   ],
   "source": [
    "# Phase-1: Audio encoder (Whisper)\n",
    "audio_model_name = cfg.models.audio_model_name\n",
    "\n",
    "audio_encoder = None\n",
    "if audio_model_name is not None:\n",
    "    audio_encoder = load_audio_encoder(\n",
    "        model_name=audio_model_name,\n",
    "        device=str(device),\n",
    "        dtype=dtype,\n",
    "        target_sr=16000,\n",
    "        feature_strategy=\"layers_concat\",  # or \"auto\"\n",
    "        layer_indices=[2, -2],\n",
    "        pool=\"mean\",\n",
    "    )\n",
    "    print(\"Audio encoder loaded:\", audio_model_name)\n",
    "else:\n",
    "    print(\"No audio model specified in config.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc33e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70061818f5b8433fa71fce52bb9ec3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded: qwen/Qwen2.5-3B-Instruct\n",
      "Image token IDs on LLM: {'image_start': 151665, 'image_patch': 151666, 'image_end': 151667}\n"
     ]
    }
   ],
   "source": [
    "# Phase-1: Decoder-only LLM\n",
    "llm_model_name = cfg.models.llm_model_name\n",
    "\n",
    "llm = load_decoder_llm(\n",
    "    model_name=llm_model_name,\n",
    "    device=str(device),\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    add_image_special_tokens=True,   # set False if you don't need them yet\n",
    ")\n",
    "\n",
    "print(\"LLM loaded:\", llm_model_name)\n",
    "print(\"Image token IDs on LLM:\", getattr(llm, \"image_token_ids\", {}))\n",
    "\n",
    "# Task-specific decoders wrapping the same LLM\n",
    "text_decoder  = TextDecoder(llm)\n",
    "audio_decoder = AudioDecoder(llm)\n",
    "video_decoder = VideoDecoder(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cca32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Freeze] All params in vision_encoder set to requires_grad=False\n",
      "[Freeze] All params in audio_encoder set to requires_grad=False\n",
      "[Freeze] All params in decoder_llm set to requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Phase-1.4: Freeze base encoders (we only train bottleneck)\n",
    "# ---------------------------------------------------------\n",
    "def maybe_freeze(module, name: str):\n",
    "    if module is None:\n",
    "        return\n",
    "    if hasattr(module, \"parameters\"):\n",
    "        for p in module.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(f\"[Freeze] All params in {name} set to requires_grad=False\")\n",
    "    else:\n",
    "        print(f\"[Warn] {name} has no .parameters(), skipping freeze\")\n",
    "\n",
    "maybe_freeze(vision_encoder, \"vision_encoder\")\n",
    "maybe_freeze(audio_encoder, \"audio_encoder\")\n",
    "\n",
    "# For the LLM we typically also freeze all weights for this POC\n",
    "# (later you can add LoRA/PEFT on top)\n",
    "if hasattr(llm, \"model\"):\n",
    "    base_llm = llm.model\n",
    "else:\n",
    "    base_llm = llm  # if load_decoder_llm returns the raw HF model\n",
    "\n",
    "maybe_freeze(base_llm, \"decoder_llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca8fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Phase-1.5: Infer feature dims & Perceiver config hooks\n",
    "# ---------------------------------------------------------\n",
    "def get_feature_dim(encoder, name: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Try a few common patterns to get the output feature dim.\n",
    "    Adjust this if your encoder abstraction exposes a different attribute.\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        return None\n",
    "\n",
    "    for attr in (\"output_dim\", \"feature_dim\", \"hidden_size\", \"embed_dim\"):\n",
    "        if hasattr(encoder, attr):\n",
    "            dim = getattr(encoder, attr)\n",
    "            print(f\"[Dim] {name} {attr} = {dim}\")\n",
    "            return dim\n",
    "\n",
    "    # If your encoders donâ€™t expose a dim attribute,\n",
    "    # you can later swap this to run a tiny dummy forward pass instead.\n",
    "    raise AttributeError(\n",
    "        f\"Could not infer feature dim for {name}. \"\n",
    "        \"Expose `.output_dim` or `.feature_dim` on your encoder class.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3afcbab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Could not infer feature dim for vision_encoder. Expose `.output_dim` or `.feature_dim` on your encoder class.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vision_feat_dim = \u001b[43mget_feature_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvision_encoder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m audio_feat_dim = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_feature_dim\u001b[39m\u001b[34m(encoder, name)\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dim\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# If your encoders donâ€™t expose a dim attribute,\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# you can later swap this to run a tiny dummy forward pass instead.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m     21\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not infer feature dim for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExpose `.output_dim` or `.feature_dim` on your encoder class.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: Could not infer feature dim for vision_encoder. Expose `.output_dim` or `.feature_dim` on your encoder class."
     ]
    }
   ],
   "source": [
    "vision_feat_dim = get_feature_dim(vision_encoder, \"vision_encoder\")\n",
    "\n",
    "audio_feat_dim = None\n",
    "if audio_encoder is not None:\n",
    "    audio_feat_dim = get_feature_dim(audio_encoder, \"audio_encoder\")\n",
    "\n",
    "# LLM hidden size â€“ used by the projector & MRL head\n",
    "llm_hidden_dim = getattr(base_llm.config, \"hidden_size\", None)\n",
    "if llm_hidden_dim is None:\n",
    "    raise ValueError(\"Could not find `hidden_size` on LLM config.\")\n",
    "\n",
    "print(f\"[Dim] LLM hidden size = {llm_hidden_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25f31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) convenience flags / sub-configs youâ€™ll use later\n",
    "models_cfg   = cfg.models\n",
    "perc_cfg     = getattr(cfg, \"perceiver\", None)\n",
    "loss_cfg     = getattr(cfg, \"loss\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0456cd0",
   "metadata": {},
   "source": [
    "### Phase-2: - Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2eb380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86739a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b47aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2624a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
