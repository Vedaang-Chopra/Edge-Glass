{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9249a68",
   "metadata": {},
   "source": [
    "### Step 1 — Load and freeze CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"  # good default\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae938de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.to(device)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "sum_params = sum(p.numel() for p in clip_model.parameters())\n",
    "sum_trainable = sum(p.numel() for p in clip_model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {sum_params:,}\")\n",
    "print(f\"Trainable params: {sum_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347379cd",
   "metadata": {},
   "source": [
    "### Load the PixMo dataset for alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ce7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "pixmo_ds = load_dataset(\"allenai/pixmo-cap\", split=\"all\")\n",
    "print(pixmo_ds)\n",
    "print(pixmo_ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb11cc",
   "metadata": {},
   "source": [
    "#### Since Pixmo is image URL's, we first fetch the images in an array to load them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b62ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch one image from URL\n",
    "# ------------------------------------------------------------\n",
    "def fetch_image(url: str) -> Image.Image:\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small working subset (e.g., first 16)\n",
    "N_EXAMPLES = 500\n",
    "subset = pixmo_ds.select(range(N_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Worker for multiprocessing\n",
    "# ------------------------------------------------------------\n",
    "def _fetch_single(args):\n",
    "    idx, ex = args\n",
    "    url = ex[\"image_url\"]\n",
    "    cap = ex[\"caption\"]\n",
    "    try:\n",
    "        img = fetch_image(url)\n",
    "        return idx, img, cap, None\n",
    "    except Exception as e:\n",
    "        return idx, None, None, e\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parallel image–caption collector\n",
    "# ------------------------------------------------------------\n",
    "def collect_image_caption_pairs_mp(subset, processes=None):\n",
    "    processes = processes or min(8, cpu_count())\n",
    "\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "    # prepare input as (idx, example) pairs\n",
    "    tasks = [(i, ex) for i, ex in enumerate(subset)]\n",
    "\n",
    "    with Pool(processes=processes) as pool:\n",
    "        for idx, img, cap, err in pool.imap_unordered(_fetch_single, tasks):\n",
    "            print(\"Fetched index\", idx)\n",
    "            if err:\n",
    "                print(f\"Skipping index {idx} due to error: {err}\")\n",
    "                continue\n",
    "            images.append(img)\n",
    "            captions.append(cap)\n",
    "\n",
    "    print(\"Collected\", len(images), \"image-caption pairs\")\n",
    "    return images, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd25f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions = collect_image_caption_pairs_mp(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa106c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(images[0])\n",
    "print(captions[0][:400], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22320174",
   "metadata": {},
   "source": [
    "### Encoding images wof Pixmo with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images_texts_from_pil(images, captions, batch_size=8):\n",
    "    all_img_embs = []\n",
    "    all_txt_embs = []\n",
    "\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_txts = captions[i:i+batch_size]\n",
    "\n",
    "        inputs = clip_processor(\n",
    "            text=batch_txts,\n",
    "            images=batch_imgs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,              # ✅ <--- this is the key\n",
    "            max_length=77\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            img_emb = outputs.image_embeds       # (B, d)\n",
    "            txt_emb = outputs.text_embeds        # (B, d)\n",
    "\n",
    "        img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "        txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        all_img_embs.append(img_emb.cpu())\n",
    "        all_txt_embs.append(txt_emb.cpu())\n",
    "\n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    all_txt_embs = torch.cat(all_txt_embs, dim=0)\n",
    "\n",
    "    return all_img_embs, all_txt_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226628d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb, txt_emb = encode_images_texts_from_pil(images, captions, batch_size=8)\n",
    "print(\"Image embeddings shape:\", img_emb.shape)\n",
    "print(\"Text embeddings shape :\", txt_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12d78c",
   "metadata": {},
   "source": [
    "### Similarity matrix + retrieval on PixMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = img_emb.numpy()\n",
    "txt_np = txt_emb.numpy()\n",
    "\n",
    "sim_matrix = img_np @ txt_np.T\n",
    "print(\"Similarity matrix shape:\", sim_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab78706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1_retrieval(sim_matrix, captions):\n",
    "    N = sim_matrix.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        j = sim_matrix[i].argmax()\n",
    "        is_correct = (i == j)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        print(f\"Image {i}: best caption idx = {j} | correct={is_correct}\")\n",
    "        print(\"  GT caption (truncated):\", captions[i][:120].replace(\"\\n\", \" \"), \"...\")\n",
    "        print(\"  Top caption (truncated):\", captions[j][:120].replace(\"\\n\", \" \"), \"...\")\n",
    "        print()\n",
    "    print(f\"Top-1 accuracy (identity pairing): {correct}/{N} = {correct / N:.2f}\")\n",
    "\n",
    "top1_retrieval(sim_matrix, captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb361785",
   "metadata": {},
   "source": [
    "### Save PixMo embeddings for the projector step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = Path(\"artifacts/clip_projector_poc_pixmo\")\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(SAVE_ROOT / \"img_emb.npy\", img_np)\n",
    "np.save(SAVE_ROOT / \"txt_emb.npy\", txt_np)\n",
    "\n",
    "with open(SAVE_ROOT / \"captions.txt\", \"w\") as f:\n",
    "    for cap in captions:\n",
    "        f.write(cap.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "print(\"Saved embeddings and captions to:\", SAVE_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da4123",
   "metadata": {},
   "source": [
    "### Loading those saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SAVE_ROOT = Path(\"artifacts/clip_projector_poc_pixmo\")\n",
    "\n",
    "img_np = np.load(SAVE_ROOT / \"img_emb.npy\")\n",
    "txt_np = np.load(SAVE_ROOT / \"txt_emb.npy\")\n",
    "\n",
    "print(\"Loaded shapes:\", img_np.shape, txt_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aafd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.from_numpy(img_np).float()\n",
    "txt_tensor = torch.from_numpy(txt_np).float()\n",
    "\n",
    "print(img_tensor.shape, img_tensor.dtype)\n",
    "print(txt_tensor.shape, txt_tensor.dtype)\n",
    "\n",
    "dataset = TensorDataset(img_tensor, txt_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff26984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "d_clip = img_tensor.shape[1]     # typically 512\n",
    "d_proj = 1024                    # you can change this\n",
    "\n",
    "class ProjectorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = out_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "P_img = ProjectorMLP(d_clip, d_proj).to(device)\n",
    "P_txt = ProjectorMLP(d_clip, d_proj).to(device)\n",
    "\n",
    "sum_p_img = sum(p.numel() for p in P_img.parameters())\n",
    "sum_p_txt = sum(p.numel() for p in P_txt.parameters())\n",
    "print(f\"P_img params: {sum_p_img:,}, P_txt params: {sum_p_txt:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(z_img, z_txt, temperature=0.07):\n",
    "    \"\"\"\n",
    "    z_img, z_txt: (B, d_proj)\n",
    "    Returns: scalar loss, img2txt_acc, txt2img_acc\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    z_img = F.normalize(z_img, dim=-1)\n",
    "    z_txt = F.normalize(z_txt, dim=-1)\n",
    "\n",
    "    # Similarity matrix: (B, B)\n",
    "    logits = z_img @ z_txt.T    # cosine sims\n",
    "    logits = logits / temperature\n",
    "\n",
    "    B = logits.shape[0]\n",
    "    targets = torch.arange(B, device=logits.device)\n",
    "\n",
    "    # Image -> text (rows over texts)\n",
    "    loss_i2t = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # Text -> image (columns over images)\n",
    "    loss_t2i = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "    loss = (loss_i2t + loss_t2i) / 2.0\n",
    "\n",
    "    # Compute top-1 accuracy for monitoring\n",
    "    with torch.no_grad():\n",
    "        pred_i2t = logits.argmax(dim=1)\n",
    "        pred_t2i = logits.argmax(dim=0)\n",
    "        acc_i2t = (pred_i2t == targets).float().mean().item()\n",
    "        acc_t2i = (pred_t2i == targets).float().mean().item()\n",
    "\n",
    "    return loss, acc_i2t, acc_t2i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275bbbb",
   "metadata": {},
   "source": [
    "### Step 9 — DataLoader + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901abdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64    # if your N_EXAMPLES is small, you can use 16 or 8\n",
    "LR = 1e-3\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(P_img.parameters()) + list(P_txt.parameters()),\n",
    "    lr=LR,\n",
    "    weight_decay=1e-4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aeed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    P_img.train()\n",
    "    P_txt.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc_i2t = 0.0\n",
    "    epoch_acc_t2i = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for img_batch, txt_batch in loader:\n",
    "        img_batch = img_batch.to(device)\n",
    "        txt_batch = txt_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z_img = P_img(img_batch)      # (B, d_proj)\n",
    "        z_txt = P_txt(txt_batch)      # (B, d_proj)\n",
    "\n",
    "        loss, acc_i2t, acc_t2i = contrastive_loss(z_img, z_txt, temperature=0.07)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc_i2t += acc_i2t\n",
    "        epoch_acc_t2i += acc_t2i\n",
    "        n_batches += 1\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"loss={epoch_loss/n_batches:.4f} | \"\n",
    "        f\"img→txt acc={epoch_acc_i2t/n_batches:.3f} | \"\n",
    "        f\"txt→img acc={epoch_acc_t2i/n_batches:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e49747",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_img.eval()\n",
    "P_txt.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_img = img_tensor.to(device)\n",
    "    all_txt = txt_tensor.to(device)\n",
    "\n",
    "    z_img_all = P_img(all_img)\n",
    "    z_txt_all = P_txt(all_txt)\n",
    "\n",
    "    z_img_all = F.normalize(z_img_all, dim=-1)\n",
    "    z_txt_all = F.normalize(z_txt_all, dim=-1)\n",
    "\n",
    "sim_proj = (z_img_all @ z_txt_all.T).cpu().numpy()\n",
    "print(\"Projected similarity matrix shape:\", sim_proj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload captions for readability\n",
    "caps = []\n",
    "with open(SAVE_ROOT / \"captions.txt\") as f:\n",
    "    for line in f:\n",
    "        caps.append(line.strip())\n",
    "\n",
    "top1_retrieval(sim_proj, caps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
