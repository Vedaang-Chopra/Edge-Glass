{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv -q pip install transformers datasets accelerate bitsandbytes einops\n",
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e124b",
   "metadata": {},
   "source": [
    "### Phase-1 : - Setup & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49127c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5439eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/notebooks/practice_poc/04_decoder_poc/wandb/run-20251122_155431-hn3udlwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/trm-math-poc/runs/hn3udlwu' target=\"_blank\">effortless-puddle-2</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/trm-math-poc' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/trm-math-poc' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/trm-math-poc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/trm-math-poc/runs/hn3udlwu' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/trm-math-poc/runs/hn3udlwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vedaangchopra_gatech/trm-math-poc/runs/hn3udlwu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ffdf81a1430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    # HF bits\n",
    "    baseline_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # or \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    dataset_name = \"openai/gsm8k\"\n",
    "    dataset_subset = \"main\"   # gsm8k has \"main\", \"socratic\"\n",
    "\n",
    "    # Data sizes\n",
    "    max_train_examples = 5000    # for TRM specialist training\n",
    "    max_eval_examples  = 300     # for both baseline & TRM\n",
    "\n",
    "    # TRM model hyperparams (tiny)\n",
    "    vocab_size = 2000            # if using a BPE tokenizer; or small char vocab if you prefer\n",
    "    d_model = 256\n",
    "    n_layers = 2                 # for reasoning block\n",
    "    n_heads = 4\n",
    "    max_seq_len = 256\n",
    "    H_cycles = 4                 # outer cycles (reasoning loops)\n",
    "    L_cycles = 2                 # inner loops\n",
    "    halt_max_steps = 4           # max ACT steps per example\n",
    "\n",
    "    # Training\n",
    "    batch_size = 32\n",
    "    lr = 3e-4\n",
    "    num_epochs = 5\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = Config()\n",
    "device = torch.device(cfg.device)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"trm-math-poc\",\n",
    "    config=vars(cfg),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6674f3",
   "metadata": {},
   "source": [
    "### Phase 2 â€“ Load Dataset & Extract Numeric Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f38f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(cfg.dataset_name, cfg.dataset_subset)\n",
    "raw_train = raw_ds[\"train\"]\n",
    "raw_test  = raw_ds[\"test\"]\n",
    "\n",
    "print(raw_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8205dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PATTERN = re.compile(r\"(-?\\d+\\.?\\d*)\")\n",
    "\n",
    "def extract_numeric_answer(ans_str: str):\n",
    "    # GSM8K uses \"#### 42\" at the end â€” we grab the last number\n",
    "    matches = ANSWER_PATTERN.findall(ans_str)\n",
    "    return matches[-1] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174bf273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7473, 1319)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_and_prepare_split(split):\n",
    "    data = []\n",
    "    for ex in split:\n",
    "        a = extract_numeric_answer(ex[\"answer\"])\n",
    "        if a is None:\n",
    "            continue\n",
    "        data.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"answer\": a,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = filter_and_prepare_split(raw_train)\n",
    "test_data  = filter_and_prepare_split(raw_test)\n",
    "\n",
    "len(train_data), len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02df73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "train_data = train_data[:cfg.max_train_examples]\n",
    "eval_data  = test_data[:cfg.max_eval_examples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bbcfe",
   "metadata": {},
   "source": [
    "### Phase-3: - Baseline: Qwen / Llama evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b49f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.baseline_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.baseline_model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87fe7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(q: str) -> str:\n",
    "    return (\n",
    "        \"You are a math solver.\\n\"\n",
    "        \"Solve the following problem step by step in your head, \"\n",
    "        \"but only output the final numeric answer.\\n\\n\"\n",
    "        f\"Question: {q}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_llm(question: str, max_new_tokens: int = 64) -> str:\n",
    "    prompt = make_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # Take only text after \"Answer:\"\n",
    "    if \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1]\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93030992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] 10/200 acc=0.000\n",
      "[LLM] 20/200 acc=0.000\n",
      "[LLM] 30/200 acc=0.000\n",
      "[LLM] 40/200 acc=0.000\n",
      "[LLM] 50/200 acc=0.040\n",
      "[LLM] 60/200 acc=0.033\n",
      "[LLM] 70/200 acc=0.029\n",
      "[LLM] 80/200 acc=0.025\n",
      "[LLM] 90/200 acc=0.022\n",
      "[LLM] 100/200 acc=0.020\n",
      "[LLM] 110/200 acc=0.018\n",
      "[LLM] 120/200 acc=0.025\n",
      "[LLM] 130/200 acc=0.031\n",
      "[LLM] 140/200 acc=0.036\n",
      "[LLM] 150/200 acc=0.033\n",
      "[LLM] 160/200 acc=0.031\n",
      "[LLM] 170/200 acc=0.029\n",
      "[LLM] 180/200 acc=0.033\n",
      "[LLM] 190/200 acc=0.032\n",
      "[LLM] 200/200 acc=0.030\n",
      "Baseline LLM acc: 0.03\n"
     ]
    }
   ],
   "source": [
    "def eval_llm_on_gsm8k(data, n_eval=200):\n",
    "    correct = 0\n",
    "    records = []\n",
    "    for i, ex in enumerate(data[:n_eval]):\n",
    "        q, gold = ex[\"question\"], ex[\"answer\"]\n",
    "        pred_str = generate_answer_llm(q)\n",
    "        pred_num = extract_numeric_answer(pred_str)\n",
    "        is_correct = (pred_num == gold)\n",
    "        correct += int(is_correct)\n",
    "\n",
    "        records.append({\n",
    "            \"question\": q,\n",
    "            \"gold\": gold,\n",
    "            \"raw_pred\": pred_str,\n",
    "            \"pred_num\": pred_num,\n",
    "            \"correct\": is_correct,\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            acc = correct / (i + 1)\n",
    "            print(f\"[LLM] {i+1}/{n_eval} acc={acc:.3f}\")\n",
    "            wandb.log({\"baseline/step\": i + 1, \"baseline/acc\": acc})\n",
    "\n",
    "    acc = correct / min(len(data), n_eval)\n",
    "    wandb.log({\"baseline/final_acc\": acc})\n",
    "    return acc, records\n",
    "\n",
    "baseline_acc, baseline_records = eval_llm_on_gsm8k(eval_data)\n",
    "print(\"Baseline LLM acc:\", baseline_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d6927",
   "metadata": {},
   "source": [
    "### Phase-4: - TRM-style tiny model (specialist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CHAR VOCAB SETUP (REPLACE YOUR CURRENT CHARSET/VOCAB + ENCODE/DECODE) ----\n",
    "\n",
    "# Build charset from train_data\n",
    "charset = set()\n",
    "for ex in train_data:\n",
    "    charset.update(ex[\"question\"])\n",
    "    charset.update(ex[\"answer\"])\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "itos = special_tokens + sorted(list(charset))\n",
    "stoi = {ch: i for i, ch in enumerate(itos)}\n",
    "\n",
    "PAD_ID = stoi[\"<pad>\"]\n",
    "BOS_ID = stoi[\"<bos>\"]\n",
    "EOS_ID = stoi[\"<eos>\"]\n",
    "\n",
    "cfg.vocab_size = len(itos)\n",
    "\n",
    "# Use a fixed max_seq_len based on your earlier analysis or keep what you had\n",
    "# Just make sure cfg.max_seq_len is set BEFORE creating the model.\n",
    "# Example:\n",
    "# max_q_len = max(len(ex[\"question\"]) for ex in train_data)\n",
    "# max_a_len = max(len(ex[\"answer\"]) for ex in train_data)\n",
    "# cfg.max_seq_len = max_q_len + max_a_len + 10  # a bit of slack\n",
    "\n",
    "\n",
    "def encode_text(s: str, max_len: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Encode string into fixed-length [BOS, ..., EOS/PAD] with safe clamping.\n",
    "    \"\"\"\n",
    "    # Map chars to ids, unknown -> PAD\n",
    "    ids = [BOS_ID] + [stoi.get(ch, PAD_ID) for ch in s]\n",
    "\n",
    "    # Clamp to valid vocab range just in case\n",
    "    ids = [min(max(i, 0), cfg.vocab_size - 1) for i in ids]\n",
    "\n",
    "    # Reserve last slot for EOS\n",
    "    ids = ids[: max_len - 1] + [EOS_ID]\n",
    "\n",
    "    # Pad if needed\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def decode_ids(ids: list[int]) -> str:\n",
    "    \"\"\"\n",
    "    Decode ids back into string, skipping special tokens.\n",
    "    \"\"\"\n",
    "    chars = []\n",
    "    for idx in ids:\n",
    "        if idx < 0 or idx >= len(itos):\n",
    "            continue\n",
    "        ch = itos[idx]\n",
    "        if ch in (\"<pad>\", \"<bos>\", \"<eos>\"):\n",
    "            continue\n",
    "        chars.append(ch)\n",
    "    return \"\".join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548e7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q_len = max(len(ex[\"question\"]) for ex in train_data)\n",
    "max_a_len = max(len(ex[\"answer\"]) for ex in train_data)\n",
    "cfg.max_seq_len = max_q_len + max_a_len + 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8545b46",
   "metadata": {},
   "source": [
    "### Dataset class for TRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98c3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KCharDataset(Dataset):\n",
    "    def __init__(self, data, max_len, answer_only_loss=True):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.answer_only_loss = answer_only_loss\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        q = ex[\"question\"]\n",
    "        a = ex[\"answer\"]\n",
    "        # You can tweak formatting; simple version:\n",
    "        prompt = q + \" Answer:\"\n",
    "        full = prompt + \" \" + a\n",
    "\n",
    "        ids = encode_text(full, self.max_len)\n",
    "        input_ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        # LM target = next char\n",
    "        labels = input_ids.clone()\n",
    "        # Optionally, we can ignore loss on non-answer parts\n",
    "        if self.answer_only_loss:\n",
    "            # ignore everything before start of answer\n",
    "            ans_start = len(encode_text(prompt + \" \", self.max_len)) - 1\n",
    "            labels[:ans_start] = -100  # ignore index\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "train_ds = GSM8KCharDataset(train_data, cfg.max_seq_len, answer_only_loss=True)\n",
    "eval_ds  = GSM8KCharDataset(eval_data,  cfg.max_seq_len, answer_only_loss=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "eval_loader  = DataLoader(eval_ds,  batch_size=cfg.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e649d5",
   "metadata": {},
   "source": [
    "#### TRM-style block (tiny transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128b913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, expansion: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model * expansion)),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(d_model * expansion), d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, D]\n",
    "        attn_out, _ = self.attn(x, x, x, need_weights=False)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1407c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyMathTRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Tiny TRM-style model with:\n",
    "      - single latent z\n",
    "      - H_cycles outer loops, L_cycles inner loops\n",
    "      - deep recursion with no_grad() on earlier cycles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TRMBlock(cfg.d_model, cfg.n_heads) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size)\n",
    "        self.q_head = nn.Linear(cfg.d_model, 1)  # halting logit from first position\n",
    "\n",
    "        # Learned initial latent state (broadcast later)\n",
    "        self.z_init = nn.Parameter(\n",
    "            torch.randn(1, 1, cfg.d_model) / math.sqrt(cfg.d_model)\n",
    "        )\n",
    "\n",
    "    def forward_once(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        z: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        One 'latent recursion' pass with gradients.\n",
    "        input_ids: [B, L]\n",
    "        z:         [B, L, D]\n",
    "        \"\"\"\n",
    "        B, L = input_ids.shape\n",
    "        tok_emb = self.embed(input_ids)  # [B, L, D]\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(L, device=input_ids.device)\n",
    "        pos_emb = self.pos_emb(positions)[None, :, :].expand(B, L, -1)  # [B, L, D]\n",
    "\n",
    "        # Combine: token + position + latent\n",
    "        x = tok_emb + pos_emb + z  # [B, L, D]\n",
    "\n",
    "        # Pass through a small stack of transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        new_z = x\n",
    "        logits = self.lm_head(x)                     # [B, L, V]\n",
    "        q_logit = self.q_head(x[:, 0, :]).squeeze(-1)  # [B]\n",
    "        return new_z, logits, q_logit\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor | None = None,\n",
    "        carry_z: torch.Tensor | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        TRM-style recursion:\n",
    "          - If carry_z is None, start from z_init.\n",
    "          - Run H_cycles - 1 passes under no_grad.\n",
    "          - Run final H-cycle with grads and compute loss if labels provided.\n",
    "          - Return new_z (detached), logits, q_logit, loss.\n",
    "        \"\"\"\n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        # Initialise or fix latent state size\n",
    "        if carry_z is None or carry_z.shape[0] != B or carry_z.shape[1] != L:\n",
    "            z = self.z_init.expand(B, L, -1)\n",
    "        else:\n",
    "            z = carry_z\n",
    "\n",
    "        # H_cycles - 1 cycles with no_grad (deep recursion, no backprop)\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.cfg.H_cycles - 1):\n",
    "                for _ in range(self.cfg.L_cycles):\n",
    "                    z, _, _ = self.forward_once(input_ids, z)\n",
    "\n",
    "        # Final cycle with gradients\n",
    "        for _ in range(self.cfg.L_cycles):\n",
    "            z, logits, q_logit = self.forward_once(input_ids, z)\n",
    "\n",
    "        new_z = z.detach()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Standard LM loss over characters (answer_only mask handled in labels)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.cfg.vocab_size),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "\n",
    "        return new_z, logits, q_logit, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b503f",
   "metadata": {},
   "source": [
    "### Training loop with ACT-style supervision + W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1967e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 50: loss=0.0011\n",
      "Epoch 0 step 100: loss=0.0005\n",
      "Epoch 0 step 150: loss=0.0003\n",
      "Epoch 1 step 200: loss=0.0002\n",
      "Epoch 1 step 250: loss=0.0002\n",
      "Epoch 1 step 300: loss=0.0001\n",
      "Epoch 2 step 350: loss=0.0001\n",
      "Epoch 2 step 400: loss=0.0001\n",
      "Epoch 2 step 450: loss=0.0001\n",
      "Epoch 3 step 500: loss=0.0001\n",
      "Eval: {'eval_loss': 5.334195452936304e-05, 'eval_ppl': 1.0000533433772367}\n",
      "Epoch 3 step 550: loss=0.0000\n",
      "Epoch 3 step 600: loss=0.0000\n",
      "Epoch 4 step 650: loss=0.0000\n",
      "Epoch 4 step 700: loss=0.0000\n",
      "Epoch 4 step 750: loss=0.0000\n"
     ]
    }
   ],
   "source": [
    "# ---- TRAINING LOOP (REPLACE YOUR CURRENT TRAINING LOOP) ----\n",
    "\n",
    "trm_model = TinyMathTRM(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(trm_model.parameters(), lr=cfg.lr)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "def evaluate_trm(model, eval_loader, max_batches: int | None = None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for b_idx, batch in enumerate(eval_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            _, logits, q_logit, loss = model(input_ids, labels=labels, carry_z=None)\n",
    "            if loss is None:\n",
    "                continue\n",
    "\n",
    "            tokens = (labels != -100).sum().item()\n",
    "            total_loss += loss.item() * tokens\n",
    "            total_tokens += tokens\n",
    "\n",
    "            if max_batches is not None and (b_idx + 1) >= max_batches:\n",
    "                break\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return {\"eval_loss\": avg_loss, \"eval_ppl\": ppl}\n",
    "\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    trm_model.train()\n",
    "    carry_z = None\n",
    "\n",
    "    for batch in train_loader:\n",
    "        global_step += 1\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Reset carry_z if shape mismatches\n",
    "        if carry_z is None or carry_z.shape[0] != input_ids.shape[0] or carry_z.shape[1] != input_ids.shape[1]:\n",
    "            carry_z = None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        carry_z, logits, q_logit, loss = trm_model(input_ids, labels=labels, carry_z=carry_z)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trm_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            wandb.log({\"trm/train_loss\": loss.item(), \"step\": global_step})\n",
    "            print(f\"Epoch {epoch} step {global_step}: loss={loss.item():.4f}\")\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            metrics = evaluate_trm(trm_model, eval_loader, max_batches=50)\n",
    "            wandb.log({f\"trm/{k}\": v for k, v in metrics.items()} | {\"step\": global_step})\n",
    "            print(\"Eval:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfae616",
   "metadata": {},
   "source": [
    "#### Adding halting loss (more TRM-like, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5b7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to implement halting loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75ce2c",
   "metadata": {},
   "source": [
    "### Measuring TRM accuracy on GSM8K subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34bb7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATION + EVAL HELPERS (REPLACE YOUR CURRENT VERSIONS) ----\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_trm(model: TinyMathTRM, question: str, max_answer_len: int = 16) -> str:\n",
    "    \"\"\"\n",
    "    Fixed-length generation: we don't change sequence length,\n",
    "    we fill answer chars into PAD slots in-place.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prompt = question + \" Answer:\"\n",
    "\n",
    "    # 1) Encode prompt into fixed-length ids\n",
    "    prompt_ids = encode_text(prompt, cfg.max_seq_len)  # [L]\n",
    "\n",
    "    # 2) Find first PAD position: where the answer starts\n",
    "    try:\n",
    "        pad_idx = prompt_ids.index(PAD_ID)\n",
    "    except ValueError:\n",
    "        # Prompt filled everything; reserve tail for answer\n",
    "        pad_idx = cfg.max_seq_len - max_answer_len\n",
    "\n",
    "    ans_start = pad_idx\n",
    "\n",
    "    # 3) Tensor [1, L]\n",
    "    input_ids = torch.tensor([prompt_ids], device=device)\n",
    "    carry_z = None\n",
    "\n",
    "    # 4) Iteratively fill answer positions\n",
    "    for t in range(max_answer_len):\n",
    "        carry_z, logits, q_logit, _ = model(input_ids, labels=None, carry_z=carry_z)\n",
    "\n",
    "        pos = ans_start + t\n",
    "        if pos >= cfg.max_seq_len:\n",
    "            break\n",
    "\n",
    "        prev_pos = max(pos - 1, 0)\n",
    "        next_id = logits[0, prev_pos, :].argmax().item()\n",
    "\n",
    "        # ðŸ”’ clamp to valid vocab range to avoid CUDA device-side asserts\n",
    "        if next_id < 0 or next_id >= cfg.vocab_size:\n",
    "            next_id = EOS_ID\n",
    "\n",
    "        input_ids[0, pos] = next_id\n",
    "\n",
    "        if next_id == EOS_ID:\n",
    "            break\n",
    "\n",
    "    # 5) Decode and extract answer substring\n",
    "    text = decode_ids(input_ids[0].tolist())\n",
    "    if \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd631d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_trm_on_gsm8k(model: TinyMathTRM, data, n_eval: int = 200):\n",
    "    correct = 0\n",
    "    for i, ex in enumerate(data[:n_eval]):\n",
    "        q, gold = ex[\"question\"], ex[\"answer\"]\n",
    "        pred_str = generate_answer_trm(model, q)\n",
    "        pred_num = extract_numeric_answer(pred_str)\n",
    "        is_correct = (pred_num == gold)\n",
    "        correct += int(is_correct)\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            acc = correct / (i + 1)\n",
    "            print(f\"[TRM] {i+1}/{n_eval} acc={acc:.3f}\")\n",
    "            wandb.log({\"trm/acc_eval\": acc, \"trm/step_eval\": i + 1})\n",
    "\n",
    "    acc = correct / min(len(data), n_eval)\n",
    "    wandb.log({\"trm/final_acc\": acc})\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "095b4d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRM] 20/300 acc=0.000\n",
      "[TRM] 40/300 acc=0.000\n",
      "[TRM] 60/300 acc=0.000\n",
      "[TRM] 80/300 acc=0.000\n",
      "[TRM] 100/300 acc=0.000\n",
      "[TRM] 120/300 acc=0.000\n",
      "[TRM] 140/300 acc=0.000\n",
      "[TRM] 160/300 acc=0.000\n",
      "[TRM] 180/300 acc=0.000\n",
      "[TRM] 200/300 acc=0.000\n",
      "[TRM] 220/300 acc=0.000\n",
      "[TRM] 240/300 acc=0.000\n",
      "[TRM] 260/300 acc=0.000\n",
      "[TRM] 280/300 acc=0.000\n",
      "[TRM] 300/300 acc=0.000\n",
      "TRM acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Call after training:\n",
    "trm_acc = eval_trm_on_gsm8k(trm_model, eval_data, n_eval=cfg.max_eval_examples)\n",
    "print(\"TRM acc:\", trm_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
