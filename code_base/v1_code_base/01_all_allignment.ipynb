{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d297b69a",
   "metadata": {},
   "source": [
    "### Part 0 â€“ Imports, config, and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 0 â€“ Imports, config, and utilities\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Device & dtype ----\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# prefer bfloat16 on newer GPUs, else float16\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    default_dtype = torch.bfloat16\n",
    "else:\n",
    "    default_dtype = torch.float16\n",
    "\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Default dtype:\", default_dtype)\n",
    "\n",
    "\n",
    "# ---- Repro utilities ----\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5569da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 0.1 â€“ Global config (OPTIMIZED)\n",
    "# ============================================\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- Model names ---\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    audio_sample_rate: int = 16000\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "    # --- Dimensions ---\n",
    "    encoder_dim_vision: int = 768     # CLIP-base dim\n",
    "    encoder_dim_audio: int = 512      # Whisper-base dim\n",
    "    llm_hidden_size: int = 3584       # Qwen 7B dim\n",
    "    \n",
    "    # === IMPROVEMENT 1: Model Capacity ===\n",
    "    perceiver_dim: int = 768          # Increased from 512 to match Vision\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 6     # Increased from 2 to 6 (Deeper logic)\n",
    "    num_attn_heads: int = 8\n",
    "    mlp_ratio: float = 4.0\n",
    "\n",
    "    # --- Matryoshka loss (MRL) ---\n",
    "    use_mrl: bool = True\n",
    "    # Adjusted MRL dims to match new perceiver_dim\n",
    "    mrl_dims: Tuple[int, ...] = (128, 256, 512, 768, 3584) \n",
    "    mrl_temperature: float = 0.07\n",
    "    mrl_weight: float = 0.1\n",
    "\n",
    "    # === IMPROVEMENT 2: Training Dynamics ===\n",
    "    # Contrastive loss needs large batches. \n",
    "    # If you get OOM, lower to 32 but use Gradient Cache if possible.\n",
    "    batch_size_vision: int = 512       \n",
    "    batch_size_audio: int = 512       \n",
    "    \n",
    "    # Train longer (200 steps is too short)\n",
    "    max_train_steps_vision: int = 1000 \n",
    "    max_train_steps_audio: int = 2000 \n",
    "    \n",
    "    learning_rate: float = 5e-4       # Slightly higher LR for Perceiver\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # === IMPROVEMENT 3: Data Scale ===\n",
    "    # We need more than 3k samples for alignment\n",
    "    librispeech_max_samples: int = 20000 \n",
    "    vision_max_samples: int = 25000     \n",
    "    max_audio_duration_s: float =  50\n",
    "    \n",
    "    # --- Training Dynamics ---\n",
    "    max_grad_norm: float = 1.0        # Fix: Adds gradient clipping (prevents exploding gradients)\n",
    "    num_rounds: int = 1               # Fix: Moving this to config\n",
    "    grad_accum_steps: int = 1         # Fix: Explicitly define this (default was 4 in loop)\n",
    "\n",
    "    # --- Paths & Misc ---\n",
    "    vision_features_root: Path = Path(\"./features_vision\")\n",
    "    audio_features_root: Path = Path(\"./features_audio_librispeech\")\n",
    "    seed: int = 42\n",
    "    log_every_steps: int = 20\n",
    "    save_dir: Path = Path(\"./runs_perceiver_mrl_qwen\")\n",
    "    run_name: str = \"optimized_alignment_run\"\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "print(\"Optimized Config Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e223ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# W&B Init\n",
    "# --------------------------------------------\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "run_name = cfg.run_name if hasattr(cfg, \"run_name\") else \"tri_modal_alignment\"\n",
    "\n",
    "wandb.init(\n",
    "    project=getattr(cfg, \"wandb_project\", \"edgeglass-multimodal\"),\n",
    "    name=run_name,\n",
    "    config=asdict(cfg),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afbb49",
   "metadata": {},
   "source": [
    "### Phase-1: - Loading the Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 1 â€“ Load models: vision, audio, text (Qwen2.5-7B)\n",
    "# ============================================\n",
    "\n",
    "# ------------------------------\n",
    "# 1.1 Vision encoder (CLIP-style)\n",
    "# ------------------------------\n",
    "# For now we use CLIP as a simple vision encoder.\n",
    "# Later you can swap this for your PixMo vision encoder or precomputed features.\n",
    "\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "print(\"\\nLoading vision encoder:\", cfg.vision_model_name)\n",
    "vision_processor = CLIPImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "vision_model = CLIPVisionModel.from_pretrained(\n",
    "    cfg.vision_model_name,\n",
    "    torch_dtype=default_dtype,\n",
    "    device_map=None,\n",
    ").to(device)\n",
    "vision_model.eval()\n",
    "\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ðŸ”¥ Add this:\n",
    "cfg.encoder_dim_vision = vision_model.config.hidden_size\n",
    "print(\"Vision encoder_dim_vision:\", cfg.encoder_dim_vision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 1.2 Audio encoder (Whisper)\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nLoading audio encoder:\", cfg.audio_model_name)\n",
    "audio_processor = WhisperProcessor.from_pretrained(cfg.audio_model_name)\n",
    "audio_model = WhisperModel.from_pretrained(\n",
    "    cfg.audio_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None,\n",
    ").to(device)\n",
    "audio_model.eval()\n",
    "\n",
    "for p in audio_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "cfg.encoder_dim_audio = audio_model.config.d_model\n",
    "print(\"Audio hidden size:\", cfg.encoder_dim_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad61b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 1.3 Qwen2.5-7B (text encoder/decoder)\n",
    "# ------------------------------\n",
    "print(\"\\nLoading Qwen2.5-7B:\", cfg.llm_model_name)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.llm_model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "if qwen_tokenizer.pad_token is None:\n",
    "    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.llm_model_name,\n",
    "    torch_dtype=default_dtype,\n",
    "    device_map=None,  # <--- IMPORTANT: Disable auto split\n",
    "    # attn_implementation=\"flash_attention_2\" # <--- Enable for H200 speedup\n",
    ")\n",
    "qwen_model.eval()\n",
    "\n",
    "for p in qwen_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¥ Robust extraction: handle int / list / tuple\n",
    "hidden_size = getattr(qwen_model.config, \"hidden_size\", None)\n",
    "if hidden_size is None:\n",
    "    raise ValueError(\"Could not find hidden_size in Qwen config!\")\n",
    "\n",
    "if isinstance(hidden_size, (list, tuple)):\n",
    "    hidden_size = hidden_size[0]\n",
    "\n",
    "cfg.llm_hidden_size = int(hidden_size)\n",
    "\n",
    "print(\"Qwen hidden_size (from config):\", hidden_size)\n",
    "print(\"cfg.llm_hidden_size:\", cfg.llm_hidden_size, type(cfg.llm_hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we capture the full Qwen hidden dimension\n",
    "# For Qwen2.5-7B, this is likely 3584\n",
    "if hasattr(qwen_model.config, \"hidden_size\"):\n",
    "    cfg.llm_hidden_size = qwen_model.config.hidden_size\n",
    "else:\n",
    "    # Fallback if config structure is different\n",
    "    cfg.llm_hidden_size = 3584 \n",
    "\n",
    "print(f\"Full LLM Hidden Size: {cfg.llm_hidden_size}\")\n",
    "\n",
    "# FIX: The MRL loss MUST include the full embedding dimension.\n",
    "# Previous config was (128, 256, 512).\n",
    "# If we don't add 3584, the projector weights for indices 512 -> 3584 will never update.\n",
    "\n",
    "current_mrl_dims = list(cfg.mrl_dims)\n",
    "\n",
    "# Only append if it's not already there\n",
    "if cfg.llm_hidden_size not in current_mrl_dims:\n",
    "    current_mrl_dims.append(cfg.llm_hidden_size)\n",
    "\n",
    "# Sort and freeze back to tuple\n",
    "cfg.mrl_dims = tuple(sorted(current_mrl_dims))\n",
    "\n",
    "print(f\"âœ… Corrected MRL Dimensions: {cfg.mrl_dims}\")\n",
    "# Expected output: (128, 256, 512, 3584)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85102543",
   "metadata": {},
   "source": [
    "### Phase-2: - Adding MLP layer for MRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 2 â€“ Quick text embedding helper (for later MRL)\n",
    "# ============================================\n",
    "\n",
    "def encode_text_with_qwen(\n",
    "    texts: List[str],\n",
    "    max_length: int = 64,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize a batch of texts and return:\n",
    "        - input_ids\n",
    "        - attention_mask\n",
    "        - token_embeddings (from embedding layer, no LM forward yet)\n",
    "    \"\"\"\n",
    "    model_device = next(qwen_model.parameters()).device\n",
    "    \n",
    "    \n",
    "    enc = qwen_tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = enc.input_ids.to(model_device)\n",
    "    attn_mask = enc.attention_mask.to(model_device)\n",
    "\n",
    "    # (B, L, D_llm)\n",
    "    token_embs = qwen_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # (B, L, D)\n",
    "    # token_embs = qwen_model.get_input_embeddings()(enc.input_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"token_embs\": token_embs,\n",
    "    }\n",
    "\n",
    "print(\"Text embedding helper ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8523df",
   "metadata": {},
   "source": [
    "### Phase-3: - Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a453",
   "metadata": {},
   "source": [
    "#### Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 3 â€“ LibriSpeech (Streaming) Audioâ€“Text Dataset\n",
    "# ============================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nLoading LibriSpeech ASR (streaming mode)...\")\n",
    "\n",
    "# Load only train.clean.100 from the giant 124GB dataset\n",
    "librispeech_raw = load_dataset(\n",
    "    \"openslr/librispeech_asr\",\n",
    "    \"all\",\n",
    "    streaming=True,\n",
    "    split=\"train.clean.100\"\n",
    ")\n",
    "\n",
    "print(\"Loaded streaming dataset:\", librispeech_raw)\n",
    "\n",
    "# Disable automatic decoding â†’ we want raw bytes for librosa\n",
    "audio_stream = librispeech_raw.decode(False)\n",
    "\n",
    "# We will collect up to cfg.librispeech_max_samples\n",
    "max_samples = cfg.librispeech_max_samples  # rename in your config if needed\n",
    "subset = []\n",
    "\n",
    "print(f\"\\nTaking up to {max_samples} examples in streaming mode...\")\n",
    "\n",
    "for ex in audio_stream:\n",
    "    subset.append(ex)\n",
    "    if len(subset) >= max_samples:\n",
    "        break\n",
    "\n",
    "print(\"\\nSubset collected:\", len(subset))\n",
    "print(\"Keys:\", subset[0].keys())\n",
    "print(\"Example 0:\", subset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f01d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: convert LibriSpeech streaming example â†’ waveform\n",
    "def load_waveform_from_streaming_example(example, target_sr=16000):\n",
    "    audio_info = example[\"audio\"]\n",
    "\n",
    "    audio_bytes = audio_info[\"bytes\"]\n",
    "    if audio_bytes is None:\n",
    "        raise ValueError(\"No audio bytes in example.\")\n",
    "\n",
    "    # Convert raw bytes â†’ file-like object\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "\n",
    "    # librosa loads PCM data and resamples to target_sr\n",
    "    wav, sr = librosa.load(audio_file, sr=target_sr)\n",
    "\n",
    "    return wav, sr\n",
    "\n",
    "\n",
    "# Helper: compute duration in seconds\n",
    "def compute_duration(wav, sr):\n",
    "    return len(wav) / float(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec95398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll filter to keep only clips <= cfg.max_audio_duration_s\n",
    "filtered = []\n",
    "\n",
    "print(\"\\nFiltering by duration â‰¤\", cfg.max_audio_duration_s, \"seconds...\")\n",
    "\n",
    "for ex in subset:\n",
    "    wav, sr = load_waveform_from_streaming_example(ex, cfg.audio_sample_rate)\n",
    "    dur = compute_duration(wav, sr)\n",
    "\n",
    "    # if dur <= cfg.max_audio_duration_s:\n",
    "    if True:\n",
    "        filtered.append({\n",
    "            \"waveform\": wav,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"duration\": dur,\n",
    "            \"text\": ex[\"text\"]\n",
    "        })\n",
    "print(\"After duration filtering:\", len(filtered), \"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShowing a few filtered samples...\")\n",
    "for i in range(min(5, len(filtered))):\n",
    "    ex = filtered[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(\"  Duration:\", round(ex[\"duration\"], 2), \"s\")\n",
    "    print(\"  Transcript:\", ex[\"text\"])\n",
    "    print(\"  Waveform shape:\", ex[\"waveform\"].shape)\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# New PixmoVisionDataset (uses HF 'image' column if available)\n",
    "# ============================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "print(\"\\nLoading PixMo-Cap visionâ€“text dataset (allenai/pixmo-cap)...\")\n",
    "\n",
    "pixmo_raw = load_dataset(\"allenai/pixmo-cap\", split=\"train\")\n",
    "print(\"PixMo-Cap split size:\", len(pixmo_raw))\n",
    "print(\"PixMo columns:\", pixmo_raw.column_names)\n",
    "\n",
    "# We only need a small subset for the POC\n",
    "vision_max = getattr(cfg, \"vision_max_samples\", 2048)\n",
    "if len(pixmo_raw) > vision_max:\n",
    "    pixmo_subset = pixmo_raw.shuffle(seed=cfg.seed).select(range(vision_max))\n",
    "else:\n",
    "    pixmo_subset = pixmo_raw\n",
    "\n",
    "print(\"PixMo subset size:\", len(pixmo_subset))\n",
    "\n",
    "# Fields from the dataset card:\n",
    "#  - \"image_url\": URL to the image\n",
    "#  - \"caption\": long caption text\n",
    "img_col = \"image_url\"\n",
    "txt_col = \"caption\"\n",
    "\n",
    "cols = pixmo_raw.column_names\n",
    "HAS_IMAGE_COL = \"image\" in cols\n",
    "\n",
    "if HAS_IMAGE_COL:\n",
    "    img_col = \"image\"\n",
    "else:\n",
    "    img_col = \"image_url\"\n",
    "\n",
    "txt_col = \"caption\"\n",
    "\n",
    "print(f\"Using image column: {img_col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed385814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PixmoVisionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    On-the-fly image loading + CLIP feature extraction.\n",
    "\n",
    "    If 'image' column exists: uses HF-managed images (no manual HTTP).\n",
    "    Else: falls back to 'image_url' with robust skipping of bad URLs.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"features\": Tensor(T, d_vision),\n",
    "          \"text\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, vision_model, vision_processor, max_retries: int = 5):\n",
    "        self.ds = hf_dataset\n",
    "        self.vision_model = vision_model\n",
    "        self.vision_processor = vision_processor\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def _load_image_from_url(self, url: str) -> Image.Image:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        # do NOT let this propagate; we'll catch in __getitem__\n",
    "        resp.raise_for_status()\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _encode_image(self, img: Image.Image):\n",
    "        proc = self.vision_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = proc[\"pixel_values\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.vision_model(pixel_values=pixel_values)\n",
    "            # (1, T, d_vision)\n",
    "            feats = out.last_hidden_state.squeeze(0).to(\"cpu\")  # (T, d_vision)\n",
    "        return feats\n",
    "\n",
    "    def _get_example(self, idx: int):\n",
    "        ex = self.ds[idx]\n",
    "        caption = ex[txt_col]\n",
    "\n",
    "        if HAS_IMAGE_COL:\n",
    "            # HF has already downloaded/cached images; this is usually a PIL.Image\n",
    "            img = ex[img_col]\n",
    "            if not isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "        else:\n",
    "            url = ex[img_col]\n",
    "            img = self._load_image_from_url(url)\n",
    "\n",
    "        feats = self._encode_image(img)\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": caption,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Try up to max_retries times with different indices if something fails\n",
    "        (HTTP error, decoding error, etc).\n",
    "        \"\"\"\n",
    "        n = len(self.ds)\n",
    "        attempt = 0\n",
    "        cur_idx = idx\n",
    "\n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                return self._get_example(cur_idx)\n",
    "            except Exception as e:\n",
    "                # print(f\"[PixmoVisionDataset] Failed idx={cur_idx}, attempt={attempt+1}, err={e}\")\n",
    "                attempt += 1\n",
    "                cur_idx = (cur_idx + 1) % n\n",
    "\n",
    "        # Final fallback: try random indices\n",
    "        for _ in range(self.max_retries):\n",
    "            j = random.randint(0, n - 1)\n",
    "            try:\n",
    "                return self._get_example(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        raise RuntimeError(\"PixmoVisionDataset: could not load any valid images after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde847ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66bea03",
   "metadata": {},
   "source": [
    "### Part-4:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 4 â€“ Audio features dataset (FIXED)\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import transforms as T_audio\n",
    "\n",
    "def whisper_encode_sequence(wav: np.ndarray, sr: int, duration_sec: float):\n",
    "    \"\"\"\n",
    "    Encodes audio and SLICES out the padding (Crucial Fix).\n",
    "    \"\"\"\n",
    "    # 1. Process raw waveform -> log-Mel (pad to 30s internally)\n",
    "    inputs = audio_processor(\n",
    "        wav,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_features = inputs[\"input_features\"].to(device) # (1, 80, 3000)\n",
    "\n",
    "    # 2. SpecAugment (only during training)\n",
    "    if audio_model.training:\n",
    "        freq_mask = T_audio.FrequencyMasking(freq_mask_param=15)\n",
    "        time_mask = T_audio.TimeMasking(time_mask_param=35)\n",
    "        input_features = freq_mask(input_features)\n",
    "        input_features = time_mask(input_features)\n",
    "\n",
    "    # 3. Encoder Forward\n",
    "    with torch.no_grad():\n",
    "        enc_out = audio_model.encoder(input_features)\n",
    "        hidden = enc_out.last_hidden_state.squeeze(0) # (1500, 512)\n",
    "\n",
    "    # === CRITICAL FIX: Slice to actual duration ===\n",
    "    # Whisper frame rate is 50Hz (20ms per frame)\n",
    "    # 10 seconds of audio = 500 frames. The rest (1000 frames) is garbage padding.\n",
    "    valid_frames = int(duration_sec * 50)\n",
    "    \n",
    "    # Safety clamp (min 1 frame, max 1500)\n",
    "    valid_frames = max(1, min(valid_frames, 1500))\n",
    "    \n",
    "    # Return only valid frames\n",
    "    feats = hidden[:valid_frames, :].to(torch.float16).cpu() \n",
    "    return feats\n",
    "\n",
    "class LibriSpeechAudioDataset(Dataset):\n",
    "    def __init__(self, examples, max_len: int | None = None):\n",
    "        self.examples = examples\n",
    "        if max_len is not None and max_len < len(examples):\n",
    "            self.examples = examples[:max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ex = self.examples[idx]\n",
    "        wav = ex[\"waveform\"]\n",
    "        sr = ex[\"sampling_rate\"]\n",
    "        dur = ex[\"duration\"]\n",
    "        \n",
    "        # === FIX 2: Text Normalization ===\n",
    "        # LibriSpeech is ALL CAPS. LLMs expect Normal case.\n",
    "        raw_text = ex[\"text\"]\n",
    "        text = raw_text.lower().capitalize()\n",
    "\n",
    "        # Pass duration to the encoder\n",
    "        feats = whisper_encode_sequence(wav, sr, duration_sec=dur)\n",
    "\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": text,\n",
    "            \"duration\": dur,\n",
    "        }\n",
    "\n",
    "# Re-init dataset\n",
    "audio_max = getattr(cfg, \"librispeech_max_samples\", len(filtered))\n",
    "audio_dataset = LibriSpeechAudioDataset(filtered, max_len=audio_max)\n",
    "\n",
    "print(\"Audio dataset fixed (Padding Slicing + Text Norm).\")\n",
    "print(\"Example 0 features shape:\", audio_dataset[0][\"features\"].shape) # Should NOT be (1500, 512) anymore unless audio is exactly 30s\n",
    "print(\"Example 0 text:\", audio_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049573b2",
   "metadata": {},
   "source": [
    "### Part-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 5 â€“ Unified Adapters, Perceiver Resampler & Projector\n",
    "# ============================================\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5.0 â€“ Ensure Perceiver hyperparams exist in cfg\n",
    "# --------------------------------------------\n",
    "\n",
    "if not hasattr(cfg, \"num_perceiver_layers\"):\n",
    "    cfg.num_perceiver_layers = 2          # depth of Perceiver\n",
    "if not hasattr(cfg, \"num_attn_heads\"):\n",
    "    cfg.num_attn_heads = 8                # multi-head attention\n",
    "if not hasattr(cfg, \"mlp_ratio\"):\n",
    "    cfg.mlp_ratio = 4.0                   # width of MLP inside Perceiver\n",
    "\n",
    "print(\"Perceiver config:\")\n",
    "print(\"  perceiver_dim:\", cfg.perceiver_dim)\n",
    "print(\"  num_latents:\", cfg.num_latents)\n",
    "print(\"  num_perceiver_layers:\", cfg.num_perceiver_layers)\n",
    "print(\"  num_attn_heads:\", cfg.num_attn_heads)\n",
    "print(\"  mlp_ratio:\", cfg.mlp_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.1 â€“ Modality adapters: vision & audio â†’ perceiver_dim\n",
    "# --------------------------------------------\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear adapter: maps encoder dim â†’ perceiver_dim.\n",
    "    Used separately for vision and audio encoders.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, in_dim) or (T, in_dim)\n",
    "        returns: (B, T, out_dim) or (T, out_dim)\n",
    "        \"\"\"\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "vision_adapter = ModalityAdapter(cfg.encoder_dim_vision, cfg.perceiver_dim).to(device)\n",
    "audio_adapter  = ModalityAdapter(cfg.encoder_dim_audio,  cfg.perceiver_dim).to(device)\n",
    "\n",
    "print(\"\\nAdapters created:\")\n",
    "print(\"  VisionAdapter:\", vision_adapter)\n",
    "print(\"  AudioAdapter:\", audio_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.2 â€“ Perceiver building blocks\n",
    "# --------------------------------------------\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One Perceiver layer:\n",
    "      1) Cross-attention: latents query encoder tokens\n",
    "      2) Self-attention on latents\n",
    "      3) MLP on latents\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens    = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = FeedForward(dim, mlp_ratio=mlp_ratio)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        latents: torch.Tensor,   # (B, L, D)\n",
    "        tokens: torch.Tensor,    # (B, T, D)\n",
    "        token_mask: torch.Tensor | None = None,  # (B, T) bool, 1=valid\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_mask: bool mask, True for valid tokens. Will be converted to key_padding_mask.\n",
    "        \"\"\"\n",
    "        B, L, D = latents.shape\n",
    "        _, T, _ = tokens.shape\n",
    "\n",
    "        # LayerNorm\n",
    "        q = self.ln_latents_1(latents)   # (B, L, D)\n",
    "        kv = self.ln_tokens(tokens)      # (B, T, D)\n",
    "\n",
    "        # key_padding_mask: True for *ignored* positions\n",
    "        key_padding_mask = None\n",
    "        if token_mask is not None:\n",
    "            # token_mask: True=valid â†’ invert\n",
    "            key_padding_mask = ~token_mask.bool()   # (B, T)\n",
    "\n",
    "        # 1) Cross-attention: latents query the encoder tokens\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            query=q,\n",
    "            key=kv,\n",
    "            value=kv,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        latents = latents + attn_out\n",
    "\n",
    "        # 2) Self-attention on latents\n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_attn_out, _ = self.self_attn(\n",
    "            query=q2,\n",
    "            key=q2,\n",
    "            value=q2,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        latents = latents + self_attn_out\n",
    "\n",
    "        # 3) MLP on latents\n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "\n",
    "        return latents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent array Z âˆˆ R^{L Ã— D}, cross-attends to encoder tokens X âˆˆ R^{B Ã— T Ã— D}\n",
    "    to produce a fixed number of latent tokens per example.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_latents: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_latents = num_latents\n",
    "\n",
    "        # Learned latent array (L, D)\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "\n",
    "        # Stack of Perceiver layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads=num_heads, mlp_ratio=mlp_ratio)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: torch.Tensor,         # (B, T, D)\n",
    "        token_mask: torch.Tensor | None = None,  # (B, T) bool\n",
    "    ) -> torch.Tensor:\n",
    "        B, T, D = tokens.shape\n",
    "        assert D == self.dim, f\"Expected dim={self.dim}, got {D}\"\n",
    "\n",
    "        # Expand latent array to batch: (B, L, D)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "\n",
    "        return latents  # (B, L, D)\n",
    "\n",
    "\n",
    "perceiver = PerceiverResampler(\n",
    "    dim=cfg.perceiver_dim,\n",
    "    num_latents=cfg.num_latents,\n",
    "    num_layers=cfg.num_perceiver_layers,\n",
    "    num_heads=cfg.num_attn_heads,\n",
    "    mlp_ratio=cfg.mlp_ratio,\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nPerceiverResampler created:\")\n",
    "print(perceiver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 5.3 â€“ Projector: Perceiver â†’ Qwen hidden space\n",
    "# --------------------------------------------\n",
    "\n",
    "projector = nn.Linear(cfg.perceiver_dim, cfg.llm_hidden_size).to(device)\n",
    "print(\"\\nProjector created:\")\n",
    "print(\"  projector:\", projector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 5.4 â€“ Quick shape sanity check with fake batch\n",
    "# --------------------------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    B = 2\n",
    "    # Fake vision sequence: (B, T_v, d_vision)\n",
    "    T_v = 32\n",
    "    fake_vision = torch.randn(B, T_v, cfg.encoder_dim_vision, device=device, dtype=default_dtype)\n",
    "    fake_mask   = torch.ones(B, T_v, dtype=torch.bool, device=device)\n",
    "\n",
    "    # 1) Adapt to perceiver_dim\n",
    "    v_tokens = vision_adapter(fake_vision)           # (B, T_v, D_perc)\n",
    "\n",
    "    # 2) Perceiver latents\n",
    "    latents = perceiver(v_tokens, fake_mask)         # (B, L, D_perc)\n",
    "\n",
    "    # 3) Project to Qwen hidden dim\n",
    "    z_llm = projector(latents)                       # (B, L, D_llm)\n",
    "\n",
    "print(\"\\nSanity check:\")\n",
    "print(\"  v_tokens shape:\", v_tokens.shape)\n",
    "print(\"  latents shape:\", latents.shape)\n",
    "print(\"  z_llm shape:\", z_llm.shape)\n",
    "print(\"Done Part 5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dddf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n== Sanity check dims ==\")\n",
    "print(\"encoder_dim_vision:\", cfg.encoder_dim_vision, type(cfg.encoder_dim_vision))\n",
    "print(\"encoder_dim_audio:\", cfg.encoder_dim_audio, type(cfg.encoder_dim_audio))\n",
    "print(\"perceiver_dim:\", cfg.perceiver_dim, type(cfg.perceiver_dim))\n",
    "print(\"llm_hidden_size:\", cfg.llm_hidden_size, type(cfg.llm_hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec960b4",
   "metadata": {},
   "source": [
    "### Part-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e969f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 6 â€“ Collate, Matryoshka loss, Forward Step\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.1 â€“ Collate functions for vision & audio\n",
    "# --------------------------------------------\n",
    "\n",
    "def collate_features_with_text(batch):\n",
    "    \"\"\"\n",
    "    Generic collate:\n",
    "        batch: list of dicts with\n",
    "            \"features\": (T_i, D_enc)\n",
    "            \"text\": str\n",
    "            (optionally \"duration\")\n",
    "    Returns:\n",
    "        encoder_feats: (B, T_max, D_enc)\n",
    "        encoder_mask:  (B, T_max) bool\n",
    "        texts: list[str]\n",
    "        durations: list[float] | None\n",
    "    \"\"\"\n",
    "    feats = [torch.as_tensor(ex[\"features\"], dtype=default_dtype) for ex in batch]  # list[(T_i, D_enc)]\n",
    "    lengths = [f.size(0) for f in feats]\n",
    "\n",
    "    # Pad to max length\n",
    "    encoder_feats = pad_sequence(feats, batch_first=True)  # (B, T_max, D_enc)\n",
    "\n",
    "    B, T_max, _ = encoder_feats.shape\n",
    "    encoder_mask = torch.zeros(B, T_max, dtype=torch.bool)\n",
    "    for i, L in enumerate(lengths):\n",
    "        encoder_mask[i, :L] = True\n",
    "\n",
    "    texts = [ex[\"text\"] for ex in batch]\n",
    "    durations = [ex.get(\"duration\", None) for ex in batch]\n",
    "\n",
    "    return {\n",
    "        \"encoder_feats\": encoder_feats,    # (B, T_max, D_enc)\n",
    "        \"encoder_mask\": encoder_mask,      # (B, T_max)\n",
    "        \"texts\": texts,\n",
    "        \"durations\": durations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e869e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_dataset = PixmoVisionDataset(\n",
    "    pixmo_subset,\n",
    "    vision_model=vision_model,\n",
    "    vision_processor=vision_processor,\n",
    ")\n",
    "\n",
    "print(\"Vision dataset ready (HF image-based if available).\")\n",
    "sample_v = vision_dataset[0]\n",
    "print(\"  features shape:\", sample_v[\"features\"].shape)\n",
    "print(\"  text snippet:\", sample_v[\"text\"][:120], \"...\")\n",
    "\n",
    "vision_loader = DataLoader(\n",
    "    vision_dataset,\n",
    "    batch_size=cfg.batch_size_vision,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision & audio loaders (youâ€™ll use these in Part 7 for training)\n",
    "vision_loader = DataLoader(\n",
    "    vision_dataset,\n",
    "    batch_size=cfg.batch_size_vision,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n",
    "\n",
    "audio_loader = DataLoader(\n",
    "    audio_dataset,\n",
    "    batch_size=cfg.batch_size_audio,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n",
    "\n",
    "print(\"Vision loader & audio loader ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35553fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 6.2 â€“ Matryoshka (MRL) contrastive loss\n",
    "# --------------------------------------------\n",
    "\n",
    "# def matryoshka_contrastive_loss(\n",
    "#     z_mod: torch.Tensor,    # (B, D)\n",
    "#     z_txt: torch.Tensor,    # (B, D)\n",
    "#     trunc_dims: tuple[int, ...],\n",
    "#     temperature: float = 0.07,\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Matryoshka-style symmetric InfoNCE at multiple truncation dims.\n",
    "\n",
    "#     For each d in trunc_dims:\n",
    "#       - truncate embeddings to first d dims\n",
    "#       - L2-normalize\n",
    "#       - compute similarity matrix\n",
    "#       - compute symmetric cross-entropy (modâ†’text and textâ†’mod)\n",
    "#     Then average across all dims.\n",
    "#     \"\"\"\n",
    "#     assert z_mod.shape == z_txt.shape\n",
    "#     B, D = z_mod.shape\n",
    "#     max_d = max(trunc_dims)\n",
    "#     assert max_d <= D, f\"Max trunc dim {max_d} exceeds embedding dim {D}\"\n",
    "\n",
    "#     losses = []\n",
    "#     targets = torch.arange(B, device=z_mod.device)\n",
    "\n",
    "#     for d in trunc_dims:\n",
    "#         zm = F.normalize(z_mod[:, :d], dim=-1)  # (B, d)\n",
    "#         zt = F.normalize(z_txt[:, :d], dim=-1)  # (B, d)\n",
    "\n",
    "#         logits = zm @ zt.T / temperature        # (B, B)\n",
    "#         loss_m2t = F.cross_entropy(logits, targets)\n",
    "#         loss_t2m = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "#         losses.append(0.5 * (loss_m2t + loss_t2m))\n",
    "\n",
    "#     return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.2 â€“ Matryoshka (MRL) contrastive loss\n",
    "# --------------------------------------------\n",
    "\n",
    "def matryoshka_contrastive_loss(\n",
    "    z_mod: torch.Tensor,    # (B, D)\n",
    "    z_txt: torch.Tensor,    # (B, D)\n",
    "    trunc_dims: tuple[int, ...],\n",
    "    temperature: float = 0.07,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Matryoshka-style symmetric InfoNCE at multiple truncation dims.\n",
    "\n",
    "    For each d in trunc_dims:\n",
    "      - truncate embeddings to first d dims\n",
    "      - L2-normalize\n",
    "      - compute similarity matrix\n",
    "      - compute symmetric cross-entropy (modâ†’text and textâ†’mod)\n",
    "    Then average across all dims.\n",
    "    \"\"\"\n",
    "    assert z_mod.shape == z_txt.shape, \"z_mod and z_txt must have same shape\"\n",
    "    B, D = z_mod.shape\n",
    "\n",
    "    # Ensure both embeddings share the same dtype (important under bf16 mixed precision)\n",
    "    if z_mod.dtype != z_txt.dtype:\n",
    "        z_txt = z_txt.to(z_mod.dtype)\n",
    "\n",
    "    # Sanity check truncation dims\n",
    "    max_d = max(trunc_dims)\n",
    "    assert max_d <= D, f\"Max trunc dim {max_d} exceeds embedding dim {D}\"\n",
    "\n",
    "    # Make temperature a tensor on the right device/dtype\n",
    "    temp = torch.as_tensor(temperature, device=z_mod.device, dtype=z_mod.dtype)\n",
    "\n",
    "    losses = []\n",
    "    targets = torch.arange(B, device=z_mod.device)\n",
    "\n",
    "    for d in trunc_dims:\n",
    "        # 1) Truncate and L2-normalize\n",
    "        zm = F.normalize(z_mod[:, :d], dim=-1)  # (B, d)\n",
    "        zt = F.normalize(z_txt[:, :d], dim=-1)  # (B, d)\n",
    "\n",
    "        # 2) Similarity matrix\n",
    "        logits = zm @ zt.T / temp               # (B, B)\n",
    "\n",
    "        # 3) Symmetric cross-entropy\n",
    "        loss_m2t = F.cross_entropy(logits, targets)\n",
    "        loss_t2m = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "        losses.append(0.5 * (loss_m2t + loss_t2m))\n",
    "\n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 6.3 â€“ Helpers for global text & modality embeddings\n",
    "# --------------------------------------------\n",
    "\n",
    "def pooled_text_embedding(texts: list[str], max_length: int = 64) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        h_text: (B, D_llm) pooled text embeddings\n",
    "        text_tok_info: dict with token_embs, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    tok_out = encode_text_with_qwen(texts, max_length=max_length)  # uses qwen_model embedding layer\n",
    "    token_embs = tok_out[\"token_embs\"]          # (B, L, D_llm)\n",
    "    attn_mask = tok_out[\"attention_mask\"]      # (B, L)\n",
    "\n",
    "    mask = attn_mask.unsqueeze(-1).to(token_embs.device)  # (B, L, 1)\n",
    "        \n",
    "    # masked mean-pooling over tokens\n",
    "    mask = attn_mask.unsqueeze(-1)             # (B, L, 1)\n",
    "    denom = mask.sum(dim=1).clamp_min(1)       # (B, 1)\n",
    "    h_text = (token_embs * mask).sum(dim=1) / denom  # (B, D_llm)\n",
    "\n",
    "\n",
    "    return h_text, tok_out\n",
    "\n",
    "\n",
    "def pooled_modality_embedding(latent_tokens_llm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    latent_tokens_llm: (B, L, D_llm) from projector(perceiver(...))\n",
    "    Returns:\n",
    "        h_mod: (B, D_llm)\n",
    "    \"\"\"\n",
    "    return latent_tokens_llm.mean(dim=1)  # simple mean over latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4fc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 6.4 â€“ Unified alignment forward step (vision or audio)\n",
    "# --------------------------------------------\n",
    "\n",
    "# def forward_alignment_step(\n",
    "#     batch: dict,\n",
    "#     modality: str = \"vision\",   # \"vision\" or \"audio\"\n",
    "# ) -> tuple[torch.Tensor, dict]:\n",
    "#     \"\"\"\n",
    "#     One step of alignment loss for a batch.\n",
    "\n",
    "#     batch keys from collate_features_with_text:\n",
    "#         - encoder_feats: (B, T, D_enc)\n",
    "#         - encoder_mask:  (B, T) bool\n",
    "#         - texts: list[str]\n",
    "\n",
    "#     modality:\n",
    "#         \"vision\" â†’ use vision_adapter\n",
    "#         \"audio\"  â†’ use audio_adapter\n",
    "#     \"\"\"\n",
    "#     encoder_feats = batch[\"encoder_feats\"].to(device)   # (B, T, D_enc)\n",
    "#     encoder_mask  = batch[\"encoder_mask\"].to(device)    # (B, T)\n",
    "#     texts         = batch[\"texts\"]                      # list[str]\n",
    "\n",
    "#     # 1) Modality adapter â†’ Perceiver dim\n",
    "#     if modality == \"vision\":\n",
    "#         tokens = vision_adapter(encoder_feats)          # (B, T, D_perc)\n",
    "#     elif modality == \"audio\":\n",
    "#         tokens = audio_adapter(encoder_feats)           # (B, T, D_perc)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "#     # 2) Perceiver resampler â†’ latent tokens\n",
    "#     latents = perceiver(tokens, encoder_mask)           # (B, L, D_perc)\n",
    "\n",
    "#     # 3) Project to Qwen hidden space\n",
    "#     z_llm = projector(latents)                          # (B, L, D_llm)\n",
    "\n",
    "#     # 4) Global modality embedding (for MRL)\n",
    "#     h_mod = pooled_modality_embedding(z_llm)            # (B, D_llm)\n",
    "\n",
    "#     # 5) Global text embedding from Qwen\n",
    "#     h_txt, tok_info = pooled_text_embedding(texts, max_length=64)  # (B, D_llm)\n",
    "\n",
    "#     # 6) Matryoshka contrastive loss\n",
    "#     mrl_loss = matryoshka_contrastive_loss(\n",
    "#         h_mod,\n",
    "#         h_txt,\n",
    "#         trunc_dims=cfg.mrl_dims,\n",
    "#         temperature=cfg.mrl_temperature,\n",
    "#     )\n",
    "\n",
    "#     # For now we focus on alignment-only POC â†’ total_loss = mrl_loss\n",
    "#     total_loss = mrl_loss\n",
    "\n",
    "#     metrics = {\n",
    "#         \"loss\":        float(total_loss.detach().cpu()),\n",
    "#         \"mrl_loss\":    float(mrl_loss.detach().cpu()),\n",
    "#         \"modality\":    modality,\n",
    "#         \"batch_size\":  int(h_mod.size(0)),\n",
    "#     }\n",
    "\n",
    "#     return total_loss, metrics\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.4 â€“ Unified alignment forward step (Fixed)\n",
    "# --------------------------------------------\n",
    "\n",
    "def forward_alignment_step(\n",
    "    batch: dict,\n",
    "    accelerator,\n",
    "    modality: str = \"vision\",   # \"vision\" or \"audio\"\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    One step of alignment loss for a batch.\n",
    "    \"\"\"\n",
    "    encoder_feats = batch[\"encoder_feats\"].to(device)   # (B, T, D_enc)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)    # (B, T)\n",
    "    texts         = batch[\"texts\"]                      # list[str]\n",
    "\n",
    "    # 1) Modality adapter -> Perceiver dim\n",
    "    if modality == \"vision\":\n",
    "        tokens = vision_adapter(encoder_feats)          # (B, T, D_perc)\n",
    "    elif modality == \"audio\":\n",
    "        tokens = audio_adapter(encoder_feats)           # (B, T, D_perc)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    # 2) Perceiver resampler -> latent tokens\n",
    "    latents = perceiver(tokens, encoder_mask)           # (B, L, D_perc)\n",
    "\n",
    "    # 3) Project to Qwen hidden space\n",
    "    z_llm_local = projector(latents)                          # (B, L, D_llm)\n",
    "    \n",
    "    # Safety check: Ensure projector output matches config\n",
    "    assert z_llm.shape[-1] == cfg.llm_hidden_size, \\\n",
    "        f\"Projector output {z_llm.shape[-1]} != Config {cfg.llm_hidden_size}\"\n",
    "\n",
    "    # 2. Get Local Embeddings\n",
    "    h_mod_local = pooled_modality_embedding(z_llm_local)\n",
    "    h_txt_local, _ = pooled_text_embedding(texts, max_length=64)\n",
    "    \n",
    "    \n",
    "    # 4) Global modality embedding (Mean Pooling over latents)\n",
    "    h_mod = pooled_modality_embedding(z_llm)            # (B, D_llm)\n",
    "\n",
    "    # 5) Global text embedding from Qwen (Pre-computed or on-the-fly)\n",
    "    # Note: encode_text_with_qwen returns raw embeddings, not LM outputs, \n",
    "    # which is correct for alignment.\n",
    "    h_txt, tok_info = pooled_text_embedding(texts, max_length=64)  # (B, D_llm)\n",
    "\n",
    "    h_mod_global = accelerator.gather(h_mod_local) \n",
    "    h_txt_global = accelerator.gather(h_txt_local)\n",
    "    \n",
    "    # 6) Matryoshka contrastive loss\n",
    "    # We pass the corrected cfg.mrl_dims here (e.g., 128, 256, 512, 3584)\n",
    "    mrl_loss = matryoshka_contrastive_loss(\n",
    "        h_mod_global,\n",
    "        h_txt_global,\n",
    "        trunc_dims=cfg.mrl_dims,\n",
    "        temperature=cfg.mrl_temperature,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\":        float(mrl_loss.detach().cpu()),\n",
    "        \"mrl_loss\":    float(mrl_loss.detach().cpu()),\n",
    "        \"modality\":    modality,\n",
    "        \"batch_size\":  int(h_mod.size(0)),\n",
    "    }\n",
    "\n",
    "    return mrl_loss, metrics\n",
    "print(\"\\nPart 6 ready: collate, MRL, and forward_alignment_step defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0eb8",
   "metadata": {},
   "source": [
    "### Part-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 7 â€“ Training loops (vision & audio alignment)\n",
    "# ============================================\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 7.0 â€“ Collect trainable parameters\n",
    "# --------------------------------------------\n",
    "\n",
    "# We ONLY train:\n",
    "#   - vision_adapter\n",
    "#   - audio_adapter\n",
    "#   - perceiver\n",
    "#   - projector\n",
    "# Qwen, CLIP, and Whisper are frozen.\n",
    "\n",
    "trainable_modules = nn.ModuleList([\n",
    "    vision_adapter,\n",
    "    audio_adapter,\n",
    "    perceiver,\n",
    "    projector,\n",
    "])\n",
    "\n",
    "for name, p in trainable_modules.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"Trainable:\", name, p.shape)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [p for p in trainable_modules.parameters() if p.requires_grad],\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizer ready with\", sum(p.numel() for p in trainable_modules.parameters() if p.requires_grad), \"trainable params.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(trainable_modules, log=\"all\", log_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 7.1 â€“ Generic training epoch for one modality\n",
    "# --------------------------------------------\n",
    "def train_one_epoch(\n",
    "    dataloader,\n",
    "    modality,\n",
    "    max_steps,\n",
    "    optimizer,      # Passed in\n",
    "    scheduler,      # Passed in\n",
    "    accelerator,\n",
    "    log_prefix=\"\"\n",
    "):\n",
    "    trainable_modules.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # We assume dataloader is already prepared by accelerator\n",
    "    pbar = tqdm(dataloader, total=max_steps, desc=f\"{log_prefix}train-{modality}\", leave=False)\n",
    "\n",
    "    for step, batch in enumerate(pbar, start=1):\n",
    "        if step > max_steps:\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward step\n",
    "        loss, metrics = forward_alignment_step(batch, accelerator, modality=modality)\n",
    "        \n",
    "        # Backward step (Accelerate handles the scaling/unscaling)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if cfg.max_grad_norm is not None:\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(trainable_modules.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += metrics[\"loss\"]\n",
    "        num_batches += 1\n",
    "        avg_loss = running_loss / num_batches\n",
    "\n",
    "        # W&B logging (only on main process)\n",
    "        if accelerator.is_main_process:\n",
    "            wandb.log({\n",
    "                f\"{modality}/train/loss\": metrics[\"loss\"],\n",
    "                f\"{modality}/train/avg_loss\": avg_loss,\n",
    "                f\"{modality}/train/mrl_loss\": metrics[\"mrl_loss\"],\n",
    "            })\n",
    "            \n",
    "            if step % cfg.log_every_steps == 0:\n",
    "                pbar.set_postfix({\"loss\": f\"{metrics['loss']:.4f}\"})\n",
    "\n",
    "    return running_loss / max(1, num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 7.2 â€“ Simple retrieval eval (sanity check)\n",
    "# --------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_retrieval(\n",
    "    dataset,\n",
    "    modality: str,\n",
    "    num_samples: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Very small retrieval sanity check:\n",
    "      - take num_samples examples\n",
    "      - compute modality & text embeddings\n",
    "      - compute similarity matrix\n",
    "      - report Recall@1 (how often correct text is most similar)\n",
    "\n",
    "    Works for both vision_dataset and audio_dataset.\n",
    "    \"\"\"\n",
    "    trainable_modules.eval()\n",
    "\n",
    "    # Build a tiny batch with collate\n",
    "    from math import ceil\n",
    "    B = min(num_samples, len(dataset))\n",
    "    # Manual batching using DataLoader with our collate\n",
    "    tmp_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=B,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_features_with_text,\n",
    "    )\n",
    "    batch = next(iter(tmp_loader))\n",
    "\n",
    "    # Forward until we get h_mod and h_txt (without loss)\n",
    "    encoder_feats = batch[\"encoder_feats\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    texts         = batch[\"texts\"]\n",
    "\n",
    "    if modality == \"vision\":\n",
    "        tokens = vision_adapter(encoder_feats)\n",
    "    elif modality == \"audio\":\n",
    "        tokens = audio_adapter(encoder_feats)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    latents = perceiver(tokens, encoder_mask)\n",
    "    z_llm   = projector(latents)\n",
    "\n",
    "    h_mod = pooled_modality_embedding(z_llm)      # (B, D_llm)\n",
    "    h_txt, _ = pooled_text_embedding(texts)      # (B, D_llm)\n",
    "\n",
    "    # Normalize\n",
    "    h_mod = F.normalize(h_mod, dim=-1)\n",
    "    h_txt = F.normalize(h_txt, dim=-1)\n",
    "\n",
    "    # Similarity matrix (B, B)\n",
    "    sims = h_mod @ h_txt.T\n",
    "\n",
    "    # For each modality embedding, check if its diagonal text is top-1\n",
    "    ranks = sims.argsort(dim=-1, descending=True)\n",
    "    correct_top1 = (ranks[:, 0] == torch.arange(B, device=ranks.device)).float().mean().item()\n",
    "\n",
    "    print(f\"[Eval {modality}] Retrieval Recall@1 on {B} samples: {correct_top1:.3f}\")\n",
    "    return correct_top1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6590c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    # 1. Initialize Accelerator FIRST\n",
    "    # We must create it here so each process gets its own instance.\n",
    "    accelerator = Accelerator(mixed_precision=\"no\", log_with=\"wandb\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    # --- move all modules to device BEFORE training ---\n",
    "    perceiver.to(device)\n",
    "    projector.to(device)\n",
    "    # pooled_modality_embedding.to(device)\n",
    "    qwen_model.to(device)   # <-- IMPORTANT\n",
    "\n",
    "    # Now we can safely use it\n",
    "    print(f\"Process {accelerator.process_index} starting...\")\n",
    "\n",
    "    # 2. Setup Optimizer (Must be created fresh inside the function)\n",
    "    optimizer = AdamW(\n",
    "        [p for p in trainable_modules.parameters() if p.requires_grad],\n",
    "        lr=cfg.learning_rate,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "    )\n",
    "\n",
    "    # 3. Setup Scheduler\n",
    "    total_steps = (cfg.max_train_steps_vision + cfg.max_train_steps_audio) * cfg.num_rounds\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 4. Prepare everything ONCE\n",
    "    # We reference the global models/loaders, but Accelerate wraps them for this specific process.\n",
    "    (\n",
    "        vision_adapter_acc, \n",
    "        audio_adapter_acc, \n",
    "        perceiver_acc, \n",
    "        projector_acc, \n",
    "        optimizer_acc, \n",
    "        vision_loader_acc, \n",
    "        audio_loader_acc,\n",
    "        scheduler_acc\n",
    "    ) = accelerator.prepare(\n",
    "        vision_adapter, \n",
    "        audio_adapter, \n",
    "        perceiver, \n",
    "        projector, \n",
    "        optimizer, \n",
    "        vision_loader, \n",
    "        audio_loader,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    # 5. The Training Loop\n",
    "    for round_idx in range(cfg.num_rounds):\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"\\n========== Round {round_idx+1}/{cfg.num_rounds} ==========\")\n",
    "\n",
    "        # --- Vision Training ---\n",
    "        train_one_epoch(\n",
    "            dataloader=vision_loader_acc,\n",
    "            modality=\"vision\",\n",
    "            max_steps=cfg.max_train_steps_vision,\n",
    "            optimizer=optimizer_acc,\n",
    "            scheduler=scheduler_acc,\n",
    "            accelerator=accelerator,\n",
    "            log_prefix=f\"round{round_idx+1}-\"\n",
    "        )\n",
    "        \n",
    "        # Wait for all GPUs to finish before evaluation\n",
    "        accelerator.wait_for_everyone()\n",
    "        \n",
    "        # Run eval only on the main process to avoid duplicate logs/prints\n",
    "        if accelerator.is_main_process:\n",
    "            eval_retrieval(vision_dataset, modality=\"vision\", num_samples=32)\n",
    "\n",
    "        # --- Audio Training ---\n",
    "        train_one_epoch(\n",
    "            dataloader=audio_loader_acc,\n",
    "            modality=\"audio\",\n",
    "            max_steps=cfg.max_train_steps_audio,\n",
    "            optimizer=optimizer_acc,\n",
    "            scheduler=scheduler_acc,\n",
    "            accelerator=accelerator,\n",
    "            log_prefix=f\"round{round_idx+1}-\"\n",
    "        )\n",
    "        \n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            eval_retrieval(audio_dataset, modality=\"audio\", num_samples=32)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"MASTER_PORT\"] = \"29999\"   # choose any free port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52227f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# num_processes = Number of GPUs you have\n",
    "notebook_launcher(training_function, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --------------------------------------------\n",
    "# # 7.3 â€“ Run a small POC training loop\n",
    "# # --------------------------------------------\n",
    "\n",
    "# # You can adjust these to be very small for a first run:\n",
    "# vision_steps = getattr(cfg, \"max_train_steps_vision\", 100)\n",
    "# audio_steps  = getattr(cfg, \"max_train_steps_audio\", 100)\n",
    "\n",
    "# num_rounds = 1  # or >1 if you want to alternate vision/audio multiple times\n",
    "\n",
    "# for round_idx in range(num_rounds):\n",
    "#     print(f\"\\n========== Training Round {round_idx+1}/{num_rounds} ==========\")\n",
    "\n",
    "#     # ---- Visionâ€“text alignment ----\n",
    "#     print(\"\\n--- Visionâ€“Text alignment ---\")\n",
    "#     train_one_epoch(\n",
    "#         dataloader=vision_loader,\n",
    "#         modality=\"vision\",\n",
    "#         max_steps=vision_steps,\n",
    "#         log_prefix=f\"round{round_idx+1}-\",\n",
    "#     )\n",
    "#     eval_retrieval(vision_dataset, modality=\"vision\", num_samples=32)\n",
    "\n",
    "#     # ---- Audioâ€“text alignment ----\n",
    "#     print(\"\\n--- Audioâ€“Text alignment ---\")\n",
    "#     train_one_epoch(\n",
    "#         dataloader=audio_loader,\n",
    "#         modality=\"audio\",\n",
    "#         max_steps=audio_steps,\n",
    "#         log_prefix=f\"round{round_idx+1}-\",\n",
    "#     )\n",
    "#     eval_retrieval(audio_dataset, modality=\"audio\", num_samples=32)\n",
    "\n",
    "# print(\"\\nTraining POC finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2406bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d03782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9ea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7227303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bc59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfd174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ea31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bd03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6342a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b761e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
