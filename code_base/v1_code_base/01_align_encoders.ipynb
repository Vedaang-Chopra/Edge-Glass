{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280e1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167e9013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vedaangchopra/all_data/complete_technical_work/all_projects_implemented/Edge Assistant/code_base/v1_code_base/wandb/run-20251124_102106-6np05ku3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/6np05ku3' target=\"_blank\">phase0_global_setup</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/6np05ku3' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/6np05ku3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: mps, dtype: torch.float16\n",
      "[Config] root_dir: /Users/vedaangchopra/all_data/complete_technical_work/all_projects_implemented/Edge Assistant/code_base/v1_code_base/edge_glass\n",
      "[Config] features_dir: /Users/vedaangchopra/all_data/complete_technical_work/all_projects_implemented/Edge Assistant/code_base/v1_code_base/edge_glass/features\n",
      "Using device: mps\n",
      "Using dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "from imports.configs.config import setup_from_yaml, ModelsConfig\n",
    "# ðŸ”¹ Phase 0 â€“ Global setup\n",
    "cfg = setup_from_yaml(\"imports/configs/config.yaml\")\n",
    "\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "audio_train_loader = True\n",
    "print(\"Using device:\", device)\n",
    "print(\"Using dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90be62",
   "metadata": {},
   "source": [
    "### Step-1: - Load the Encoded Dataset -> For Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024cfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.dataset import PixmoFeatureDataset, LibriSpeechFeatureDataset\n",
    "from imports.dataset import LibriSpeechFeatureDataset\n",
    "from imports.dataset import collate_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ca1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# assuming cfg is already loaded and has cfg.datasets\n",
    "device = cfg.torch_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8efa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision\n",
    "pixmo_train_ds = PixmoFeatureDataset(cfg.datasets.pixmo_train_path)\n",
    "pixmo_val_ds   = PixmoFeatureDataset(cfg.datasets.pixmo_val_path)\n",
    "\n",
    "# Audio (optional)\n",
    "audio_train_ds = None\n",
    "if cfg.datasets.use_librispeech and cfg.datasets.librispeech_train_path is not None:\n",
    "    audio_train_ds = LibriSpeechFeatureDataset(cfg.datasets.librispeech_train_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e39b5",
   "metadata": {},
   "source": [
    "#### Step-1: - Load the Pixmo-Cap Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be1b7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixMo train: 873\n",
      "PixMo val: 89\n"
     ]
    }
   ],
   "source": [
    "pixmo_train_ds = PixmoFeatureDataset(\"./data/data/pixmo/train_index.json\")\n",
    "pixmo_val_ds   = PixmoFeatureDataset(\"./data/data/pixmo/val_index.json\")\n",
    "\n",
    "print(\"PixMo train:\", len(pixmo_train_ds))\n",
    "print(\"PixMo val:\", len(pixmo_val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2ca4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1536]) In this meme, a close-up photograph of a bald-headed Black man is prominently fe\n"
     ]
    }
   ],
   "source": [
    "# Inspect one example\n",
    "ex = pixmo_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"text\"][:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4435e67",
   "metadata": {},
   "source": [
    "#### Step-2: - Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14704bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LibriSpeech feature dataset: 338\n",
      "torch.Size([1500, 512]) IN THE EXERCISE OF THE EXECUTIVE POWER THE PRESIDENT OF THE UNITED STATES IS CONSTANTLY SUBJECT TO A\n"
     ]
    }
   ],
   "source": [
    "audio_train_ds = LibriSpeechFeatureDataset(\"./data/data/librispeech/train_index.json\")\n",
    "\n",
    "print(\"Loaded LibriSpeech feature dataset:\", len(audio_train_ds))\n",
    "ex = audio_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d2c5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(paths=PathsConfig(root_dir='/Users/vedaangchopra/all_data/complete_technical_work/all_projects_implemented/Edge Assistant/code_base/v1_code_base/edge_glass', features_dir='/Users/vedaangchopra/all_data/complete_technical_work/all_projects_implemented/Edge Assistant/code_base/v1_code_base/edge_glass/features'), models=ModelsConfig(vision_model_name='openai/clip-vit-base-patch32', llm_model_name='qwen/Qwen2.5-3B-Instruct', audio_model_name='openai/whisper-base'), architecture=ArchitectureConfig(perceiver_dim=None, num_latents=64, num_perceiver_layers=4, num_attn_heads=8, mlp_ratio=4.0), training=TrainingConfig(batch_size=16, num_epochs=5, learning_rate='3e-4', weight_decay=0.01, warmup_steps=500, max_grad_norm=1.0, log_every_steps=50, train_subset_size=2000, val_subset_size=500), mrl=MRLConfig(mrl_dims=[1024, 512, 256], mrl_weight=1.0, mrl_temp=0.07), misc=MiscConfig(dtype='float16', seed=42, device='auto', use_wandb=True, wandb_project='edge_glass', wandb_run_name='phase0_global_setup'), datasets=DatasetsConfig(pixmo_train_index='./data/data/pixmo/train_index.json', pixmo_val_index='./data/data/pixmo/val_index.json', librispeech_train_index='./data/data/librispeech/train_index.json', use_pixmo=True, use_librispeech=True), torch_device=device(type='mps'), torch_dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c3f8e",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dcd3d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load LLM tokenizer based on config.yaml\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.models.llm_model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "llm_tokenizer.truncation_side = \"right\"\n",
    "\n",
    "print(\"Loaded tokenizer:\", cfg.models.llm_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ffcbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b873f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cfg.torch_device\n",
    "tokenizer = llm_tokenizer   # previously loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "679bda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_collate = partial(collate_alignment, tokenizer=llm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dbda714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader config â†’ num_workers=0, pin_memory=False\n"
     ]
    }
   ],
   "source": [
    "# Choose workers & pin_memory based on device\n",
    "if device.type == \"cuda\":\n",
    "    num_workers = 4       # you can bump this if GPU + good CPU\n",
    "    pin_memory = True\n",
    "else:\n",
    "    # macOS (CPU or MPS) and generic CPU: be conservative\n",
    "    num_workers = 0       # safest in Jupyter / macOS\n",
    "    pin_memory = False\n",
    "\n",
    "print(f\"DataLoader config â†’ num_workers={num_workers}, pin_memory={pin_memory}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc576c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixmo_train_loader = DataLoader(\n",
    "    pixmo_train_ds,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=alignment_collate,\n",
    ")\n",
    "\n",
    "pixmo_val_loader = DataLoader(\n",
    "    pixmo_val_ds,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=alignment_collate,\n",
    ")\n",
    "\n",
    "if audio_train_ds is not None:\n",
    "    audio_train_loader = DataLoader(\n",
    "        audio_train_ds,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=alignment_collate,\n",
    "    )\n",
    "else:\n",
    "    audio_train_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8731220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc09c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision batch encoder_feats: torch.Size([16, 256, 1536])\n",
      "Vision batch tokens: torch.Size([16, 322])\n",
      "Vision modalities: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_v = next(iter(pixmo_train_loader))\n",
    "print(\"Vision batch encoder_feats:\", batch_v[\"features\"].shape)\n",
    "print(\"Vision batch tokens:\", batch_v[\"input_ids\"].shape)\n",
    "print(\"Vision modalities:\", batch_v[\"modality_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "749aa3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio batch encoder_feats: torch.Size([16, 1500, 512])\n",
      "Audio batch tokens: torch.Size([16, 54])\n",
      "Audio modalities: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "if audio_train_loader:\n",
    "    batch_a = next(iter(audio_train_loader))\n",
    "    print(\"Audio batch encoder_feats:\", batch_a[\"features\"].shape)\n",
    "    print(\"Audio batch tokens:\", batch_a[\"input_ids\"].shape)\n",
    "    print(\"Audio modalities:\", batch_a[\"modality_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44b65b",
   "metadata": {},
   "source": [
    "### Step-2:- Load the Architecture(Encoders + Alignment Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0dd6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.encoders import VisionEncoder, AudioEncoder\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "import torch, torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61122129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load frozen encoders\n",
    "vision_enc = VisionEncoder(\n",
    "    model_name=\"facebook/dinov2-base\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "# Use a real image if available\n",
    "try:\n",
    "    img1 = Image.open(\"/mnt/data/sample1.jpg\")\n",
    "    img2 = Image.open(\"/mnt/data/sample2.jpg\")\n",
    "    images = [img1, img2]\n",
    "except:\n",
    "    # fallback: create dummy RGB images of size 224x224\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    dummy = (np.random.rand(224,224,3) * 255).astype('uint8')\n",
    "    images = [Image.fromarray(dummy), Image.fromarray(dummy)]\n",
    "    \n",
    "    \n",
    "vision_out = vision_enc.encode_images(images)\n",
    "vision_feats, vision_mask = vision_out[\"feats\"], vision_out[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699620",
   "metadata": {},
   "source": [
    "#### Test the encoders with random image and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6b7fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_enc = AudioEncoder(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    device=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 1) (B, T) tensor\n",
    "B, T = 2, 16000 * 3\n",
    "waveforms = torch.randn(B, T)\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f437df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "out = audio_enc.encode_waveforms(waveforms, sample_rates=sr)\n",
    "print(\"Case 1 feats:\", out[\"feats\"].shape, \"mask:\", out[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40273004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 2 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n",
      "Case 3 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "# 2) (B, 1, T) tensor\n",
    "waveforms_3d = waveforms.unsqueeze(1)\n",
    "out2 = audio_enc.encode_waveforms(waveforms_3d, sample_rates=sr)\n",
    "print(\"Case 2 feats:\", out2[\"feats\"].shape, \"mask:\", out2[\"mask\"].shape)\n",
    "\n",
    "# 3) list[Tensor] with slightly different shapes (simulating variable length)\n",
    "waveforms_list = [torch.randn(T), torch.randn(T // 2)]\n",
    "out3 = audio_enc.encode_waveforms(waveforms_list, sample_rates=sr)\n",
    "print(\"Case 3 feats:\", out3[\"feats\"].shape, \"mask:\", out3[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a8871",
   "metadata": {},
   "source": [
    "#### Load the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72499992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.model import MultiModalAlignmentModel  # or from current cell\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "747360c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalAlignmentModel(\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (model): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): AudioEncoder(\n",
       "    (model): WhisperModel(\n",
       "      (encoder): WhisperEncoder(\n",
       "        (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (embed_positions): Embedding(1500, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperEncoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): WhisperDecoder(\n",
       "        (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
       "        (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperDecoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (perceiver): PerceiverLatentEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x PerceiverBlock(\n",
       "        (enc_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (ln_latents_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): ProjectorMLP(\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiModalAlignmentModel(\n",
    "    d_shared=512,\n",
    "    d_latent=512,\n",
    "    d_align=1024,\n",
    "    num_latents=32,   # smaller for viz\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    use_perceiver=True,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c886bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(name, t):\n",
    "    if t is None:\n",
    "        print(f\"{name}: None\")\n",
    "    else:\n",
    "        print(f\"{name}: shape={tuple(t.shape)}, dtype={t.dtype}, \"\n",
    "              f\"mean={t.mean().item():.4f}, std={t.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "513ea77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "# Vision: dummy pixel values (already \"preprocessed\"-ish tensor)\n",
    "dummy_images = torch.randn(B, 3, 224, 224, device=device)\n",
    "\n",
    "# Audio: dummy raw waveforms (e.g. 3 seconds at 16kHz)\n",
    "dummy_waveforms = torch.randn(B, 16000 * 3, device=device)\n",
    "dummy_sr = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b00b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision/encoder_feats: shape=(2, 256, 1536), dtype=torch.float32, mean=-0.0202, std=1.1425\n",
      "vision/mask: shape=(2, 256), dtype=torch.float32, mean=1.0000, std=0.0000\n",
      "vision/shared_feats (after adapter): shape=(2, 256, 512), dtype=torch.float32, mean=-0.0095, std=0.6621\n",
      "vision/perceiver_latents: shape=(2, 32, 512), dtype=torch.float32, mean=-0.0542, std=0.5422\n",
      "vision/tokens_after_projector: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0005, std=0.1944\n",
      "vision/pooled: shape=(2, 1024), dtype=torch.float32, mean=0.0005, std=0.1938\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # ---- 1) Encoder ----\n",
    "    enc_v = model.vision_encoder.encode_images(dummy_images)\n",
    "    v_feats = enc_v[\"feats\"].to(device)   # (B, T_v, D_v)\n",
    "    v_mask  = enc_v[\"mask\"]              # (B, T_v) bool\n",
    "\n",
    "    describe(\"vision/encoder_feats\", v_feats)\n",
    "    describe(\"vision/mask\", v_mask.float())\n",
    "\n",
    "    # ---- 2) Adapter to shared dim ----\n",
    "    model._ensure_vision_adapter(v_feats.size(-1))\n",
    "    v_shared = model.vision_adapter(v_feats)   # (B, T_v, d_shared)\n",
    "    describe(\"vision/shared_feats (after adapter)\", v_shared)\n",
    "\n",
    "    # ---- 3) Perceiver latents ----\n",
    "    if model.use_perceiver:\n",
    "        v_latents = model.perceiver(v_shared, encoder_mask=v_mask)  # (B, L, d_latent)\n",
    "        describe(\"vision/perceiver_latents\", v_latents)\n",
    "\n",
    "        # ---- 4) Projector (token-level) ----\n",
    "        v_tokens = model.projector(v_latents)                        # (B, L, d_align)\n",
    "        describe(\"vision/tokens_after_projector\", v_tokens)\n",
    "\n",
    "        # ---- 5) Pooled alignment embedding ----\n",
    "        v_pooled = v_tokens.mean(dim=1)                              # (B, d_align)\n",
    "        describe(\"vision/pooled\", v_pooled)\n",
    "    else:\n",
    "        print(\"Perceiver disabled; using pooled-only path.\")\n",
    "        v_pooled_in = model._pool_masked_mean(v_shared, v_mask)\n",
    "        v_pooled = model.projector(v_pooled_in)\n",
    "        v_latents = None\n",
    "        v_tokens = None\n",
    "        describe(\"vision/pooled\", v_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ee8d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio/encoder_feats: shape=(2, 1500, 512), dtype=torch.float32, mean=-0.0190, std=1.3893\n",
      "audio/mask: shape=(2, 1500), dtype=torch.float32, mean=1.0000, std=0.0000\n",
      "audio/shared_feats (after adapter): shape=(2, 1500, 512), dtype=torch.float32, mean=-0.0087, std=0.7757\n",
      "audio/perceiver_latents: shape=(2, 32, 512), dtype=torch.float32, mean=-0.0570, std=0.5787\n",
      "audio/tokens_after_projector: shape=(2, 32, 1024), dtype=torch.float32, mean=-0.0038, std=0.1963\n",
      "audio/pooled: shape=(2, 1024), dtype=torch.float32, mean=-0.0038, std=0.1962\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # ---- 1) Encoder ----\n",
    "    enc_a = model.audio_encoder.encode_waveforms(dummy_waveforms, sample_rates=dummy_sr)\n",
    "    a_feats = enc_a[\"feats\"].to(device)   # (B, T_a, D_a)\n",
    "    a_mask  = enc_a[\"mask\"]              # (B, T_a) bool\n",
    "\n",
    "    describe(\"audio/encoder_feats\", a_feats)\n",
    "    describe(\"audio/mask\", a_mask.float())\n",
    "\n",
    "    # ---- 2) Adapter to shared dim ----\n",
    "    model._ensure_audio_adapter(a_feats.size(-1))\n",
    "    a_shared = model.audio_adapter(a_feats)   # (B, T_a, d_shared)\n",
    "    describe(\"audio/shared_feats (after adapter)\", a_shared)\n",
    "\n",
    "    # ---- 3) Perceiver latents ----\n",
    "    if model.use_perceiver:\n",
    "        a_latents = model.perceiver(a_shared, encoder_mask=a_mask)  # (B, L, d_latent)\n",
    "        describe(\"audio/perceiver_latents\", a_latents)\n",
    "\n",
    "        # ---- 4) Projector (token-level) ----\n",
    "        a_tokens = model.projector(a_latents)                        # (B, L, d_align)\n",
    "        describe(\"audio/tokens_after_projector\", a_tokens)\n",
    "\n",
    "        # ---- 5) Pooled alignment embedding ----\n",
    "        a_pooled = a_tokens.mean(dim=1)                              # (B, d_align)\n",
    "        describe(\"audio/pooled\", a_pooled)\n",
    "    else:\n",
    "        print(\"Perceiver disabled; using pooled-only path.\")\n",
    "        a_pooled_in = model._pool_masked_mean(a_shared, a_mask)\n",
    "        a_pooled = model.projector(a_pooled_in)\n",
    "        a_latents = None\n",
    "        a_tokens = None\n",
    "        describe(\"audio/pooled\", a_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61995bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_vision()['tokens']: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0005, std=0.1944\n",
      "encode_vision()['pooled']: shape=(2, 1024), dtype=torch.float32, mean=0.0005, std=0.1938\n",
      "encode_audio()['tokens']: shape=(2, 32, 1024), dtype=torch.float32, mean=-0.0038, std=0.1963\n",
      "encode_audio()['pooled']: shape=(2, 1024), dtype=torch.float32, mean=-0.0038, std=0.1962\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    v_enc_api = model.encode_vision(dummy_images)\n",
    "    a_enc_api = model.encode_audio(dummy_waveforms, dummy_sr)\n",
    "\n",
    "describe(\"encode_vision()['tokens']\", v_enc_api[\"tokens\"])\n",
    "describe(\"encode_vision()['pooled']\", v_enc_api[\"pooled\"])\n",
    "\n",
    "describe(\"encode_audio()['tokens']\", a_enc_api[\"tokens\"])\n",
    "describe(\"encode_audio()['pooled']\", a_enc_api[\"pooled\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53674222",
   "metadata": {},
   "source": [
    "### Plotting the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac89f84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullAlignmentGraphWrapper(\n",
       "  (core): MultiModalAlignmentModel(\n",
       "    (vision_encoder): VisionEncoder(\n",
       "      (model): Dinov2Model(\n",
       "        (embeddings): Dinov2Embeddings(\n",
       "          (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "            (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): Dinov2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x Dinov2Layer(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attention): Dinov2Attention(\n",
       "                (attention): Dinov2SelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (output): Dinov2SelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layer_scale1): Dinov2LayerScale()\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Dinov2MLP(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_scale2): Dinov2LayerScale()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (audio_encoder): AudioEncoder(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 512)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (perceiver): PerceiverLatentEncoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x PerceiverBlock(\n",
       "          (enc_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (ln_latents_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ln_latents_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ln_latents_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (mlp): FeedForward(\n",
       "            (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector): ProjectorMLP(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (vision_adapter): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (audio_adapter): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imports.model import FullAlignmentGraphWrapper\n",
    "\n",
    "\n",
    "wrapper = FullAlignmentGraphWrapper(model).to(device)\n",
    "wrapper.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bab0e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision: dummy pixel_values (B, 3, H, W)\n",
    "dummy_pixel_values = torch.randn(2, 3, 224, 224, device=device)\n",
    "\n",
    "# Audio: dummy raw waveforms (B, T)\n",
    "dummy_waveforms = torch.randn(2, 16000 * 3, device=device)  # 3 seconds @ 16kHz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e5405e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = wrapper(dummy_pixel_values, dummy_waveforms)\n",
    "\n",
    "print(\"Output shape:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c9259b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model parameter devices & dtypes (first 20) ===\n",
      "vision_encoder.model.embeddings.cls_token cpu torch.float32\n",
      "vision_encoder.model.embeddings.mask_token cpu torch.float32\n",
      "vision_encoder.model.embeddings.position_embeddings cpu torch.float32\n",
      "vision_encoder.model.embeddings.patch_embeddings.projection.weight cpu torch.float32\n",
      "vision_encoder.model.embeddings.patch_embeddings.projection.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.norm1.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.norm1.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.query.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.query.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.key.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.key.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.value.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.attention.value.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.output.dense.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.attention.output.dense.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.layer_scale1.lambda1 cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.norm2.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.norm2.bias cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.mlp.fc1.weight cpu torch.float32\n",
      "vision_encoder.model.encoder.layer.0.mlp.fc1.bias cpu torch.float32\n",
      "vision_encoder.model param -> cpu torch.float32\n",
      "audio_encoder.model param -> cpu torch.float32\n",
      "dummy_pixel_values -> mps:0 torch.float32\n",
      "dummy_waveforms -> mps:0 torch.float32\n",
      "wrapper.core param -> cpu torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verification: print devices and dtypes of model params and inputs\n",
    "print('=== Model parameter devices & dtypes (first 20) ===')\n",
    "count = 0\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, p.device, p.dtype)\n",
    "    count += 1\n",
    "    if count >= 20:\n",
    "        break\n",
    "\n",
    "# Check specific submodules\n",
    "if hasattr(model, 'vision_encoder') and hasattr(model.vision_encoder, 'model'):\n",
    "    try:\n",
    "        vp = next(model.vision_encoder.model.parameters())\n",
    "        print('vision_encoder.model param ->', vp.device, vp.dtype)\n",
    "    except StopIteration:\n",
    "        print('vision_encoder.model has no parameters')\n",
    "\n",
    "if hasattr(model, 'audio_encoder') and hasattr(model.audio_encoder, 'model'):\n",
    "    try:\n",
    "        ap = next(model.audio_encoder.model.parameters())\n",
    "        print('audio_encoder.model param ->', ap.device, ap.dtype)\n",
    "    except StopIteration:\n",
    "        print('audio_encoder.model has no parameters')\n",
    "\n",
    "# Inputs\n",
    "print('dummy_pixel_values ->', dummy_pixel_values.device, dummy_pixel_values.dtype)\n",
    "print('dummy_waveforms ->', dummy_waveforms.device, dummy_waveforms.dtype)\n",
    "\n",
    "# Also ensure wrapper/core moved to device\n",
    "try:\n",
    "    cp = next(wrapper.core.parameters())\n",
    "    print('wrapper.core param ->', cp.device, cp.dtype)\n",
    "except StopIteration:\n",
    "    print('wrapper.core has no parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e867943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchview import draw_graph\n",
    "\n",
    "# graph = draw_graph(\n",
    "#     wrapper,\n",
    "#     input_data=(dummy_pixel_values, dummy_waveforms),\n",
    "#     graph_name=\"EdgeGlassAlignmentFull\",\n",
    "#     expand_nested=True,\n",
    "#     depth=3,                 # increase to 4 for more detail\n",
    "#     save_graph=True,\n",
    "#     directory=\"arch_plots\",\n",
    "#     filename=\"edgeglass_alignment_full_graph\",\n",
    "# )\n",
    "\n",
    "# # graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308155ed",
   "metadata": {},
   "source": [
    "### Step-3: - Alignment Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e6d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414ee79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
