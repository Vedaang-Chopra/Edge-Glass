{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "167e9013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stage1_mrl_alignment</strong> at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass-align/runs/ajhtkyqb' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass-align/runs/ajhtkyqb</a><br> View project at: <a href='https://wandb.ai/vedaangchopra_gatech/edgeglass-align' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edgeglass-align</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251123_170238-ajhtkyqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/wandb/run-20251123_170455-jtk6snhw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jtk6snhw' target=\"_blank\">phase0_global_setup</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jtk6snhw' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass/runs/jtk6snhw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: cuda, dtype: torch.float16\n",
      "[Config] root_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass\n",
      "[Config] features_dir: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass/features\n",
      "Using device: cuda\n",
      "Using dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "from imports.configs.config import setup_from_yaml, ModelsConfig\n",
    "# üîπ Phase 0 ‚Äì Global setup\n",
    "cfg = setup_from_yaml(\"imports/configs/config.yaml\")\n",
    "\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Using dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90be62",
   "metadata": {},
   "source": [
    "### Step-1: - Load the Encoded Dataset -> For Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024cfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e39b5",
   "metadata": {},
   "source": [
    "#### Step-1: - Load the Pixmo-Cap Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class PixmoFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the pre-extracted image features.\n",
    "    It also fixes old paths like 'data/pixmo/...' -> 'data/data/pixmo/...'\n",
    "    \"\"\"\n",
    "    def __init__(self, index_file: str | Path):\n",
    "        index_file = Path(index_file)\n",
    "        with open(index_file, \"r\") as f:\n",
    "            self.index = json.load(f)\n",
    "\n",
    "        # Base dir where your index lives (e.g. ./data/data/pixmo)\n",
    "        self.base_dir = index_file.parent\n",
    "\n",
    "    def _fix_path(self, raw_path: str) -> Path:\n",
    "        p = Path(raw_path)\n",
    "\n",
    "        # If it's already absolute and exists, just return\n",
    "        if p.is_absolute() and p.exists():\n",
    "            return p\n",
    "\n",
    "        # Common case: path stored as \"data/pixmo/features/xxx.pt\"\n",
    "        # but actual is \"data/data/pixmo/features/xxx.pt\"\n",
    "        s = str(p)\n",
    "\n",
    "        if \"data/pixmo\" in s and not p.exists():\n",
    "            s = s.replace(\"data/pixmo\", \"data/data/pixmo\")\n",
    "            p2 = Path(s)\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "\n",
    "        # Otherwise, try resolving relative to the index directory\n",
    "        p3 = (self.base_dir / p.name)  # fallback: same dir, same filename\n",
    "        if p3.exists():\n",
    "            return p3\n",
    "\n",
    "        # Last resort: just return the original; will raise if missing\n",
    "        return p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.index[idx]\n",
    "        raw_path = meta[\"file\"]\n",
    "        file_path = self._fix_path(raw_path)\n",
    "\n",
    "        blob = torch.load(file_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": blob[\"features\"],          # (num_patches, feat_dim)\n",
    "            \"caption\": blob[\"caption\"],            # raw caption text\n",
    "            \"num_patches\": meta[\"num_patches\"],\n",
    "            \"orig_idx\": meta[\"orig_idx\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1b7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixMo train: 26366\n",
      "PixMo val: 4376\n"
     ]
    }
   ],
   "source": [
    "pixmo_train_ds = PixmoFeatureDataset(\"./data/data/pixmo/train_index.json\")\n",
    "pixmo_val_ds   = PixmoFeatureDataset(\"./data/data/pixmo/val_index.json\")\n",
    "\n",
    "print(\"PixMo train:\", len(pixmo_train_ds))\n",
    "print(\"PixMo val:\", len(pixmo_val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2ca4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1536]) The image depicts a pixelated, retro-style space shooter game set against a nigh\n"
     ]
    }
   ],
   "source": [
    "# Inspect one example\n",
    "ex = pixmo_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"caption\"][:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4435e67",
   "metadata": {},
   "source": [
    "#### Step-2: - Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db52ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "class LibriSpeechFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads pre-extracted Whisper features.\n",
    "    Fixes broken paths like 'data/librispeech/...' -> 'data/data/librispeech/...'\n",
    "    just like PixmoFeatureDataset does.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_file: str | Path):\n",
    "        index_file = Path(index_file)\n",
    "        with open(index_file, \"r\") as f:\n",
    "            self.index = json.load(f)\n",
    "\n",
    "        # Base directory where index.json lives\n",
    "        self.base_dir = index_file.parent\n",
    "\n",
    "    def _fix_path(self, raw_path: str) -> Path:\n",
    "        \"\"\"\n",
    "        Try multiple strategies to fix incorrect dataset paths.\n",
    "        1. Use raw path if absolute + exists\n",
    "        2. Fix common 'data/librispeech' ‚Üí 'data/data/librispeech'\n",
    "        3. Try rewriting relative to index dir\n",
    "        \"\"\"\n",
    "        p = Path(raw_path)\n",
    "\n",
    "        # 1. If fully absolute and exists ‚Üí OK\n",
    "        if p.is_absolute() and p.exists():\n",
    "            return p\n",
    "\n",
    "        s = str(p)\n",
    "\n",
    "        # 2. Common mismatch:\n",
    "        #    raw: \"data/librispeech/features/train_feat_123.pt\"\n",
    "        #    actual: \"data/data/librispeech/features/train_feat_123.pt\"\n",
    "        if \"data/librispeech\" in s and not p.exists():\n",
    "            s2 = s.replace(\"data/librispeech\", \"data/data/librispeech\")\n",
    "            p2 = Path(s2)\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "\n",
    "        # 3. Fallback: resolve relative to the index directory\n",
    "        #    (useful if someone moved the index folder)\n",
    "        p3 = self.base_dir / p.name\n",
    "        if p3.exists():\n",
    "            return p3\n",
    "\n",
    "        # ‚ùå Last fallback ‚Üí just return original (torch.load will raise)\n",
    "        return p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.index[idx]\n",
    "\n",
    "        raw_path = meta[\"file\"]\n",
    "        file_path = self._fix_path(raw_path)\n",
    "\n",
    "        blob = torch.load(file_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": blob[\"features\"],      # (T_enc, d_audio)\n",
    "            \"text\": blob[\"text\"],\n",
    "            \"duration\": blob[\"duration\"],\n",
    "            \"sampling_rate\": blob[\"sampling_rate\"],\n",
    "            \"orig_idx\": blob[\"orig_idx\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14704bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LibriSpeech feature dataset: 8677\n",
      "torch.Size([1500, 512]) CONCERNING THE DISEASE THAT HEROD FELL INTO AND THE SEDITION WHICH THE JEWS RAISED THEREUPON\n"
     ]
    }
   ],
   "source": [
    "audio_train_ds = LibriSpeechFeatureDataset(\"./data/data/librispeech/train_index.json\")\n",
    "\n",
    "print(\"Loaded LibriSpeech feature dataset:\", len(audio_train_ds))\n",
    "\n",
    "ex = audio_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67d2c5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(paths=PathsConfig(root_dir='/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass', features_dir='/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/edge_glass/features'), models=ModelsConfig(vision_model_name='openai/clip-vit-base-patch32', llm_model_name='qwen/Qwen2.5-3B-Instruct', audio_model_name='openai/whisper-base'), architecture=ArchitectureConfig(perceiver_dim=None, num_latents=64, num_perceiver_layers=4, num_attn_heads=8, mlp_ratio=4.0), training=TrainingConfig(batch_size=16, num_epochs=5, learning_rate='3e-4', weight_decay=0.01, warmup_steps=500, max_grad_norm=1.0, log_every_steps=50, train_subset_size=2000, val_subset_size=500), mrl=MRLConfig(mrl_dims=[1024, 512, 256], mrl_weight=1.0, mrl_temp=0.07), misc=MiscConfig(dtype='float16', seed=42, device='auto', use_wandb=True, wandb_project='edge_glass', wandb_run_name='phase0_global_setup'), torch_device=device(type='cuda'), torch_dtype=torch.float16)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c3f8e",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc576c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_alignment(batch, tokenizer, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    batch: list of dicts with keys:\n",
    "      - features: (L_i, D)\n",
    "      - text: str\n",
    "      - modality: \"vision\" or \"audio\"\n",
    "    \"\"\"\n",
    "    # 1) Feature padding\n",
    "    seqs = [b[\"features\"] for b in batch]             # list of (L_i, D)\n",
    "    lengths = [s.size(0) for s in seqs]\n",
    "    max_len = max(lengths)\n",
    "    feat_dim = seqs[0].size(1)\n",
    "    B = len(batch)\n",
    "\n",
    "    feats = torch.zeros(B, max_len, feat_dim, dtype=seqs[0].dtype)\n",
    "    feat_mask = torch.zeros(B, max_len, dtype=torch.bool)\n",
    "\n",
    "    for i, (s, L) in enumerate(zip(seqs, lengths)):\n",
    "        feats[i, :L] = s\n",
    "        feat_mask[i, :L] = True   # True where there is real data\n",
    "\n",
    "    # 2) Text tokenization (for LLM)\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    tok = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 3) Modality as tensor of ints (0 = vision, 1 = audio), if you want\n",
    "    modality_strs = [b.get(\"modality\", \"vision\") for b in batch]\n",
    "    modality_ids = torch.tensor(\n",
    "        [0 if m == \"vision\" else 1 for m in modality_strs],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    batch_out = {\n",
    "        \"features\": feats.to(device),                 # (B, max_L, D)\n",
    "        \"feature_mask\": feat_mask.to(device),         # (B, max_L)\n",
    "        \"input_ids\": tok[\"input_ids\"].to(device),     # (B, T_text)\n",
    "        \"attention_mask\": tok[\"attention_mask\"].to(device),\n",
    "        \"modality_ids\": modality_ids.to(device),      # (B,)\n",
    "        \"raw_text\": texts,\n",
    "    }\n",
    "    return batch_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "vision_collate = partial(collate_alignment, tokenizer=llm_tokenizer, device=device)\n",
    "\n",
    "pixmo_train_loader = DataLoader(\n",
    "    pixmo_train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=vision_collate,\n",
    ")\n",
    "\n",
    "pixmo_val_loader = DataLoader(\n",
    "    pixmo_val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=vision_collate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc09c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train_loader = DataLoader(\n",
    "    audio_train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=vision_collate,   # same collate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44b65b",
   "metadata": {},
   "source": [
    "### Step-2:- Load the Architecture(Encoders + Alignment Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0dd6596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imports.encoders import VisionEncoder, AudioEncoder\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "import torch, torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7c0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16  # or bfloat16 / float32 depending on your env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61122129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load frozen encoders\n",
    "vision_enc = VisionEncoder(\n",
    "    model_name=\"facebook/dinov2-base\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "# Use a real image if available\n",
    "try:\n",
    "    img1 = Image.open(\"/mnt/data/sample1.jpg\")\n",
    "    img2 = Image.open(\"/mnt/data/sample2.jpg\")\n",
    "    images = [img1, img2]\n",
    "except:\n",
    "    # fallback: create dummy RGB images of size 224x224\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    dummy = (np.random.rand(224,224,3) * 255).astype('uint8')\n",
    "    images = [Image.fromarray(dummy), Image.fromarray(dummy)]\n",
    "    \n",
    "    \n",
    "vision_out = vision_enc.encode_images(images)\n",
    "vision_feats, vision_mask = vision_out[\"feats\"], vision_out[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699620",
   "metadata": {},
   "source": [
    "#### Test the encoders with random image and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b7fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_enc = AudioEncoder(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    device=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 1) (B, T) tensor\n",
    "B, T = 2, 16000 * 3\n",
    "waveforms = torch.randn(B, T)\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f437df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "out = audio_enc.encode_waveforms(waveforms, sample_rates=sr)\n",
    "print(\"Case 1 feats:\", out[\"feats\"].shape, \"mask:\", out[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40273004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 2 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n",
      "Case 3 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "# 2) (B, 1, T) tensor\n",
    "waveforms_3d = waveforms.unsqueeze(1)\n",
    "out2 = audio_enc.encode_waveforms(waveforms_3d, sample_rates=sr)\n",
    "print(\"Case 2 feats:\", out2[\"feats\"].shape, \"mask:\", out2[\"mask\"].shape)\n",
    "\n",
    "# 3) list[Tensor] with slightly different shapes (simulating variable length)\n",
    "waveforms_list = [torch.randn(T), torch.randn(T // 2)]\n",
    "out3 = audio_enc.encode_waveforms(waveforms_list, sample_rates=sr)\n",
    "print(\"Case 3 feats:\", out3[\"feats\"].shape, \"mask:\", out3[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a8871",
   "metadata": {},
   "source": [
    "#### Load the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72499992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.model import MultiModalAlignmentModel  # or from current cell\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalAlignmentModel(\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (model): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): AudioEncoder(\n",
       "    (model): WhisperModel(\n",
       "      (encoder): WhisperEncoder(\n",
       "        (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (embed_positions): Embedding(1500, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperEncoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): WhisperDecoder(\n",
       "        (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
       "        (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x WhisperDecoderLayer(\n",
       "            (self_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): WhisperAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (perceiver): PerceiverLatentEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x PerceiverBlock(\n",
       "        (enc_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (ln_latents_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_latents_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): ProjectorMLP(\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiModalAlignmentModel(\n",
    "    d_shared=512,\n",
    "    d_latent=512,\n",
    "    d_align=1024,\n",
    "    num_latents=32,   # smaller for viz\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    use_perceiver=True,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c886bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(name, t):\n",
    "    if t is None:\n",
    "        print(f\"{name}: None\")\n",
    "    else:\n",
    "        print(f\"{name}: shape={tuple(t.shape)}, dtype={t.dtype}, \"\n",
    "              f\"mean={t.mean().item():.4f}, std={t.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "513ea77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "\n",
    "# Vision: dummy pixel values (already \"preprocessed\"-ish tensor)\n",
    "dummy_images = torch.randn(B, 3, 224, 224, device=device)\n",
    "\n",
    "# Audio: dummy raw waveforms (e.g. 3 seconds at 16kHz)\n",
    "dummy_waveforms = torch.randn(B, 16000 * 3, device=device)\n",
    "dummy_sr = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b00b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision/encoder_feats: shape=(2, 256, 1536), dtype=torch.float32, mean=-0.0171, std=1.1842\n",
      "vision/mask: shape=(2, 256), dtype=torch.float32, mean=1.0000, std=0.0000\n",
      "vision/shared_feats (after adapter): shape=(2, 256, 512), dtype=torch.float32, mean=0.0004, std=0.6689\n",
      "vision/perceiver_latents: shape=(2, 32, 512), dtype=torch.float32, mean=0.0417, std=0.5438\n",
      "vision/tokens_after_projector: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0071, std=0.1993\n",
      "vision/pooled: shape=(2, 1024), dtype=torch.float32, mean=0.0071, std=0.1986\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # ---- 1) Encoder ----\n",
    "    enc_v = model.vision_encoder.encode_images(dummy_images)\n",
    "    v_feats = enc_v[\"feats\"].to(device)   # (B, T_v, D_v)\n",
    "    v_mask  = enc_v[\"mask\"]              # (B, T_v) bool\n",
    "\n",
    "    describe(\"vision/encoder_feats\", v_feats)\n",
    "    describe(\"vision/mask\", v_mask.float())\n",
    "\n",
    "    # ---- 2) Adapter to shared dim ----\n",
    "    model._ensure_vision_adapter(v_feats.size(-1))\n",
    "    v_shared = model.vision_adapter(v_feats)   # (B, T_v, d_shared)\n",
    "    describe(\"vision/shared_feats (after adapter)\", v_shared)\n",
    "\n",
    "    # ---- 3) Perceiver latents ----\n",
    "    if model.use_perceiver:\n",
    "        v_latents = model.perceiver(v_shared, encoder_mask=v_mask)  # (B, L, d_latent)\n",
    "        describe(\"vision/perceiver_latents\", v_latents)\n",
    "\n",
    "        # ---- 4) Projector (token-level) ----\n",
    "        v_tokens = model.projector(v_latents)                        # (B, L, d_align)\n",
    "        describe(\"vision/tokens_after_projector\", v_tokens)\n",
    "\n",
    "        # ---- 5) Pooled alignment embedding ----\n",
    "        v_pooled = v_tokens.mean(dim=1)                              # (B, d_align)\n",
    "        describe(\"vision/pooled\", v_pooled)\n",
    "    else:\n",
    "        print(\"Perceiver disabled; using pooled-only path.\")\n",
    "        v_pooled_in = model._pool_masked_mean(v_shared, v_mask)\n",
    "        v_pooled = model.projector(v_pooled_in)\n",
    "        v_latents = None\n",
    "        v_tokens = None\n",
    "        describe(\"vision/pooled\", v_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ee8d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio/encoder_feats: shape=(2, 1500, 512), dtype=torch.float32, mean=-0.0190, std=1.3926\n",
      "audio/mask: shape=(2, 1500), dtype=torch.float32, mean=1.0000, std=0.0000\n",
      "audio/shared_feats (after adapter): shape=(2, 1500, 512), dtype=torch.float32, mean=-0.0090, std=0.7969\n",
      "audio/perceiver_latents: shape=(2, 32, 512), dtype=torch.float32, mean=-0.0019, std=0.5730\n",
      "audio/tokens_after_projector: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0041, std=0.1987\n",
      "audio/pooled: shape=(2, 1024), dtype=torch.float32, mean=0.0041, std=0.1985\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # ---- 1) Encoder ----\n",
    "    enc_a = model.audio_encoder.encode_waveforms(dummy_waveforms, sample_rates=dummy_sr)\n",
    "    a_feats = enc_a[\"feats\"].to(device)   # (B, T_a, D_a)\n",
    "    a_mask  = enc_a[\"mask\"]              # (B, T_a) bool\n",
    "\n",
    "    describe(\"audio/encoder_feats\", a_feats)\n",
    "    describe(\"audio/mask\", a_mask.float())\n",
    "\n",
    "    # ---- 2) Adapter to shared dim ----\n",
    "    model._ensure_audio_adapter(a_feats.size(-1))\n",
    "    a_shared = model.audio_adapter(a_feats)   # (B, T_a, d_shared)\n",
    "    describe(\"audio/shared_feats (after adapter)\", a_shared)\n",
    "\n",
    "    # ---- 3) Perceiver latents ----\n",
    "    if model.use_perceiver:\n",
    "        a_latents = model.perceiver(a_shared, encoder_mask=a_mask)  # (B, L, d_latent)\n",
    "        describe(\"audio/perceiver_latents\", a_latents)\n",
    "\n",
    "        # ---- 4) Projector (token-level) ----\n",
    "        a_tokens = model.projector(a_latents)                        # (B, L, d_align)\n",
    "        describe(\"audio/tokens_after_projector\", a_tokens)\n",
    "\n",
    "        # ---- 5) Pooled alignment embedding ----\n",
    "        a_pooled = a_tokens.mean(dim=1)                              # (B, d_align)\n",
    "        describe(\"audio/pooled\", a_pooled)\n",
    "    else:\n",
    "        print(\"Perceiver disabled; using pooled-only path.\")\n",
    "        a_pooled_in = model._pool_masked_mean(a_shared, a_mask)\n",
    "        a_pooled = model.projector(a_pooled_in)\n",
    "        a_latents = None\n",
    "        a_tokens = None\n",
    "        describe(\"audio/pooled\", a_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61995bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_vision()['tokens']: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0071, std=0.1993\n",
      "encode_vision()['pooled']: shape=(2, 1024), dtype=torch.float32, mean=0.0071, std=0.1986\n",
      "encode_audio()['tokens']: shape=(2, 32, 1024), dtype=torch.float32, mean=0.0041, std=0.1987\n",
      "encode_audio()['pooled']: shape=(2, 1024), dtype=torch.float32, mean=0.0041, std=0.1985\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    v_enc_api = model.encode_vision(dummy_images)\n",
    "    a_enc_api = model.encode_audio(dummy_waveforms, dummy_sr)\n",
    "\n",
    "describe(\"encode_vision()['tokens']\", v_enc_api[\"tokens\"])\n",
    "describe(\"encode_vision()['pooled']\", v_enc_api[\"pooled\"])\n",
    "\n",
    "describe(\"encode_audio()['tokens']\", a_enc_api[\"tokens\"])\n",
    "describe(\"encode_audio()['pooled']\", a_enc_api[\"pooled\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53674222",
   "metadata": {},
   "source": [
    "### Plotting the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac89f84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullAlignmentGraphWrapper(\n",
       "  (core): MultiModalAlignmentModel(\n",
       "    (vision_encoder): VisionEncoder(\n",
       "      (model): Dinov2Model(\n",
       "        (embeddings): Dinov2Embeddings(\n",
       "          (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "            (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): Dinov2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x Dinov2Layer(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attention): Dinov2Attention(\n",
       "                (attention): Dinov2SelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (output): Dinov2SelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layer_scale1): Dinov2LayerScale()\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Dinov2MLP(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_scale2): Dinov2LayerScale()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (audio_encoder): AudioEncoder(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 512)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (perceiver): PerceiverLatentEncoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x PerceiverBlock(\n",
       "          (enc_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (ln_latents_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ln_latents_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ln_latents_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (mlp): FeedForward(\n",
       "            (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector): ProjectorMLP(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from imports.model import FullAlignmentGraphWrapper\n",
    "\n",
    "\n",
    "wrapper = FullAlignmentGraphWrapper(model).to(device)\n",
    "wrapper.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bab0e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision: dummy pixel_values (B, 3, H, W)\n",
    "dummy_pixel_values = torch.randn(2, 3, 224, 224, device=device)\n",
    "\n",
    "# Audio: dummy raw waveforms (B, T)\n",
    "dummy_waveforms = torch.randn(2, 16000 * 3, device=device)  # 3 seconds @ 16kHz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e5405e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = wrapper(dummy_pixel_values, dummy_waveforms)\n",
    "\n",
    "print(\"Output shape:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e867943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "graph = draw_graph(\n",
    "    wrapper,\n",
    "    input_data=(dummy_pixel_values, dummy_waveforms),\n",
    "    graph_name=\"EdgeGlassAlignmentFull\",\n",
    "    expand_nested=True,\n",
    "    depth=3,                 # increase to 4 for more detail\n",
    "    save_graph=True,\n",
    "    directory=\"arch_plots\",\n",
    "    filename=\"edgeglass_alignment_full_graph\",\n",
    ")\n",
    "\n",
    "# graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ae36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308155ed",
   "metadata": {},
   "source": [
    "### Step-3: - Alignment Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "426b730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.align_training.steps import AlignmentModules, AlignmentConfig\n",
    "from imports.align_training.training import (\n",
    "    build_alignment_optimizer,\n",
    "    train_alignment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_loader = DataLoader(\n",
    "    pixmo_train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,  # returns encoder_feats, encoder_mask, texts\n",
    ")\n",
    "\n",
    "audio_loader = DataLoader(\n",
    "    audio_train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n",
    "\n",
    "train_loaders = {\n",
    "    \"vision\": vision_loader,\n",
    "    \"audio\": audio_loader,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_modules = torch.nn.Module()\n",
    "trainable_modules.vision_adapter = vision_adapter\n",
    "trainable_modules.audio_adapter  = audio_adapter\n",
    "trainable_modules.perceiver      = perceiver\n",
    "trainable_modules.projector      = projector\n",
    "\n",
    "optimizer = build_alignment_optimizer(\n",
    "    trainable_modules=trainable_modules,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33a485",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vision_adapter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimports\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malign_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     build_alignment_optimizer,\n\u001b[32m      4\u001b[39m     train_alignment,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# You already have cfg, vision_adapter, audio_adapter, perceiver, projector, device\u001b[39;00m\n\u001b[32m      9\u001b[39m modules = AlignmentModules(\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     vision_adapter=\u001b[43mvision_adapter\u001b[49m,\n\u001b[32m     11\u001b[39m     audio_adapter=audio_adapter,\n\u001b[32m     12\u001b[39m     perceiver=perceiver,\n\u001b[32m     13\u001b[39m     projector=projector,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m align_cfg = AlignmentConfig(\n\u001b[32m     17\u001b[39m     mrl_dims=\u001b[38;5;28mtuple\u001b[39m(cfg.mrl_dims),\n\u001b[32m     18\u001b[39m     mrl_temperature=cfg.mrl_temperature,\n\u001b[32m     19\u001b[39m     max_text_length=\u001b[32m64\u001b[39m,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'vision_adapter' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# You already have cfg, vision_adapter, audio_adapter, perceiver, projector, device\n",
    "\n",
    "modules = AlignmentModules(\n",
    "    vision_adapter=vision_adapter,\n",
    "    audio_adapter=audio_adapter,\n",
    "    perceiver=perceiver,\n",
    "    projector=projector,\n",
    ")\n",
    "\n",
    "align_cfg = AlignmentConfig(\n",
    "    mrl_dims=tuple(cfg.mrl_dims),\n",
    "    mrl_temperature=cfg.mrl_temperature,\n",
    "    max_text_length=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a6c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "833324f7",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf184de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_alignment(\n",
    "    train_loaders=train_loaders,\n",
    "    modules=modules,\n",
    "    cfg=align_cfg,\n",
    "    text_embed_fn=text_embed_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=cfg.num_epochs,\n",
    "    log_every=50,\n",
    "    log_fn=wandb_log_fn,\n",
    "    modalities=(\"vision\", \"audio\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e6d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414ee79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
