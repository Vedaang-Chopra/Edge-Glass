{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90be62",
   "metadata": {},
   "source": [
    "### Step-1: - Load the Encoded Dataset -> For Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024cfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e39b5",
   "metadata": {},
   "source": [
    "#### Step-1: - Load the Pixmo-Cap Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class PixmoFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the pre-extracted image features.\n",
    "    It also fixes old paths like 'data/pixmo/...' -> 'data/data/pixmo/...'\n",
    "    \"\"\"\n",
    "    def __init__(self, index_file: str | Path):\n",
    "        index_file = Path(index_file)\n",
    "        with open(index_file, \"r\") as f:\n",
    "            self.index = json.load(f)\n",
    "\n",
    "        # Base dir where your index lives (e.g. ./data/data/pixmo)\n",
    "        self.base_dir = index_file.parent\n",
    "\n",
    "    def _fix_path(self, raw_path: str) -> Path:\n",
    "        p = Path(raw_path)\n",
    "\n",
    "        # If it's already absolute and exists, just return\n",
    "        if p.is_absolute() and p.exists():\n",
    "            return p\n",
    "\n",
    "        # Common case: path stored as \"data/pixmo/features/xxx.pt\"\n",
    "        # but actual is \"data/data/pixmo/features/xxx.pt\"\n",
    "        s = str(p)\n",
    "\n",
    "        if \"data/pixmo\" in s and not p.exists():\n",
    "            s = s.replace(\"data/pixmo\", \"data/data/pixmo\")\n",
    "            p2 = Path(s)\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "\n",
    "        # Otherwise, try resolving relative to the index directory\n",
    "        p3 = (self.base_dir / p.name)  # fallback: same dir, same filename\n",
    "        if p3.exists():\n",
    "            return p3\n",
    "\n",
    "        # Last resort: just return the original; will raise if missing\n",
    "        return p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.index[idx]\n",
    "        raw_path = meta[\"file\"]\n",
    "        file_path = self._fix_path(raw_path)\n",
    "\n",
    "        blob = torch.load(file_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": blob[\"features\"],          # (num_patches, feat_dim)\n",
    "            \"caption\": blob[\"caption\"],            # raw caption text\n",
    "            \"num_patches\": meta[\"num_patches\"],\n",
    "            \"orig_idx\": meta[\"orig_idx\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1b7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixMo train: 26366\n",
      "PixMo val: 4376\n"
     ]
    }
   ],
   "source": [
    "pixmo_train_ds = PixmoFeatureDataset(\"./data/data/pixmo/train_index.json\")\n",
    "pixmo_val_ds   = PixmoFeatureDataset(\"./data/data/pixmo/val_index.json\")\n",
    "\n",
    "print(\"PixMo train:\", len(pixmo_train_ds))\n",
    "print(\"PixMo val:\", len(pixmo_val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2ca4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1536]) The image depicts a pixelated, retro-style space shooter game set against a nigh\n"
     ]
    }
   ],
   "source": [
    "# Inspect one example\n",
    "ex = pixmo_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"caption\"][:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4435e67",
   "metadata": {},
   "source": [
    "#### Step-2: - Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db52ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class LibriSpeechFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads pre-extracted Whisper features.\n",
    "    Fixes broken paths like 'data/librispeech/...' -> 'data/data/librispeech/...'\n",
    "    just like PixmoFeatureDataset does.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_file: str | Path):\n",
    "        index_file = Path(index_file)\n",
    "        with open(index_file, \"r\") as f:\n",
    "            self.index = json.load(f)\n",
    "\n",
    "        # Base directory where index.json lives\n",
    "        self.base_dir = index_file.parent\n",
    "\n",
    "    def _fix_path(self, raw_path: str) -> Path:\n",
    "        \"\"\"\n",
    "        Try multiple strategies to fix incorrect dataset paths.\n",
    "        1. Use raw path if absolute + exists\n",
    "        2. Fix common 'data/librispeech' → 'data/data/librispeech'\n",
    "        3. Try rewriting relative to index dir\n",
    "        \"\"\"\n",
    "        p = Path(raw_path)\n",
    "\n",
    "        # 1. If fully absolute and exists → OK\n",
    "        if p.is_absolute() and p.exists():\n",
    "            return p\n",
    "\n",
    "        s = str(p)\n",
    "\n",
    "        # 2. Common mismatch:\n",
    "        #    raw: \"data/librispeech/features/train_feat_123.pt\"\n",
    "        #    actual: \"data/data/librispeech/features/train_feat_123.pt\"\n",
    "        if \"data/librispeech\" in s and not p.exists():\n",
    "            s2 = s.replace(\"data/librispeech\", \"data/data/librispeech\")\n",
    "            p2 = Path(s2)\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "\n",
    "        # 3. Fallback: resolve relative to the index directory\n",
    "        #    (useful if someone moved the index folder)\n",
    "        p3 = self.base_dir / p.name\n",
    "        if p3.exists():\n",
    "            return p3\n",
    "\n",
    "        # ❌ Last fallback → just return original (torch.load will raise)\n",
    "        return p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.index[idx]\n",
    "\n",
    "        raw_path = meta[\"file\"]\n",
    "        file_path = self._fix_path(raw_path)\n",
    "\n",
    "        blob = torch.load(file_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": blob[\"features\"],      # (T_enc, d_audio)\n",
    "            \"text\": blob[\"text\"],\n",
    "            \"duration\": blob[\"duration\"],\n",
    "            \"sampling_rate\": blob[\"sampling_rate\"],\n",
    "            \"orig_idx\": blob[\"orig_idx\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14704bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LibriSpeech feature dataset: 8677\n",
      "torch.Size([1500, 512]) CONCERNING THE DISEASE THAT HEROD FELL INTO AND THE SEDITION WHICH THE JEWS RAISED THEREUPON\n"
     ]
    }
   ],
   "source": [
    "audio_train_ds = LibriSpeechFeatureDataset(\"./data/data/librispeech/train_index.json\")\n",
    "\n",
    "print(\"Loaded LibriSpeech feature dataset:\", len(audio_train_ds))\n",
    "\n",
    "ex = audio_train_ds[0]\n",
    "print(ex[\"features\"].shape, ex[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44b65b",
   "metadata": {},
   "source": [
    "### Step-2:- Load the Architecture(Encoders + Alignment Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0dd6596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imports.encoders import VisionEncoder, AudioEncoder\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "import torch, torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7c0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16  # or bfloat16 / float32 depending on your env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61122129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load frozen encoders\n",
    "vision_enc = VisionEncoder(\n",
    "    model_name=\"facebook/dinov2-base\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "# Use a real image if available\n",
    "try:\n",
    "    img1 = Image.open(\"/mnt/data/sample1.jpg\")\n",
    "    img2 = Image.open(\"/mnt/data/sample2.jpg\")\n",
    "    images = [img1, img2]\n",
    "except:\n",
    "    # fallback: create dummy RGB images of size 224x224\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    dummy = (np.random.rand(224,224,3) * 255).astype('uint8')\n",
    "    images = [Image.fromarray(dummy), Image.fromarray(dummy)]\n",
    "    \n",
    "    \n",
    "vision_out = vision_enc.encode_images(images)\n",
    "vision_feats, vision_mask = vision_out[\"feats\"], vision_out[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699620",
   "metadata": {},
   "source": [
    "#### Test the encoders with random image and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b7fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_enc = AudioEncoder(\n",
    "    model_name=\"openai/whisper-base\",\n",
    "    device=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 1) (B, T) tensor\n",
    "B, T = 2, 16000 * 3\n",
    "waveforms = torch.randn(B, T)\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f437df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "out = audio_enc.encode_waveforms(waveforms, sample_rates=sr)\n",
    "print(\"Case 1 feats:\", out[\"feats\"].shape, \"mask:\", out[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40273004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 2 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n",
      "Case 3 feats: torch.Size([2, 1500, 512]) mask: torch.Size([2, 1500])\n"
     ]
    }
   ],
   "source": [
    "# 2) (B, 1, T) tensor\n",
    "waveforms_3d = waveforms.unsqueeze(1)\n",
    "out2 = audio_enc.encode_waveforms(waveforms_3d, sample_rates=sr)\n",
    "print(\"Case 2 feats:\", out2[\"feats\"].shape, \"mask:\", out2[\"mask\"].shape)\n",
    "\n",
    "# 3) list[Tensor] with slightly different shapes (simulating variable length)\n",
    "waveforms_list = [torch.randn(T), torch.randn(T // 2)]\n",
    "out3 = audio_enc.encode_waveforms(waveforms_list, sample_rates=sr)\n",
    "print(\"Case 3 feats:\", out3[\"feats\"].shape, \"mask:\", out3[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a8871",
   "metadata": {},
   "source": [
    "#### Load the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72499992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.model import MultiModalAlignmentModel  # or from current cell\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalAlignmentModel(\n",
    "    d_shared=512,\n",
    "    d_latent=512,\n",
    "    d_align=1024,\n",
    "    num_latents=32,   # smaller for viz\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    use_perceiver=True,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d41bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e867943d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchgraph see error message",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchview/torchview.py:261\u001b[39m, in \u001b[36mforward_prop\u001b[39m\u001b[34m(model, x, device, model_graph, mode, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Mapping):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchview/recorder_tensor.py:154\u001b[39m, in \u001b[36mmodule_forward_wrapper.<locals>._module_forward_wrapper\u001b[39m\u001b[34m(mod, *args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# TODO: check if output contains RecorderTensor\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# this seems not to be necessary so far\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m out = \u001b[43m_orig_module_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m model_graph.context_tracker[\u001b[33m'\u001b[39m\u001b[33mcurrent_depth\u001b[39m\u001b[33m'\u001b[39m] = cur_depth\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/imports/model.py:191\u001b[39m, in \u001b[36mMultiModalAlignmentModel.forward\u001b[39m\u001b[34m(self, images, waveforms, sample_rates)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mConvenience forward that encodes both modalities and returns aligned pooled embeddings.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m a = \u001b[38;5;28mself\u001b[39m.encode_audio(waveforms, sample_rates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/imports/model.py:128\u001b[39m, in \u001b[36mMultiModalAlignmentModel.encode_vision\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03mimages: list[PIL.Image] or preprocessed tensor\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m \u001b[33;03m    }\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m enc_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m feats, mask = enc_out[\u001b[33m\"\u001b[39m\u001b[33mfeats\u001b[39m\u001b[33m\"\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device), enc_out[\u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# (B, T_v, D_v)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/v1_code_base/imports/encoders.py:169\u001b[39m, in \u001b[36mVisionEncoder.encode_images\u001b[39m\u001b[34m(self, images, return_mask)\u001b[39m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPIL is required to pass raw images to VisionEncoder.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m pixel_values = inputs[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# (B, C, H, W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:732\u001b[39m, in \u001b[36mBaseImageProcessorFast.__call__\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:757\u001b[39m, in \u001b[36mBaseImageProcessorFast.preprocess\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdata_format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:776\u001b[39m, in \u001b[36mBaseImageProcessorFast._preprocess_image_like_inputs\u001b[39m\u001b[34m(self, images, do_convert_rgb, input_data_format, device, *args, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# Prepare input images\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preprocess(images, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:633\u001b[39m, in \u001b[36mBaseImageProcessorFast._prepare_image_like_inputs\u001b[39m\u001b[34m(self, images, do_convert_rgb, input_data_format, device, expected_ndims)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# Get structured images (potentially nested)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_images_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_ndims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m process_image_partial = partial(\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m._process_image, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n\u001b[32m    637\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:564\u001b[39m, in \u001b[36mBaseImageProcessorFast._prepare_images_structure\u001b[39m\u001b[34m(self, images, expected_ndims)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Checks for `str` in case of URL/local path and optionally loads images\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m make_flat_list_of_images(images, expected_ndims=expected_ndims)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_base.py:530\u001b[39m, in \u001b[36mImageProcessingMixin.fetch_images\u001b[39m\u001b[34m(self, image_url_or_urls)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m image_url_or_urls]\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/image_processing_base.py:536\u001b[39m, in \u001b[36mImageProcessingMixin.fetch_images\u001b[39m\u001b[34m(self, image_url_or_urls)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33monly a single or a list of entries is supported but got type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image_url_or_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: only a single or a list of entries is supported but got type=<class 'NoneType'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m dummy_wave = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m16000\u001b[39m)\n\u001b[32m     10\u001b[39m dummy_sr = \u001b[32m16000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m graph = \u001b[43mdraw_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_wave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_sr\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# images=None placeholder\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpand_nested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43march_plots\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malignment_model_torchview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m graph.visual_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchview/torchview.py:225\u001b[39m, in \u001b[36mdraw_graph\u001b[39m\u001b[34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, collect_attributes, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m input_recorder_tensor, kwargs_record_tensor, input_nodes = process_input(\n\u001b[32m    217\u001b[39m     input_data, input_size, kwargs, device, dtypes, collect_attributes\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m model_graph = ComputationGraph(\n\u001b[32m    221\u001b[39m     visual_graph, input_nodes, show_shapes, expand_nested,\n\u001b[32m    222\u001b[39m     hide_inner_tensors, hide_module_functions, roll, depth, collect_attributes\n\u001b[32m    223\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_recorder_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_record_tensor\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m model_graph.fill_visual_graph()\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_graph:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchview/torchview.py:269\u001b[39m, in \u001b[36mforward_prop\u001b[39m\u001b[34m(model, x, device, model_graph, mode, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnknown input type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    270\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchgraph see error message\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    273\u001b[39m     model.train(saved_model_mode)\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to run torchgraph see error message"
     ]
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "dummy_img = (np.random.rand(224, 224, 3) * 255).astype(\"uint8\")\n",
    "img1 = Image.fromarray(dummy_img)\n",
    "img2 = Image.fromarray(dummy_img)\n",
    "images = [img1, img2]\n",
    "\n",
    "\n",
    "dummy_wave = torch.randn(2, 16000)\n",
    "dummy_sr = 16000\n",
    "\n",
    "graph = draw_graph(\n",
    "    model, \n",
    "    input_data=( [None]*2, dummy_wave, dummy_sr ),  # images=None placeholder\n",
    "    expand_nested=True,\n",
    "    save_graph=True,\n",
    "    directory=\"arch_plots\",\n",
    "    filename=\"alignment_model_torchview\",\n",
    ")\n",
    "graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d9cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8227efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f39f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12526137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4de35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2c739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dd8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc57206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ae316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ae36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33a485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
