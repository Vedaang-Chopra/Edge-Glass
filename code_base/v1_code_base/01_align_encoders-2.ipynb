{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eef5dd",
   "metadata": {},
   "source": [
    "## Main Notebook -> For Running Alignment (Stage-1 Training)\n",
    "\n",
    "This notebook is used for alignment training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from imports.configs.config import setup_from_yaml\n",
    "from imports.dataset import PixmoFeatureDataset, LibriSpeechFeatureDataset, collate_alignment\n",
    "from imports.perceiver import PerceiverLatentEncoder, ProjectorMLP\n",
    "from imports.align_training.text_encoder import HFTextEncoderConfig, HFTextEncoder\n",
    "from imports.align_training.steps import AlignmentModules, AlignmentConfig\n",
    "from imports.align_training.training import build_alignment_optimizer, train_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = setup_from_yaml(\"imports/configs/config.yaml\")  # uses your Config + YAML loader\n",
    "device = cfg.torch_device\n",
    "dtype = cfg.torch_dtype\n",
    "print(\"Device:\", device, \"dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530dd21e",
   "metadata": {},
   "source": [
    "### 6.2 Build datasets & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.models.llm_model_name)\n",
    "\n",
    "# Vision (PixMo) features\n",
    "pixmo_train = PixmoFeatureDataset(cfg.datasets.pixmo_train_index)\n",
    "pixmo_val   = PixmoFeatureDataset(cfg.datasets.pixmo_val_index)\n",
    "\n",
    "# Optional small subsets for quick testing\n",
    "if cfg.training.train_subset_size and cfg.training.train_subset_size < len(pixmo_train):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_train = Subset(pixmo_train, range(cfg.training.train_subset_size))\n",
    "\n",
    "if cfg.training.val_subset_size and cfg.training.val_subset_size < len(pixmo_val):\n",
    "    from torch.utils.data import Subset\n",
    "    pixmo_val = Subset(pixmo_val, range(cfg.training.val_subset_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nvidia\n",
    "\n",
    "vision_train_loader = DataLoader(\n",
    "    pixmo_train,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n",
    "vision_val_loader = DataLoader(\n",
    "    pixmo_val,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda b: collate_alignment(b, tokenizer),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# collate_fn = partial(collate_alignment, tokenizer=tokenizer)\n",
    "\n",
    "# vision_train_loader = DataLoader(\n",
    "#     pixmo_train,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,  # <-- IMPORTANT for now on macOS\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# vision_val_loader = DataLoader(\n",
    "#     pixmo_val,\n",
    "#     batch_size=cfg.training.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,  # <-- same here\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = {\n",
    "    \"vision\": vision_train_loader,\n",
    "    # \"audio\": audio_train_loader,  # add later if you want\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.dataset import LibriSpeechFeatureDataset, collate_alignment\n",
    "from functools import partial\n",
    "\n",
    "# Only if you actually set librispeech_train_index in config.yaml\n",
    "if cfg.datasets.use_librispeech and cfg.datasets.librispeech_train_index is not None:\n",
    "    print(\"Loading LibriSpeech feature dataset from:\", cfg.datasets.librispeech_train_index)\n",
    "    audio_train = LibriSpeechFeatureDataset(cfg.datasets.librispeech_train_index)\n",
    "\n",
    "    # Optional: subset for quick debugging\n",
    "    if cfg.training.train_subset_size and cfg.training.train_subset_size < len(audio_train):\n",
    "        from torch.utils.data import Subset\n",
    "        audio_train = Subset(audio_train, range(cfg.training.train_subset_size))\n",
    "\n",
    "    # Reuse the same collate fn as vision, with tokenizer bound\n",
    "    audio_collate_fn = partial(collate_alignment, tokenizer=tokenizer)\n",
    "\n",
    "    audio_train_loader = DataLoader(\n",
    "        audio_train,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,             # IMPORTANT on macOS; avoids pickling issues\n",
    "        collate_fn=audio_collate_fn,\n",
    "    )\n",
    "\n",
    "    train_loaders[\"audio\"] = audio_train_loader\n",
    "    print(\"Audio train samples:\", len(audio_train))\n",
    "else:\n",
    "    print(\"LibriSpeech not enabled or librispeech_train_index missing in config.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ddd7",
   "metadata": {},
   "source": [
    "### 6.3 Build text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cba83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cfg = HFTextEncoderConfig(\n",
    "    model_name=cfg.models.llm_model_name,\n",
    "    max_length=128,\n",
    "    trainable=False,  # Stage-1: keep frozen\n",
    ")\n",
    "\n",
    "text_encoder = HFTextEncoder(\n",
    "    cfg=txt_cfg,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "d_text = text_encoder.hidden_size\n",
    "print(\"Text hidden size:\", d_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa06dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embed_fn(texts: list[str], max_length: int) -> torch.Tensor:\n",
    "    # We could override max_length by rebuilding text_encoder, but usually\n",
    "    # HFTextEncoderConfig.max_length is enough, so we ignore this arg.\n",
    "    with torch.no_grad():\n",
    "        return text_encoder.encode(texts).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12937dcf",
   "metadata": {},
   "source": [
    "### 6.4 Build adapters, Perceiver, projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek one example to get feature dim\n",
    "sample = pixmo_train[0] if not isinstance(pixmo_train, torch.utils.data.Subset) else pixmo_train.dataset[pixmo_train.indices[0]]\n",
    "d_feat_v = sample[\"features\"].shape[-1]\n",
    "print(\"Vision feature dim:\", d_feat_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = audio_train[0] if not isinstance(audio_train, torch.utils.data.Subset) else audio_train.dataset[audio_train.indices[0]]\n",
    "d_feat_a = sample[\"features\"].shape[-1]\n",
    "print(\"Audio feature dim:\", d_feat_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_perceiver = cfg.architecture.perceiver_dim or d_feat_v\n",
    "print(\"Perceiver dim:\", d_perceiver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_adapter = nn.Linear(d_feat_v, d_perceiver).to(device=device, dtype=dtype)\n",
    "audio_adapter = nn.Linear(d_feat_a, d_perceiver).to(device=device, dtype=dtype)\n",
    "\n",
    "perceiver = PerceiverLatentEncoder(\n",
    "    num_latents=cfg.architecture.num_latents,\n",
    "    d_latent=d_perceiver,\n",
    "    d_input=d_perceiver,\n",
    "    num_layers=cfg.architecture.num_perceiver_layers,\n",
    "    num_heads=cfg.architecture.num_attn_heads,\n",
    "    mlp_ratio=cfg.architecture.mlp_ratio,\n",
    "    dropout=0.1,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "projector = ProjectorMLP(\n",
    "    d_in=d_perceiver,\n",
    "    d_out=d_text,\n",
    "    hidden_factor=2.0,\n",
    "    dropout=0.1,\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "modules = AlignmentModules(\n",
    "    vision_adapter=vision_adapter,\n",
    "    audio_adapter=None,  # add audio later if needed\n",
    "    perceiver=perceiver,\n",
    "    projector=projector,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_adapter = nn.Linear(d_feat_v, d_perceiver).to(device=device, dtype=dtype)\n",
    "perceiver = PerceiverLatentEncoder(...)\n",
    "projector = ProjectorMLP(...)\n",
    "modules = AlignmentModules(\n",
    "    vision_adapter=vision_adapter,\n",
    "    audio_adapter=None,\n",
    "    perceiver=perceiver,\n",
    "    projector=projector,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d387c15",
   "metadata": {},
   "source": [
    "### 6.5 Alignment config + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb622e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRL radii from config; default to full dim if None\n",
    "mrl_dims = tuple(cfg.mrl.mrl_dims) if cfg.mrl.mrl_dims is not None else (d_text,)\n",
    "\n",
    "align_cfg = AlignmentConfig(\n",
    "    mrl_dims=mrl_dims,\n",
    "    mrl_temperature=cfg.mrl.mrl_temp,\n",
    "    max_text_length=64,  # arbitrary; text_encoder already truncates internally\n",
    ")\n",
    "\n",
    "optimizer = build_alignment_optimizer(\n",
    "    modules=modules,\n",
    "    learning_rate=float(cfg.training.learning_rate),\n",
    "    weight_decay=float(cfg.training.weight_decay),\n",
    ")\n",
    "print(\"Trainable params:\", sum(p.numel() for p in optimizer.param_groups[0][\"params\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bf941",
   "metadata": {},
   "source": [
    "### 6.6 Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "# train_alignment(\n",
    "#     train_loaders=train_loaders,\n",
    "#     modules=modules,\n",
    "#     cfg=align_cfg,\n",
    "#     text_embed_fn=text_embed_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     num_epochs=mean_epochs,\n",
    "#     log_every=cfg.training.log_every_steps,\n",
    "#     log_fn=None,   # or pass wandb.log\n",
    "#     modalities=(\"vision\",),  # add \"audio\" when you wire audio\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_epochs = cfg.training.num_epochs\n",
    "\n",
    "train_alignment(\n",
    "    train_loaders=train_loaders,\n",
    "    modules=modules,\n",
    "    cfg=align_cfg,\n",
    "    text_embed_fn=text_embed_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=mean_epochs,\n",
    "    log_every=cfg.training.log_every_steps,\n",
    "    log_fn=None,\n",
    "    modalities=(\"vision\",),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0d323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb09fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
