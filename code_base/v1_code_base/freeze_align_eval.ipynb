{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab4dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 0. Setup\n",
    "# =============================\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import random\n",
    "import time\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# --- Imports for models ---\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    CLIPVisionModel, \n",
    "    CLIPImageProcessor, \n",
    "    WhisperModel, \n",
    "    WhisperProcessor\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f4aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adapter device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set device for adapters (Qwen manages its own device map)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using adapter device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. Configuration\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Checkpoint path (Ensure this matches where you saved it!)\n",
    "    ckpt_path: Path = Path(\"./runs_perceiver_mrl_qwen/multimodal_adapter_poc_1.pt\")\n",
    "    \n",
    "    # Models\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    \n",
    "    # Architecture Dims (Must match training!)\n",
    "    perceiver_dim: int = 512\n",
    "    num_latents: int = 64\n",
    "    num_perceiver_layers: int = 2\n",
    "    num_attn_heads: int = 8\n",
    "    mlp_ratio: float = 4.0\n",
    "    \n",
    "    # Filled dynamically later\n",
    "    encoder_dim_vision: int = 768 \n",
    "    encoder_dim_audio: int = 512 \n",
    "    llm_hidden_size: int = 3584\n",
    "    \n",
    "    # Data for Eval\n",
    "    batch_size: int = 8\n",
    "    vision_max_samples: int = 100  # Small subset for fast eval\n",
    "    audio_max_samples: int = 100\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692b03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 2. Architecture Definitions (Must match training code)\n",
    "# ============================================================\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "        self.mlp = FeedForward(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, latents, tokens, token_mask=None):\n",
    "        # Cross-Attention\n",
    "        q = self.ln_latents_1(latents)\n",
    "        kv = self.ln_tokens(tokens)\n",
    "        # Create key_padding_mask (True for ignored positions)\n",
    "        key_padding_mask = ~token_mask.bool() if token_mask is not None else None\n",
    "        \n",
    "        attn_out, _ = self.cross_attn(q, kv, kv, key_padding_mask=key_padding_mask, need_weights=False)\n",
    "        latents = latents + attn_out\n",
    "        \n",
    "        # Self-Attention\n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_out, _ = self.self_attn(q2, q2, q2, need_weights=False)\n",
    "        latents = latents + self_out\n",
    "        \n",
    "        # MLP\n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "        return latents\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, dim, num_latents, num_layers, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads, mlp_ratio) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, tokens, token_mask=None):\n",
    "        B = tokens.shape[0]\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a126d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. The Aligned Model Wrapper (Fixed for Device Map)\n",
    "# ============================================================\n",
    "\n",
    "class AlignedModel(nn.Module):\n",
    "    def __init__(self, vision_adapter, audio_adapter, perceiver, projector, qwen_model, qwen_tokenizer):\n",
    "        super().__init__()\n",
    "        self.vision_adapter = vision_adapter\n",
    "        self.audio_adapter = audio_adapter\n",
    "        self.perceiver = perceiver\n",
    "        self.projector = projector\n",
    "        self.qwen_model = qwen_model\n",
    "        self.qwen_tokenizer = qwen_tokenizer\n",
    "\n",
    "    def encode_image_features(self, features, mask):\n",
    "        # Move inputs to the same device as the adapter (which is on 'device')\n",
    "        adapter_dev = next(self.vision_adapter.parameters()).device\n",
    "        features = features.to(adapter_dev)\n",
    "        mask = mask.to(adapter_dev)\n",
    "        \n",
    "        tokens = self.vision_adapter(features)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        z_llm = self.projector(latents)\n",
    "        return z_llm.mean(dim=1) # Pooling for retrieval\n",
    "\n",
    "    def encode_audio_features(self, features, mask):\n",
    "        adapter_dev = next(self.audio_adapter.parameters()).device\n",
    "        features = features.to(adapter_dev)\n",
    "        mask = mask.to(adapter_dev)\n",
    "        \n",
    "        tokens = self.audio_adapter(features)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        z_llm = self.projector(latents)\n",
    "        return z_llm.mean(dim=1)\n",
    "\n",
    "    def encode_text_raw(self, texts: list[str]):\n",
    "        # Find Qwen's device for embeddings\n",
    "        qwen_dev = self.qwen_model.model.embed_tokens.weight.device\n",
    "        \n",
    "        enc = self.qwen_tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=64, return_tensors=\"pt\"\n",
    "        ).to(qwen_dev)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embs = self.qwen_model.get_input_embeddings()(enc.input_ids)\n",
    "            \n",
    "        mask = enc.attention_mask.unsqueeze(-1)\n",
    "        sum_embs = (token_embs * mask).sum(dim=1)\n",
    "        count = mask.sum(dim=1).clamp(min=1)\n",
    "        return sum_embs / count\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, modality_feats, modality_type, prompt_text, max_new_tokens=50):\n",
    "        \"\"\"\n",
    "        Generates text from an image or audio feature input.\n",
    "        \"\"\"\n",
    "        # 1. Project Modality (on adapter device)\n",
    "        if modality_type == \"vision\":\n",
    "            adapter = self.vision_adapter\n",
    "        elif modality_type == \"audio\":\n",
    "            adapter = self.audio_adapter\n",
    "            \n",
    "        adapter_dev = next(adapter.parameters()).device\n",
    "        modality_feats = modality_feats.to(adapter_dev)\n",
    "        \n",
    "        # Create mask (B=1)\n",
    "        mask = torch.ones(1, modality_feats.shape[1], dtype=torch.bool, device=adapter_dev)\n",
    "        \n",
    "        tokens = adapter(modality_feats)\n",
    "        latents = self.perceiver(tokens, mask)\n",
    "        inputs_embeds_modality = self.projector(latents) # (1, 64, 3584)\n",
    "\n",
    "        # 2. Embed Text (on Qwen device)\n",
    "        qwen_dev = self.qwen_model.model.embed_tokens.weight.device\n",
    "        qwen_dtype = self.qwen_model.model.embed_tokens.weight.dtype\n",
    "        \n",
    "        # Move modality embeds to Qwen device for concatenation\n",
    "        inputs_embeds_modality = inputs_embeds_modality.to(qwen_dev)\n",
    "        \n",
    "        text_inputs = self.qwen_tokenizer(prompt_text, return_tensors=\"pt\").to(qwen_dev)\n",
    "        inputs_embeds_text = self.qwen_model.get_input_embeddings()(text_inputs.input_ids)\n",
    "\n",
    "        # 3. Concatenate\n",
    "        final_embeds = torch.cat([inputs_embeds_modality, inputs_embeds_text], dim=1)\n",
    "        # CRITICAL FIX: Force the combined embeddings to match Qwen's weight dtype (BF16)\n",
    "        if final_embeds.dtype != qwen_dtype:\n",
    "            final_embeds = final_embeds.to(qwen_dtype)\n",
    "            \n",
    "        # 4. Generate\n",
    "        out = self.qwen_model.generate(\n",
    "            inputs_embeds=final_embeds,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        return self.qwen_tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00af441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Load Model Function\n",
    "# ============================================================\n",
    "\n",
    "def load_full_model(cfg_obj):\n",
    "    print(\"\\n--- Loading Full Inference System ---\")\n",
    "    print(\"1. Loading Frozen Qwen (Auto Device Map)...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg_obj.llm_model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # Load Qwen with device_map=\"auto\" -> Allows splitting/offloading\n",
    "    qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg_obj.llm_model_name, \n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "\n",
    "    print(\"2. Initializing Adapters...\")\n",
    "    v_adapt = ModalityAdapter(cfg_obj.encoder_dim_vision, cfg_obj.perceiver_dim)\n",
    "    a_adapt = ModalityAdapter(cfg_obj.encoder_dim_audio, cfg_obj.perceiver_dim)\n",
    "    \n",
    "    perc = PerceiverResampler(\n",
    "        dim=cfg_obj.perceiver_dim, \n",
    "        num_latents=cfg_obj.num_latents,\n",
    "        num_layers=cfg_obj.num_perceiver_layers,\n",
    "        num_heads=cfg_obj.num_attn_heads,\n",
    "        mlp_ratio=cfg_obj.mlp_ratio\n",
    "    )\n",
    "    \n",
    "    proj = nn.Linear(cfg_obj.perceiver_dim, cfg_obj.llm_hidden_size)\n",
    "\n",
    "    # Load Weights\n",
    "    print(f\"3. Loading adapter weights from {cfg_obj.ckpt_path}...\")\n",
    "    try:\n",
    "        ckpt = torch.load(cfg_obj.ckpt_path, map_location=\"cpu\")\n",
    "        v_adapt.load_state_dict(ckpt[\"vision_adapter\"])\n",
    "        a_adapt.load_state_dict(ckpt[\"audio_adapter\"])\n",
    "        perc.load_state_dict(ckpt[\"perceiver\"])\n",
    "        proj.load_state_dict(ckpt[\"projector\"])\n",
    "        print(\"✅ Weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Checkpoint not found! Using random init (expect garbage results).\")\n",
    "\n",
    "    # MOVE ADAPTERS TO DEVICE (Manual)\n",
    "    # Qwen handles its own split via device_map=\"auto\"\n",
    "    # We assume adapters fit on the main execution device (cuda:0 or cpu)\n",
    "    adapter_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    v_adapt.to(adapter_device).eval()\n",
    "    a_adapt.to(adapter_device).eval()\n",
    "    perc.to(adapter_device).eval()\n",
    "    proj.to(adapter_device).eval()\n",
    "\n",
    "    # Create Wrapper (Do NOT call .to() on this wrapper!)\n",
    "    model = AlignedModel(v_adapt, a_adapt, perc, proj, qwen, tokenizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb01872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Full Inference System ---\n",
      "1. Loading Frozen Qwen (Auto Device Map)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6daa6ffb789438e96de92e7e51ba0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Initializing Adapters...\n",
      "3. Loading adapter weights from runs_perceiver_mrl_qwen/multimodal_adapter_poc_1.pt...\n",
      "✅ Weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "eval_model = load_full_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b46e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(modality_fn, items):\n",
    "    all_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(items):\n",
    "            e = modality_fn(x)\n",
    "            all_embeds.append(e.cpu().numpy())\n",
    "    return np.vstack(all_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1718b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_rank(query_embed, gallery_embed):\n",
    "    sims = cosine_similarity(query_embed[None, :], gallery_embed)[0]\n",
    "    sorted_idx = np.argsort(-sims)  # descending\n",
    "    return sorted_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(query_embeds, gallery_embeds, true_indices, k=1):\n",
    "    hits = 0\n",
    "    for i in range(len(query_embeds)):\n",
    "        idxs = retrieval_rank(query_embeds[i], gallery_embeds)[:k]\n",
    "        if true_indices[i] in idxs:\n",
    "            hits += 1\n",
    "    return hits / len(query_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f54652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cka(X, Y):\n",
    "    X = X - X.mean(0, keepdims=True)\n",
    "    Y = Y - Y.mean(0, keepdims=True)\n",
    "    numerator = np.linalg.norm(X.T @ Y, ord='fro') ** 2\n",
    "    denom = np.linalg.norm(X.T @ X, ord='fro') * np.linalg.norm(Y.T @ Y, ord='fro')\n",
    "    return numerator / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca91ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_stats(E, name=\"\"):\n",
    "    norms = np.linalg.norm(E, axis=1)\n",
    "    print(f\"\\n{name} Embedding stats:\")\n",
    "    print(\"  mean norm:\", norms.mean())\n",
    "    print(\"  std:\", norms.std())\n",
    "    print(\"  min:\", norms.min())\n",
    "    print(\"  max:\", norms.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac4d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb65d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9c14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
