{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93fdbda",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 1 â€“ Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8088881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import random\n",
    "import io\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.dummy import Pool as ThreadPool  # threads (Jupyter-friendly)\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(hf_name='openslr/librispeech_asr', hf_config='all', hf_split='train.clean.360', librispeech_max_samples=1000, audio_sample_rate=16000, max_audio_duration_s=12.0, whisper_model_name='openai/whisper-base', root_dir=PosixPath('data/librispeech'), features_dir=PosixPath('data/librispeech/features'), device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # HF LibriSpeech\n",
    "    hf_name: str = \"openslr/librispeech_asr\"\n",
    "    hf_config: str = \"all\"                # matches your snippet\n",
    "    hf_split: str = \"train.clean.360\"     # subset for alignment\n",
    "\n",
    "    # Sampling & filtering\n",
    "    librispeech_max_samples: int = 1000   # how many to take from streaming\n",
    "    audio_sample_rate: int = 16_000       # target sample rate\n",
    "    max_audio_duration_s: float = 12.0    # filter long clips\n",
    "\n",
    "    # Whisper encoder\n",
    "    whisper_model_name: str = \"openai/whisper-base\"  # or tiny / small etc.\n",
    "\n",
    "    # Paths\n",
    "    root_dir: Path = Path(\"./data/librispeech\")\n",
    "    features_dir: Path = Path(\"./data/librispeech/features\")\n",
    "\n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.features_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bb362",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 2 â€“ Load Whisper Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4250262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper encoder: openai/whisper-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d606ac6065a24ca5b0f558725d630645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8356192782443a78c88bbcb616a9cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f26207ffa4575bb864cc7648a34da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee22ee155ac54521b34b622d7a15a243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616e243a5900414b8cde2506a224acc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f99ba18423f40c28ca197136cbdf023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19771fc7e886411882a93fbadb9094ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66795ee01bc4d09afe6768a94f7de3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b5ca626bfc48dcb4e61843a7df8ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740323f4268340e797890db4d6e1f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper encoder dim (d_audio): 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperModel\n",
    "\n",
    "print(\"Loading Whisper encoder:\", cfg.whisper_model_name)\n",
    "\n",
    "audio_processor = WhisperProcessor.from_pretrained(cfg.whisper_model_name)\n",
    "audio_model = WhisperModel.from_pretrained(cfg.whisper_model_name)\n",
    "audio_model.eval().to(cfg.device)\n",
    "\n",
    "d_audio = audio_model.config.d_model\n",
    "print(\"Whisper encoder dim (d_audio):\", d_audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03fd82",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 3 â€“ Streaming LibriSpeech, Subset & Duration Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bedd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LibriSpeech ASR in streaming mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf43ec7584014c38a5cae7fe012887df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e428a432ba3b4ec5ac0f5ecff38eeb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3ab67a7c94179afc8a6191add9cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4fb1704e6433199c68fdd58a136a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded streaming dataset: IterableDataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_shards: 48\n",
      "})\n",
      "\n",
      "Taking up to 1000 examples in streaming mode...\n",
      "\n",
      "Subset collected: 1000\n",
      "Keys: dict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\n",
      "Example 0 (truncated): {'file': '/home/albert/.cache/huggingface/datasets/downloads/extracted/41321bb258f165f6163b1b0335f41c1ae65f86988d636209307b7ed94af929a4/1487-133273-0000.flac', 'text': 'THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT', 'speaker_id': 1487, 'chapter_id': 133273, 'id': '1487-133273-0000'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading LibriSpeech ASR in streaming mode...\")\n",
    "\n",
    "librispeech_raw = load_dataset(\n",
    "    cfg.hf_name,\n",
    "    cfg.hf_config,\n",
    "    streaming=True,\n",
    "    split=cfg.hf_split,\n",
    ")\n",
    "\n",
    "print(\"Loaded streaming dataset:\", librispeech_raw)\n",
    "\n",
    "# Disable automatic decoding â†’ we want raw bytes for librosa\n",
    "audio_stream = librispeech_raw.decode(False)\n",
    "\n",
    "max_samples = cfg.librispeech_max_samples\n",
    "subset = []\n",
    "\n",
    "print(f\"\\nTaking up to {max_samples} examples in streaming mode...\")\n",
    "\n",
    "for ex in audio_stream:\n",
    "    subset.append(ex)\n",
    "    if len(subset) >= max_samples:\n",
    "        break\n",
    "\n",
    "print(\"\\nSubset collected:\", len(subset))\n",
    "if len(subset) > 0:\n",
    "    print(\"Keys:\", subset[0].keys())\n",
    "    print(\"Example 0 (truncated):\", {k: subset[0][k] for k in subset[0].keys() if k != \"audio\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3806d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform_from_streaming_example(example, target_sr=16_000):\n",
    "    \"\"\"\n",
    "    Convert LibriSpeech streaming example -> mono waveform @ target_sr.\n",
    "    \"\"\"\n",
    "    audio_info = example[\"audio\"]\n",
    "    audio_bytes = audio_info[\"bytes\"]\n",
    "    if audio_bytes is None:\n",
    "        raise ValueError(\"No audio bytes in example.\")\n",
    "\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "    wav, sr = librosa.load(audio_file, sr=target_sr)\n",
    "    return wav, sr\n",
    "\n",
    "def compute_duration(wav, sr):\n",
    "    return len(wav) / float(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering by duration â‰¤ 12.0 seconds...\n",
      "After duration filtering: 338 examples\n"
     ]
    }
   ],
   "source": [
    "filtered = []\n",
    "\n",
    "print(\"\\nFiltering by duration â‰¤\", cfg.max_audio_duration_s, \"seconds...\")\n",
    "\n",
    "for ex in subset:\n",
    "    try:\n",
    "        wav, sr = load_waveform_from_streaming_example(ex, cfg.audio_sample_rate)\n",
    "    except Exception as e:\n",
    "        # Log decode issues quietly if you like, or just skip\n",
    "        log_path = cfg.root_dir / \"decode_skipped.log\"\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(f\"decode_error\\ttext={ex.get('text', '')[:80]!r}\\terror={repr(e)}\\n\")\n",
    "        continue\n",
    "\n",
    "    dur = compute_duration(wav, sr)\n",
    "\n",
    "    if dur <= cfg.max_audio_duration_s:\n",
    "        filtered.append({\n",
    "            \"waveform\": wav,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"duration\": dur,\n",
    "            \"text\": ex[\"text\"],\n",
    "        })\n",
    "\n",
    "print(\"After duration filtering:\", len(filtered), \"examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b379767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing a few filtered samples...\n",
      "\n",
      "Sample 0:\n",
      "  Duration: 11.79 s\n",
      "  Transcript: IN THE EXERCISE OF THE EXECUTIVE POWER THE PRESIDENT OF THE UNITED STATES IS CONSTANTLY SUBJECT TO A JEALOUS SCRUTINY HE MAY MAKE BUT HE CANNOT CONCLUDE A TREATY\n",
      "  Waveform shape: (188560,)\n",
      "\n",
      "Sample 1:\n",
      "  Duration: 10.94 s\n",
      "  Transcript: PUBLIC OPINION IS THE PREDOMINANT AUTHORITY IN BOTH OF THEM THE FUNDAMENTAL PRINCIPLE OF LEGISLATION A PRINCIPLE ESSENTIALLY REPUBLICAN IS THE SAME IN BOTH COUNTRIES\n",
      "  Waveform shape: (174960,)\n",
      "\n",
      "Sample 2:\n",
      "  Duration: 11.47 s\n",
      "  Transcript: THE CONTRAST WOULD HAVE BEEN RENDERED STILL MORE STRIKING I HAVE REMARKED THAT THE AUTHORITY OF THE PRESIDENT IN THE UNITED STATES IS ONLY EXERCISED WITHIN THE LIMITS OF A PARTIAL SOVEREIGNTY\n",
      "  Waveform shape: (183600,)\n",
      "\n",
      "Sample 3:\n",
      "  Duration: 7.39 s\n",
      "  Transcript: AND THOSE WHICH IT WOULD CARRY INTO EFFECT THE PRESIDENT OF THE UNITED STATES IS THE COMMANDER IN CHIEF OF THE ARMY\n",
      "  Waveform shape: (118240,)\n",
      "\n",
      "Sample 4:\n",
      "  Duration: 3.29 s\n",
      "  Transcript: WHICH CIRCUMSTANCES DO NOT PERMIT HIM TO EMPLOY\n",
      "  Waveform shape: (52720,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShowing a few filtered samples...\")\n",
    "\n",
    "for i in range(min(5, len(filtered))):\n",
    "    ex = filtered[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(\"  Duration:\", round(ex[\"duration\"], 2), \"s\")\n",
    "    print(\"  Transcript:\", ex[\"text\"])\n",
    "    print(\"  Waveform shape:\", ex[\"waveform\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9064a35",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 4 â€“ Whisper Encoding Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b95b96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_encode_sequence(wav: np.ndarray, sr: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    wav: 1D numpy array (time,)\n",
    "    sr:  sampling rate (expected 16k)\n",
    "    Returns:\n",
    "        feats: Tensor(T_enc, d_audio) on CPU (float16)\n",
    "    \"\"\"\n",
    "    inputs = audio_processor(\n",
    "        wav,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_features = inputs[\"input_features\"].to(cfg.device)  # (1, T_mel, 80)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out = audio_model.encoder(input_features)\n",
    "        hidden = enc_out.last_hidden_state  # (1, T_enc, d_audio)\n",
    "\n",
    "    feats = hidden.squeeze(0).to(torch.float16).cpu()  # (T_enc, d_audio)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416143dd",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 5 â€“ Per-Example Processing & Saving (with quiet logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8b6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_audio_example(ex, idx: int, split_name: str = \"train\"):\n",
    "    \"\"\"\n",
    "    ex: entry from `filtered` list\n",
    "    idx: index within that list\n",
    "    split_name: typically \"train\" for this notebook\n",
    "\n",
    "    Saves:\n",
    "      - <cfg.features_dir>/<split_name>_feat_<idx>.pt\n",
    "\n",
    "    Returns:\n",
    "      metadata dict or None if error.\n",
    "    \"\"\"\n",
    "    wav = ex[\"waveform\"]\n",
    "    sr = ex[\"sampling_rate\"]\n",
    "    dur = ex[\"duration\"]\n",
    "    text = ex[\"text\"]\n",
    "\n",
    "    try:\n",
    "        feats = whisper_encode_sequence(wav, sr)  # (T_enc, d_audio)\n",
    "    except Exception as e:\n",
    "        log_path = cfg.root_dir / f\"{split_name}_skipped.log\"\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"idx={idx}\\tduration={dur:.3f}\\ttext={text[:80]!r}\\terror={repr(e)}\\n\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    fname = f\"{split_name}_feat_{idx}.pt\"\n",
    "    fpath = cfg.features_dir / fname\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"features\": feats,     # (T_enc, d_audio)\n",
    "            \"text\": text,          # raw transcript\n",
    "            \"duration\": dur,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"orig_idx\": idx,\n",
    "        },\n",
    "        fpath,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"orig_idx\": idx,\n",
    "        \"file\": str(fpath),\n",
    "        \"num_frames\": feats.shape[0],\n",
    "        \"feat_dim\": feats.shape[1],\n",
    "        \"duration\": dur,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046e3be",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 6 â€“ Threaded Builder (train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3b92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_audio(args):\n",
    "    i, ex, split_name = args\n",
    "    return process_and_save_audio_example(ex, idx=i, split_name=split_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c41027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_audio_split_index_threaded(\n",
    "    examples,\n",
    "    split_name: str = \"train\",\n",
    "    workers: Optional[int] = None,\n",
    "    chunksize: int = 16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract & cache Whisper features for LibriSpeech examples using threads.\n",
    "\n",
    "    Creates:\n",
    "      - <cfg.root_dir>/<split_name>_index.json\n",
    "      - .pt feature files under cfg.features_dir\n",
    "    \"\"\"\n",
    "    if workers is None:\n",
    "        workers = min(8, cpu_count())\n",
    "\n",
    "    total = len(examples)\n",
    "    tasks = ((i, ex, split_name) for i, ex in enumerate(examples))\n",
    "    split_index = []\n",
    "\n",
    "    with ThreadPool(processes=workers) as pool:\n",
    "        for meta in tqdm(\n",
    "            pool.imap_unordered(_process_audio, tasks, chunksize=chunksize),\n",
    "            total=total,\n",
    "            desc=f\"Extracting {split_name} audio features (threads)\",\n",
    "        ):\n",
    "            if meta is not None:\n",
    "                split_index.append(meta)\n",
    "\n",
    "    index_path = cfg.root_dir / f\"{split_name}_index.json\"\n",
    "    with open(index_path, \"w\") as f:\n",
    "        json.dump(split_index, f, indent=2)\n",
    "\n",
    "    # Quiet summary\n",
    "    log_path = cfg.root_dir / f\"{split_name}_skipped.log\"\n",
    "    if log_path.exists():\n",
    "        n_skipped = sum(1 for _ in open(log_path, \"r\"))\n",
    "        print(\n",
    "            f\"Saved {split_name} index with {len(split_index)} items \"\n",
    "            f\"(skipped {n_skipped} examples, see {log_path})\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Saved {split_name} index with {len(split_index)} items (no skips).\")\n",
    "\n",
    "    return split_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 7 â€“ Run the Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e38f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building audio feature dataset from filtered LibriSpeech samples...\n",
      "Filtered examples: 338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train audio features (threads): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:25<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train index with 338 items (no skips).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding audio feature dataset from filtered LibriSpeech samples...\")\n",
    "print(\"Filtered examples:\", len(filtered))\n",
    "\n",
    "audio_index = build_audio_split_index_threaded(\n",
    "    filtered,\n",
    "    split_name=\"train\",\n",
    "    workers=4,\n",
    "    chunksize=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc1567",
   "metadata": {},
   "source": [
    "### ðŸ§© Cell 8 â€“ Sanity Check a Saved Feature File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced8e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338 train items\n",
      "First meta: {'orig_idx': 0, 'file': 'data/librispeech/features/train_feat_0.pt', 'num_frames': 1500, 'feat_dim': 512, 'duration': 11.785}\n",
      "Loaded features shape: torch.Size([1500, 512])\n",
      "Duration: 11.79 s\n",
      "Transcript snippet: IN THE EXERCISE OF THE EXECUTIVE POWER THE PRESIDENT OF THE UNITED STATES IS CONSTANTLY SUBJECT TO A JEALOUS SCRUTINY HE MAY MAKE BUT HE CANNOT CONCLUDE A TREATY ...\n"
     ]
    }
   ],
   "source": [
    "index_path = cfg.root_dir / \"train_index.json\"\n",
    "\n",
    "with open(index_path, \"r\") as f:\n",
    "    train_index = json.load(f)\n",
    "\n",
    "print(\"Loaded\", len(train_index), \"train items\")\n",
    "\n",
    "if len(train_index) > 0:\n",
    "    first_meta = train_index[0]\n",
    "    print(\"First meta:\", first_meta)\n",
    "\n",
    "    sample_path = first_meta[\"file\"]\n",
    "    blob = torch.load(sample_path)\n",
    "    feats = blob[\"features\"]\n",
    "    text = blob[\"text\"]\n",
    "    duration = blob[\"duration\"]\n",
    "\n",
    "    print(\"Loaded features shape:\", feats.shape)\n",
    "    print(\"Duration:\", round(duration, 2), \"s\")\n",
    "    print(\"Transcript snippet:\", text[:200], \"...\")\n",
    "else:\n",
    "    print(\"No train examples cached â€“ check earlier logs for errors.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
