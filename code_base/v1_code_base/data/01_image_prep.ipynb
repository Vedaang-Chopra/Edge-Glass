{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead86133",
   "metadata": {},
   "source": [
    "### Phase-0: - Setting up the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4b2b6",
   "metadata": {},
   "source": [
    "#### Step-1: - Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20a248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from multiprocessing.dummy import Pool as ThreadPool  # threads (Jupyter-friendly)\n",
    "from multiprocessing import cpu_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671dee37",
   "metadata": {},
   "source": [
    "#### Step-2:- Setting the Seed and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51082f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(dataset_name='allenai/pixmo-cap', train_subset_size=1000, val_subset_size=100, vision_model_name='facebook/dinov2-base', root_dir=PosixPath('data/pixmo'), features_dir=PosixPath('data/pixmo/features'), device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # HF dataset\n",
    "    dataset_name: str = \"allenai/pixmo-cap\"\n",
    "    train_subset_size: int = 1000   # you can change later\n",
    "    val_subset_size: int = 100\n",
    "\n",
    "    # Vision encoder\n",
    "    vision_model_name: str = \"facebook/dinov2-base\"  # or whatever you use\n",
    "\n",
    "    # Paths\n",
    "    root_dir: Path = Path(\"./data/pixmo\")\n",
    "    features_dir: Path = Path(\"./data/pixmo/features\")\n",
    "\n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.features_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5abed5",
   "metadata": {},
   "source": [
    "### Load Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe036ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision encoder: facebook/dinov2-base\n",
      "Vision hidden size: 768\n",
      "Perceiver dim (2 * hidden_size): 1536\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "print(\"Loading vision encoder:\", cfg.vision_model_name)\n",
    "\n",
    "vision_processor = AutoImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "vision_model = AutoModel.from_pretrained(cfg.vision_model_name, output_hidden_states=True)\n",
    "vision_model.eval().to(cfg.device)\n",
    "\n",
    "vision_hidden_size = vision_model.config.hidden_size\n",
    "perceiver_dim = 2 * vision_hidden_size\n",
    "print(\"Vision hidden size:\", vision_hidden_size)\n",
    "print(\"Perceiver dim (2 * hidden_size):\", perceiver_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c7f5a",
   "metadata": {},
   "source": [
    "### Cell 3 – Load, Filter, and Split PixMo-Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c2fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: allenai/pixmo-cap\n",
      "Dataset({\n",
      "    features: ['image_url', 'caption', 'transcripts'],\n",
      "    num_rows: 717042\n",
      "})\n",
      "Columns: ['image_url', 'caption', 'transcripts']\n"
     ]
    }
   ],
   "source": [
    "print(\"Using dataset:\", cfg.dataset_name)\n",
    "pixmo_raw = load_dataset(cfg.dataset_name, split=\"train\")\n",
    "print(pixmo_raw)\n",
    "print(\"Columns:\", pixmo_raw.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aea5d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 717042/717042 [00:02<00:00, 348291.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 717042\n",
      "Filtered size: 717042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def has_image_and_caption(ex):\n",
    "    url = ex.get(\"image_url\", None)\n",
    "    cap = ex.get(\"caption\", None)\n",
    "    return (url is not None) and (len(str(url)) > 0) and (cap is not None) and (len(cap.strip()) > 0)\n",
    "\n",
    "pixmo_filtered = pixmo_raw.filter(has_image_and_caption)\n",
    "\n",
    "print(\"Original size:\", len(pixmo_raw))\n",
    "print(\"Filtered size:\", len(pixmo_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4e766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 1000\n",
      "Val subset size: 100\n"
     ]
    }
   ],
   "source": [
    "# Train/val split with subset sizes\n",
    "total = len(pixmo_filtered)\n",
    "if total == 0:\n",
    "    raise ValueError(\"No valid examples left after filtering – check dataset or filter.\")\n",
    "\n",
    "train_size = min(cfg.train_subset_size, total - cfg.val_subset_size)\n",
    "val_size = min(cfg.val_subset_size, total - train_size)\n",
    "\n",
    "if train_size <= 0 or val_size <= 0:\n",
    "    # fallback: simple 90/10 split if config is too aggressive\n",
    "    print(\"⚠️ Falling back to 90/10 split due to small dataset.\")\n",
    "    split = pixmo_filtered.train_test_split(test_size=0.1, seed=SEED)\n",
    "    pixmo_train, pixmo_val = split[\"train\"], split[\"test\"]\n",
    "else:\n",
    "    pixmo_train = pixmo_filtered.select(range(train_size))\n",
    "    pixmo_val   = pixmo_filtered.select(range(train_size, train_size + val_size))\n",
    "\n",
    "print(\"Train subset size:\", len(pixmo_train))\n",
    "print(\"Val subset size:\", len(pixmo_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25381e28",
   "metadata": {},
   "source": [
    "### Image Fetch & Encoder Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108ee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(url: str) -> Image.Image:\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248c6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_patches(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    img: PIL.Image\n",
    "    returns: torch.Tensor of shape (num_patches, feat_dim_concat) on CPU\n",
    "    \"\"\"\n",
    "    inputs = vision_processor(images=img, return_tensors=\"pt\").to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        hidden_states = outputs.hidden_states  # tuple of length (num_layers + 1)\n",
    "\n",
    "    num_layers_plus_embed = len(hidden_states)\n",
    "    if num_layers_plus_embed < 4:\n",
    "        raise ValueError(f\"Not enough layers ({num_layers_plus_embed}) to take 2nd and 2nd-to-last.\")\n",
    "\n",
    "    h_low  = hidden_states[1]    # (B, seq_len, D)\n",
    "    h_high = hidden_states[-2]   # (B, seq_len, D)\n",
    "\n",
    "    h_low_patches  = h_low[:, 1:, :]    # drop CLS at position 0\n",
    "    h_high_patches = h_high[:, 1:, :]\n",
    "\n",
    "    feats = torch.cat([h_low_patches, h_high_patches], dim=-1)  # (B, num_patches, 2D)\n",
    "    feats = feats[0].to(torch.float16).cpu()  # (num_patches, 2D)\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d3f6f",
   "metadata": {},
   "source": [
    "### Per-Example Processing & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d6ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_example(ex, idx: int, split_name: str):\n",
    "    \"\"\"\n",
    "    ex: example from pixmo_train or pixmo_val\n",
    "    idx: index in that split\n",
    "    split_name: \"train\" or \"val\"\n",
    "    returns: metadata dict if success, None if failure\n",
    "    \"\"\"\n",
    "    url = ex[\"image_url\"]\n",
    "    caption = ex[\"caption\"].strip()\n",
    "\n",
    "    try:\n",
    "        img = fetch_image(url)\n",
    "        feats = encode_image_to_patches(img)  # (num_patches, feat_dim)\n",
    "    except Exception as e:\n",
    "        # ❌ No noisy print here\n",
    "        # ✅ Quietly append to a log file\n",
    "        log_path = cfg.root_dir / f\"{split_name}_skipped.log\"\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"idx={idx}\\turl={url}\\terror={repr(e)}\\n\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    fname = f\"{split_name}_feat_{idx}.pt\"\n",
    "    fpath = cfg.features_dir / fname\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"features\": feats,      # (num_patches, feat_dim), float16\n",
    "            \"caption\": caption,     # raw text\n",
    "            \"image_url\": url,\n",
    "            \"orig_idx\": idx,\n",
    "        },\n",
    "        fpath,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"orig_idx\": idx,\n",
    "        \"file\": str(fpath),\n",
    "        \"num_patches\": feats.shape[0],\n",
    "        \"feat_dim\": feats.shape[1],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c0bee",
   "metadata": {},
   "source": [
    "### Threaded Builders for Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cfdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_example(args):\n",
    "    i, ex, split_name = args\n",
    "    return process_and_save_example(ex, idx=i, split_name=split_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf92b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_index_threaded(dataset, split_name: str, workers: Optional[int] = None, chunksize: int = 32):\n",
    "    \"\"\"\n",
    "    Extract & cache features for a given split using threads.\n",
    "    Creates:\n",
    "      - <cfg.root_dir>/<split_name>_index.json\n",
    "      - .pt feature files under cfg.features_dir\n",
    "    \"\"\"\n",
    "    if workers is None:\n",
    "        workers = min(8, cpu_count())\n",
    "\n",
    "    try:\n",
    "        total = len(dataset)\n",
    "    except TypeError:\n",
    "        total = None\n",
    "\n",
    "    tasks = ((i, ex, split_name) for i, ex in enumerate(dataset))\n",
    "    split_index = []\n",
    "\n",
    "    with ThreadPool(processes=workers) as pool:\n",
    "        for meta in tqdm(\n",
    "            pool.imap_unordered(_process_example, tasks, chunksize=chunksize),\n",
    "            total=total,\n",
    "            desc=f\"Extracting {split_name} features (threads)\",\n",
    "        ):\n",
    "            if meta is not None:\n",
    "                split_index.append(meta)\n",
    "\n",
    "    index_path = cfg.root_dir / f\"{split_name}_index.json\"\n",
    "    with open(index_path, \"w\") as f:\n",
    "        json.dump(split_index, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {split_name} index with {len(split_index)} items to {index_path}\")\n",
    "    return split_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fc78c",
   "metadata": {},
   "source": [
    "### Run the Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features (threads):  22%|██▎       | 225/1000 [00:50<01:17, 10.05it/s]/opt/homebrew/Caskroom/miniconda/base/envs/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Extracting train features (threads): 100%|██████████| 1000/1000 [02:49<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train index with 873 items to data/pixmo/train_index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting val features (threads): 100%|██████████| 100/100 [00:14<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved val index with 89 items to data/pixmo/val_index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the main \"build\" step for this notebook\n",
    "\n",
    "train_index = build_split_index_threaded(pixmo_train, \"train\", workers=4)\n",
    "val_index   = build_split_index_threaded(pixmo_val,   \"val\",   workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e2007",
   "metadata": {},
   "source": [
    "### Sanity Check a Saved Feature File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bba3053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 873 train items\n",
      "First meta: {'orig_idx': 64, 'file': 'data/pixmo/features/train_feat_64.pt', 'num_patches': 256, 'feat_dim': 1536}\n",
      "Loaded features shape: torch.Size([256, 1536])\n",
      "Caption snippet: In this meme, a close-up photograph of a bald-headed Black man is prominently featured, capturing a raw moment of emotion with tears streaming down his face and lips pursed. The man, with a dark compl ...\n"
     ]
    }
   ],
   "source": [
    "index_path = cfg.root_dir / \"train_index.json\"\n",
    "\n",
    "with open(index_path, \"r\") as f:\n",
    "    train_index = json.load(f)\n",
    "\n",
    "print(\"Loaded\", len(train_index), \"train items\")\n",
    "\n",
    "if len(train_index) > 0:\n",
    "    first_meta = train_index[0]\n",
    "    print(\"First meta:\", first_meta)\n",
    "\n",
    "    sample_path = first_meta[\"file\"]\n",
    "    blob = torch.load(sample_path)\n",
    "    feats = blob[\"features\"]\n",
    "    caption = blob[\"caption\"]\n",
    "    print(\"Loaded features shape:\", feats.shape)\n",
    "    print(\"Caption snippet:\", caption[:200], \"...\")\n",
    "else:\n",
    "    print(\"No train examples cached – check earlier logs for errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384a833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ce3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
