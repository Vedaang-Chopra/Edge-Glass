{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Part 0 â€“ Imports, config, and utilities\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Device & dtype ----\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# prefer bfloat16 on newer GPUs, else float16\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    default_dtype = torch.bfloat16\n",
    "else:\n",
    "    default_dtype = torch.float16\n",
    "\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Default dtype:\", default_dtype)\n",
    "\n",
    "\n",
    "# ---- Repro utilities ----\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5569da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Part 0.1 â€“ Global config\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- Model names ---\n",
    "    # Vision encoder (you can swap to your PixMo vision backbone later)\n",
    "    vision_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "    # Audio encoder (Whisper)\n",
    "    audio_model_name: str = \"openai/whisper-base\"\n",
    "    audio_sample_rate: int = 16000\n",
    "\n",
    "    # Text encoder/decoder\n",
    "    llm_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "    # --- Dimensions (will be filled after loading models) ---\n",
    "    encoder_dim_vision: Optional[int] = None\n",
    "    encoder_dim_audio: Optional[int] = None\n",
    "    perceiver_dim: int = 512          # unified bottleneck dim\n",
    "    llm_hidden_size: Optional[int] = None\n",
    "\n",
    "    num_latents: int = 64             # Perceiver latent length\n",
    "\n",
    "    # --- Matryoshka loss (MRL) ---\n",
    "    use_mrl: bool = True\n",
    "    mrl_dims: Tuple[int, ...] = (128, 256, 512)\n",
    "    mrl_temperature: float = 0.07\n",
    "    mrl_weight: float = 0.1\n",
    "\n",
    "    # --- Training (we'll tune later) ---\n",
    "    batch_size_vision: int = 16\n",
    "    batch_size_audio: int = 16\n",
    "    max_train_steps_vision: int = 200\n",
    "    max_train_steps_audio: int = 200\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "        # --- Data paths & limits ---\n",
    "    # Vision feature cache (you can reuse PixMo / CLIP features here)\n",
    "    vision_features_root: Path = Path(\"./features_vision\")\n",
    "\n",
    "    # Audio feature cache for LibriSpeech (Whisper embeddings)\n",
    "    audio_features_root: Path = Path(\"./features_audio_librispeech\")\n",
    "\n",
    "    # LibriSpeech POC limits\n",
    "    librispeech_max_samples: int = 3000   # total train subset for alignment\n",
    "    max_audio_duration_s: float = 12.0    # filter very long clips\n",
    "\n",
    "    # --- Misc ---\n",
    "    seed: int = 42\n",
    "    log_every_steps: int = 20\n",
    "    save_dir: Path = Path(\"./runs_perceiver_mrl_qwen\")\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.vision_features_root.mkdir(parents=True, exist_ok=True)\n",
    "cfg.audio_features_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Config:\", asdict(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e223ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from dataclasses import asdict  # if not already imported\n",
    "\n",
    "run_name = cfg.run_name if hasattr(cfg, \"run_name\") else \"tri_modal_alignment\"\n",
    "\n",
    "wandb.init(\n",
    "    project=getattr(cfg, \"wandb_project\", \"edgeglass-multimodal\"),\n",
    "    name=run_name,\n",
    "    config=asdict(cfg),\n",
    ")\n",
    "\n",
    "# Optional: watch model modules (Perceiver + adapters + projector)\n",
    "wandb.watch(trainable_modules, log=\"all\", log_freq=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afbb49",
   "metadata": {},
   "source": [
    "### Phase-1: - Loading the Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 1 â€“ Load models: vision, audio, text (Qwen2.5-7B)\n",
    "# ============================================\n",
    "\n",
    "# ------------------------------\n",
    "# 1.1 Vision encoder (CLIP-style)\n",
    "# ------------------------------\n",
    "# For now we use CLIP as a simple vision encoder.\n",
    "# Later you can swap this for your PixMo vision encoder or precomputed features.\n",
    "\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "print(\"\\nLoading vision encoder:\", cfg.vision_model_name)\n",
    "vision_processor = CLIPImageProcessor.from_pretrained(cfg.vision_model_name)\n",
    "vision_model = CLIPVisionModel.from_pretrained(\n",
    "    cfg.vision_model_name,\n",
    "    torch_dtype=default_dtype,\n",
    "    device_map=None,\n",
    ").to(device)\n",
    "vision_model.eval()\n",
    "\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ðŸ”¥ Add this:\n",
    "cfg.encoder_dim_vision = vision_model.config.hidden_size\n",
    "print(\"Vision encoder_dim_vision:\", cfg.encoder_dim_vision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 1.2 Audio encoder (Whisper)\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nLoading audio encoder:\", cfg.audio_model_name)\n",
    "audio_processor = WhisperProcessor.from_pretrained(cfg.audio_model_name)\n",
    "audio_model = WhisperModel.from_pretrained(\n",
    "    cfg.audio_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None,\n",
    ").to(device)\n",
    "audio_model.eval()\n",
    "\n",
    "for p in audio_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "cfg.encoder_dim_audio = audio_model.config.d_model\n",
    "print(\"Audio hidden size:\", cfg.encoder_dim_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad61b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 1.3 Qwen2.5-7B (text encoder/decoder)\n",
    "# ------------------------------\n",
    "print(\"\\nLoading Qwen2.5-7B:\", cfg.llm_model_name)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.llm_model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "if qwen_tokenizer.pad_token is None:\n",
    "    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.llm_model_name,\n",
    "    torch_dtype=default_dtype,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "qwen_model.eval()\n",
    "\n",
    "for p in qwen_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ðŸ”¥ Robust extraction: handle int / list / tuple\n",
    "hidden_size = getattr(qwen_model.config, \"hidden_size\", None)\n",
    "if hidden_size is None:\n",
    "    raise ValueError(\"Could not find hidden_size in Qwen config!\")\n",
    "\n",
    "if isinstance(hidden_size, (list, tuple)):\n",
    "    hidden_size = hidden_size[0]\n",
    "\n",
    "cfg.llm_hidden_size = int(hidden_size)\n",
    "\n",
    "print(\"Qwen hidden_size (from config):\", hidden_size)\n",
    "print(\"cfg.llm_hidden_size:\", cfg.llm_hidden_size, type(cfg.llm_hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85102543",
   "metadata": {},
   "source": [
    "### Phase-2: - Adding MLP layer for MRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 2 â€“ Quick text embedding helper (for later MRL)\n",
    "# ============================================\n",
    "\n",
    "def encode_text_with_qwen(\n",
    "    texts: List[str],\n",
    "    max_length: int = 64,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize a batch of texts and return:\n",
    "        - input_ids\n",
    "        - attention_mask\n",
    "        - token_embeddings (from embedding layer, no LM forward yet)\n",
    "    \"\"\"\n",
    "    enc = qwen_tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # (B, L, D)\n",
    "    token_embs = qwen_model.get_input_embeddings()(enc.input_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"token_embs\": token_embs,\n",
    "    }\n",
    "\n",
    "print(\"Text embedding helper ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8523df",
   "metadata": {},
   "source": [
    "### Phase-3: - Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a453",
   "metadata": {},
   "source": [
    "#### Load the Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 3 â€“ LibriSpeech (Streaming) Audioâ€“Text Dataset\n",
    "# ============================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nLoading LibriSpeech ASR (streaming mode)...\")\n",
    "\n",
    "# Load only train.clean.100 from the giant 124GB dataset\n",
    "librispeech_raw = load_dataset(\n",
    "    \"openslr/librispeech_asr\",\n",
    "    \"all\",\n",
    "    streaming=True,\n",
    "    split=\"train.clean.100\"\n",
    ")\n",
    "\n",
    "print(\"Loaded streaming dataset:\", librispeech_raw)\n",
    "\n",
    "# Disable automatic decoding â†’ we want raw bytes for librosa\n",
    "audio_stream = librispeech_raw.decode(False)\n",
    "\n",
    "# We will collect up to cfg.librispeech_max_samples\n",
    "max_samples = cfg.librispeech_max_samples  # rename in your config if needed\n",
    "subset = []\n",
    "\n",
    "print(f\"\\nTaking up to {max_samples} examples in streaming mode...\")\n",
    "\n",
    "for ex in audio_stream:\n",
    "    subset.append(ex)\n",
    "    if len(subset) >= max_samples:\n",
    "        break\n",
    "\n",
    "print(\"\\nSubset collected:\", len(subset))\n",
    "print(\"Keys:\", subset[0].keys())\n",
    "print(\"Example 0:\", subset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f01d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: convert LibriSpeech streaming example â†’ waveform\n",
    "def load_waveform_from_streaming_example(example, target_sr=16000):\n",
    "    audio_info = example[\"audio\"]\n",
    "\n",
    "    audio_bytes = audio_info[\"bytes\"]\n",
    "    if audio_bytes is None:\n",
    "        raise ValueError(\"No audio bytes in example.\")\n",
    "\n",
    "    # Convert raw bytes â†’ file-like object\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "\n",
    "    # librosa loads PCM data and resamples to target_sr\n",
    "    wav, sr = librosa.load(audio_file, sr=target_sr)\n",
    "\n",
    "    return wav, sr\n",
    "\n",
    "\n",
    "# Helper: compute duration in seconds\n",
    "def compute_duration(wav, sr):\n",
    "    return len(wav) / float(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec95398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll filter to keep only clips <= cfg.max_audio_duration_s\n",
    "filtered = []\n",
    "\n",
    "print(\"\\nFiltering by duration â‰¤\", cfg.max_audio_duration_s, \"seconds...\")\n",
    "\n",
    "for ex in subset:\n",
    "    wav, sr = load_waveform_from_streaming_example(ex, cfg.audio_sample_rate)\n",
    "    dur = compute_duration(wav, sr)\n",
    "\n",
    "    if dur <= cfg.max_audio_duration_s:\n",
    "        filtered.append({\n",
    "            \"waveform\": wav,\n",
    "            \"sampling_rate\": sr,\n",
    "            \"duration\": dur,\n",
    "            \"text\": ex[\"text\"]\n",
    "        })\n",
    "\n",
    "print(\"After duration filtering:\", len(filtered), \"examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShowing a few filtered samples...\")\n",
    "\n",
    "for i in range(min(5, len(filtered))):\n",
    "    ex = filtered[i]\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(\"  Duration:\", round(ex[\"duration\"], 2), \"s\")\n",
    "    print(\"  Transcript:\", ex[\"text\"])\n",
    "    print(\"  Waveform shape:\", ex[\"waveform\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# New PixmoVisionDataset (uses HF 'image' column if available)\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "cols = pixmo_raw.column_names\n",
    "HAS_IMAGE_COL = \"image\" in cols\n",
    "\n",
    "if HAS_IMAGE_COL:\n",
    "    img_col = \"image\"\n",
    "else:\n",
    "    img_col = \"image_url\"\n",
    "\n",
    "txt_col = \"caption\"\n",
    "\n",
    "print(f\"Using image column: {img_col}\")\n",
    "\n",
    "\n",
    "class PixmoVisionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    On-the-fly image loading + CLIP feature extraction.\n",
    "\n",
    "    If 'image' column exists: uses HF-managed images (no manual HTTP).\n",
    "    Else: falls back to 'image_url' with robust skipping of bad URLs.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"features\": Tensor(T, d_vision),\n",
    "          \"text\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, vision_model, vision_processor, max_retries: int = 5):\n",
    "        self.ds = hf_dataset\n",
    "        self.vision_model = vision_model\n",
    "        self.vision_processor = vision_processor\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def _load_image_from_url(self, url: str) -> Image.Image:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        # do NOT let this propagate; we'll catch in __getitem__\n",
    "        resp.raise_for_status()\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _encode_image(self, img: Image.Image):\n",
    "        proc = self.vision_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = proc[\"pixel_values\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.vision_model(pixel_values=pixel_values)\n",
    "            # (1, T, d_vision)\n",
    "            feats = out.last_hidden_state.squeeze(0).to(\"cpu\")  # (T, d_vision)\n",
    "        return feats\n",
    "\n",
    "    def _get_example(self, idx: int):\n",
    "        ex = self.ds[idx]\n",
    "        caption = ex[txt_col]\n",
    "\n",
    "        if HAS_IMAGE_COL:\n",
    "            # HF has already downloaded/cached images; this is usually a PIL.Image\n",
    "            img = ex[img_col]\n",
    "            if not isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "        else:\n",
    "            url = ex[img_col]\n",
    "            img = self._load_image_from_url(url)\n",
    "\n",
    "        feats = self._encode_image(img)\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": caption,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Try up to max_retries times with different indices if something fails\n",
    "        (HTTP error, decoding error, etc).\n",
    "        \"\"\"\n",
    "        n = len(self.ds)\n",
    "        attempt = 0\n",
    "        cur_idx = idx\n",
    "\n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                return self._get_example(cur_idx)\n",
    "            except Exception as e:\n",
    "                # print(f\"[PixmoVisionDataset] Failed idx={cur_idx}, attempt={attempt+1}, err={e}\")\n",
    "                attempt += 1\n",
    "                cur_idx = (cur_idx + 1) % n\n",
    "\n",
    "        # Final fallback: try random indices\n",
    "        for _ in range(self.max_retries):\n",
    "            j = random.randint(0, n - 1)\n",
    "            try:\n",
    "                return self._get_example(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        raise RuntimeError(\"PixmoVisionDataset: could not load any valid images after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde847ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_dataset = PixmoVisionDataset(\n",
    "    pixmo_subset,\n",
    "    vision_model=vision_model,\n",
    "    vision_processor=vision_processor,\n",
    ")\n",
    "\n",
    "print(\"Vision dataset ready (HF image-based if available).\")\n",
    "sample_v = vision_dataset[0]\n",
    "print(\"  features shape:\", sample_v[\"features\"].shape)\n",
    "print(\"  text snippet:\", sample_v[\"text\"][:120], \"...\")\n",
    "\n",
    "vision_loader = DataLoader(\n",
    "    vision_dataset,\n",
    "    batch_size=cfg.batch_size_vision,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bea03",
   "metadata": {},
   "source": [
    "### Part-4:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 4 â€“ Audio features dataset (LibriSpeech + Whisper)\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# We assume:\n",
    "#  - `filtered` has been built in Part 3 (streaming LibriSpeech)\n",
    "#  - Each entry: {\"waveform\": np.ndarray, \"sampling_rate\": int, \"duration\": float, \"text\": str}\n",
    "print(\"\\nBuilding LibriSpeech audioâ€“text dataset from filtered streaming subset...\")\n",
    "print(\"Filtered LibriSpeech examples:\", len(filtered))\n",
    "\n",
    "\n",
    "def whisper_encode_sequence(wav: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    wav: 1D numpy array (time,)\n",
    "    sr:  sampling rate (expected 16k)\n",
    "    Returns:\n",
    "        feats: Tensor(T_enc, d_audio) on CPU (float16)\n",
    "    \"\"\"\n",
    "    # WhisperProcessor: raw waveform -> log-Mel spectrogram features\n",
    "    inputs = audio_processor(\n",
    "        wav,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_features = inputs[\"input_features\"].to(device)  # (1, T_mel, 80)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out = audio_model.encoder(input_features)\n",
    "        hidden = enc_out.last_hidden_state  # (1, T_enc, d_audio)\n",
    "\n",
    "    feats = hidden.squeeze(0).to(torch.float16).cpu()  # (T_enc, d_audio)\n",
    "    return feats\n",
    "\n",
    "\n",
    "class LibriSpeechAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset over the in-memory filtered LibriSpeech examples.\n",
    "    Returns:\n",
    "        {\n",
    "          \"features\": Tensor(T_enc, d_audio),\n",
    "          \"text\": str,\n",
    "          \"duration\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, examples, max_len: int | None = None):\n",
    "        self.examples = examples\n",
    "        if max_len is not None and max_len < len(examples):\n",
    "            # Optionally cut down further for faster experiments\n",
    "            self.examples = examples[:max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ex = self.examples[idx]\n",
    "        wav = ex[\"waveform\"]\n",
    "        sr = ex[\"sampling_rate\"]\n",
    "        text = ex[\"text\"]\n",
    "        dur = ex[\"duration\"]\n",
    "\n",
    "        feats = whisper_encode_sequence(wav, sr)  # (T_enc, d_audio)\n",
    "\n",
    "        return {\n",
    "            \"features\": feats,\n",
    "            \"text\": text,\n",
    "            \"duration\": dur,\n",
    "        }\n",
    "\n",
    "\n",
    "audio_max = getattr(cfg, \"librispeech_max_samples\", len(filtered))\n",
    "audio_dataset = LibriSpeechAudioDataset(filtered, max_len=audio_max)\n",
    "\n",
    "print(\"Audio dataset ready. Example:\")\n",
    "sample_a = audio_dataset[0]\n",
    "print(\"  features shape:\", sample_a[\"features\"].shape)\n",
    "print(\"  duration:\", round(sample_a[\"duration\"], 2), \"s\")\n",
    "print(\"  text:\", sample_a[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049573b2",
   "metadata": {},
   "source": [
    "### Part-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 5 â€“ Unified Adapters, Perceiver Resampler & Projector\n",
    "# ============================================\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5.0 â€“ Ensure Perceiver hyperparams exist in cfg\n",
    "# --------------------------------------------\n",
    "\n",
    "if not hasattr(cfg, \"num_perceiver_layers\"):\n",
    "    cfg.num_perceiver_layers = 2          # depth of Perceiver\n",
    "if not hasattr(cfg, \"num_attn_heads\"):\n",
    "    cfg.num_attn_heads = 8                # multi-head attention\n",
    "if not hasattr(cfg, \"mlp_ratio\"):\n",
    "    cfg.mlp_ratio = 4.0                   # width of MLP inside Perceiver\n",
    "\n",
    "print(\"Perceiver config:\")\n",
    "print(\"  perceiver_dim:\", cfg.perceiver_dim)\n",
    "print(\"  num_latents:\", cfg.num_latents)\n",
    "print(\"  num_perceiver_layers:\", cfg.num_perceiver_layers)\n",
    "print(\"  num_attn_heads:\", cfg.num_attn_heads)\n",
    "print(\"  mlp_ratio:\", cfg.mlp_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.1 â€“ Modality adapters: vision & audio â†’ perceiver_dim\n",
    "# --------------------------------------------\n",
    "\n",
    "class ModalityAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear adapter: maps encoder dim â†’ perceiver_dim.\n",
    "    Used separately for vision and audio encoders.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, in_dim) or (T, in_dim)\n",
    "        returns: (B, T, out_dim) or (T, out_dim)\n",
    "        \"\"\"\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "vision_adapter = ModalityAdapter(cfg.encoder_dim_vision, cfg.perceiver_dim).to(device)\n",
    "audio_adapter  = ModalityAdapter(cfg.encoder_dim_audio,  cfg.perceiver_dim).to(device)\n",
    "\n",
    "print(\"\\nAdapters created:\")\n",
    "print(\"  VisionAdapter:\", vision_adapter)\n",
    "print(\"  AudioAdapter:\", audio_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.2 â€“ Perceiver building blocks\n",
    "# --------------------------------------------\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PerceiverLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One Perceiver layer:\n",
    "      1) Cross-attention: latents query encoder tokens\n",
    "      2) Self-attention on latents\n",
    "      3) MLP on latents\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.ln_latents_1 = nn.LayerNorm(dim)\n",
    "        self.ln_tokens    = nn.LayerNorm(dim)\n",
    "        self.ln_latents_2 = nn.LayerNorm(dim)\n",
    "        self.ln_latents_3 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = FeedForward(dim, mlp_ratio=mlp_ratio)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        latents: torch.Tensor,   # (B, L, D)\n",
    "        tokens: torch.Tensor,    # (B, T, D)\n",
    "        token_mask: torch.Tensor | None = None,  # (B, T) bool, 1=valid\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_mask: bool mask, True for valid tokens. Will be converted to key_padding_mask.\n",
    "        \"\"\"\n",
    "        B, L, D = latents.shape\n",
    "        _, T, _ = tokens.shape\n",
    "\n",
    "        # LayerNorm\n",
    "        q = self.ln_latents_1(latents)   # (B, L, D)\n",
    "        kv = self.ln_tokens(tokens)      # (B, T, D)\n",
    "\n",
    "        # key_padding_mask: True for *ignored* positions\n",
    "        key_padding_mask = None\n",
    "        if token_mask is not None:\n",
    "            # token_mask: True=valid â†’ invert\n",
    "            key_padding_mask = ~token_mask.bool()   # (B, T)\n",
    "\n",
    "        # 1) Cross-attention: latents query the encoder tokens\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            query=q,\n",
    "            key=kv,\n",
    "            value=kv,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        latents = latents + attn_out\n",
    "\n",
    "        # 2) Self-attention on latents\n",
    "        q2 = self.ln_latents_2(latents)\n",
    "        self_attn_out, _ = self.self_attn(\n",
    "            query=q2,\n",
    "            key=q2,\n",
    "            value=q2,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        latents = latents + self_attn_out\n",
    "\n",
    "        # 3) MLP on latents\n",
    "        latents = latents + self.mlp(self.ln_latents_3(latents))\n",
    "\n",
    "        return latents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent array Z âˆˆ R^{L Ã— D}, cross-attends to encoder tokens X âˆˆ R^{B Ã— T Ã— D}\n",
    "    to produce a fixed number of latent tokens per example.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_latents: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_latents = num_latents\n",
    "\n",
    "        # Learned latent array (L, D)\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim) / math.sqrt(dim))\n",
    "\n",
    "        # Stack of Perceiver layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            PerceiverLayer(dim, num_heads=num_heads, mlp_ratio=mlp_ratio)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: torch.Tensor,         # (B, T, D)\n",
    "        token_mask: torch.Tensor | None = None,  # (B, T) bool\n",
    "    ) -> torch.Tensor:\n",
    "        B, T, D = tokens.shape\n",
    "        assert D == self.dim, f\"Expected dim={self.dim}, got {D}\"\n",
    "\n",
    "        # Expand latent array to batch: (B, L, D)\n",
    "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            latents = layer(latents, tokens, token_mask)\n",
    "\n",
    "        return latents  # (B, L, D)\n",
    "\n",
    "\n",
    "perceiver = PerceiverResampler(\n",
    "    dim=cfg.perceiver_dim,\n",
    "    num_latents=cfg.num_latents,\n",
    "    num_layers=cfg.num_perceiver_layers,\n",
    "    num_heads=cfg.num_attn_heads,\n",
    "    mlp_ratio=cfg.mlp_ratio,\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nPerceiverResampler created:\")\n",
    "print(perceiver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 5.3 â€“ Projector: Perceiver â†’ Qwen hidden space\n",
    "# --------------------------------------------\n",
    "\n",
    "projector = nn.Linear(cfg.perceiver_dim, cfg.llm_hidden_size).to(device)\n",
    "print(\"\\nProjector created:\")\n",
    "print(\"  projector:\", projector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 5.4 â€“ Quick shape sanity check with fake batch\n",
    "# --------------------------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    B = 2\n",
    "    # Fake vision sequence: (B, T_v, d_vision)\n",
    "    T_v = 32\n",
    "    fake_vision = torch.randn(B, T_v, cfg.encoder_dim_vision, device=device, dtype=default_dtype)\n",
    "    fake_mask   = torch.ones(B, T_v, dtype=torch.bool, device=device)\n",
    "\n",
    "    # 1) Adapt to perceiver_dim\n",
    "    v_tokens = vision_adapter(fake_vision)           # (B, T_v, D_perc)\n",
    "\n",
    "    # 2) Perceiver latents\n",
    "    latents = perceiver(v_tokens, fake_mask)         # (B, L, D_perc)\n",
    "\n",
    "    # 3) Project to Qwen hidden dim\n",
    "    z_llm = projector(latents)                       # (B, L, D_llm)\n",
    "\n",
    "print(\"\\nSanity check:\")\n",
    "print(\"  v_tokens shape:\", v_tokens.shape)\n",
    "print(\"  latents shape:\", latents.shape)\n",
    "print(\"  z_llm shape:\", z_llm.shape)\n",
    "print(\"Done Part 5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dddf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n== Sanity check dims ==\")\n",
    "print(\"encoder_dim_vision:\", cfg.encoder_dim_vision, type(cfg.encoder_dim_vision))\n",
    "print(\"encoder_dim_audio:\", cfg.encoder_dim_audio, type(cfg.encoder_dim_audio))\n",
    "print(\"perceiver_dim:\", cfg.perceiver_dim, type(cfg.perceiver_dim))\n",
    "print(\"llm_hidden_size:\", cfg.llm_hidden_size, type(cfg.llm_hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec960b4",
   "metadata": {},
   "source": [
    "### Part-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e969f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 6 â€“ Collate, Matryoshka loss, Forward Step\n",
    "# ============================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.1 â€“ Collate functions for vision & audio\n",
    "# --------------------------------------------\n",
    "\n",
    "def collate_features_with_text(batch):\n",
    "    \"\"\"\n",
    "    Generic collate:\n",
    "        batch: list of dicts with\n",
    "            \"features\": (T_i, D_enc)\n",
    "            \"text\": str\n",
    "            (optionally \"duration\")\n",
    "    Returns:\n",
    "        encoder_feats: (B, T_max, D_enc)\n",
    "        encoder_mask:  (B, T_max) bool\n",
    "        texts: list[str]\n",
    "        durations: list[float] | None\n",
    "    \"\"\"\n",
    "    feats = [torch.as_tensor(ex[\"features\"], dtype=default_dtype) for ex in batch]  # list[(T_i, D_enc)]\n",
    "    lengths = [f.size(0) for f in feats]\n",
    "\n",
    "    # Pad to max length\n",
    "    encoder_feats = pad_sequence(feats, batch_first=True)  # (B, T_max, D_enc)\n",
    "\n",
    "    B, T_max, _ = encoder_feats.shape\n",
    "    encoder_mask = torch.zeros(B, T_max, dtype=torch.bool)\n",
    "    for i, L in enumerate(lengths):\n",
    "        encoder_mask[i, :L] = True\n",
    "\n",
    "    texts = [ex[\"text\"] for ex in batch]\n",
    "    durations = [ex.get(\"duration\", None) for ex in batch]\n",
    "\n",
    "    return {\n",
    "        \"encoder_feats\": encoder_feats,    # (B, T_max, D_enc)\n",
    "        \"encoder_mask\": encoder_mask,      # (B, T_max)\n",
    "        \"texts\": texts,\n",
    "        \"durations\": durations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision & audio loaders (youâ€™ll use these in Part 7 for training)\n",
    "vision_loader = DataLoader(\n",
    "    vision_dataset,\n",
    "    batch_size=cfg.batch_size_vision,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n",
    "\n",
    "audio_loader = DataLoader(\n",
    "    audio_dataset,\n",
    "    batch_size=cfg.batch_size_audio,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_features_with_text,\n",
    ")\n",
    "\n",
    "print(\"Vision loader & audio loader ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35553fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 6.2 â€“ Matryoshka (MRL) contrastive loss\n",
    "# --------------------------------------------\n",
    "\n",
    "def matryoshka_contrastive_loss(\n",
    "    z_mod: torch.Tensor,    # (B, D)\n",
    "    z_txt: torch.Tensor,    # (B, D)\n",
    "    trunc_dims: tuple[int, ...],\n",
    "    temperature: float = 0.07,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Matryoshka-style symmetric InfoNCE at multiple truncation dims.\n",
    "\n",
    "    For each d in trunc_dims:\n",
    "      - truncate embeddings to first d dims\n",
    "      - L2-normalize\n",
    "      - compute similarity matrix\n",
    "      - compute symmetric cross-entropy (modâ†’text and textâ†’mod)\n",
    "    Then average across all dims.\n",
    "    \"\"\"\n",
    "    assert z_mod.shape == z_txt.shape\n",
    "    B, D = z_mod.shape\n",
    "    max_d = max(trunc_dims)\n",
    "    assert max_d <= D, f\"Max trunc dim {max_d} exceeds embedding dim {D}\"\n",
    "\n",
    "    losses = []\n",
    "    targets = torch.arange(B, device=z_mod.device)\n",
    "\n",
    "    for d in trunc_dims:\n",
    "        zm = F.normalize(z_mod[:, :d], dim=-1)  # (B, d)\n",
    "        zt = F.normalize(z_txt[:, :d], dim=-1)  # (B, d)\n",
    "\n",
    "        logits = zm @ zt.T / temperature        # (B, B)\n",
    "        loss_m2t = F.cross_entropy(logits, targets)\n",
    "        loss_t2m = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "        losses.append(0.5 * (loss_m2t + loss_t2m))\n",
    "\n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 6.3 â€“ Helpers for global text & modality embeddings\n",
    "# --------------------------------------------\n",
    "\n",
    "def pooled_text_embedding(texts: list[str], max_length: int = 64) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        h_text: (B, D_llm) pooled text embeddings\n",
    "        text_tok_info: dict with token_embs, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    tok_out = encode_text_with_qwen(texts, max_length=max_length)  # uses qwen_model embedding layer\n",
    "    token_embs = tok_out[\"token_embs\"]          # (B, L, D_llm)\n",
    "    attn_mask = tok_out[\"attention_mask\"]      # (B, L)\n",
    "\n",
    "    # masked mean-pooling over tokens\n",
    "    mask = attn_mask.unsqueeze(-1)             # (B, L, 1)\n",
    "    denom = mask.sum(dim=1).clamp_min(1)       # (B, 1)\n",
    "    h_text = (token_embs * mask).sum(dim=1) / denom  # (B, D_llm)\n",
    "\n",
    "    return h_text, tok_out\n",
    "\n",
    "\n",
    "def pooled_modality_embedding(latent_tokens_llm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    latent_tokens_llm: (B, L, D_llm) from projector(perceiver(...))\n",
    "    Returns:\n",
    "        h_mod: (B, D_llm)\n",
    "    \"\"\"\n",
    "    return latent_tokens_llm.mean(dim=1)  # simple mean over latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4fc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.4 â€“ Unified alignment forward step (vision or audio)\n",
    "# --------------------------------------------\n",
    "\n",
    "def forward_alignment_step(\n",
    "    batch: dict,\n",
    "    modality: str = \"vision\",   # \"vision\" or \"audio\"\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    One step of alignment loss for a batch.\n",
    "\n",
    "    batch keys from collate_features_with_text:\n",
    "        - encoder_feats: (B, T, D_enc)\n",
    "        - encoder_mask:  (B, T) bool\n",
    "        - texts: list[str]\n",
    "\n",
    "    modality:\n",
    "        \"vision\" â†’ use vision_adapter\n",
    "        \"audio\"  â†’ use audio_adapter\n",
    "    \"\"\"\n",
    "    encoder_feats = batch[\"encoder_feats\"].to(device)   # (B, T, D_enc)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)    # (B, T)\n",
    "    texts         = batch[\"texts\"]                      # list[str]\n",
    "\n",
    "    # 1) Modality adapter â†’ Perceiver dim\n",
    "    if modality == \"vision\":\n",
    "        tokens = vision_adapter(encoder_feats)          # (B, T, D_perc)\n",
    "    elif modality == \"audio\":\n",
    "        tokens = audio_adapter(encoder_feats)           # (B, T, D_perc)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    # 2) Perceiver resampler â†’ latent tokens\n",
    "    latents = perceiver(tokens, encoder_mask)           # (B, L, D_perc)\n",
    "\n",
    "    # 3) Project to Qwen hidden space\n",
    "    z_llm = projector(latents)                          # (B, L, D_llm)\n",
    "\n",
    "    # 4) Global modality embedding (for MRL)\n",
    "    h_mod = pooled_modality_embedding(z_llm)            # (B, D_llm)\n",
    "\n",
    "    # 5) Global text embedding from Qwen\n",
    "    h_txt, tok_info = pooled_text_embedding(texts, max_length=64)  # (B, D_llm)\n",
    "\n",
    "    # 6) Matryoshka contrastive loss\n",
    "    mrl_loss = matryoshka_contrastive_loss(\n",
    "        h_mod,\n",
    "        h_txt,\n",
    "        trunc_dims=cfg.mrl_dims,\n",
    "        temperature=cfg.mrl_temperature,\n",
    "    )\n",
    "\n",
    "    # For now we focus on alignment-only POC â†’ total_loss = mrl_loss\n",
    "    total_loss = mrl_loss\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\":        float(total_loss.detach().cpu()),\n",
    "        \"mrl_loss\":    float(mrl_loss.detach().cpu()),\n",
    "        \"modality\":    modality,\n",
    "        \"batch_size\":  int(h_mod.size(0)),\n",
    "    }\n",
    "\n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "print(\"\\nPart 6 ready: collate, MRL, and forward_alignment_step defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0eb8",
   "metadata": {},
   "source": [
    "### Part-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Part 7 â€“ Training loops (vision & audio alignment)\n",
    "# ============================================\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 7.0 â€“ Collect trainable parameters\n",
    "# --------------------------------------------\n",
    "\n",
    "# We ONLY train:\n",
    "#   - vision_adapter\n",
    "#   - audio_adapter\n",
    "#   - perceiver\n",
    "#   - projector\n",
    "# Qwen, CLIP, and Whisper are frozen.\n",
    "\n",
    "trainable_modules = nn.ModuleList([\n",
    "    vision_adapter,\n",
    "    audio_adapter,\n",
    "    perceiver,\n",
    "    projector,\n",
    "])\n",
    "\n",
    "for name, p in trainable_modules.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"Trainable:\", name, p.shape)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [p for p in trainable_modules.parameters() if p.requires_grad],\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizer ready with\", sum(p.numel() for p in trainable_modules.parameters() if p.requires_grad), \"trainable params.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 7.1 â€“ Generic training epoch for one modality\n",
    "# --------------------------------------------\n",
    "\n",
    "def train_one_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    modality: str,\n",
    "    max_steps: int,\n",
    "    log_prefix: str = \"\",\n",
    "):\n",
    "    trainable_modules.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=max_steps, desc=f\"{log_prefix}train-{modality}\", leave=False)\n",
    "\n",
    "    for step, batch in enumerate(pbar, start=1):\n",
    "        if step > max_steps:\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, metrics = forward_alignment_step(batch, modality=modality)\n",
    "        loss.backward()\n",
    "\n",
    "        if hasattr(cfg, \"max_grad_norm\") and cfg.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_modules.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += metrics[\"loss\"]\n",
    "        num_batches += 1\n",
    "        avg_loss = running_loss / num_batches\n",
    "\n",
    "        # âœ… W&B logging\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"{modality}/train/loss\": metrics[\"loss\"],\n",
    "                f\"{modality}/train/avg_loss\": avg_loss,\n",
    "                f\"{modality}/train/mrl_loss\": metrics[\"mrl_loss\"],\n",
    "                f\"{modality}/train/batch_size\": metrics[\"batch_size\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if step % cfg.log_every_steps == 0 or step == 1:\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{metrics['loss']:.4f}\",\n",
    "                \"avg_loss\": f\"{avg_loss:.4f}\",\n",
    "            })\n",
    "\n",
    "    avg_epoch_loss = running_loss / max(1, num_batches)\n",
    "    print(f\"[{log_prefix} {modality}] avg loss: {avg_epoch_loss:.4f}\")\n",
    "    # âœ… epoch-level log\n",
    "    wandb.log({f\"{modality}/train/epoch_loss\": avg_epoch_loss})\n",
    "    return avg_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 7.2 â€“ Simple retrieval eval (sanity check)\n",
    "# --------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_retrieval(\n",
    "    dataset,\n",
    "    modality: str,\n",
    "    num_samples: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Very small retrieval sanity check:\n",
    "      - take num_samples examples\n",
    "      - compute modality & text embeddings\n",
    "      - compute similarity matrix\n",
    "      - report Recall@1 (how often correct text is most similar)\n",
    "\n",
    "    Works for both vision_dataset and audio_dataset.\n",
    "    \"\"\"\n",
    "    trainable_modules.eval()\n",
    "\n",
    "    # Build a tiny batch with collate\n",
    "    from math import ceil\n",
    "    B = min(num_samples, len(dataset))\n",
    "    # Manual batching using DataLoader with our collate\n",
    "    tmp_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=B,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_features_with_text,\n",
    "    )\n",
    "    batch = next(iter(tmp_loader))\n",
    "\n",
    "    # Forward until we get h_mod and h_txt (without loss)\n",
    "    encoder_feats = batch[\"encoder_feats\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    texts         = batch[\"texts\"]\n",
    "\n",
    "    if modality == \"vision\":\n",
    "        tokens = vision_adapter(encoder_feats)\n",
    "    elif modality == \"audio\":\n",
    "        tokens = audio_adapter(encoder_feats)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "    latents = perceiver(tokens, encoder_mask)\n",
    "    z_llm   = projector(latents)\n",
    "\n",
    "    h_mod = pooled_modality_embedding(z_llm)      # (B, D_llm)\n",
    "    h_txt, _ = pooled_text_embedding(texts)      # (B, D_llm)\n",
    "\n",
    "    # Normalize\n",
    "    h_mod = F.normalize(h_mod, dim=-1)\n",
    "    h_txt = F.normalize(h_txt, dim=-1)\n",
    "\n",
    "    # Similarity matrix (B, B)\n",
    "    sims = h_mod @ h_txt.T\n",
    "\n",
    "    # For each modality embedding, check if its diagonal text is top-1\n",
    "    ranks = sims.argsort(dim=-1, descending=True)\n",
    "    correct_top1 = (ranks[:, 0] == torch.arange(B, device=ranks.device)).float().mean().item()\n",
    "\n",
    "    print(f\"[Eval {modality}] Retrieval Recall@1 on {B} samples: {correct_top1:.3f}\")\n",
    "    return correct_top1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# 7.3 â€“ Run a small POC training loop\n",
    "# --------------------------------------------\n",
    "\n",
    "# You can adjust these to be very small for a first run:\n",
    "vision_steps = getattr(cfg, \"max_train_steps_vision\", 100)\n",
    "audio_steps  = getattr(cfg, \"max_train_steps_audio\", 100)\n",
    "\n",
    "num_rounds = 1  # or >1 if you want to alternate vision/audio multiple times\n",
    "\n",
    "for round_idx in range(num_rounds):\n",
    "    print(f\"\\n========== Training Round {round_idx+1}/{num_rounds} ==========\")\n",
    "\n",
    "    # ---- Visionâ€“text alignment ----\n",
    "    print(\"\\n--- Visionâ€“Text alignment ---\")\n",
    "    train_one_epoch(\n",
    "        dataloader=vision_loader,\n",
    "        modality=\"vision\",\n",
    "        max_steps=vision_steps,\n",
    "        log_prefix=f\"round{round_idx+1}-\",\n",
    "    )\n",
    "    eval_retrieval(vision_dataset, modality=\"vision\", num_samples=32)\n",
    "\n",
    "    # ---- Audioâ€“text alignment ----\n",
    "    print(\"\\n--- Audioâ€“Text alignment ---\")\n",
    "    train_one_epoch(\n",
    "        dataloader=audio_loader,\n",
    "        modality=\"audio\",\n",
    "        max_steps=audio_steps,\n",
    "        log_prefix=f\"round{round_idx+1}-\",\n",
    "    )\n",
    "    eval_retrieval(audio_dataset, modality=\"audio\", num_samples=32)\n",
    "\n",
    "print(\"\\nTraining POC finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633bab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52227f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2406bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d03782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9ea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7227303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bc59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfd174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ea31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bd03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6342a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b761e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
