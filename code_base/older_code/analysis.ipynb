{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement open_clip (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for open_clip\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee05d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# ---- Quick config ----\n",
    "USE_REAL_MODELS = True     # flip to True to use CLIP / Whisper / MiniLM encoders\n",
    "DEVICE = \"cuda:0\"\n",
    "SEED = 42\n",
    "\n",
    "# Data sizes for quick runs (adjust later)\n",
    "N_IT_PAIRS = 3000           # image-text pairs\n",
    "N_AT_PAIRS = 3000           # audio-text pairs\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "# Embedding sizes\n",
    "EMB_VIS  = 512\n",
    "EMB_AUD  = 512\n",
    "EMB_TXT  = 384\n",
    "\n",
    "# Adapter output dimension (base/budget sweep will vary)\n",
    "ADAPT_OUT_DIMS = [128, 256, 512]\n",
    "\n",
    "# InfoNCE / Training\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LR = 3e-4\n",
    "TAU = 0.07   # temperature\n",
    "\n",
    "# Repro\n",
    "import random, numpy as np, torch\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = F.normalize(a, dim=-1)\n",
    "    b = F.normalize(b, dim=-1)\n",
    "    return a @ b.T  # [N, M]\n",
    "\n",
    "def recall_at_k(sim, k=1):\n",
    "    # sim: [N, M], rows are queries, cols are candidates; assume gold index matches (i->i)\n",
    "    ranks = sim.argsort(dim=1, descending=True)\n",
    "    correct = torch.arange(sim.size(0), device=sim.device).unsqueeze(1)\n",
    "    hit = (ranks[:, :k] == correct).any(dim=1).float()\n",
    "    return hit.mean().item()\n",
    "\n",
    "def mAP(sim):\n",
    "    # Simple mAP assuming 1 positive per row at same index\n",
    "    ranks = sim.argsort(dim=1, descending=True)\n",
    "    idx = torch.arange(sim.size(0), device=sim.device)\n",
    "    pos_rank = (ranks == idx.unsqueeze(1)).nonzero()[:,1] + 1  # 1-based\n",
    "    return (1.0 / pos_rank.float()).mean().item()\n",
    "\n",
    "def info_nce_loss(z_q, z_k, tau=0.07):\n",
    "    # z_q, z_k: [B, D]\n",
    "    z_q = F.normalize(z_q, dim=-1)\n",
    "    z_k = F.normalize(z_k, dim=-1)\n",
    "    logits = (z_q @ z_k.T) / tau\n",
    "    labels = torch.arange(z_q.size(0), device=z_q.device)\n",
    "    loss_qk = F.cross_entropy(logits, labels)\n",
    "    loss_kq = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_qk + loss_kq) / 2\n",
    "\n",
    "def tsne_plot(emb_list, colors, title=\"t-SNE\"):\n",
    "    x = torch.cat(emb_list, dim=0).detach().cpu().numpy()\n",
    "    tsne = TSNE(n_components=2, init='pca', perplexity=30, learning_rate='auto')\n",
    "    xy = tsne.fit_transform(x)\n",
    "    n = [e.shape[0] for e in emb_list]\n",
    "    idxs = np.cumsum([0]+n)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for i,(c,label) in enumerate(colors):\n",
    "        plt.scatter(xy[idxs[i]:idxs[i+1],0], xy[idxs[i]:idxs[i+1],1], s=6, alpha=0.7, label=label)\n",
    "    plt.title(title); plt.legend(); plt.show()\n",
    "\n",
    "def plot_pos_neg_hist(pos_sims, neg_sims, title=\"Cosine hist\"):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(pos_sims, bins=50, alpha=0.7, label=\"positives\")\n",
    "    plt.hist(neg_sims, bins=50, alpha=0.7, label=\"negatives\")\n",
    "    plt.legend(); plt.title(title); plt.xlabel(\"cosine\"); plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "def split_train_val(N, val_frac=0.1):\n",
    "    N_val = int(N*val_frac)\n",
    "    idx = torch.randperm(N)\n",
    "    return idx[N_val:], idx[:N_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'open_clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Build encoder handles\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_REAL_MODELS:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     enc = \u001b[43mbuild_real_encoders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     Dv = \u001b[32m512\u001b[39m; Dt = \u001b[32m384\u001b[39m; Da = \u001b[32m768\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mbuild_real_encoders\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_real_encoders\u001b[39m():\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    Returns dict with encode_image, encode_text, encode_audio callables.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m    Expect user to have:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m      - transformers (for Whisper encoder-only)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'open_clip'"
     ]
    }
   ],
   "source": [
    "# Synthetic encoders: fast, deterministic, good for pipeline & plots today\n",
    "class FakeEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        self.proj = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # initialize to be near-orthogonal-ish\n",
    "        nn.init.orthogonal_(self.proj.weight)\n",
    "    def forward(self, x):\n",
    "        # x is ignored; just return learned anchors (we’ll feed IDs as one-hot)\n",
    "        return F.normalize(self.proj(x), dim=-1)\n",
    "\n",
    "def make_one_hot_ids(n, dim):\n",
    "    eye = torch.eye(dim)[:n] if dim >= n else F.pad(torch.eye(dim), (0,0,0,n-dim))\n",
    "    return eye[:n].to(DEVICE)\n",
    "\n",
    "# Real encoders (optional): CLIP, MiniLM, Whisper\n",
    "def build_real_encoders():\n",
    "    \"\"\"\n",
    "    Returns dict with encode_image, encode_text, encode_audio callables.\n",
    "    Expect user to have:\n",
    "      - open_clip_torch\n",
    "      - sentence_transformers\n",
    "      - transformers (for Whisper encoder-only)\n",
    "    \"\"\"\n",
    "    import open_clip, torch\n",
    "    from PIL import Image\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from transformers import WhisperModel, WhisperFeatureExtractor\n",
    "    import torchaudio\n",
    "\n",
    "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\", pretrained=\"laion2b_s34b_b88k\", device=DEVICE\n",
    "    )\n",
    "    clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "    txt_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "    whisper = WhisperModel.from_pretrained(\"openai/whisper-small\").to(DEVICE).eval()\n",
    "    feat_ext = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(pil_list):\n",
    "        imgs = torch.stack([clip_preprocess(im).to(DEVICE) for im in pil_list])\n",
    "        return F.normalize(clip_model.encode_image(imgs), dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(str_list):\n",
    "        return F.normalize(torch.tensor(txt_model.encode(str_list, convert_to_numpy=True, normalize_embeddings=True)).to(DEVICE), dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_audio(wave_list, sr_list):\n",
    "        # Whisper: use encoder states mean pooled as an embedding proxy\n",
    "        embs = []\n",
    "        for wav, sr in zip(wave_list, sr_list):\n",
    "            if sr != 16000:\n",
    "                wav = torchaudio.functional.resample(torch.tensor(wav), sr, 16000).numpy()\n",
    "            inputs = feat_ext(wav, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            enc = whisper.encoder(inputs.input_features.to(DEVICE)).last_hidden_state.mean(dim=1)\n",
    "            embs.append(F.normalize(enc, dim=-1))\n",
    "        return torch.cat(embs, dim=0)\n",
    "\n",
    "    return dict(encode_image=encode_image, encode_text=encode_text, encode_audio=encode_audio)\n",
    "\n",
    "# Build encoder handles\n",
    "if USE_REAL_MODELS:\n",
    "    enc = build_real_encoders()\n",
    "    Dv = 512; Dt = 384; Da = 768\n",
    "else:\n",
    "    Dv, Dt, Da = EMB_VIS, EMB_TXT, EMB_AUD\n",
    "    # one-hot ID spaces to simulate “semantics” via shared caption IDs\n",
    "    # We’ll generate data below.\n",
    "    enc = dict(encode_image=None, encode_text=None, encode_audio=None)\n",
    "\n",
    "print(\"Encoders ready. Real:\", USE_REAL_MODELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_scatter__value)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mReal-mode loaders not yet implemented in this snippet. Use synthetic for now or plug your loaders.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     synth = \u001b[43mmake_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_IT_PAIRS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_AT_PAIRS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     V, T_it, A, T_at = encode_synthetic(synth, Dv, Dt, Da)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Train/val splits by index alignment\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmake_synthetic_data\u001b[39m\u001b[34m(n_it, n_at, vocab)\u001b[39m\n\u001b[32m     13\u001b[39m oh_dim = \u001b[38;5;28mmax\u001b[39m(vocab, \u001b[38;5;28mmax\u001b[39m(n_it, n_at))\n\u001b[32m     14\u001b[39m img_ids = make_one_hot_ids(n_it, oh_dim)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m txt_ids_it = torch.zeros_like(img_ids); \u001b[43mtxt_ids_it\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap_ids_it\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m; txt_ids_it = txt_ids_it.to(DEVICE)\n\u001b[32m     16\u001b[39m aud_ids = make_one_hot_ids(n_at, oh_dim)\n\u001b[32m     17\u001b[39m txt_ids_at = torch.zeros_like(aud_ids); txt_ids_at.scatter_(\u001b[32m1\u001b[39m, cap_ids_at.view(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m).cpu(), \u001b[32m1.0\u001b[39m); txt_ids_at = txt_ids_at.to(DEVICE)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_scatter__value)"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_synthetic_data(n_it=3000, n_at=3000, vocab=4000):\n",
    "    \"\"\"\n",
    "    Generate:\n",
    "      - shared caption IDs for i<->t and a<->t\n",
    "      - embeddings from one-hot IDs passed through FakeEncoder as \"frozen encoders\"\n",
    "    \"\"\"\n",
    "    # Shared caption IDs simulate cross-modality semantics\n",
    "    cap_ids_it = torch.randint(0, vocab, (n_it,))\n",
    "    cap_ids_at = torch.randint(0, vocab, (n_at,))\n",
    "    # “Inputs” for fake encoders are one-hots in a common index space\n",
    "    oh_dim = max(vocab, max(n_it, n_at))\n",
    "    img_ids = make_one_hot_ids(n_it, oh_dim)\n",
    "    txt_ids_it = torch.zeros_like(img_ids); txt_ids_it.scatter_(1, cap_ids_it.view(-1,1).cpu(), 1.0); txt_ids_it = txt_ids_it.to(DEVICE)\n",
    "    aud_ids = make_one_hot_ids(n_at, oh_dim)\n",
    "    txt_ids_at = torch.zeros_like(aud_ids); txt_ids_at.scatter_(1, cap_ids_at.view(-1,1).cpu(), 1.0); txt_ids_at = txt_ids_at.to(DEVICE)\n",
    "    return dict(\n",
    "        img_ids=img_ids, txt_ids_it=txt_ids_it, cap_ids_it=cap_ids_it.to(DEVICE),\n",
    "        aud_ids=aud_ids, txt_ids_at=txt_ids_at, cap_ids_at=cap_ids_at.to(DEVICE),\n",
    "        oh_dim=oh_dim\n",
    "    )\n",
    "\n",
    "def encode_synthetic(fake: Dict, Dv, Dt, Da):\n",
    "    enc_v = FakeEncoder(fake['oh_dim'], Dv).to(DEVICE).eval()\n",
    "    enc_t = FakeEncoder(fake['oh_dim'], Dt).to(DEVICE).eval()\n",
    "    enc_a = FakeEncoder(fake['oh_dim'], Da).to(DEVICE).eval()\n",
    "    with torch.no_grad():\n",
    "        v = enc_v(fake['img_ids'])\n",
    "        t_it = enc_t(fake['txt_ids_it'])\n",
    "        a = enc_a(fake['aud_ids'])\n",
    "        t_at = enc_t(fake['txt_ids_at'])\n",
    "    return v, t_it, a, t_at\n",
    "\n",
    "# Optional: real loaders (fill in later if needed)\n",
    "def load_real_it_pairs(limit):\n",
    "    \"\"\"\n",
    "    Return list_of_PIL_images, list_of_captions (same length)\n",
    "    Implement with HF `datasets` coco_captions or your local COCO subset.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Plug your real (image, text) loader here.\")\n",
    "\n",
    "def load_real_at_pairs(limit):\n",
    "    \"\"\"\n",
    "    Return list_of_waveforms, list_of_sample_rates, list_of_captions\n",
    "    Implement with AudioCaps/Clotho loaders in your environment.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Plug your real (audio, text) loader here.\")\n",
    "\n",
    "# Build working arrays\n",
    "if USE_REAL_MODELS:\n",
    "    # Example scaffold (uncomment when you fill loaders)\n",
    "    # pil_imgs, caps_it = load_real_it_pairs(N_IT_PAIRS)\n",
    "    # wavs, srs, caps_at = load_real_at_pairs(N_AT_PAIRS)\n",
    "    # v = enc['encode_image'](pil_imgs)\n",
    "    # t_it = enc['encode_text'](caps_it)\n",
    "    # a = enc['encode_audio'](wavs, srs)\n",
    "    # t_at = enc['encode_text'](caps_at)\n",
    "    raise RuntimeError(\"Real-mode loaders not yet implemented in this snippet. Use synthetic for now or plug your loaders.\")\n",
    "else:\n",
    "    synth = make_synthetic_data(N_IT_PAIRS, N_AT_PAIRS, vocab=4000)\n",
    "    V, T_it, A, T_at = encode_synthetic(synth, Dv, Dt, Da)\n",
    "\n",
    "# Train/val splits by index alignment\n",
    "tr_it, va_it = split_train_val(V.shape[0], VAL_FRAC)\n",
    "tr_at, va_at = split_train_val(A.shape[0], VAL_FRAC)\n",
    "\n",
    "print(\"Data ready. Synthetic:\", not USE_REAL_MODELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.LayerNorm(out_dim),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def train_pairwise(V, T_it, A, T_at, out_dim=256, epochs=2, tau=0.07):\n",
    "    va = Adapter(V.shape[1], out_dim).to(DEVICE)\n",
    "    ta = Adapter(T_it.shape[1], out_dim).to(DEVICE)\n",
    "    aa = Adapter(A.shape[1], out_dim).to(DEVICE)\n",
    "    # We share text adapter for both it/at; easy to change if you want per-pair text adapters\n",
    "    opt = torch.optim.AdamW(list(va.parameters()) + list(ta.parameters()) + list(aa.parameters()), lr=LR)\n",
    "\n",
    "    # Prepare quick index sets\n",
    "    it_idx = tr_it\n",
    "    at_idx = tr_at\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        va.train(); ta.train(); aa.train()\n",
    "        # minibatches over the *larger* split\n",
    "        max_len = max(len(it_idx), len(at_idx))\n",
    "        perm_it = it_idx[torch.randperm(len(it_idx))]\n",
    "        perm_at = at_idx[torch.randperm(len(at_idx))]\n",
    "        for s in range(0, max_len, BATCH_SIZE):\n",
    "            # gather it batch\n",
    "            it_slice = perm_it[s:s+BATCH_SIZE]\n",
    "            at_slice = perm_at[s:s+BATCH_SIZE]\n",
    "            # handle length mismatch by wrapping indices\n",
    "            if len(it_slice)==0 and len(at_slice)==0: break\n",
    "            loss = 0.0\n",
    "\n",
    "            if len(it_slice)>0:\n",
    "                vq = va(V[it_slice])\n",
    "                tk = ta(T_it[it_slice])\n",
    "                loss += info_nce_loss(vq, tk, tau)\n",
    "\n",
    "            if len(at_slice)>0:\n",
    "                aq = aa(A[at_slice])\n",
    "                tk2 = ta(T_at[at_slice])  # same text adapter\n",
    "                loss += info_nce_loss(aq, tk2, tau)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # quick val metrics\n",
    "        va.eval(); ta.eval(); aa.eval()\n",
    "        with torch.no_grad():\n",
    "            vq = F.normalize(va(V[va_it]), dim=-1); tk = F.normalize(ta(T_it[va_it]), dim=-1)\n",
    "            aq = F.normalize(aa(A[va_at]), dim=-1); tk2 = F.normalize(ta(T_at[va_at]), dim=-1)\n",
    "\n",
    "            sim_it = vq @ tk.T\n",
    "            sim_at = aq @ tk2.T\n",
    "\n",
    "            r1_it = recall_at_k(sim_it, 1); r5_it = recall_at_k(sim_it, 5)\n",
    "            r1_at = recall_at_k(sim_at, 1); r5_at = recall_at_k(sim_at, 5)\n",
    "\n",
    "        print(f\"[ep {ep}] it R@1={r1_it:.3f} R@5={r5_it:.3f} | at R@1={r1_at:.3f} R@5={r5_at:.3f}\")\n",
    "\n",
    "    return va, ta, aa\n",
    "\n",
    "# Train quick baseline\n",
    "adV, adT, adA = train_pairwise(V, T_it, A, T_at, out_dim=ADAPT_OUT_DIMS[-1], epochs=EPOCHS, tau=TAU)\n",
    "print(\"Adapters trained (pairwise baseline).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_all(adV, adT, adA, V, T_it, A, T_at, split=('val','val','val','val')):\n",
    "    adV.eval(); adT.eval(); adA.eval()\n",
    "    # choose split indices\n",
    "    I = va_it if split[0]=='val' else tr_it\n",
    "    IT = va_it if split[1]=='val' else tr_it\n",
    "    J = va_at if split[2]=='val' else tr_at\n",
    "    AT = va_at if split[3]=='val' else tr_at\n",
    "\n",
    "    v = F.normalize(adV(V[I]), dim=-1)\n",
    "    t1 = F.normalize(adT(T_it[IT]), dim=-1)\n",
    "    a = F.normalize(adA(A[J]), dim=-1)\n",
    "    t2 = F.normalize(adT(T_at[AT]), dim=-1)\n",
    "\n",
    "    # i<->t\n",
    "    sim_it = v @ t1.T\n",
    "    # a<->t\n",
    "    sim_at = a @ t2.T\n",
    "    # i<->a (emergent via trained adapters, no direct loss yet)\n",
    "    # align by their own indices sizes; use min to form matched pairs\n",
    "    N = min(v.shape[0], a.shape[0])\n",
    "    sim_ia = v[:N] @ a[:N].T\n",
    "\n",
    "    metrics = dict(\n",
    "        it_R1 = recall_at_k(sim_it, 1),\n",
    "        it_R5 = recall_at_k(sim_it, 5),\n",
    "        at_R1 = recall_at_k(sim_at, 1),\n",
    "        at_R5 = recall_at_k(sim_at, 5),\n",
    "        ia_R1 = recall_at_k(sim_ia, 1),\n",
    "        ia_R5 = recall_at_k(sim_ia, 5),\n",
    "        it_mAP = mAP(sim_it),\n",
    "        at_mAP = mAP(sim_at),\n",
    "        ia_mAP = mAP(sim_ia)\n",
    "    )\n",
    "    return metrics, sim_it, sim_at, sim_ia\n",
    "\n",
    "metrics, sim_it, sim_at, sim_ia = eval_all(adV, adT, adA, V, T_it, A, T_at, split=('val','val','val','val'))\n",
    "print(metrics)\n",
    "\n",
    "# Cosine histograms (positives are diagonal; negatives are off-diagonal)\n",
    "def diag_offdiag(sim):\n",
    "    pos = sim.diag().detach().cpu().numpy()\n",
    "    neg = sim.detach().cpu().numpy()[~np.eye(sim.shape[0], dtype=bool)]\n",
    "    return pos, neg\n",
    "\n",
    "p_it, n_it = diag_offdiag(sim_it)\n",
    "p_at, n_at = diag_offdiag(sim_at)\n",
    "p_ia, n_ia = diag_offdiag(sim_ia)\n",
    "\n",
    "plot_pos_neg_hist(p_it, n_it, title=\"i↔t cosine\")\n",
    "plot_pos_neg_hist(p_at, n_at, title=\"a↔t cosine\")\n",
    "plot_pos_neg_hist(p_ia, n_ia, title=\"i↔a cosine (emergent)\")\n",
    "\n",
    "# t-SNE (mixed modalities)\n",
    "# Use a small sample for speed\n",
    "K = 600\n",
    "v_s = F.normalize(adV(V[va_it][:K]), dim=-1)\n",
    "t_s = F.normalize(adT(T_it[va_it][:K]), dim=-1)\n",
    "a_s = F.normalize(adA(A[va_at][:K]), dim=-1)\n",
    "tsne_plot([v_s, t_s, a_s], colors=[('C0','image'),('C1','text'),('C2','audio')], title=\"t-SNE of aligned embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_consistency(V, T_it, A, T_at, out_dim=256, epochs=2, tau=0.07, w_cons=0.2):\n",
    "    va = Adapter(V.shape[1], out_dim).to(DEVICE)\n",
    "    ta = Adapter(T_it.shape[1], out_dim).to(DEVICE)\n",
    "    aa = Adapter(A.shape[1], out_dim).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(list(va.parameters()) + list(ta.parameters()) + list(aa.parameters()), lr=LR)\n",
    "\n",
    "    it_idx = tr_it\n",
    "    at_idx = tr_at\n",
    "\n",
    "    # create simple caption-matched synthetic i-a positives via nearest in text space\n",
    "    # (for real data: group by same caption string or high-sim threshold)\n",
    "    # Here we use *indexes* as proxy (controlled synthetic setting).\n",
    "    for ep in range(1, epochs+1):\n",
    "        va.train(); ta.train(); aa.train()\n",
    "        max_len = max(len(it_idx), len(at_idx))\n",
    "        perm_it = it_idx[torch.randperm(len(it_idx))]\n",
    "        perm_at = at_idx[torch.randperm(len(at_idx))]\n",
    "        for s in range(0, max_len, BATCH_SIZE):\n",
    "            it_slice = perm_it[s:s+BATCH_SIZE]\n",
    "            at_slice = perm_at[s:s+BATCH_SIZE]\n",
    "            if len(it_slice)==0 and len(at_slice)==0: break\n",
    "            loss = 0.0\n",
    "\n",
    "            if len(it_slice)>0:\n",
    "                vq = va(V[it_slice]); tk = ta(T_it[it_slice])\n",
    "                loss += info_nce_loss(vq, tk, tau)\n",
    "\n",
    "            if len(at_slice)>0:\n",
    "                aq = aa(A[at_slice]); tk2 = ta(T_at[at_slice])\n",
    "                loss += info_nce_loss(aq, tk2, tau)\n",
    "\n",
    "            # consistency: push image/audio of *same batch index* closer\n",
    "            if len(it_slice)>0 and len(at_slice)>0:\n",
    "                K = min(len(it_slice), len(at_slice))\n",
    "                vq_c = F.normalize(va(V[it_slice[:K]]), dim=-1)\n",
    "                aq_c = F.normalize(aa(A[at_slice[:K]]), dim=-1)\n",
    "                # InfoNCE between v and a as if paired\n",
    "                loss += w_cons * info_nce_loss(vq_c, aq_c, tau)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        # quick log\n",
    "        va.eval(); ta.eval(); aa.eval()\n",
    "        with torch.no_grad():\n",
    "            vq = F.normalize(va(V[va_it]), dim=-1); tk = F.normalize(ta(T_it[va_it]), dim=-1)\n",
    "            aq = F.normalize(aa(A[va_at]), dim=-1); tk2 = F.normalize(ta(T_at[va_at]), dim=-1)\n",
    "            sim_it = vq @ tk.T; sim_at = aq @ tk2.T\n",
    "        print(f\"[ep {ep}] it R@1={recall_at_k(sim_it,1):.3f} | at R@1={recall_at_k(sim_at,1):.3f}\")\n",
    "\n",
    "    return va, ta, aa\n",
    "\n",
    "# Train a quick joint model with consistency\n",
    "adV_joint, adT_joint, adA_joint = train_with_consistency(V, T_it, A, T_at, out_dim=ADAPT_OUT_DIMS[-1], epochs=EPOCHS, tau=TAU, w_cons=0.2)\n",
    "\n",
    "metrics_joint, _, _, sim_ia_joint = eval_all(adV_joint, adT_joint, adA_joint, V, T_it, A, T_at)\n",
    "print(\"Joint metrics:\", metrics_joint)\n",
    "\n",
    "p_ia_j, n_ia_j = diag_offdiag(sim_ia_joint)\n",
    "plot_pos_neg_hist(p_ia_j, n_ia_j, title=\"i↔a cosine (with consistency)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def budget_curve(adV, adT, adA, dims=[128,256,512]):\n",
    "    print(\"Budget curve (dim vs R@1 for i↔t / a↔t / i↔a):\")\n",
    "    baseV, baseT, baseA = adV, adT, adA\n",
    "    for d in dims:\n",
    "        redV = Adapter(baseV.net[0].in_features, d).to(DEVICE); redV.load_state_dict(baseV.state_dict(), strict=False)\n",
    "        redT = Adapter(baseT.net[0].in_features, d).to(DEVICE); redT.load_state_dict(baseT.state_dict(), strict=False)\n",
    "        redA = Adapter(baseA.net[0].in_features, d).to(DEVICE); redA.load_state_dict(baseA.state_dict(), strict=False)\n",
    "        m, sim_it, sim_at, sim_ia = eval_all(redV, redT, redA, V, T_it, A, T_at)\n",
    "        print(f\"  d={d:4d} | i↔t R@1={m['it_R1']:.3f}  a↔t R@1={m['at_R1']:.3f}  i↔a R@1={m['ia_R1']:.3f}\")\n",
    "\n",
    "print(\"Pairwise baseline curve:\")\n",
    "budget_curve(adV, adT, adA, ADAPT_OUT_DIMS)\n",
    "\n",
    "print(\"Joint (consistency) curve:\")\n",
    "budget_curve(adV_joint, adT_joint, adA_joint, ADAPT_OUT_DIMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_control_shuffle_text(T_tensor, idx):\n",
    "    # Shuffle the alignment between embeddings & indices\n",
    "    shuf = idx[torch.randperm(len(idx))]\n",
    "    return T_tensor[shuf]\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_with_text_shuffle(adV, adT, adA):\n",
    "    v = F.normalize(adV(V[va_it]), dim=-1)\n",
    "    t = F.normalize(adT(negative_control_shuffle_text(T_it, va_it)), dim=-1)\n",
    "    a = F.normalize(adA(A[va_at]), dim=-1)\n",
    "    t2 = F.normalize(adT(negative_control_shuffle_text(T_at, va_at)), dim=-1)\n",
    "    sim_it = v @ t.T; sim_at = a @ t2.T\n",
    "    print(f\"NegCtrl — i↔t R@1={recall_at_k(sim_it,1):.3f}  a↔t R@1={recall_at_k(sim_at,1):.3f}\")\n",
    "\n",
    "print(\"Negative control (pairwise):\")\n",
    "eval_with_text_shuffle(adV, adT, adA)\n",
    "\n",
    "print(\"Negative control (joint):\")\n",
    "eval_with_text_shuffle(adV_joint, adT_joint, adA_joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd215754",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def oracle_text_bridge_R1(adV, adT, adA):\n",
    "    # Rank i->t and t->a, then compose ranks for an oracle upper bound\n",
    "    v = F.normalize(adV(V[va_it]), dim=-1)\n",
    "    t1 = F.normalize(adT(T_it[va_it]), dim=-1)\n",
    "    a = F.normalize(adA(A[va_at]), dim=-1)\n",
    "    t2 = F.normalize(adT(T_at[va_at]), dim=-1)\n",
    "\n",
    "    sim_i_t = v @ t1.T\n",
    "    sim_t_a = t2 @ a.T\n",
    "\n",
    "    # For each image i, pick best text, then that text's best audio\n",
    "    top_t = sim_i_t.argmax(dim=1)          # [Ni]\n",
    "    sim_i_a_via_t = sim_t_a[top_t]         # [Ni, Na]\n",
    "    r1 = recall_at_k(sim_i_a_via_t, 1)\n",
    "    return r1\n",
    "\n",
    "print(\"Oracle text bridge upper bound R@1 (pairwise adapters):\", oracle_text_bridge_R1(adV, adT, adA))\n",
    "print(\"Oracle text bridge upper bound R@1 (joint adapters):   \", oracle_text_bridge_R1(adV_joint, adT_joint, adA_joint))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "py311_main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
