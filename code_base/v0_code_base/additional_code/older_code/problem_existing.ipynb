{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408fda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri-Modal Alignment Demo (Phases 0–3)\n",
    "# Author: ChatGPT (for Vedaang)\n",
    "#\n",
    "# This notebook implements Phases 0–3 of your experiment plan:\n",
    "#  - Phase 0: Setup, datasets, frozen encoders, sanity checks\n",
    "#  - Phase 1: Pairwise-only training (i<->t and a<->t separately) + full tri-direction eval\n",
    "#  - Phase 2: Image-Hub training (i<->t + i<->a) with pseudo (i,a,t) triplets via CLIP retrieval\n",
    "#  - Phase 3: Tri-modal adapters with cycle consistency (shared space, frozen encoders)\n",
    "#\n",
    "# Outputs: retrieval metrics (R@1/5/10, mAP), cosine histograms, comparison tables.\n",
    "#\n",
    "# Notes:\n",
    "# - Uses small, controllable subsets to be runnable on a single GPU.\n",
    "# - Uses public HF datasets (COCO captions, AudioCaps) and HF encoders (CLIP, Whisper, MiniLM).\n",
    "# - Encoders are *frozen*; we train light adapters/projectors.\n",
    "# - Pseudo (i,a,t) triplets are created by retrieving a COCO image for each AudioCaps caption via CLIP.\n",
    "#\n",
    "# Before running, ensure internet access in the kernel (to fetch models/datasets) and enough disk cache.\n",
    "# If offline, pre-download datasets/models in ~/.cache/huggingface.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0. Setup\n",
    "# * Installs (if needed)\n",
    "# * Imports & Config\n",
    "# * Reproducibility and device\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3dc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, math, random, time, json, itertools, functools, gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# Light installs: uncomment if you need to install in the environment\n",
    "# !pip -q install transformers datasets torchaudio tqdm scikit-learn matplotlib pillow\n",
    "\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoProcessor,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    WhisperFeatureExtractor, WhisperModel,\n",
    ")\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a0b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 0.1 Config\n",
    "# You can tweak subset sizes and dims here for quick iterations.\n",
    "\n",
    "# %%\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data sizes (keep small for quick runs; scale later)\n",
    "    coco_images_train: int = 4000   # number of COCO images for (i,t) train\n",
    "    coco_images_val: int = 1000     # number for (i,t) eval\n",
    "    audiocaps_train: int = 4000     # number of AudioCaps audio clips for (a,t) train\n",
    "    audiocaps_val: int = 1000       # number for (a,t) eval\n",
    "\n",
    "    # Subset used as candidate image pool for pseudo (i,a,t) triplets\n",
    "    image_pool_for_triplets: int = 5000\n",
    "\n",
    "    # Common embedding dim for adapters\n",
    "    embed_dim: int = 512\n",
    "\n",
    "    # Matryoshka (optional, Phase 5) – placeholders\n",
    "    matryoshka_widths: List[int] = (64, 128, 256, 512)\n",
    "\n",
    "    # Audio settings\n",
    "    target_sr: int = 16000\n",
    "    max_audio_sec: float = 12.0\n",
    "\n",
    "    # Image settings\n",
    "    image_size: int = 224\n",
    "\n",
    "    # Train settings\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    epochs_pairwise: int = 3\n",
    "    epochs_hub: int = 3\n",
    "    epochs_trimodal: int = 3\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.01\n",
    "    fp16: bool = True\n",
    "\n",
    "    # Eval\n",
    "    eval_batch: int = 128\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd196d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 0.2 Encoders (Frozen)\n",
    "# - Vision: CLIP ViT-B/32\n",
    "# - Text: MiniLM (sentence-transformers/all-MiniLM-L6-v2)\n",
    "# - Audio: Whisper encoder (mean-pooled hidden states)\n",
    "\n",
    "# %%\n",
    "# Vision (CLIP)\n",
    "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_id).eval().to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Text (MiniLM)\n",
    "text_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "text_model = AutoModel.from_pretrained(text_model_id).eval().to(device)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_id)\n",
    "for p in text_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Audio (Whisper encoder)\n",
    "whisper_id = \"openai/whisper-small\"\n",
    "whisper_enc = WhisperModel.from_pretrained(whisper_id).encoder.eval().to(device)\n",
    "whisper_feat = WhisperFeatureExtractor.from_pretrained(whisper_id)\n",
    "for p in whisper_enc.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03dba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 0.3 Helper: Frozen encoder wrappers -> fixed-size vectors\n",
    "\n",
    "# %%\n",
    "@torch.inference_mode()\n",
    "def encode_text(texts: List[str]) -> torch.Tensor:\n",
    "    # Mean Pool over last hidden states\n",
    "    toks = text_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    out = text_model(**toks)\n",
    "    last = out.last_hidden_state  # [B, L, H]\n",
    "    mask = toks.attention_mask.unsqueeze(-1)  # [B, L, 1]\n",
    "    summed = (last * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1)\n",
    "    emb = summed / denom\n",
    "    emb = F.normalize(emb, dim=-1)\n",
    "    return emb\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_image_pil(pil_list: List[\"PIL.Image.Image\"]) -> torch.Tensor:\n",
    "    inputs = clip_processor(images=pil_list, return_tensors=\"pt\").to(device)\n",
    "    out = clip_model.get_image_features(**inputs)  # [B, 512]\n",
    "    emb = F.normalize(out, dim=-1)\n",
    "    return emb\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_text_with_clip(texts: List[str]) -> torch.Tensor:\n",
    "    # For CLIP retrieval step only (text->image retrieval)\n",
    "    inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    out = clip_model.get_text_features(**inputs)\n",
    "    return F.normalize(out, dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_audio_wave(wave: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    # wave: [1, T] mono\n",
    "    if sr != CFG.target_sr:\n",
    "        wave = torchaudio.functional.resample(wave, sr, CFG.target_sr)\n",
    "    # trim/pad to max_audio_sec\n",
    "    max_len = int(CFG.target_sr * CFG.max_audio_sec)\n",
    "    if wave.size(1) > max_len:\n",
    "        wave = wave[:, :max_len]\n",
    "    else:\n",
    "        pad = max_len - wave.size(1)\n",
    "        if pad > 0:\n",
    "            wave = F.pad(wave, (0, pad))\n",
    "    inputs = whisper_feat(wave.squeeze(0).cpu().numpy(), sampling_rate=CFG.target_sr, return_tensors=\"pt\")\n",
    "    # Whisper expects features shaped for decoder usually; for encoder, we use input_features.\n",
    "    feats = inputs[\"input_features\"].to(device)\n",
    "    out = whisper_enc(feats)  # Base encoder forward\n",
    "    last = out.last_hidden_state  # [B, T, H]\n",
    "    emb = last.mean(dim=1)  # temporal mean-pool\n",
    "    emb = F.normalize(emb, dim=-1)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddbfd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# --- helpers --------------------------------------------------------------\n",
    "\n",
    "def _load_head(dataset_name: str, split: str, n: int, streaming: bool = True):\n",
    "    \"\"\"\n",
    "    Return the first n samples of `split` from `dataset_name`.\n",
    "    - streaming=True: returns an IterableDataset via .take(n) (no full download)\n",
    "    - streaming=False: materializes only the first n rows to disk/ram\n",
    "    \"\"\"\n",
    "    if streaming:\n",
    "        return load_dataset(dataset_name, split=split, streaming=True).take(n)\n",
    "    else:\n",
    "        return load_dataset(dataset_name, split=f\"{split}[:{n}]\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _first_caption_field(example):\n",
    "    # For COCO: 'captions' is a list of dicts [{'text': ...}, ...]\n",
    "    caps = example.get(\"captions\") or []\n",
    "    if caps:\n",
    "        return caps[0][\"text\"] if isinstance(caps[0], dict) and \"text\" in caps[0] else str(caps[0])\n",
    "    return example.get(\"caption\", \"\")\n",
    "\n",
    "# ---------- COCO (images + text) ----------\n",
    "def load_coco_subsets(train_n: int, val_n: int):\n",
    "    \"\"\"\n",
    "    sentence-transformers/coco-captions has only 'train'.\n",
    "    We create disjoint train/val by skipping into the same stream.\n",
    "    Everything is streaming: no full download, no Arrow files written.\n",
    "    \"\"\"\n",
    "    def pick_first_caption(example):\n",
    "        return {\"image\": example[\"image\"], \"caption\": _first_caption_field(example)}\n",
    "\n",
    "    # Fresh stream for train\n",
    "    coco_train_stream = load_dataset(\"sentence-transformers/coco-captions\",\n",
    "                                     split=\"train\", streaming=True)\n",
    "    COCO_TRAIN = coco_train_stream.map(pick_first_caption).take(train_n)\n",
    "\n",
    "    # Fresh stream for val (skip the first `train_n` to avoid overlap)\n",
    "    coco_val_stream = load_dataset(\"sentence-transformers/coco-captions\",\n",
    "                                   split=\"train\", streaming=True)\n",
    "    COCO_VAL = coco_val_stream.map(pick_first_caption).skip(train_n).take(val_n)\n",
    "    return COCO_TRAIN, COCO_VAL\n",
    "\n",
    "# ---------- AudioCaps (audio + text) ----------\n",
    "def load_audiocaps_subsets(train_n: int, val_n: int):\n",
    "    \"\"\"\n",
    "    AudioCaps has 'train', 'validation', 'test' on the Hub.\n",
    "    We try 'validation' for val, fall back to 'test' if needed.\n",
    "    Streaming the Audio feature avoids big writes to disk.\n",
    "    \"\"\"\n",
    "    def norm_audio(example):\n",
    "        # Keep HF Audio feature (lazy decode). Add normalized caption.\n",
    "        return {\"audio\": example[\"audio\"], \"caption\": _first_caption_field(example)}\n",
    "\n",
    "    AUDIO_TRAIN = load_dataset(\"d0rj/audiocaps\", split=\"train\", streaming=True).map(norm_audio).take(train_n)\n",
    "\n",
    "    try:\n",
    "        AUDIO_VAL = load_dataset(\"d0rj/audiocaps\", split=\"validation\", streaming=True).map(norm_audio).take(val_n)\n",
    "    except Exception:\n",
    "        AUDIO_VAL = load_dataset(\"d0rj/audiocaps\", split=\"test\", streaming=True).map(norm_audio).take(val_n)\n",
    "\n",
    "    return AUDIO_TRAIN, AUDIO_VAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27af2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets (streaming, no full download)...\")\n",
    "COCO_TRAIN, COCO_VAL = load_coco_subsets(CFG.coco_images_train, CFG.coco_images_val)\n",
    "AUDIO_TRAIN, AUDIO_VAL = load_audiocaps_subsets(CFG.audiocaps_train, CFG.audiocaps_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44164a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "253454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek one example from each (doesn't materialize the whole thing)\n",
    "print(next(iter(COCO_TRAIN)))\n",
    "print(next(iter(COCO_VAL)))\n",
    "print(next(iter(AUDIO_TRAIN)))\n",
    "print(next(iter(AUDIO_VAL)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b99fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 0.5 DataLoaders\n",
    "\n",
    "# %%\n",
    "class CocoITDataset(Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.ds = hf_ds\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[int(idx)]\n",
    "        img = ex[\"image\"]  # PIL\n",
    "        cap = ex[\"caption\"]\n",
    "        return img, cap\n",
    "\n",
    "class AudioTDataset(Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.ds = hf_ds\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[int(idx)]\n",
    "        wave = torch.tensor(ex[\"audio\"]).float().unsqueeze(0)  # [1, T]\n",
    "        sr = int(ex[\"sr\"])\n",
    "        cap = ex[\"caption\"]\n",
    "        return wave, sr, cap\n",
    "\n",
    "coco_train_loader = DataLoader(CocoITDataset(COCO_TRAIN), batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, collate_fn=lambda b: list(zip(*b)))\n",
    "coco_val_loader = DataLoader(CocoITDataset(COCO_VAL), batch_size=CFG.eval_batch, shuffle=False, num_workers=CFG.num_workers, collate_fn=lambda b: list(zip(*b)))\n",
    "\n",
    "audio_train_loader = DataLoader(AudioTDataset(AUDIO_TRAIN), batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, collate_fn=lambda b: list(zip(*b)))\n",
    "audio_val_loader = DataLoader(AudioTDataset(AUDIO_VAL), batch_size=CFG.eval_batch, shuffle=False, num_workers=CFG.num_workers, collate_fn=lambda b: list(zip(*b)))\n",
    "\n",
    "print(\"Loaders ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 0.6 Pseudo (i,a,t) Triplets via CLIP Retrieval (for Phases 2 & 3)\n",
    "# For each AudioCaps caption, find the nearest COCO image (from a pool) using CLIP text<->image similarity.\n",
    "\n",
    "# %%\n",
    "# Build an image pool from COCO_TRAIN + COCO_VAL\n",
    "pool_imgs = list(itertools.islice((ex[\"image\"] for ex in COCO_TRAIN), CFG.image_pool_for_triplets))\n",
    "if len(pool_imgs) < CFG.image_pool_for_triplets:\n",
    "    # top-up with validation if needed\n",
    "    remaining = CFG.image_pool_for_triplets - len(pool_imgs)\n",
    "    pool_imgs += list(itertools.islice((ex[\"image\"] for ex in COCO_VAL), remaining))\n",
    "print(\"Image pool size:\", len(pool_imgs))\n",
    "\n",
    "# Precompute CLIP image embeddings for the pool\n",
    "img_pool_embs = []\n",
    "BS = 64\n",
    "for i in tqdm(range(0, len(pool_imgs), BS), desc=\"Encoding image pool\"):\n",
    "    batch = pool_imgs[i:i+BS]\n",
    "    with torch.inference_mode():\n",
    "        e = encode_image_pil(batch)\n",
    "    img_pool_embs.append(e.cpu())\n",
    "img_pool_embs = torch.cat(img_pool_embs, dim=0)  # [P, 512]\n",
    "img_pool_embs = F.normalize(img_pool_embs, dim=-1)\n",
    "\n",
    "# Retrieve top-1 image for each AudioCaps caption in the *train* split to create pseudo triplets\n",
    "pseudo_triplets = []  # list of (PIL.Image, np.array audio, sr, caption)\n",
    "text_caps = []\n",
    "for i in tqdm(range(len(AUDIO_TRAIN)), desc=\"Retrieving images for AudioCaps captions\"):\n",
    "    cap = AUDIO_TRAIN[i][\"caption\"]\n",
    "    text_caps.append(cap)\n",
    "\n",
    "# Batch-encode text with CLIP for speed\n",
    "cap_embs = []\n",
    "for i in tqdm(range(0, len(text_caps), BS), desc=\"Encoding caps (CLIP)\"):\n",
    "    e = encode_text_with_clip(text_caps[i:i+BS]).detach().cpu()\n",
    "    cap_embs.append(e)\n",
    "cap_embs = torch.cat(cap_embs, dim=0)  # [N, 512]\n",
    "\n",
    "# cosine sim -> nearest image index\n",
    "cap_embs = F.normalize(cap_embs, dim=-1)\n",
    "img_pool_embs_t = img_pool_embs.t()  # [512, P]\n",
    "sims = cap_embs @ img_pool_embs_t  # [N, P]\n",
    "nearest = sims.argmax(dim=1).tolist()\n",
    "\n",
    "for idx, img_idx in enumerate(nearest):\n",
    "    img = pool_imgs[img_idx]\n",
    "    wave = torch.tensor(AUDIO_TRAIN[idx][\"audio\"]).float().numpy()\n",
    "    sr = int(AUDIO_TRAIN[idx][\"sr\"])\n",
    "    cap = AUDIO_TRAIN[idx][\"caption\"]\n",
    "    pseudo_triplets.append({\"image\": img, \"audio\": wave, \"sr\": sr, \"caption\": cap})\n",
    "\n",
    "print(\"Pseudo triplets created:\", len(pseudo_triplets))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0.7 Adapters & Losses\n",
    "\n",
    "# %%\n",
    "class LinearAdapter(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "    def forward(self, x):\n",
    "        z = self.proj(x)\n",
    "        z = self.ln(z)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "class MLPAdapter(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int = 1024):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        z = self.ln(z)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "# InfoNCE loss (symmetric)\n",
    "\n",
    "def info_nce(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.07):\n",
    "    # z1, z2: [B, D], normalized\n",
    "    logits = (z1 @ z2.t()) / temperature\n",
    "    targets = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss = (F.cross_entropy(logits, targets) + F.cross_entropy(logits.t(), targets)) / 2\n",
    "    return loss\n",
    "\n",
    "# Cycle consistency: pull z_v and z_a together if they share same caption\n",
    "\n",
    "def cycle_consistency(z_a: torch.Tensor, z_v: torch.Tensor, margin: float = 0.0):\n",
    "    # Positive pairs are aligned by index in the batch\n",
    "    # L2 or cosine: we use cosine distance (1 - cos)\n",
    "    sim = (z_a * z_v).sum(dim=-1)\n",
    "    loss = (1 - sim).mean()\n",
    "    return loss\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0.8 Utilities: batching encode, metrics, plots\n",
    "\n",
    "# %%\n",
    "@torch.inference_mode()\n",
    "def batch_encode_images(pils: List[Image.Image]) -> torch.Tensor:\n",
    "    embs = []\n",
    "    for i in range(0, len(pils), CFG.eval_batch):\n",
    "        embs.append(encode_image_pil(pils[i:i+CFG.eval_batch]).cpu())\n",
    "    return F.normalize(torch.cat(embs, dim=0), dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def batch_encode_texts(texts: List[str]) -> torch.Tensor:\n",
    "    embs = []\n",
    "    for i in range(0, len(texts), CFG.eval_batch):\n",
    "        embs.append(encode_text(texts[i:i+CFG.eval_batch]).cpu())\n",
    "    return F.normalize(torch.cat(embs, dim=0), dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def batch_encode_audio(waves: List[np.ndarray], srs: List[int]) -> torch.Tensor:\n",
    "    embs = []\n",
    "    for i in range(0, len(waves), CFG.eval_batch):\n",
    "        chunk = waves[i:i+CFG.eval_batch]\n",
    "        srs_chunk = srs[i:i+CFG.eval_batch]\n",
    "        sub = []\n",
    "        for w, sr in zip(chunk, srs_chunk):\n",
    "            t = torch.tensor(w).float().unsqueeze(0).to(device)\n",
    "            sub.append(encode_audio_wave(t, sr))\n",
    "        embs.append(torch.cat(sub, dim=0).detach().cpu())\n",
    "    return F.normalize(torch.cat(embs, dim=0), dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def recall_at_k(sim: torch.Tensor, k: int = 1) -> float:\n",
    "    # sim: [N, N] (query x gallery) cosine similarities; diagonal are positives\n",
    "    topk = sim.topk(k, dim=1).indices\n",
    "    correct = torch.arange(sim.size(0)).view(-1, 1)\n",
    "    hits = (topk == correct).any(dim=1).float().mean().item()\n",
    "    return hits\n",
    "\n",
    "@torch.inference_mode()\n",
    "def map_score(sim: torch.Tensor) -> float:\n",
    "    # Diagonal is the only positive per query\n",
    "    N = sim.size(0)\n",
    "    y_true = torch.zeros((N, N), dtype=torch.float32)\n",
    "    y_true[torch.arange(N), torch.arange(N)] = 1.0\n",
    "    y_score = sim.cpu().numpy()\n",
    "    ap = [average_precision_score(y_true[i].numpy(), y_score[i]) for i in range(N)]\n",
    "    return float(np.mean(ap))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_retrieval(z_q: torch.Tensor, z_g: torch.Tensor, direction: str):\n",
    "    # cosine similarity matrix\n",
    "    z_q = F.normalize(z_q, dim=-1)\n",
    "    z_g = F.normalize(z_g, dim=-1)\n",
    "    sim = z_q @ z_g.t()\n",
    "    return {\n",
    "        f\"{direction}_R@1\": recall_at_k(sim, 1),\n",
    "        f\"{direction}_R@5\": recall_at_k(sim, 5),\n",
    "        f\"{direction}_R@10\": recall_at_k(sim, 10),\n",
    "        f\"{direction}_mAP\": map_score(sim)\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_pos_neg_hist(z1: torch.Tensor, z2: torch.Tensor, title: str):\n",
    "    # Build positives (diag) and negatives (off-diag) cosines\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "    sim = (z1 @ z2.t()).cpu().numpy()\n",
    "    pos = sim.diagonal()\n",
    "    neg = sim[~np.eye(sim.shape[0], dtype=bool)]\n",
    "    plt.figure()\n",
    "    plt.hist(pos, bins=40, alpha=0.6, label=\"positives\")\n",
    "    plt.hist(neg, bins=40, alpha=0.6, label=\"negatives\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"cosine similarity\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Phase 0 Sanity: Encode & Baseline (no training)\n",
    "\n",
    "# %%\n",
    "# Build eval tensors for (i,t) and (a,t)\n",
    "val_imgs = [ex[\"image\"] for ex in COCO_VAL]\n",
    "val_caps_it = [ex[\"caption\"] for ex in COCO_VAL]\n",
    "\n",
    "val_audio = [ex[\"audio\"] for ex in AUDIO_VAL]\n",
    "val_srs = [int(ex[\"sr\"]) for ex in AUDIO_VAL]\n",
    "val_caps_at = [ex[\"caption\"] for ex in AUDIO_VAL]\n",
    "\n",
    "print(\"Encoding eval sets...\")\n",
    "Z_i_val = batch_encode_images(val_imgs)\n",
    "Z_t_it_val = batch_encode_texts(val_caps_it)\n",
    "Z_a_val = batch_encode_audio(val_audio, val_srs)\n",
    "Z_t_at_val = batch_encode_texts(val_caps_at)\n",
    "\n",
    "print(\"Eval (no adapters, just frozen encoders, cosine retrieval):\")\n",
    "res_baseline = {}\n",
    "res_baseline.update(eval_retrieval(Z_t_it_val, Z_i_val, \"t->i\"))\n",
    "res_baseline.update(eval_retrieval(Z_i_val, Z_t_it_val, \"i->t\"))\n",
    "res_baseline.update(eval_retrieval(Z_t_at_val, Z_a_val, \"t->a\"))\n",
    "res_baseline.update(eval_retrieval(Z_a_val, Z_t_at_val, \"a->t\"))\n",
    "print(json.dumps(res_baseline, indent=2))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Phase 1 — Pairwise-only training\n",
    "# Train independent adapters for (i<->t) and (a<->t) with InfoNCE. Then *evaluate all three* directions, including i<->a (which should be weak).\n",
    "\n",
    "# %%\n",
    "# Infer encoder output dims\n",
    "with torch.inference_mode():\n",
    "    d_img = Z_i_val.size(1)\n",
    "    d_txt = Z_t_it_val.size(1)\n",
    "    d_aud = Z_a_val.size(1)\n",
    "print(\"Encoder dims:\", d_img, d_txt, d_aud)\n",
    "\n",
    "# Projectors for Pairwise (separate text heads)\n",
    "proj_v = LinearAdapter(d_img, CFG.embed_dim).to(device)\n",
    "proj_t_v = LinearAdapter(d_txt, CFG.embed_dim).to(device)\n",
    "proj_a = LinearAdapter(d_aud, CFG.embed_dim).to(device)\n",
    "proj_t_a = LinearAdapter(d_txt, CFG.embed_dim).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(list(proj_v.parameters()) + list(proj_t_v.parameters()), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG.fp16 and device.type==\"cuda\")\n",
    "\n",
    "print(\"Training (i<->t) pairwise...\")\n",
    "for epoch in range(CFG.epochs_pairwise):\n",
    "    proj_v.train(); proj_t_v.train()\n",
    "    pbar = tqdm(coco_train_loader, desc=f\"[Pairwise i<->t] epoch {epoch+1}\")\n",
    "    for imgs, caps in pbar:\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.fp16 and device.type==\"cuda\"):\n",
    "            # encode\n",
    "            z_i = encode_image_pil(list(imgs))\n",
    "            z_t = encode_text(list(caps))\n",
    "            # project\n",
    "            z_i = proj_v(z_i)\n",
    "            z_t = proj_t_v(z_t)\n",
    "            loss = info_nce(z_i, z_t)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "opt2 = torch.optim.AdamW(list(proj_a.parameters()) + list(proj_t_a.parameters()), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "print(\"Training (a<->t) pairwise...\")\n",
    "for epoch in range(CFG.epochs_pairwise):\n",
    "    proj_a.train(); proj_t_a.train()\n",
    "    pbar = tqdm(audio_train_loader, desc=f\"[Pairwise a<->t] epoch {epoch+1}\")\n",
    "    for waves, srs, caps in pbar:\n",
    "        # batch encode audio\n",
    "        z_a_list = []\n",
    "        for w, sr in zip(waves, srs):\n",
    "            w = w.to(device)\n",
    "            z_a_list.append(encode_audio_wave(w, int(sr)))\n",
    "        z_a = torch.cat(z_a_list, dim=0)\n",
    "        z_t = encode_text(list(caps))\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.fp16 and device.type==\"cuda\"):\n",
    "            z_a = proj_a(z_a)\n",
    "            z_t = proj_t_a(z_t)\n",
    "            loss = info_nce(z_a, z_t)\n",
    "        opt2.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt2)\n",
    "        scaler.update()\n",
    "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "# Eval Phase 1\n",
    "@torch.inference_mode()\n",
    "def eval_phase1():\n",
    "    proj_v.eval(); proj_t_v.eval(); proj_a.eval(); proj_t_a.eval()\n",
    "    Zi = proj_v(Z_i_val.to(device)).cpu()\n",
    "    Zt_it = proj_t_v(Z_t_it_val.to(device)).cpu()\n",
    "    Za = proj_a(Z_a_val.to(device)).cpu()\n",
    "    Zt_at = proj_t_a(Z_t_at_val.to(device)).cpu()\n",
    "\n",
    "    # t<->i (using t head from (i,t) model)\n",
    "    res = {}\n",
    "    res.update(eval_retrieval(Zt_it, Zi, \"t->i\"))\n",
    "    res.update(eval_retrieval(Zi, Zt_it, \"i->t\"))\n",
    "\n",
    "    # t<->a (using t head from (a,t) model)\n",
    "    res.update(eval_retrieval(Zt_at, Za, \"t->a\"))\n",
    "    res.update(eval_retrieval(Za, Zt_at, \"a->t\"))\n",
    "\n",
    "    # i<->a (mismatch: image head trained with t_v, audio head trained with t_a) → expected to be poor\n",
    "    res.update(eval_retrieval(Zi, Za, \"i->a\"))\n",
    "    res.update(eval_retrieval(Za, Zi, \"a->i\"))\n",
    "    return res, Zi, Zt_it, Za, Zt_at\n",
    "\n",
    "res_p1, Zi_p1, Zt_it_p1, Za_p1, Zt_at_p1 = eval_phase1()\n",
    "print(\"Phase 1 results:\")\n",
    "print(json.dumps(res_p1, indent=2))\n",
    "\n",
    "# Histograms\n",
    "plot_pos_neg_hist(Zt_it_p1, Zi_p1, title=\"Phase 1: t<->i cosine\")\n",
    "plot_pos_neg_hist(Zt_at_p1, Za_p1, title=\"Phase 1: t<->a cosine\")\n",
    "plot_pos_neg_hist(Zi_p1, Za_p1, title=\"Phase 1: i<->a cosine (expected weak)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Phase 2 — Image Hub (i<->t + i<->a)\n",
    "# Use pseudo triplets to supervise i<->a; keep encoders frozen; train small adapters for all three.\n",
    "\n",
    "# %%\n",
    "# Fresh adapters for hub training\n",
    "hub_v = LinearAdapter(d_img, CFG.embed_dim).to(device)\n",
    "hub_t = LinearAdapter(d_txt, CFG.embed_dim).to(device)\n",
    "hub_a = LinearAdapter(d_aud, CFG.embed_dim).to(device)\n",
    "\n",
    "opt_hub = torch.optim.AdamW(list(hub_v.parameters()) + list(hub_t.parameters()) + list(hub_a.parameters()), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "# Build simple loaders for pseudo (i,a,t) triplets\n",
    "class PseudoTripletDS(Dataset):\n",
    "    def __init__(self, triples):\n",
    "        self.triples = triples\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.triples[idx]\n",
    "        return ex[\"image\"], ex[\"audio\"], int(ex[\"sr\"]), ex[\"caption\"]\n",
    "\n",
    "triplet_loader = DataLoader(PseudoTripletDS(pseudo_triplets), batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, collate_fn=lambda b: list(zip(*b)))\n",
    "\n",
    "print(\"Training Hub (i<->t + i<->a)...\")\n",
    "for epoch in range(CFG.epochs_hub):\n",
    "    hub_v.train(); hub_t.train(); hub_a.train()\n",
    "    pbar = tqdm(triplet_loader, desc=f\"[Hub i<->t,i<->a] epoch {epoch+1}\")\n",
    "    for imgs, auds, srs, caps in pbar:\n",
    "        # Encode\n",
    "        z_i = encode_image_pil(list(imgs))\n",
    "        z_t = encode_text(list(caps))\n",
    "        z_a_list = []\n",
    "        for a, sr in zip(auds, srs):\n",
    "            w = torch.tensor(a).float().unsqueeze(0).to(device)\n",
    "            z_a_list.append(encode_audio_wave(w, int(sr)))\n",
    "        z_a = torch.cat(z_a_list, dim=0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.fp16 and device.type==\"cuda\"):\n",
    "            zi = hub_v(z_i)\n",
    "            zt = hub_t(z_t)\n",
    "            za = hub_a(z_a)\n",
    "            # Hub losses: i<->t and i<->a only\n",
    "            loss = info_nce(zi, zt) + info_nce(zi, za)\n",
    "        opt_hub.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt_hub)\n",
    "        scaler.update()\n",
    "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_phase2():\n",
    "    hub_v.eval(); hub_t.eval(); hub_a.eval()\n",
    "    Zi = hub_v(Z_i_val.to(device)).cpu()\n",
    "    Zt_it = hub_t(Z_t_it_val.to(device)).cpu()\n",
    "    Za = hub_a(Z_a_val.to(device)).cpu()\n",
    "    Zt_at = hub_t(Z_t_at_val.to(device)).cpu()  # same text head\n",
    "\n",
    "    res = {}\n",
    "    # i<->t\n",
    "    res.update(eval_retrieval(Zt_it, Zi, \"t->i\"))\n",
    "    res.update(eval_retrieval(Zi, Zt_it, \"i->t\"))\n",
    "    # i<->a\n",
    "    res.update(eval_retrieval(Zi, Za, \"i->a\"))\n",
    "    res.update(eval_retrieval(Za, Zi, \"a->i\"))\n",
    "    # a<->t (note: no direct a<->t loss; test generalization)\n",
    "    res.update(eval_retrieval(Zt_at, Za, \"t->a\"))\n",
    "    res.update(eval_retrieval(Za, Zt_at, \"a->t\"))\n",
    "    return res, Zi, Zt_it, Za, Zt_at\n",
    "\n",
    "res_p2, Zi_p2, Zt_it_p2, Za_p2, Zt_at_p2 = eval_phase2()\n",
    "print(\"Phase 2 (Hub) results:\")\n",
    "print(json.dumps(res_p2, indent=2))\n",
    "\n",
    "plot_pos_neg_hist(Zi_p2, Za_p2, title=\"Phase 2: i<->a cosine (hub)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Phase 3 — Tri-modal Adapters + Cycle Consistency (Ours)\n",
    "# Jointly train v/a/t adapters in a *shared* space with InfoNCE on (i<->t) and (a<->t), plus cycle loss tying (i,a) that share the same caption.\n",
    "\n",
    "# %%\n",
    "tri_v = LinearAdapter(d_img, CFG.embed_dim).to(device)\n",
    "tri_t = LinearAdapter(d_txt, CFG.embed_dim).to(device)\n",
    "tri_a = LinearAdapter(d_aud, CFG.embed_dim).to(device)\n",
    "\n",
    "opt_tri = torch.optim.AdamW(list(tri_v.parameters()) + list(tri_t.parameters()) + list(tri_a.parameters()), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "print(\"Training Tri-modal (i<->t, a<->t) + Cycle(i~a|t)...\")\n",
    "for epoch in range(CFG.epochs_trimodal):\n",
    "    tri_v.train(); tri_t.train(); tri_a.train()\n",
    "    pbar = tqdm(triplet_loader, desc=f\"[Tri-Modal + Cycle] epoch {epoch+1}\")\n",
    "    for imgs, auds, srs, caps in pbar:\n",
    "        # Encode\n",
    "        z_i = encode_image_pil(list(imgs))\n",
    "        z_t = encode_text(list(caps))\n",
    "        z_a_list = []\n",
    "        for a, sr in zip(auds, srs):\n",
    "            w = torch.tensor(a).float().unsqueeze(0).to(device)\n",
    "            z_a_list.append(encode_audio_wave(w, int(sr)))\n",
    "        z_a = torch.cat(z_a_list, dim=0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.fp16 and device.type==\"cuda\"):\n",
    "            zi = tri_v(z_i)\n",
    "            zt = tri_t(z_t)\n",
    "            za = tri_a(z_a)\n",
    "            loss = info_nce(zi, zt) + info_nce(za, zt) + 0.2 * cycle_consistency(za, zi)\n",
    "        opt_tri.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt_tri)\n",
    "        scaler.update()\n",
    "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_phase3():\n",
    "    tri_v.eval(); tri_t.eval(); tri_a.eval()\n",
    "    Zi = tri_v(Z_i_val.to(device)).cpu()\n",
    "    Zt_it = tri_t(Z_t_it_val.to(device)).cpu()\n",
    "    Za = tri_a(Z_a_val.to(device)).cpu()\n",
    "    Zt_at = tri_t(Z_t_at_val.to(device)).cpu()\n",
    "\n",
    "    res = {}\n",
    "    # All six directions\n",
    "    res.update(eval_retrieval(Zt_it, Zi, \"t->i\"))\n",
    "    res.update(eval_retrieval(Zi, Zt_it, \"i->t\"))\n",
    "\n",
    "    res.update(eval_retrieval(Zt_at, Za, \"t->a\"))\n",
    "    res.update(eval_retrieval(Za, Zt_at, \"a->t\"))\n",
    "\n",
    "    res.update(eval_retrieval(Zi, Za, \"i->a\"))\n",
    "    res.update(eval_retrieval(Za, Zi, \"a->i\"))\n",
    "    return res, Zi, Zt_it, Za, Zt_at\n",
    "\n",
    "res_p3, Zi_p3, Zt_it_p3, Za_p3, Zt_at_p3 = eval_phase3()\n",
    "print(\"Phase 3 (Ours) results:\")\n",
    "print(json.dumps(res_p3, indent=2))\n",
    "\n",
    "plot_pos_neg_hist(Zi_p3, Za_p3, title=\"Phase 3: i<->a cosine (ours)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Comparison Table (Phases 1–3)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "\n",
    "# unify keys\n",
    "keys = [\n",
    "    \"t->i_R@1\",\"t->i_R@5\",\"t->i_R@10\",\"t->i_mAP\",\n",
    "    \"i->t_R@1\",\"i->t_R@5\",\"i->t_R@10\",\"i->t_mAP\",\n",
    "    \"t->a_R@1\",\"t->a_R@5\",\"t->a_R@10\",\"t->a_mAP\",\n",
    "    \"a->t_R@1\",\"a->t_R@5\",\"a->t_R@10\",\"a->t_mAP\",\n",
    "    \"i->a_R@1\",\"i->a_R@5\",\"i->a_R@10\",\"i->a_mAP\",\n",
    "    \"a->i_R@1\",\"a->i_R@5\",\"a->i_R@10\",\"a->i_mAP\",\n",
    "]\n",
    "\n",
    "def row_of(res: Dict[str, float]):\n",
    "    return [res.get(k, float('nan')) for k in keys]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [row_of(res_p1), row_of(res_p2), row_of(res_p3)],\n",
    "    index=[\"Pairwise-only\", \"Hub (image)\", \"Ours (Tri+Cycle)\"],\n",
    "    columns=keys,\n",
    ")\n",
    "print(df.round(4))\n",
    "\n",
    "# Matplotlib bar chart: focus on i<->a R@1 across the three settings\n",
    "plt.figure()\n",
    "vals = [res_p1.get(\"i->a_R@1\", 0.0), res_p2.get(\"i->a_R@1\", 0.0), res_p3.get(\"i->a_R@1\", 0.0)]\n",
    "plt.bar([\"Pairwise\", \"Hub\", \"Ours\"], vals)\n",
    "plt.title(\"i->a R@1 across settings\")\n",
    "plt.ylabel(\"Recall@1\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Save Artifacts\n",
    "\n",
    "# %%\n",
    "out_dir = Path(\"artifacts\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({\"proj_v\": proj_v.state_dict(), \"proj_t_v\": proj_t_v.state_dict()}, out_dir/\"phase1_it.pt\")\n",
    "torch.save({\"proj_a\": proj_a.state_dict(), \"proj_t_a\": proj_t_a.state_dict()}, out_dir/\"phase1_at.pt\")\n",
    "torch.save({\"hub_v\": hub_v.state_dict(), \"hub_t\": hub_t.state_dict(), \"hub_a\": hub_a.state_dict()}, out_dir/\"phase2_hub.pt\")\n",
    "torch.save({\"tri_v\": tri_v.state_dict(), \"tri_t\": tri_t.state_dict(), \"tri_a\": tri_a.state_dict()}, out_dir/\"phase3_ours.pt\")\n",
    "\n",
    "df.to_csv(out_dir/\"comparison_phase1_2_3.csv\", index=True)\n",
    "\n",
    "print(\"Saved:\", list(out_dir.iterdir()))\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Notes & Next Steps\n",
    "# - Phases 4–7 can build on these adapters:\n",
    "#   * Phase 4: profile FLOPs/VRAM (use torch.cuda.max_memory_allocated, measure timestamps, count parameters in adapters).\n",
    "#   * Phase 5: add Matryoshka heads (multiple width slices in adapters) + token resampling.\n",
    "#   * Phase 6: apply audio noise / image blur and re-run eval.\n",
    "#   * Phase 7: ablations (turn off cycle, swap LinearAdapter with MLPAdapter, vary pseudo triplet %).\n",
    "# - If memory becomes a bottleneck, reduce subset sizes in Config.\n",
    "# - To speed up triplet retrieval, cache CLIP image pool embeddings to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515186e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7a717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b42a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "py311_main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
