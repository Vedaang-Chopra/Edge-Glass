{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db945a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using code base at: /storage/ice1/1/0/vchopra37/projects/edge_glass/code_base\n"
     ]
    }
   ],
   "source": [
    "# --- Path bootstrap so \"from utils.config import ...\" works no matter where Jupyter's CWD is ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def add_codebase_to_path(marker_dir_name=\"code_base\"):\n",
    "    cwd = Path.cwd().resolve()\n",
    "    # walk up until we find \"code_base\" that contains utils/config.py\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if p.name == marker_dir_name and (p / \"utils\" / \"config.py\").exists():\n",
    "            sys.path.insert(0, str(p))  # add code_base itself\n",
    "            return p\n",
    "        # if we're at repo root and \"code_base\" is a child, handle that too\n",
    "        if (p / marker_dir_name / \"utils\" / \"config.py\").exists():\n",
    "            sys.path.insert(0, str(p / marker_dir_name))\n",
    "            return p / marker_dir_name\n",
    "    raise RuntimeError(\"Could not locate code_base/ with utils/config.py\")\n",
    "\n",
    "CODEBASE_DIR = add_codebase_to_path()\n",
    "print(\"Using code base at:\", CODEBASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33da365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform, torch\n",
    "import transformers\n",
    "from utils.config import load_config, select_device, select_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d2fa1",
   "metadata": {},
   "source": [
    "### Cell 1 — Imports & env echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb8dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.5\n",
      "Torch: 2.9.0+cu128\n",
      "Transformers: 4.57.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "HF_HOME: /home/hice1/vchopra37/scratch/hf_home\n",
      "TORCH_HOME: /home/hice1/vchopra37/scratch/torch\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))\n",
    "print(\"TORCH_HOME:\", os.environ.get(\"TORCH_HOME\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d388160",
   "metadata": {},
   "source": [
    "### Cell 2 — Load config & inspect resolved paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff739e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "DType: torch.float16\n",
      "Resolved paths: {'data': '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/data', 'out': '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/experiments', 'embeds': '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/experiments/embeddings'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(device='cuda', dtype='fp16', seeds=Seeds(python=1337, torch=1337), paths=Paths(data='./data', out='./experiments', embeds='./experiments/embeddings'), encoders=Encoders(vision='openai/clip-vit-base-patch32', text='sentence-transformers/all-MiniLM-L6-v2', audio='openai/whisper-small'), cfg_path=PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/configs/base.yaml'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_config()  # picks code_base/configs/base.yaml by default\n",
    "device = select_device(cfg)\n",
    "dtype = select_dtype(cfg)\n",
    "\n",
    "resolved = cfg.paths.resolve()\n",
    "print(\"Device:\", device)\n",
    "print(\"DType:\", dtype)\n",
    "print(\"Resolved paths:\", {k:str(v) for k,v in resolved.items()})\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cf4e3",
   "metadata": {},
   "source": [
    "### Cell 3 — Create a run artifact under code_base/experiments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c1ffbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': '0_bootstrap',\n",
       " 'device': 'cuda',\n",
       " 'dtype': 'fp16',\n",
       " 'encoders': {'vision': 'openai/clip-vit-base-patch32',\n",
       "  'text': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'audio': 'openai/whisper-small'},\n",
       " 'cfg_path': '/storage/ice1/1/0/vchopra37/projects/edge_glass/code_base/configs/base.yaml'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "run_info = {\n",
    "    \"phase\": \"0_bootstrap\",\n",
    "    \"device\": str(device),\n",
    "    \"dtype\": cfg.dtype,\n",
    "    \"encoders\": {\n",
    "        \"vision\": cfg.encoders.vision,\n",
    "        \"text\": cfg.encoders.text,\n",
    "        \"audio\": cfg.encoders.audio,\n",
    "    },\n",
    "    \"cfg_path\": str(cfg.cfg_path),\n",
    "}\n",
    "runs_dir = cfg.paths.resolve()[\"out\"] / \"runs\"\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(runs_dir / \"phase0_bootstrap.json\", \"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "run_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455b7a6",
   "metadata": {},
   "source": [
    "### Cell 4 — Micro GPU test (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f67c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul OK: torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1024, 1024, device=device, dtype=dtype if dtype!=torch.float16 or torch.cuda.is_available() else torch.float32)\n",
    "    y = x @ x.T\n",
    "    print(\"Matmul OK:\", y.shape)\n",
    "else:\n",
    "    print(\"CPU mode. Matmul OK (small):\", (torch.randn(64,64) @ torch.randn(64,64)).shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
