{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60960b6a",
   "metadata": {},
   "source": [
    "### üëâ Goal: use a frozen CLIP model to check if image ‚Üî text embeddings align.\n",
    "\n",
    "We‚Äôll just do Stage 1 ‚Üí Step E1.1 right now:\n",
    "üëâ Goal: use a frozen CLIP model to check if image ‚Üî text embeddings align.\n",
    "\n",
    "You‚Äôll:\n",
    "\n",
    "Load a few images + captions,\n",
    "\n",
    "Get CLIP embeddings,\n",
    "\n",
    "Compute cosine similarities,\n",
    "\n",
    "See if each caption matches its image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# your images and captions\n",
    "images = [\n",
    "    Image.open(requests.get(\"https://picsum.photos/id/237/400/400\", stream=True).raw),\n",
    "    Image.open(requests.get(\"https://picsum.photos/id/1025/400/400\", stream=True).raw),\n",
    "    Image.open(requests.get(\"https://picsum.photos/id/1074/400/400\", stream=True).raw)\n",
    "]\n",
    "\n",
    "captions = [\n",
    "    \"A black dog looking at the camera\",\n",
    "     \"A small dog wrapped in a blanket\",          # image 2 ‚úÖ\n",
    "    \"A close-up portrait of a lion\"              # image 3 ‚úÖ\n",
    "]\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "\n",
    "if len(images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img, cap in zip(axes, images, captions):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(cap, fontsize=10, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# embed images\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    img_emb = model.get_image_features(**inputs)\n",
    "    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# embed texts\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=captions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    txt_emb = model.get_text_features(**inputs)\n",
    "    txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb11e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity matrix\n",
    "sims = txt_emb @ img_emb.T\n",
    "sims_np = sims.cpu().numpy()\n",
    "\n",
    "for i, cap in enumerate(captions):\n",
    "    best_idx = torch.argmax(sims[i]).item()\n",
    "\n",
    "    # show the image\n",
    "    plt.imshow(images[best_idx])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"True caption:\\n{cap}\\n\\nCLIP matched image #{best_idx+1}\", fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"--- Similarities for caption {i+1} ---\")\n",
    "    for j, val in enumerate(sims_np[i]):\n",
    "        print(f\"Image {j+1}: {val:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af7e50",
   "metadata": {},
   "source": [
    "### Goal: check whether Whisper‚Äôs audio embeddings align with MiniLM‚Äôs text embeddings.\n",
    "We‚Äôll keep it as simple and visual as before ‚Äî small dataset, frozen models, cosine similarity, clear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7830173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import torchaudio\n",
    "\n",
    "samples = [\n",
    "    {\n",
    "        \"url\": \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "        \"caption\": \"A man is speaking calmly\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/2.flac\",\n",
    "        \"caption\": \"A person is laughing loudly\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/3.flac\",\n",
    "        \"caption\": \"A dog is barking\"\n",
    "    }\n",
    "]\n",
    "\n",
    "audios, captions = [], []\n",
    "for s in samples:\n",
    "    # ‚úÖ download to bytes buffer, then read with torchaudio\n",
    "    resp = requests.get(s[\"url\"])\n",
    "    wav_bytes = io.BytesIO(resp.content)\n",
    "    wav, sr = torchaudio.load(wav_bytes)\n",
    "    audios.append((wav.mean(0), sr))\n",
    "    captions.append(s[\"caption\"])\n",
    "\n",
    "print(\"Loaded\", len(audios), \"audio clips successfully ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper encoder (for audio)\n",
    "whisper_name = \"openai/whisper-tiny\"\n",
    "whisper = WhisperModel.from_pretrained(whisper_name).to(device).eval()\n",
    "whisper_proc = WhisperProcessor.from_pretrained(whisper_name)\n",
    "\n",
    "# MiniLM (for text)\n",
    "text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_emb = []\n",
    "for wav, sr in tqdm(audios, desc=\"Audio ‚Üí Whisper\"):\n",
    "    # Make sure it's a single 1D numpy array\n",
    "    audio_np = wav.squeeze().numpy()\n",
    "    inputs = whisper_proc([audio_np], sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = whisper.encoder(inputs.input_features).last_hidden_state.mean(dim=1)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    a_emb.append(feats.cpu())\n",
    "a_emb = torch.cat(a_emb, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e472ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute text embeddings for captions\n",
    "t_emb = torch.tensor(\n",
    "    text_model.encode(captions, convert_to_numpy=True, normalize_embeddings=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = (a_emb @ t_emb.T).cpu().numpy()\n",
    "\n",
    "for i, cap in enumerate(captions):\n",
    "    best = sims[i].argmax()\n",
    "    print(f\"\\nCaption {i+1}: {cap}\")\n",
    "    print(f\"Best-matched audio index: {best+1}\")\n",
    "    print(\"Similarity scores:\", sims[i].round(3))\n",
    "\n",
    "    # Simple bar plot\n",
    "    plt.bar(range(1, len(audios)+1), sims[i])\n",
    "    plt.title(f\"Audio {i+1} vs Texts\")\n",
    "    plt.xlabel(\"Text index\"); plt.ylabel(\"Cosine similarity\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9d6d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a686e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ensure everything is numpy\n",
    "a_np = a_emb.numpy()\n",
    "t_np = t_emb.numpy()\n",
    "i_np = img_emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56215f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "common_dim = min(2, t_np.shape[0], i_np.shape[0])  # safe value\n",
    "pca_t = PCA(n_components=common_dim).fit(t_np)\n",
    "pca_i = PCA(n_components=common_dim).fit(i_np)\n",
    "\n",
    "t_proj = pca_t.transform(t_np)\n",
    "i_proj = pca_i.transform(i_np)\n",
    "\n",
    "sim_t_i = cosine_similarity(t_proj, i_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Audio ‚Üí Text similarity\n",
    "sim_a_t = cosine_similarity(a_np, t_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best-matching text for each audio\n",
    "best_text_idx = np.argmax(sim_a_t, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2Ô∏è‚É£ Text ‚Üí Image similarity (CLIP space)\n",
    "# sim_t_i = cosine_similarity(t_np, i_np)\n",
    "sim_a_i = sim_a_t[np.arange(len(best_text_idx)), best_text_idx][:, None] * sim_t_i[best_text_idx]\n",
    "best_img_idx = np.argmax(sim_a_i, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff058df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final Audio ‚Üí Image score through the text pivot\n",
    "sim_a_i = sim_a_t[np.arange(len(best_text_idx)), best_text_idx][:, None] * sim_t_i[best_text_idx]\n",
    "\n",
    "# for each audio, get the image with the highest combined similarity\n",
    "best_img_idx = np.argmax(sim_a_i, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f47f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, (audio_idx, img_idx) in enumerate(zip(range(len(best_img_idx)), best_img_idx)):\n",
    "    print(f\"\\nAudio {audio_idx+1} predicted best image: {img_idx+1}\")\n",
    "    print(\"Matched caption:\", captions[best_text_idx[audio_idx]])\n",
    "\n",
    "    plt.imshow(images[img_idx])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Audio {audio_idx+1} ‚Üí Image {img_idx+1}\\nvia text '{captions[best_text_idx[audio_idx]]}'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aada8c",
   "metadata": {},
   "source": [
    "# Title: ‚ÄúZero-shot Image‚ÄìText Retrieval (CLIP-ViT-B/16)‚Äù\n",
    "\n",
    "Objective: Show that a frozen vision-language model already aligns images ‚Üî captions.\n",
    "\n",
    "Setup: CLIP ViT-B/16, COCO-2017 val, N=12 pairs, no training.\n",
    "\n",
    "Procedure: Encode images & captions ‚Üí cosine similarity ‚Üí retrieval.\n",
    "\n",
    "Metrics: Acc@1 (caption‚Üíimage), similarity heatmap, top-3 qualitative results.\n",
    "\n",
    "Takeaway: CLIP gets strong text‚Üîimage alignment out-of-the-box ‚Üí a solid baseline to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "# streaming = True lets us pull just a few samples quickly\n",
    "ds = load_dataset(\"allenai/pixmo-cap\", split=\"train\", streaming=True)\n",
    "\n",
    "# take 2 samples only\n",
    "samples = list(islice(ds, 2))\n",
    "\n",
    "print(f\"Loaded {len(samples)} examples\")\n",
    "for i, s in enumerate(samples):\n",
    "    print(f\"\\nSample {i+1} caption:\\n{s['caption'][:200]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faff2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, len(samples), figsize=(14, 5))\n",
    "if len(samples) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, s in zip(axes, samples):\n",
    "    img = Image.open(requests.get(s[\"image_url\"], stream=True).raw).convert(\"RGB\")\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(s[\"caption\"][:120] + \"‚Ä¶\", fontsize=10, wrap=True)\n",
    "    break\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d71347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, requests\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for ex in islice(ds, 60):  # overfetch a bit to skip any bad rows\n",
    "    cap = ex.get(\"caption\", None)\n",
    "    url = ex.get(\"image_url\", None)\n",
    "    if not cap or not url:\n",
    "        continue\n",
    "    try:\n",
    "        img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        continue\n",
    "    pairs.append((img, cap))\n",
    "    if len(pairs) == N:\n",
    "        break\n",
    "\n",
    "images = [im for im, _ in pairs]\n",
    "captions = [tx for _, tx in pairs]\n",
    "print(f\"Loaded {len(images)} PixMo samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3568d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- SigLIP model ----------\n",
    "model_id = \"google/siglip-base-patch16-224\"\n",
    "model = AutoModel.from_pretrained(model_id).to(device).eval()         # returns SiglipModel\n",
    "proc  = AutoProcessor.from_pretrained(model_id)                       # returns SiglipProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d358479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- Embeddings (normalized) ----------\n",
    "with torch.no_grad():\n",
    "    img_emb = model.get_image_features(**proc(images=images, return_tensors=\"pt\").to(device))\n",
    "    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "    txt_emb = model.get_text_features(**proc(text=captions, return_tensors=\"pt\", padding=True, truncation=True).to(device))\n",
    "    txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# ---------- Similarity & Acc@1 ----------\n",
    "S = (txt_emb @ img_emb.T).detach().cpu().numpy()   # [N_text, N_img]\n",
    "pred = S.argmax(axis=1)\n",
    "acc1 = (pred == np.arange(len(captions))).mean()\n",
    "print(f\"Acc@1 (caption‚Üíimage): {acc1:.2f}\")\n",
    "\n",
    "# ---------- Figure A: Heatmap ----------\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "sns.heatmap(S, annot=False, cmap=\"viridis\")\n",
    "plt.xlabel(\"image index\"); plt.ylabel(\"caption index\")\n",
    "plt.title(f\"SigLIP cosine similarity (Acc@1={acc1:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"siglip_heatmap.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# ---------- Figure B: Top-3 retrieval gallery ----------\n",
    "def show_topk_gallery(k_ids=(0,1,2), topk=3):\n",
    "    for cid in k_ids:\n",
    "        order = np.argsort(-S[cid])[:topk]\n",
    "        fig, axes = plt.subplots(1, topk, figsize=(10, 3.2))\n",
    "        for j, ax in enumerate(axes):\n",
    "            ax.imshow(images[order[j]]); ax.axis(\"off\")\n",
    "            ax.set_title(f\"rank {j+1}  s={S[cid, order[j]]:.2f}\", fontsize=9)\n",
    "        fig.suptitle(f\"Query caption:\\n{captions[cid]}\", fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"siglip_top{topk}_caption{cid}.png\", dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "show_topk_gallery(k_ids=tuple(range(min(3, len(captions)))), topk=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e6527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fd5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
