{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee260da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ef03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random, time, io\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor, CLIPProcessor, CLIPModel, WhisperModel, WhisperProcessor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import faiss\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ac7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- Repro ----------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def recall_at_k(scores: np.ndarray, gt: List[int], ks=(1,5,10)):\n",
    "    # scores: [num_queries, num_items] (higher is better)\n",
    "    ranks = np.argsort(-scores, axis=1)\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        hits = 0\n",
    "        for i, g in enumerate(gt):\n",
    "            if g in ranks[i, :k]:\n",
    "                hits += 1\n",
    "        out[f'R@{k}'] = hits / len(gt)\n",
    "    return out\n",
    "\n",
    "def mean_average_precision(scores: np.ndarray, gt: List[int]):\n",
    "    # mAP@all: binary relevance only for the single matching index g\n",
    "    APs = []\n",
    "    ranks = np.argsort(-scores, axis=1)\n",
    "    for i, g in enumerate(gt):\n",
    "        rank_list = ranks[i]\n",
    "        # precision at the rank where g appears\n",
    "        idx = np.where(rank_list == g)[0]\n",
    "        if len(idx) == 0:\n",
    "            APs.append(0.0)\n",
    "        else:\n",
    "            r = idx[0] + 1\n",
    "            APs.append(1.0 / r)\n",
    "    return float(np.mean(APs))\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray):\n",
    "    a_n = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "    b_n = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
    "    return a_n @ b_n.T\n",
    "\n",
    "def plot_cos_hist(pos_sims, neg_sims, title):\n",
    "    plt.figure(figsize=(5,3.2))\n",
    "    plt.hist(neg_sims, bins=50, alpha=0.6, label=\"negatives\")\n",
    "    plt.hist(pos_sims, bins=50, alpha=0.6, label=\"positives\")\n",
    "    plt.title(title); plt.xlabel(\"cosine similarity\"); plt.ylabel(\"count\")\n",
    "    plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76431a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # sample sizes (tune to your GPU / time budget)\n",
    "    N_COCO: int = 5000\n",
    "    N_AUDIOCAPS: int = 5000\n",
    "\n",
    "    # image size / audio sampling\n",
    "    IMG_SIZE: int = 224\n",
    "    AUDIO_SR: int = 16000\n",
    "\n",
    "    # batch sizes\n",
    "    BATCH_CLIP: int = 64\n",
    "    BATCH_AUDIO: int = 32\n",
    "    BATCH_TEXT: int = 128\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# If you already have local copies, set these (folders of images / wav + json with {image_path, caption} etc.)\n",
    "LOCAL_COCO = None          # e.g., \"/data/coco_2017_val_subset/\"\n",
    "LOCAL_AUDIOCAPS = None     # e.g., \"/data/audiocaps_subset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_pairs(n=5000, local_root=None):\n",
    "    data = []\n",
    "    try:\n",
    "        ds = load_dataset(\"coco_captions\", \"2017\", split=\"validation\")\n",
    "        # each item: {image, captions:[{id, caption}]}\n",
    "        for ex in ds.select(range(min(n, len(ds)))):\n",
    "            img = ex[\"image\"]\n",
    "            caps = [c[\"caption\"] for c in ex[\"captions\"]]\n",
    "            if len(caps)==0: \n",
    "                continue\n",
    "            data.append((img, caps[0]))\n",
    "    except Exception as e:\n",
    "        if not local_root:\n",
    "            raise RuntimeError(f\"HF coco_captions failed and no local_root provided: {e}\")\n",
    "        # Expect local_root to contain 'images/' and a jsonl with {\"image_path\":\"...\", \"caption\":\"...\"}\n",
    "        import json, glob\n",
    "        jfiles = glob.glob(os.path.join(local_root, \"*.jsonl\"))\n",
    "        assert jfiles, \"Provide a jsonl with image_path, caption\"\n",
    "        with open(jfiles[0], \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                obj = json.loads(line)\n",
    "                p = obj[\"image_path\"]; cap = obj[\"caption\"]\n",
    "                img = Image.open(os.path.join(local_root, p)).convert(\"RGB\")\n",
    "                data.append((img, cap))\n",
    "                if len(data)>=n: break\n",
    "    return data[:n]\n",
    "\n",
    "coco_pairs = load_coco_pairs(cfg.N_COCO, LOCAL_COCO)\n",
    "len(coco_pairs), coco_pairs[0][1][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53650f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiocaps_pairs(n=5000, sr=16000, local_root=None):\n",
    "    out = []\n",
    "    try:\n",
    "        ds = load_dataset(\"audiocaps\", \"balanced_train\", split=\"validation\")\n",
    "        # Some configs differ; fall back if needed\n",
    "    except:\n",
    "        ds = load_dataset(\"audiocaps\", split=\"validation\")\n",
    "\n",
    "    try:\n",
    "        for ex in ds.select(range(min(n, len(ds)))):\n",
    "            # ex has \"audio\": {'array', 'sampling_rate'}, 'caption'\n",
    "            au = ex[\"audio\"]\n",
    "            wav = torch.tensor(au[\"array\"], dtype=torch.float32)\n",
    "            wav_sr = au[\"sampling_rate\"]\n",
    "            if wav_sr != sr:\n",
    "                wav = torchaudio.functional.resample(wav, wav_sr, sr)\n",
    "            out.append((wav.numpy(), sr, ex[\"caption\"]))\n",
    "    except Exception as e:\n",
    "        if not local_root:\n",
    "            raise RuntimeError(f\"HF audiocaps failed and no local_root provided: {e}\")\n",
    "        # Expect local_root with wav files + jsonl {\"wav_path\":\"...\", \"caption\":\"...\"}\n",
    "        import json, glob, soundfile as sf\n",
    "        jfiles = glob.glob(os.path.join(local_root, \"*.jsonl\"))\n",
    "        assert jfiles, \"Provide a jsonl with wav_path, caption\"\n",
    "        with open(jfiles[0], \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                obj = json.loads(line)\n",
    "                p = obj[\"wav_path\"]; cap = obj[\"caption\"]\n",
    "                wav, wav_sr = torchaudio.load(os.path.join(local_root, p))\n",
    "                wav = torchaudio.functional.resample(wav.squeeze(0), wav_sr, sr)\n",
    "                out.append((wav.numpy(), sr, cap))\n",
    "                if len(out)>=n: break\n",
    "    return out[:n]\n",
    "\n",
    "audio_pairs = load_audiocaps_pairs(cfg.N_AUDIOCAPS, cfg.AUDIO_SR, LOCAL_AUDIOCAPS)\n",
    "len(audio_pairs), audio_pairs[0][2][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_name = \"openai/clip-vit-base-patch16\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_name).to(device).eval()\n",
    "clip_proc  = CLIPProcessor.from_pretrained(clip_name)\n",
    "\n",
    "# --- build arrays ---\n",
    "images = [im for im,_ in coco_pairs]\n",
    "texts  = [cap for _,cap in coco_pairs]\n",
    "N = len(images)\n",
    "\n",
    "# --- embed images ---\n",
    "img_emb = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, N, cfg.BATCH_CLIP), desc=\"CLIP image embed\"):\n",
    "        batch = images[i:i+cfg.BATCH_CLIP]\n",
    "        inputs = clip_proc(images=batch, return_tensors=\"pt\").to(device)\n",
    "        z = clip_model.get_image_features(**inputs)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        img_emb.append(z.cpu())\n",
    "img_emb = torch.cat(img_emb, dim=0).numpy()\n",
    "\n",
    "# --- embed texts ---\n",
    "txt_emb = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, N, cfg.BATCH_CLIP), desc=\"CLIP text embed\"):\n",
    "        batch = texts[i:i+cfg.BATCH_CLIP]\n",
    "        inputs = clip_proc(text=batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        z = clip_model.get_text_features(**inputs)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        txt_emb.append(z.cpu())\n",
    "txt_emb = torch.cat(txt_emb, dim=0).numpy()\n",
    "\n",
    "# --- retrieval image index w/ FAISS ---\n",
    "d = img_emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(img_emb)  # already normalized = use inner product == cosine\n",
    "sims_t2i = []\n",
    "bs = 256\n",
    "for i in tqdm(range(0, N, bs), desc=\"t->i sims\"):\n",
    "    q = txt_emb[i:i+bs]\n",
    "    D, I = index.search(q, k=N)  # we only need topK later, but full is ok for metrics\n",
    "    # store only top 100 to save memory\n",
    "    sims_t2i.append((D[:, :100], I[:, :100]))\n",
    "# Flatten into a dense score matrix (for metrics); we’ll just recompute when needed:\n",
    "score_t2i = cosine_sim(txt_emb, img_emb)\n",
    "score_i2t = cosine_sim(img_emb, txt_emb)\n",
    "\n",
    "gt = list(range(N))  # aligned pairs by index\n",
    "\n",
    "m_t2i = recall_at_k(score_t2i, gt); m_t2i[\"mAP\"] = mean_average_precision(score_t2i, gt)\n",
    "m_i2t = recall_at_k(score_i2t, gt); m_i2t[\"mAP\"] = mean_average_precision(score_i2t, gt)\n",
    "\n",
    "print(\"E1.1 — CLIP t->i:\", {k: round(v,4) for k,v in m_t2i.items()})\n",
    "print(\"E1.1 — CLIP i->t:\", {k: round(v,4) for k,v in m_i2t.items()})\n",
    "\n",
    "# Cosine hist: positives vs negatives (sampled)\n",
    "pos_sims = np.array([score_t2i[i, i] for i in range(N)])\n",
    "neg_idx = np.random.randint(0, N, size=(N,))\n",
    "neg_sims = score_t2i[np.arange(N), neg_idx]\n",
    "plot_cos_hist(pos_sims, neg_sims, \"CLIP t->i cosine distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Whisper encoder (no decoder) ---\n",
    "whisper_name = \"openai/whisper-base\"\n",
    "whisper = WhisperModel.from_pretrained(whisper_name).to(device).eval()\n",
    "whisper_proc = WhisperProcessor.from_pretrained(whisper_name)\n",
    "target_len_s = 10.0  # trim/pad to ~10s for consistency\n",
    "\n",
    "def whisper_embed_batch(wavs: List[np.ndarray], sr: int):\n",
    "    # pad/trim and log-mel inside processor\n",
    "    inputs = whisper_proc(wavs, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    input_feats = inputs.input_features.to(device)  # [B, 80, T]\n",
    "    with torch.no_grad():\n",
    "        z = whisper.encoder(input_feats).last_hidden_state  # [B, T', C]\n",
    "        z = z.mean(dim=1)  # simple mean-pool\n",
    "        z = F.normalize(z, dim=-1)\n",
    "    return z.cpu().numpy()\n",
    "\n",
    "# --- MiniLM text encoder ---\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "audios = [wav for (wav, sr, cap) in audio_pairs]\n",
    "a_texts = [cap for (wav, sr, cap) in audio_pairs]\n",
    "M = len(audios)\n",
    "\n",
    "# --- audio embeds ---\n",
    "a_emb = []\n",
    "for i in tqdm(range(0, M, cfg.BATCH_AUDIO), desc=\"Whisper audio embed\"):\n",
    "    batch = audios[i:i+cfg.BATCH_AUDIO]\n",
    "    a_emb.append(whisper_embed_batch(batch, cfg.AUDIO_SR))\n",
    "a_emb = np.concatenate(a_emb, axis=0)\n",
    "\n",
    "# --- text embeds ---\n",
    "t_emb = []\n",
    "for i in tqdm(range(0, M, cfg.BATCH_TEXT), desc=\"MiniLM text embed\"):\n",
    "    batch = a_texts[i:i+cfg.BATCH_TEXT]\n",
    "    z = minilm.encode(batch, batch_size=cfg.BATCH_TEXT, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n",
    "    t_emb.append(z)\n",
    "t_emb = np.concatenate(t_emb, axis=0)\n",
    "\n",
    "# --- retrieval ---\n",
    "score_a2t = cosine_sim(a_emb, t_emb)\n",
    "score_t2a = cosine_sim(t_emb, a_emb)\n",
    "gt_audio = list(range(M))\n",
    "\n",
    "m_a2t = recall_at_k(score_a2t, gt_audio); m_a2t[\"mAP\"] = mean_average_precision(score_a2t, gt_audio)\n",
    "m_t2a = recall_at_k(score_t2a, gt_audio); m_t2a[\"mAP\"] = mean_average_precision(score_t2a, gt_audio)\n",
    "print(\"E1.2 — audio->text:\", {k: round(v,4) for k,v in m_a2t.items()})\n",
    "print(\"E1.2 — text->audio:\", {k: round(v,4) for k,v in m_t2a.items()})\n",
    "\n",
    "# Cosine hist example (a->t)\n",
    "pos_sims = np.array([score_a2t[i, i] for i in range(M)])\n",
    "neg_idx = np.random.randint(0, M, size=(M,))\n",
    "neg_sims = score_a2t[np.arange(M), neg_idx]\n",
    "plot_cos_hist(pos_sims, neg_sims, \"Whisper+MiniLM a->t cosine distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = min(700, N, M)  # per-modality count for the plot\n",
    "# Use:\n",
    "# - CLIP image embeddings (img_emb)\n",
    "# - CLIP text embeddings for COCO (txt_emb)\n",
    "# - Whisper audio embeddings (a_emb)\n",
    "X = np.concatenate([img_emb[:K], txt_emb[:K], a_emb[:K]], axis=0)\n",
    "labels = ([\"image\"]*K) + ([\"text\"]*K) + ([\"audio\"]*K)\n",
    "\n",
    "# PCA -> UMAP (faster & cleaner)\n",
    "pca = PCA(n_components=50, random_state=42).fit_transform(X)\n",
    "um = umap.UMAP(n_neighbors=30, min_dist=0.1, random_state=42).fit_transform(pca)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "for mod, c in [(\"image\", 0), (\"text\", 1), (\"audio\", 2)]:\n",
    "    idx = [i for i,l in enumerate(labels) if l==mod]\n",
    "    plt.scatter(um[idx,0], um[idx,1], s=8, label=mod, alpha=0.7)\n",
    "plt.title(\"E1.4 — 2D projection (UMAP on PCA)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "py311_main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
