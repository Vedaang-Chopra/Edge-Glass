{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f8952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-10-13 11:22:59.745395: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-13 11:23:06.092460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-13 11:23:27.405891: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d244ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Repro & device\n",
    "# -----------------------\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af880844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    out_dir: str = \"./phase1_artifacts\"\n",
    "    # Data slice to keep runtime manageable. Increase for stronger curves.\n",
    "    max_train: int = 6000\n",
    "    max_val: int = 2000\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-3\n",
    "    temp: float = 0.07\n",
    "    # Target text embedding dim\n",
    "    dim_t: int = 384  # MiniLM all-MiniLM-L6-v2 returns 384-d by default\n",
    "    # Encoders\n",
    "    clip_name: str = \"openai/clip-vit-base-patch16\"\n",
    "    whisper_name: str = \"openai/whisper-small\"\n",
    "    sent_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # SpokenCOCO split names\n",
    "    dataset_name: str = \"mteb/SpeechCoco\"  # SpokenCOCO on HF\n",
    "    # mixed precision for faster embedding extraction\n",
    "    amp: bool = True\n",
    "\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b77da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpokenCOCO subset (10%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67785f2bac54ae38a598c8f45fa6248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f892db3b9fa4951afbcf758a0c6398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24506c3620c9428eb80d1278d770b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b4f905c1e645998387133f86490dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def l2(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    return x / (x.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "# -----------------------\n",
    "# Load dataset (SpokenCOCO)\n",
    "# Fields: id, image_id, audio, image, text, ...\n",
    "# Splits: train / validation\n",
    "# -----------------------\n",
    "print(\"Loading SpokenCOCO subset (10%)...\")\n",
    "ds = load_dataset(\"mteb/SpeechCoco\",streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625121a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500 Val size: 200\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from copy import deepcopy\n",
    "\n",
    "class StreamHead:\n",
    "    \"\"\"\n",
    "    Provides __iter__ and a constant __len__=n without materializing to disk.\n",
    "    `make_stream` must be a callable that returns a *fresh* HF stream each time.\n",
    "    \"\"\"\n",
    "    def __init__(self, make_stream, n):\n",
    "        self.make_stream = make_stream\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        # new iterator each time so you can re-iterate in multiple epochs\n",
    "        stream = self.make_stream()\n",
    "        return islice(stream, self.n)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n  # pretend length for code paths that call len()\n",
    "\n",
    "# Usage:\n",
    "def make_train_stream():\n",
    "    return load_dataset(\"mteb/SpeechCoco\", streaming=True, split=\"train\").shuffle(seed=42, buffer_size=5000)\n",
    "\n",
    "def make_val_stream():\n",
    "    return load_dataset(\"mteb/SpeechCoco\", streaming=True, split=\"validation\")\n",
    "\n",
    "train_raw = StreamHead(make_train_stream, n=500)\n",
    "val_raw   = StreamHead(make_val_stream,   n=200)\n",
    "print(\"Train size:\", len(train_raw), \"Val size:\", len(val_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f718c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb7cb70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'StreamHead' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'StreamHead' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def take_first_n(split_ds, n):\n",
    "# #     return split_ds.select(range(min(n, len(split_ds))))\n",
    "\n",
    "# # train_raw = take_first_n(ds[\"train\"], CFG.max_train)\n",
    "# # val_raw   = take_first_n(ds[\"validation\"], CFG.max_val)\n",
    "\n",
    "# # print(\"Train size:\", len(train_raw), \"Val size:\", len(val_raw))\n",
    "\n",
    "# # Take only small heads of each split (doesn't store to disk)\n",
    "# train_stream = ds[\"train\"].take(500)         # first 500 samples\n",
    "# val_stream   = ds[\"validation\"].take(200)    # first 200 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frozen encoders ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Build frozen encoders\n",
    "# -----------------------\n",
    "print(\"Loading frozen encoders ...\")\n",
    "clip_model = CLIPModel.from_pretrained(CFG.clip_name).eval().to(device)\n",
    "clip_proc  = CLIPProcessor.from_pretrained(CFG.clip_name)\n",
    "\n",
    "whisper_model = WhisperModel.from_pretrained(CFG.whisper_name).eval().to(device)\n",
    "whisper_proc  = WhisperProcessor.from_pretrained(CFG.whisper_name)\n",
    "\n",
    "sent_model = SentenceTransformer(CFG.sent_name, device=str(device))\n",
    "\n",
    "for p in clip_model.parameters():    p.requires_grad = False\n",
    "for p in whisper_model.parameters(): p.requires_grad = False\n",
    "# SentenceTransformer is already in eval/frozen mode for encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Embedding functions\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def embed_image(pil_images: List):\n",
    "    # Returns [N, D_v]; CLIP returns 512-d by default for ViT-B/16\n",
    "    inputs = clip_proc(images=pil_images, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.float16) if (CFG.amp and device.type==\"cuda\") else torch.no_grad():\n",
    "        img_feats = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "    return l2(img_feats.float()).cpu()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_audio(list_of_np_wavs: List, sampling_rate: int):\n",
    "    # Whisper expects 16k mono; datasets usually provide .array & .sampling_rate\n",
    "    # We'll resample via processor if needed.\n",
    "    # Strategy: run encoder, mean-pool last_hidden_state.\n",
    "    batch_inputs = whisper_proc(\n",
    "        audio=list_of_np_wavs, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_features = batch_inputs.input_features.to(device)\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.float16) if (CFG.amp and device.type==\"cuda\") else torch.no_grad():\n",
    "        enc_out = whisper_model.encoder(input_features)\n",
    "    hidden = enc_out.last_hidden_state  # [B, T, D_a]\n",
    "    a_emb = hidden.mean(dim=1)          # [B, D_a]\n",
    "    return l2(a_emb.float()).cpu()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_text(list_of_strs: List):\n",
    "    # SentenceTransformer handles batching internally; ensure normalize_embeddings=True\n",
    "    t = sent_model.encode(list_of_strs, batch_size=64, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    return t.cpu().float()  # [N, dim_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d48f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding TRAIN ... (this may take a few minutes the first time)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad5bd36ddf64e6ca2090531b4c8b8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4228fcc5291345af92ffc9abfbc1c80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed9843b41349ea9a30eef47d88ddfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b1d508a7b5435b920c95970f6a0c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only a single or a list of entries is supported but got type=<class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m V, A, T\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPre-encoding TRAIN ... (this may take a few minutes the first time)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m V_tr, A_tr, T_tr = \u001b[43mpreencode_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPre-encoding VAL ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m V_va, A_va, T_va = preencode_split(val_raw)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mpreencode_split\u001b[39m\u001b[34m(split, max_batch_images, max_batch_audio, max_batch_text)\u001b[39m\n\u001b[32m     12\u001b[39m     idxs_img.append(i)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pil_buf) == max_batch_images \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(split)-\u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         Vs.append(\u001b[43membed_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_buf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m         pil_buf.clear()\n\u001b[32m     16\u001b[39m V = torch.cat(Vs, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36membed_image\u001b[39m\u001b[34m(pil_images)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_image\u001b[39m(pil_images: List):\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Returns [N, D_v]; CLIP returns 512-d by default for ViT-B/16\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     inputs = \u001b[43mclip_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     pixel_values = inputs[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=device.type, dtype=torch.float16) \u001b[38;5;28;01mif\u001b[39;00m (CFG.amp \u001b[38;5;129;01mand\u001b[39;00m device.type==\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/processing_utils.py:594\u001b[39m, in \u001b[36mProcessorMixin.__call__\u001b[39m\u001b[34m(self, images, text, videos, audio, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m     input_data, input_kwargs = attribute_to_kwargs[attribute_name]\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attribute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m         attribute_output = \u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_kwargs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m         outputs.update(attribute_output)\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:288\u001b[39m, in \u001b[36mCLIPImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m do_convert_rgb = do_convert_rgb \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_convert_rgb\n\u001b[32m    286\u001b[39m validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=\u001b[38;5;28mself\u001b[39m._valid_processor_keys)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m images = make_flat_list_of_images(images)\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/image_processing_base.py:530\u001b[39m, in \u001b[36mImageProcessingMixin.fetch_images\u001b[39m\u001b[34m(self, image_url_or_urls)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03mConvert a single or a list of urls into the corresponding `PIL.Image` objects.\u001b[39;00m\n\u001b[32m    525\u001b[39m \n\u001b[32m    526\u001b[39m \u001b[33;03mIf a single url is passed, the return value will be a single object. If a list is passed a list of objects is\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03mreturned.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_url_or_urls\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_image(image_url_or_urls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/image_processing_base.py:530\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03mConvert a single or a list of urls into the corresponding `PIL.Image` objects.\u001b[39;00m\n\u001b[32m    525\u001b[39m \n\u001b[32m    526\u001b[39m \u001b[33;03mIf a single url is passed, the return value will be a single object. If a list is passed a list of objects is\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03mreturned.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m image_url_or_urls]\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_image(image_url_or_urls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/main_python_env/main_python_venv/lib/python3.11/site-packages/transformers/image_processing_base.py:536\u001b[39m, in \u001b[36mImageProcessingMixin.fetch_images\u001b[39m\u001b[34m(self, image_url_or_urls)\u001b[39m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_url_or_urls\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33monly a single or a list of entries is supported but got type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image_url_or_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: only a single or a list of entries is supported but got type=<class 'dict'>"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Pre-encode a slice of the dataset for speed\n",
    "# -----------------------\n",
    "def preencode_split(split, max_batch_images=32, max_batch_audio=16, max_batch_text=128):\n",
    "    Vs, As, Ts = [], [], []\n",
    "    # image/audio/text are aligned per row; keep the order for retrieval GT.\n",
    "    # 1) Images\n",
    "    pil_buf = []\n",
    "    idxs_img = []\n",
    "    for i, row in enumerate(split):\n",
    "        pil_buf.append(row[\"image\"])\n",
    "        idxs_img.append(i)\n",
    "        if len(pil_buf) == max_batch_images or i == len(split)-1:\n",
    "            Vs.append(embed_image(pil_buf))\n",
    "            pil_buf.clear()\n",
    "    V = torch.cat(Vs, dim=0)\n",
    "\n",
    "    # 2) Audio (datasets audio object provides \"array\" and \"sampling_rate\")\n",
    "    wav_buf, sr_buf = [], []\n",
    "    for i, row in enumerate(split):\n",
    "        aud = row[\"audio\"]\n",
    "        wav_buf.append(aud[\"array\"])\n",
    "        sr_buf.append(aud[\"sampling_rate\"])\n",
    "        if len(wav_buf) == max_batch_audio or i == len(split)-1:\n",
    "            # resample to first SR in batch if differs? WhisperProcessor handles list with distinct sampling_rate as separate calls\n",
    "            # For simplicity, call per-item if SRs differ in the batch\n",
    "            if len(set(sr_buf)) == 1:\n",
    "                As.append(embed_audio(wav_buf, sr_buf[0]))\n",
    "            else:\n",
    "                for w, s in zip(wav_buf, sr_buf):\n",
    "                    As.append(embed_audio([w], s))\n",
    "                As = [torch.cat(As, dim=0)]\n",
    "            wav_buf, sr_buf = [], []\n",
    "    A = torch.cat(As, dim=0)\n",
    "\n",
    "    # 3) Text\n",
    "    texts = [row[\"text\"] for row in split]\n",
    "    # chunk long lists to avoid OOM inside sentence-transformers\n",
    "    Tparts = []\n",
    "    for i in range(0, len(texts), max_batch_text):\n",
    "        Tparts.append(embed_text(texts[i:i+max_batch_text]))\n",
    "    T = torch.cat(Tparts, dim=0)\n",
    "\n",
    "    assert V.shape[0] == A.shape[0] == T.shape[0] == len(split)\n",
    "    return V, A, T\n",
    "\n",
    "print(\"Pre-encoding TRAIN ... (this may take a few minutes the first time)\")\n",
    "V_tr, A_tr, T_tr = preencode_split(train_raw)\n",
    "print(\"Pre-encoding VAL ...\")\n",
    "V_va, A_va, T_va = preencode_split(val_raw)\n",
    "\n",
    "dim_v = V_tr.shape[1]\n",
    "dim_a = A_tr.shape[1]\n",
    "print(f\"Emb dims -> vision: {dim_v}, audio: {dim_a}, text: {T_tr.shape[1]} (target={CFG.dim_t})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Torch datasets over the embeddings\n",
    "# -----------------------\n",
    "class TripletEmbedDataset(Dataset):\n",
    "    def __init__(self, V, A, T):\n",
    "        self.V = V; self.A = A; self.T = T\n",
    "        assert len(V) == len(A) == len(T)\n",
    "    def __len__(self): return len(self.V)\n",
    "    def __getitem__(self, i):\n",
    "        return self.V[i], self.A[i], self.T[i], i  # use index as \"class\" for retrieval GT\n",
    "\n",
    "train_ds = TripletEmbedDataset(V_tr, A_tr, T_tr)\n",
    "val_ds   = TripletEmbedDataset(V_va, A_va, T_va)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False,  num_workers=CFG.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Linear adapters + InfoNCE\n",
    "# -----------------------\n",
    "class LinearAdapter(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(din),\n",
    "            nn.Linear(din, dout, bias=False),\n",
    "            nn.LayerNorm(dout)\n",
    "        )\n",
    "    def forward(self, x): return l2(self.net(x))\n",
    "\n",
    "class PairwiseAligner(nn.Module):\n",
    "    def __init__(self, dim_v, dim_a, dim_t):\n",
    "        super().__init__()\n",
    "        self.v = LinearAdapter(dim_v, dim_t)\n",
    "        self.a = LinearAdapter(dim_a, dim_t)\n",
    "    def forward(self, v, a):\n",
    "        return self.v(v), self.a(a)\n",
    "\n",
    "def info_nce_symmetric(z1: torch.Tensor, z2: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    sim = (z1 @ z2.t()) / temp\n",
    "    y = torch.arange(z1.size(0), device=z1.device)\n",
    "    return 0.5*(F.cross_entropy(sim, y) + F.cross_entropy(sim.t(), y))\n",
    "\n",
    "model = PairwiseAligner(dim_v, dim_a, CFG.dim_t).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Train (v↔t and a↔t only)\n",
    "# -----------------------\n",
    "for epoch in range(1, CFG.epochs+1):\n",
    "    model.train(); losses=[]\n",
    "    for Vb, Ab, Tb, _ in train_loader:\n",
    "        Vb, Ab, Tb = Vb.to(device), Ab.to(device), Tb.to(device)\n",
    "        Vt, At = model(Vb, Ab)\n",
    "        loss = info_nce_symmetric(Vt, Tb, CFG.temp) + info_nce_symmetric(At, Tb, CFG.temp)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Eval utils\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def collect_embeds(loader, model):\n",
    "    model.eval()\n",
    "    Vt_all, At_all, T_all = [], [], []\n",
    "    for Vb, Ab, Tb, _ in loader:\n",
    "        Vb, Ab, Tb = Vb.to(device), Ab.to(device), Tb.to(device)\n",
    "        Vt, At = model(Vb, Ab)\n",
    "        Vt_all.append(Vt.cpu()); At_all.append(At.cpu()); T_all.append(Tb.cpu())\n",
    "    return torch.cat(Vt_all), torch.cat(At_all), torch.cat(T_all)\n",
    "\n",
    "def cosine_matrix(X, Y): return X @ Y.t()\n",
    "\n",
    "def recall_at_k(sim: torch.Tensor, k: int) -> float:\n",
    "    topk = torch.topk(sim, k, dim=1).indices\n",
    "    gt = torch.arange(sim.size(0)).unsqueeze(1)\n",
    "    return float((topk == gt).any(dim=1).float().mean() * 100.0)\n",
    "\n",
    "def pos_neg(sim: torch.Tensor):\n",
    "    pos = sim.diag().numpy()\n",
    "    neg = sim[~torch.eye(sim.size(0), dtype=torch.bool)].numpy()\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd3d3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate on VAL\n",
    "# -----------------------\n",
    "Vt, At, Tt = collect_embeds(val_loader, model)\n",
    "\n",
    "sim_ti = cosine_matrix(Tt, Vt)  # text->image\n",
    "sim_ta = cosine_matrix(Tt, At)  # text->audio\n",
    "sim_ia = cosine_matrix(Vt, At)  # image->audio (NO direct loss)  <-- should be weak\n",
    "\n",
    "metrics = []\n",
    "def add(name, sim):\n",
    "    metrics.append({\n",
    "        \"Pair\": name,\n",
    "        \"R@1\":  round(recall_at_k(sim, 1), 2),\n",
    "        \"R@5\":  round(recall_at_k(sim, 5), 2),\n",
    "        \"R@10\": round(recall_at_k(sim, 10), 2),\n",
    "        \"MeanCos(PosDiag)\": round(sim.diag().mean().item(), 3)\n",
    "    })\n",
    "add(\"t→i\", sim_ti); add(\"i→t\", sim_ti.t())\n",
    "add(\"t→a\", sim_ta); add(\"a→t\", sim_ta.t())\n",
    "add(\"i→a\", sim_ia); add(\"a→i\", sim_ia.t())\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "display(df)\n",
    "\n",
    "csv_path = os.path.join(CFG.out_dir, \"B0_pairwise_baseline_metrics.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved:\", csv_path)\n",
    "\n",
    "# -----------------------\n",
    "# Histograms\n",
    "# -----------------------\n",
    "def plot_hist(pos, neg, title, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(pos, bins=50, alpha=0.6, label=\"Positives\")\n",
    "    plt.hist(neg, bins=50, alpha=0.6, label=\"Negatives\")\n",
    "    plt.xlabel(\"Cosine similarity\"); plt.ylabel(\"Count\")\n",
    "    plt.title(title); plt.legend(); plt.tight_layout()\n",
    "    out = os.path.join(CFG.out_dir, fname)\n",
    "    plt.savefig(out, dpi=150); plt.show()\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "p_ti, n_ti = pos_neg(sim_ti)\n",
    "p_ta, n_ta = pos_neg(sim_ta)\n",
    "p_ia, n_ia = pos_neg(sim_ia)\n",
    "\n",
    "plot_hist(p_ti, n_ti, \"t↔i Cosine — Pos vs Neg\", \"hist_t_i.png\")\n",
    "plot_hist(p_ta, n_ta, \"t↔a Cosine — Pos vs Neg\", \"hist_t_a.png\")\n",
    "plot_hist(p_ia, n_ia, \"i↔a Cosine — Pos vs Neg (no direct loss)\", \"hist_i_a.png\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7547744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "py311_main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
