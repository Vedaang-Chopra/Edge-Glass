{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17864738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment and run:\n",
    "! uv pip install -q torch torchvision timm sentence-transformers datasets pillow tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad3b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, time, random, pathlib\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data/output\n",
    "PROJECT_ROOT = \"freeze_align_poc\"\n",
    "DATA_ROOT    = f\"{PROJECT_ROOT}/data/flickr30k\"\n",
    "IMG_DIR      = f\"{DATA_ROOT}/images\"\n",
    "CAP_FILE     = f\"{DATA_ROOT}/captions.json\"\n",
    "CKPT_PATH    = f\"{PROJECT_ROOT}/checkpoints/pair_img_txt.pt\"\n",
    "\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_ROOT}/checkpoints\", exist_ok=True)\n",
    "\n",
    "# Flickr30k subset size for quick POC\n",
    "NUM_PAIRS    = 10_000   # try 30_000 if you want a bigger run\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE   = 64\n",
    "ACCUM_STEPS  = 1        # set >1 for larger effective batch\n",
    "MAX_STEPS    = 2000     # increase for better alignment (e.g., 5k)\n",
    "LR           = 1e-3\n",
    "WD           = 0.01\n",
    "WARMUP       = 100\n",
    "LOG_EVERY    = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a split from HF and materializes to local images + captions.json\n",
    "ds = load_dataset(\"lmms-lab/flickr30k\", split=f\"test[:{NUM_PAIRS}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b0983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'caption', 'sentids', 'img_id', 'filename'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "441236cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving images: 100%|██████████| 10000/10000 [00:40<00:00, 249.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 pairs to freeze_align_poc/data/flickr30k\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for i, row in tqdm(enumerate(ds), total=len(ds), desc=\"Saving images\"):\n",
    "    image: Image.Image = row[\"image\"]\n",
    "    caption = row[\"caption\"]\n",
    "    img_path = f\"{IMG_DIR}/{i:06d}.jpg\"\n",
    "    image.save(img_path, quality=90)\n",
    "    records.append({\"image\": f\"images/{i:06d}.jpg\", \"caption\": caption})\n",
    "\n",
    "with open(CAP_FILE, \"w\") as f:\n",
    "    json.dump(records, f)\n",
    "print(f\"Saved {len(records)} pairs to {DATA_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff1cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " torch.Size([64, 3, 224, 224]),\n",
       " ['Two little girls are sitting on a yellow rubber ball toy .',\n",
       "  'The two girls are playing on a yellow sit-and-bounce .',\n",
       "  'Two children sitting atop a large yellow bounce toy .',\n",
       "  'Two girls bounce on a large yellow ball indoors .',\n",
       "  'Two smiling girls sit on a yellow bouncy ball .'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImgTxtDataset(Dataset):\n",
    "    def __init__(self, root, captions_file, transform):\n",
    "        self.root = pathlib.Path(root)\n",
    "        with open(captions_file) as f:\n",
    "            self.items = json.load(f)\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        rec = self.items[i]\n",
    "        img = Image.open(self.root / rec[\"image\"]).convert(\"RGB\")\n",
    "        return self.transform(img), rec[\"caption\"]\n",
    "\n",
    "img_tf = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=Image.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "ds = ImgTxtDataset(DATA_ROOT, CAP_FILE, img_tf)\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, caps = zip(*batch)\n",
    "    return torch.stack(imgs), list(caps)\n",
    "\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=4, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "len(ds), next(iter(loader))[0].shape, next(iter(loader))[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df76e785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timm version: 1.0.21\n",
      "available dinov2 models: ['vit_base_patch14_dinov2', 'vit_base_patch14_reg4_dinov2', 'vit_giant_patch14_dinov2', 'vit_giant_patch14_reg4_dinov2', 'vit_large_patch14_dinov2', 'vit_large_patch14_reg4_dinov2', 'vit_small_patch14_dinov2', 'vit_small_patch14_reg4_dinov2']\n",
      "Using vision model: vit_base_patch16_224\n"
     ]
    }
   ],
   "source": [
    "# Fix for vision encoder selection (DINOv2 via timm)\n",
    "import timm, torch\n",
    "\n",
    "print(\"timm version:\", timm.__version__)\n",
    "print(\"available dinov2 models:\", timm.list_models(\"*dinov2*\"))\n",
    "\n",
    "# Prefer large → base → small, depending on what's installed\n",
    "CANDIDATES = [\n",
    "    \"vit_large_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch14_dinov2.lvd142m\",\n",
    "    \"vit_small_patch14_dinov2.lvd142m\",\n",
    "]\n",
    "for name in CANDIDATES:\n",
    "    if name in timm.list_models(\"*dinov2*\"):\n",
    "        VISION_MODEL_NAME = name\n",
    "        break\n",
    "else:\n",
    "    # last-resort fallback if your timm build lacks dinov2\n",
    "    VISION_MODEL_NAME = \"vit_base_patch16_224\"\n",
    "\n",
    "print(\"Using vision model:\", VISION_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813423ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision encoder: DINOv2-L, pooled features\n",
    "vision_model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "vision_model.eval().to(device)\n",
    "for p in vision_model.parameters(): p.requires_grad = False\n",
    "vision_dim = vision_model.num_features\n",
    "\n",
    "# Text encoder: SentenceTransformer all-roberta-large-v1\n",
    "text_model = SentenceTransformer(\"sentence-transformers/all-roberta-large-v1\", device=device)\n",
    "text_model.eval()\n",
    "for p in text_model.parameters(): p.requires_grad = False\n",
    "text_dim = text_model.get_sentence_embedding_dimension()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91c3f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims: {'vision_dim': 768, 'text_dim': 1024}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "If dims differ, add a linear mapper to unify.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDims:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mdict\u001b[39m(vision_dim=vision_dim, text_dim=text_dim))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m vision_dim == text_dim == \u001b[32m1024\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIf dims differ, add a linear mapper to unify.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: If dims differ, add a linear mapper to unify."
     ]
    }
   ],
   "source": [
    "print(\"Dims:\", dict(vision_dim=vision_dim, text_dim=text_dim))\n",
    "assert vision_dim == text_dim == 1024, \"If dims differ, add a linear mapper to unify.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8005daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims: {'vision_dim': 768, 'text_dim': 1024}\n",
      "TARGET_DIM: 1024\n"
     ]
    }
   ],
   "source": [
    " # Replace your \"Dims + assert\" cell with this\n",
    "print(\"Dims:\", dict(vision_dim=vision_dim, text_dim=text_dim))\n",
    "\n",
    "# Pick a shared target dim automatically (no information loss on the larger side)\n",
    "TARGET_DIM = max(vision_dim, text_dim)     # e.g., 1024 if text=1024 and vision=768\n",
    "print(\"TARGET_DIM:\", TARGET_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (M): 17.576961\n"
     ]
    }
   ],
   "source": [
    "# Drop-in replacement for the projectors + wrapper cell\n",
    "\n",
    "def l2_normalize(x, dim=-1, eps=1e-8):\n",
    "    return x / (x.norm(dim=dim, keepdim=True) + eps)\n",
    "\n",
    "def maybe_linear(in_dim, out_dim):\n",
    "    if in_dim == out_dim:\n",
    "        return nn.Identity()\n",
    "    return nn.Linear(in_dim, out_dim)\n",
    "\n",
    "class TokenProjector(nn.Module):\n",
    "    def __init__(self, dim, hidden=None):\n",
    "        super().__init__()\n",
    "        hidden = hidden or max(1024, dim * 2)\n",
    "        self.net = nn.Sequential(nn.Linear(dim, hidden), nn.ReLU(), nn.Linear(hidden, dim))\n",
    "    def forward(self, x):  # [B, D]\n",
    "        return x + self.net(x)\n",
    "\n",
    "class GlobalProjector(nn.Module):\n",
    "    def __init__(self, dim, hidden=None):\n",
    "        super().__init__()\n",
    "        hidden = hidden or max(1024, dim * 2)\n",
    "        self.net = nn.Sequential(nn.Linear(dim, hidden), nn.ReLU(), nn.Linear(hidden, dim))\n",
    "    def forward(self, x):  # [B, D]\n",
    "        return self.net(x)\n",
    "\n",
    "class FreezeAlignIT(nn.Module):\n",
    "    def __init__(self, vision_backbone, text_backbone, vision_dim, text_dim, target_dim):\n",
    "        super().__init__()\n",
    "        self.vision = vision_backbone\n",
    "        self.text   = text_backbone\n",
    "\n",
    "        # NEW: light mappers to a shared TARGET_DIM\n",
    "        self.v_map = maybe_linear(vision_dim, target_dim)\n",
    "        self.t_map = maybe_linear(text_dim,   target_dim)\n",
    "\n",
    "        # Projectors operate in TARGET_DIM\n",
    "        self.v_token  = TokenProjector(target_dim)\n",
    "        self.v_global = GlobalProjector(target_dim)\n",
    "        self.t_token  = TokenProjector(target_dim)\n",
    "        self.t_global = GlobalProjector(target_dim)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(0.07))\n",
    "        self.target_dim  = target_dim\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _vision(self, images):           # [B, Dv]\n",
    "        return self.vision(images)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _text(self, captions, device):   # [B, Dt]\n",
    "        return self.text.encode(captions, convert_to_tensor=True, device=device)\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        v = self._vision(images)                 # [B, Dv]\n",
    "        v = self.v_map(v)                        # [B, TARGET_DIM]\n",
    "        v = self.v_token(v)\n",
    "        v = self.v_global(v)\n",
    "        return l2_normalize(v)\n",
    "\n",
    "    def encode_text(self, captions, device):\n",
    "        t = self._text(captions, device)         # [B, Dt]\n",
    "        t = self.t_map(t)                        # [B, TARGET_DIM]\n",
    "        t = self.t_token(t)\n",
    "        t = self.t_global(t)\n",
    "        return l2_normalize(t)\n",
    "\n",
    "    def forward(self, images, captions, device):\n",
    "        zi = self.encode_image(images)\n",
    "        zt = self.encode_text(captions, device)\n",
    "        return zi, zt, self.temperature.clamp(0.01, 1.0)\n",
    "\n",
    "model = FreezeAlignIT(vision_model, text_model, vision_dim, text_dim, TARGET_DIM).to(device)\n",
    "print(\"Trainable params (M):\", sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_177539/2878756575.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "def clip_loss(img_z, txt_z, temperature):\n",
    "    logits = (img_z @ txt_z.t()) / temperature\n",
    "    labels = torch.arange(len(img_z), device=img_z.device)\n",
    "    return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)) / 2\n",
    "\n",
    "\n",
    "# Recreate optimizer & scheduler after redefining model\n",
    "optim = torch.optim.AdamW(\n",
    "    [p for n,p in model.named_parameters()\n",
    "     if p.requires_grad and not n.startswith(\"vision\") and not n.startswith(\"text\")],  # includes v_map/t_map\n",
    "    lr=LR, weight_decay=WD\n",
    ")\n",
    "\n",
    "class CosineAnnealWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, max_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps; self.max_steps = max_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step <= self.warmup_steps:\n",
    "            s = step / max(1, self.warmup_steps)\n",
    "            return [base * s for base in self.base_lrs]\n",
    "        progress = (step - self.warmup_steps) / max(1, self.max_steps - self.warmup_steps)\n",
    "        factor = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return [base * factor for base in self.base_lrs]\n",
    "\n",
    "sched  = CosineAnnealWarmup(optim, warmup_steps=WARMUP, max_steps=MAX_STEPS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b16d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_dataset(model, dataset, batch=128):\n",
    "    model.eval()\n",
    "    all_img, all_txt = [], []\n",
    "    for i in range(0, len(dataset), batch):\n",
    "        imgs = [dataset[j][0] for j in range(i, min(i+batch, len(dataset)))]\n",
    "        caps = [dataset[j][1] for j in range(i, min(i+batch, len(dataset)))]\n",
    "        imgs = torch.stack(imgs).to(device, non_blocking=True)\n",
    "        zi = model.encode_image(imgs)\n",
    "        zt = model.encode_text(caps, device)\n",
    "        all_img.append(zi.cpu()); all_txt.append(zt.cpu())\n",
    "    return torch.cat(all_img), torch.cat(all_txt)\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_at_k(A, B, ks=(1,5,10)):\n",
    "    sims  = A @ B.t()\n",
    "    ranks = torch.argsort(sims, dim=1, descending=True)\n",
    "    target = torch.arange(A.size(0)).unsqueeze(1)\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        hit = (ranks[:, :k] == target).any(dim=1).float().mean().item()\n",
    "        out[f\"R@{k}\"] = hit\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95bd3d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline  Image→Text: {'R@1': 0.0, 'R@5': 9.999999747378752e-05, 'R@10': 0.0006000000284984708}\n",
      "Baseline  Text→Image: {'R@1': 9.999999747378752e-05, 'R@5': 0.0006000000284984708, 'R@10': 0.00139999995008111}\n"
     ]
    }
   ],
   "source": [
    "img_z0, txt_z0 = encode_dataset(model, ds, batch=256)\n",
    "print(\"Baseline  Image→Text:\", recall_at_k(img_z0, txt_z0))\n",
    "print(\"Baseline  Text→Image:\", recall_at_k(txt_z0, img_z0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af23b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_177539/3083396272.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    50 | loss 1.3210 | T=0.059 | lr=5.10e-04\n",
      "step   100 | loss 1.3711 | T=0.054 | lr=1.00e-03\n",
      "step   150 | loss 1.5962 | T=0.056 | lr=9.98e-04\n",
      "step   200 | loss 1.0407 | T=0.052 | lr=9.93e-04\n",
      "step   250 | loss 0.9373 | T=0.051 | lr=9.84e-04\n",
      "step   300 | loss 0.6604 | T=0.050 | lr=9.73e-04\n",
      "step   350 | loss 0.4601 | T=0.041 | lr=9.58e-04\n",
      "step   400 | loss 0.7422 | T=0.042 | lr=9.39e-04\n",
      "step   450 | loss 0.5671 | T=0.045 | lr=9.18e-04\n",
      "step   500 | loss 0.5670 | T=0.033 | lr=8.94e-04\n",
      "step   550 | loss 0.4150 | T=0.036 | lr=8.67e-04\n",
      "step   600 | loss 0.4277 | T=0.038 | lr=8.38e-04\n",
      "step   650 | loss 0.1910 | T=0.030 | lr=8.06e-04\n",
      "step   700 | loss 0.2739 | T=0.037 | lr=7.73e-04\n",
      "step   750 | loss 0.3360 | T=0.035 | lr=7.37e-04\n",
      "step   800 | loss 0.1929 | T=0.033 | lr=7.00e-04\n",
      "step   850 | loss 0.1935 | T=0.031 | lr=6.62e-04\n",
      "step   900 | loss 0.3501 | T=0.030 | lr=6.22e-04\n",
      "step   950 | loss 0.1497 | T=0.029 | lr=5.81e-04\n",
      "step  1000 | loss 0.1032 | T=0.026 | lr=5.40e-04\n",
      "step  1050 | loss 0.1127 | T=0.029 | lr=4.99e-04\n",
      "step  1100 | loss 0.1332 | T=0.029 | lr=4.58e-04\n",
      "step  1150 | loss 0.0852 | T=0.024 | lr=4.17e-04\n",
      "step  1200 | loss 0.0978 | T=0.026 | lr=3.76e-04\n",
      "step  1250 | loss 0.1118 | T=0.025 | lr=3.37e-04\n",
      "step  1300 | loss 0.0374 | T=0.022 | lr=2.98e-04\n",
      "step  1350 | loss 0.2348 | T=0.020 | lr=2.61e-04\n",
      "step  1400 | loss 0.0413 | T=0.020 | lr=2.26e-04\n",
      "step  1450 | loss 0.0449 | T=0.020 | lr=1.92e-04\n",
      "step  1500 | loss 0.0534 | T=0.019 | lr=1.61e-04\n",
      "step  1550 | loss 0.0975 | T=0.020 | lr=1.32e-04\n",
      "step  1600 | loss 0.0167 | T=0.020 | lr=1.05e-04\n",
      "step  1650 | loss 0.0700 | T=0.018 | lr=8.10e-05\n",
      "step  1700 | loss 0.0832 | T=0.018 | lr=5.99e-05\n",
      "step  1750 | loss 0.0069 | T=0.018 | lr=4.18e-05\n",
      "step  1800 | loss 0.0155 | T=0.017 | lr=2.68e-05\n",
      "step  1850 | loss 0.0137 | T=0.017 | lr=1.51e-05\n",
      "step  1900 | loss 0.0056 | T=0.017 | lr=6.68e-06\n",
      "step  1950 | loss 0.0208 | T=0.017 | lr=1.64e-06\n",
      "step  2000 | loss 0.0440 | T=0.017 | lr=6.83e-10\n",
      "Training done in 155.6s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "step, t0 = 0, time.time()\n",
    "\n",
    "for epoch in range(9999):\n",
    "    for imgs, caps in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            zi, zt, temp = model(imgs, caps, device)\n",
    "            loss = clip_loss(zi, zt, temp) / ACCUM_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optim)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optim); scaler.update()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            sched.step()\n",
    "\n",
    "        step += 1\n",
    "        if step % LOG_EVERY == 0:\n",
    "            print(f\"step {step:5d} | loss {loss.item()*ACCUM_STEPS:.4f} | T={temp.item():.3f} | lr={sched.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "    if step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "print(f\"Training done in {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04f8c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'person'),\n",
       " (1, 'airplane'),\n",
       " (2, 'flower'),\n",
       " (3, 'car'),\n",
       " (4, 'cat'),\n",
       " (5, 'car'),\n",
       " (6, 'person'),\n",
       " (7, 'person'),\n",
       " (8, 'person'),\n",
       " (9, 'person'),\n",
       " (10, 'airplane'),\n",
       " (11, 'car'),\n",
       " (12, 'dog'),\n",
       " (13, 'car'),\n",
       " (14, 'car'),\n",
       " (15, 'cat')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels  = [\"cat\", \"dog\", \"car\", \"airplane\", \"flower\", \"person\"]\n",
    "prompts = [f\"A photo of a {c}.\" for c in labels]\n",
    "\n",
    "@torch.no_grad()\n",
    "def zero_shot_scores(model, images, label_prompts):\n",
    "    txt = model.encode_text(label_prompts, device)      # [C, D]\n",
    "    vi  = model.encode_image(images.to(device))          # [B, D]\n",
    "    sims = vi @ txt.t()\n",
    "    preds = sims.argmax(dim=1)\n",
    "    return preds, sims\n",
    "\n",
    "N = min(16, len(ds))\n",
    "sample_imgs = torch.stack([ds[i][0] for i in range(N)])\n",
    "preds, sims = zero_shot_scores(model, sample_imgs, prompts)\n",
    "[(i, labels[preds[i].item()]) for i in range(N)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40965468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'freeze_align_poc/checkpoints/pair_img_txt.pt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"vision_dim\": vision_dim,\n",
    "    \"text_dim\": text_dim,\n",
    "    \"config\": {\n",
    "        \"LR\": LR, \"WD\": WD, \"MAX_STEPS\": MAX_STEPS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE, \"ACCUM_STEPS\": ACCUM_STEPS\n",
    "    }\n",
    "}, CKPT_PATH)\n",
    "CKPT_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c78256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded Image→Text: {'R@1': 0.5141000151634216, 'R@5': 0.8799999952316284, 'R@10': 0.9595000147819519}\n",
      "Reloaded Text→Image: {'R@1': 0.49570000171661377, 'R@5': 0.8748000264167786, 'R@10': 0.9567999839782715}\n"
     ]
    }
   ],
   "source": [
    "# Sanity reload\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "model.eval()\n",
    "img_z_r, txt_z_r = encode_dataset(model, ds, batch=256)\n",
    "print(\"Reloaded Image→Text:\", recall_at_k(img_z_r, txt_z_r))\n",
    "print(\"Reloaded Text→Image:\", recall_at_k(txt_z_r, img_z_r))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
