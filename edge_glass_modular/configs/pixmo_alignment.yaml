name: pixmo_vision_text_alignment
description: Vision-Text alignment with Pixmo dataset, 4096 dim embeddings, and improved training

dataset:
  name: pixmo_parquet
  batch_size: 64  # Reduced for gradient accumulation (effective batch = 64*4*2 = 512)
  base_batch_size: 64
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  image_size: 336  # Match CLIP ViT-L/14 336px input size
  max_text_length: 512
  text_dropout_prob: 0.05  # Reduced dropout for better alignment
  # Parquet file paths
  train_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_train.parquet
  val_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_val.parquet
  test_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_test.parquet

vision_encoder:
  model_name: openai/clip-vit-large-patch14-336
  projection_dim: 4096  # Top MRL dimension
  freeze: true
  trainable: false
  use_perceiver: false
  use_mrl: true
  mrl_dimensions: [2048, 1024, 512, 256, 128]  # MRL dimensions (4096 is top)
  use_attention_pooling: true  # Learnable attention pooling
  pooling_type: simple  # "simple" or "multihead"

text_encoder:
  model_name: sentence-transformers/all-mpnet-base-v2
  projection_dim: 4096  # Match vision encoder
  freeze: true
  use_mrl: true
  mrl_dimensions: [2048, 1024, 512, 256, 128]

decoder:
  type: qwen
  model_name: Qwen/Qwen2.5-7B-Instruct
  use_lora: true
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  load_in_8bit: false
  load_in_4bit: false

fusion: null  # No fusion for bimodal

losses:
  contrastive: 1.0   # CLIP loss is primary objective for alignment
  mrl: 0.5           # MRL as regularizer, not primary
  sample_single_mrl_dim: true  # Sample one MRL dim per batch

optimization:
  lr: 0.00005  # 5e-5 - lower LR for stable contrastive learning
  learning_rate: 0.00005
  weight_decay: 0.01
  betas: [0.9, 0.95]
  gradient_clip: 1.0  # max_grad_norm = 1.0
  max_grad_norm: 1.0
  grad_accum_steps: 4  # Gradient accumulation for larger effective batch
  fp16: false
  bf16: true
  mixed_precision: bf16
  warmup_ratio: 0.15  # 15% warmup for stability
  warmup_steps: null  # Will be computed from warmup_ratio
  total_steps: null  # Will be computed from epochs
  contrastive_loss_weight: 1.0  # Primary objective
  mrl_loss_weight: 0.5  # Regularizer
  lm_loss_weight: 1.0

trainer:
  epochs: 25  # More epochs for better alignment
  num_epochs: 25
  batch_size: 64
  save_every: 5  # Save checkpoint every 5 epochs
  log_every: 20  # Log every N steps
  ckpt_dir: ./checkpoints/pixmo_alignment
  output_dir: ./outputs/pixmo_alignment
  devices: 2
  strategy: ddp
  wandb_project: edge_glass_alignment
  wandb_run_name: pixmo_4096_mrl_v2  # New run name
  save_optimizer_state: true
  best_weights_only: true
  use_wandb: true

training:
  num_epochs: 25
  output_dir: ./outputs/pixmo_alignment
  save_steps: 500
  logging_steps: 20
  eval_steps: 500
  warmup_steps: null  # Computed from warmup_ratio
  gradient_accumulation_steps: 4
  wandb_project: edge_glass_alignment
  wandb_run_name: pixmo_4096_mrl_v2
