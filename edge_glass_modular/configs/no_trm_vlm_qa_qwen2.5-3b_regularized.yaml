# Baseline (No-TRM) VLM Configuration for PixMo QA (Regularized)
# Standard Qwen VLM without recursive TRM layers
# Derived from configs/trm_vlm_qa_qwen2.5-3b_regularized.yaml

name: no_trm_vlm_qa_3b_reg
description: Vision-Language Model Baseline (No TRM) for PixMo QA (Qwen 3B) - Regularized
seed: 42

# Dataset configuration
dataset:
  name: pixmo_qa
  train_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_train.parquet
  val_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_val.parquet
  test_parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_test.parquet

  image_size: 336
  max_text_length: 512
  max_question_length: 128
  max_answer_length: 256
  max_total_length: 384
  text_dropout_prob: 0.2

  batch_size: 4  # Optimized for H200
  base_batch_size: 4
  eval_batch_size: 4
  num_workers: 4
  prefetch_factor: 1
  pin_memory: true
  persistent_workers: true

# Vision encoder (loaded from pretrained alignment)
vision_encoder:
  model_name: openai/clip-vit-large-patch14-336
  projection_dim: 4096
  freeze: true
  trainable: false
  use_perceiver: true
  perceiver_num_latents: 64
  perceiver_latent_dim: 1024
  perceiver_num_layers: 4
  perceiver_num_heads: 8
  perceiver_dropout: 0.0
  use_mrl: true
  mrl_dimensions: [3072, 2048, 1536, 1024, 768, 512]
  use_attention_pooling: false
  pooling_type: mean

# Text encoder
text_encoder:
  model_name: sentence-transformers/all-mpnet-base-v2
  projection_dim: 4096
  freeze: true
  trainable: false
  use_mrl: false

# Decoder configuration
decoder:
  type: qwen
  model_name: Qwen/Qwen2.5-3B-Instruct
  use_lora: true
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.2
  lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  load_in_8bit: false
  load_in_4bit: true

# Fusion
fusion: null

# Loss configuration
losses:
  contrastive: 0.0
  mrl: 0.0
  sample_single_mrl_dim: false

# Optimization configuration
optimization:
  lr: 2.0e-5
  learning_rate: 2.0e-5
  weight_decay: 0.2
  betas: [0.9, 0.95]
  max_grad_norm: 1.0
  gradient_clip: 1.0
  grad_accum_steps: 4          # INCREASED: 1 -> 4 to hit effective BS 128 (8*4GPUs*4Accum)
  gradient_accumulation_steps: 4
  fp16: false
  bf16: true
  mixed_precision: bf16
  warmup_ratio: 0.1
  warmup_steps: null
  total_steps: null
  lr_scheduler: cosine
  min_lr_ratio: 0.1
  contrastive_loss_weight: 0.0
  mrl_loss_weight: 0.0
  lm_loss_weight: 1.0

# Trainer configuration
trainer:
  epochs: 10             # REDUCED TO 5 EPOCHS for 3-hour limit (safer than 20)
  num_epochs: 10
  batch_size: 8         # SAFE BATCH SIZE (4 was 50%, 8 should be ~90%)
  save_every: 1         # Save every epoch
  
  log_every: 1
  ckpt_dir: ./checkpoints/no_trm_vlm_qa_3b_reg
  output_dir: ./outputs/no_trm_vlm_qa_3b_reg
  devices: 1
  strategy: ddp
  use_wandb: true
  wandb_project: edge_glass_trm_vlm
  wandb_run_name: no_trm_vlm_qa_3b_reg
  eval_batch_size: 32
  save_optimizer_state: true
  best_weights_only: false

# Training configuration
training:
  num_epochs: 5
  output_dir: ./outputs/no_trm_vlm_qa_3b_reg
  save_steps: 500
  logging_steps: 20
  eval_steps: 500
  warmup_steps: null
  gradient_accumulation_steps: 4
  wandb_project: edge_glass_trm_vlm
  wandb_run_name: no_trm_vlm_qa_3b_reg

# Output configuration
output:
  checkpoint_dir: ./checkpoints/no_trm_vlm_qa_3b_reg
  log_dir: ./logs/no_trm_vlm_qa_3b_reg
  output_dir: ./outputs/no_trm_vlm_qa_3b_reg
  save_best_only: true

# Weights & Biases
wandb:
  enabled: true
  project: edge_glass_trm_vlm
  entity: null
  group: no_trm_vlm_qa_3b
  tags: [baseline, no_trm, vlm, pixmo_qa, qwen3b, regularized]
