{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a31581",
   "metadata": {},
   "source": [
    "# VLM Quantitative Evaluation\n",
    "\n",
    "This notebook evaluates the VLM on the validation set, computing Accuracy, Exact Match, and F1 scores with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeef6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path(os.path.abspath('..'))\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from config import load_config\n",
    "from models.alignment import MultimodalAlignmentModel\n",
    "from models.trm_qwen_vlm import QwenVLM\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "from data.transforms import get_image_transforms\n",
    "from evaluation.qa_metrics import evaluate_qa_metrics, compute_f1, compute_exact_match\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01170a24",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "config_path = \"../configs/trm_vlm_qa_qwen1.5.yaml\"\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "# Update this path to your trained checkpoint!\n",
    "checkpoint_path = \"../checkpoints/vlm_run/checkpoint-epoch-9\" \n",
    "# Or if you used the debug run:\n",
    "# checkpoint_path = \"../checkpoints/vlm_debug/checkpoint-epoch-0\"\n",
    "\n",
    "# alignment_checkpoint = \"../notebooks/checkpoints/pixmo_alignment/checkpoint_best.pt\n",
    "alignment_checkpoint = \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_modular/outputs/production_run_3b/checkpoint_best/pytorch_model.bin\"\n",
    "\n",
    "use_trm = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34516137",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07713958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vision Encoder...\n",
      "✓ Loaded alignment checkpoint\n",
      "Loading VLM...\n",
      "trainable params: 8,716,288 || all params: 1,552,430,592 || trainable%: 0.5615\n",
      "Loading VLM checkpoint from ../checkpoints/vlm_run/checkpoint-epoch-9\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/vlm_run/checkpoint-epoch-9'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚠ Could not find pytorch_model.bin in directory. Checkpoint loading might fail.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     state_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state_dict: state_dict = state_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     59\u001b[39m     model.load_state_dict(state_dict, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../checkpoints/vlm_run/checkpoint-epoch-9'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Aligned Vision Encoder\n",
    "print(\"Loading Vision Encoder...\")\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "alignment_config.decoder = None\n",
    "alignment_config.text_encoder = None\n",
    "\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config)\n",
    "\n",
    "if os.path.exists(alignment_checkpoint):\n",
    "    ckpt = torch.load(alignment_checkpoint, map_location='cpu', weights_only=False)\n",
    "    aligned_model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    print(\"✓ Loaded alignment checkpoint\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: Alignment checkpoint not found at {alignment_checkpoint}\")\n",
    "\n",
    "aligned_model.eval().to(device)\n",
    "for p in aligned_model.parameters(): p.requires_grad = False\n",
    "\n",
    "def encode_images(images):\n",
    "    with torch.no_grad():\n",
    "        if images.dim() == 3: images = images.unsqueeze(0)\n",
    "        images = images.to(device)\n",
    "        return aligned_model.vision_encoder(images, return_sequence=True).sequence\n",
    "\n",
    "# 2. VLM\n",
    "print(\"Loading VLM...\")\n",
    "config = load_config(config_path)\n",
    "qwen_decoder = QwenDecoder(\n",
    "    model_name=config.decoder.model_name,\n",
    "    load_in_4bit=True, # Use 4bit for inference efficiency\n",
    "    use_lora=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "vision_token_dim = alignment_config.vision_encoder.projection_dim\n",
    "model = QwenVLM(\n",
    "    qwen_decoder=qwen_decoder,\n",
    "    vision_token_dim=vision_token_dim,\n",
    "    use_trm_recursion=use_trm,\n",
    "    num_trm_layers=4,\n",
    "    num_recursion_steps=4\n",
    ").to(device)\n",
    "\n",
    "# Load Checkpoint\n",
    "print(f\"Loading VLM checkpoint from {checkpoint_path}\")\n",
    "if os.path.isdir(checkpoint_path):\n",
    "    # If saved with accelerator.save_state, it's a dir. \n",
    "    # We might need to manually load pytorch_model.bin if it exists or use accelerator\n",
    "    bin_path = Path(checkpoint_path) / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state_dict = torch.load(bin_path, map_location='cpu')\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"✓ Loaded state dict from bin file\")\n",
    "    else:\n",
    "        print(\"⚠ Could not find pytorch_model.bin in directory. Checkpoint loading might fail.\")\n",
    "else:\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"✓ Loaded state dict from file\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Models Ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef7ad8",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21016df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_transforms = get_image_transforms(config.dataset.image_size, is_training=False)\n",
    "val_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.val_parquet,\n",
    "    tokenizer=qwen_decoder.tokenizer,\n",
    "    image_transforms=val_transforms,\n",
    "    max_question_length=128,\n",
    "    max_answer_length=256,\n",
    "    limit=100 # Evaluate on 100 samples for speed, set to None for full set\n",
    ")\n",
    "\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c2963",
   "metadata": {},
   "source": [
    "## 4. Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "for i in tqdm(range(len(val_dataset))):\n",
    "    sample = val_dataset[i]\n",
    "    image = sample['image']\n",
    "    question = sample['question'] # Raw text\n",
    "    answer_gt = sample['answer']  # Raw text\n",
    "    \n",
    "    # Encode Vision\n",
    "    vision_tokens = encode_images(image)\n",
    "    \n",
    "    # Tokenize Question\n",
    "    inputs = qwen_decoder.tokenizer([question], return_tensors='pt', padding=True).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            vision_tokens=vision_tokens,\n",
    "            question_ids=inputs.input_ids,\n",
    "            max_new_tokens=64,\n",
    "            temperature=0.0 # Greedy\n",
    "        )\n",
    "    \n",
    "    pred = qwen_decoder.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Metrics\n",
    "    f1 = compute_f1(pred, answer_gt)\n",
    "    em = compute_exact_match(pred, answer_gt)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": answer_gt,\n",
    "        \"prediction\": pred,\n",
    "        \"f1\": f1,\n",
    "        \"exact_match\": em,\n",
    "        \"image_idx\": i\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"Average F1: {df['f1'].mean()*100:.2f}%\")\n",
    "print(f\"Average EM: {df['exact_match'].mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3b3a7",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86046fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Score Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['f1'], bins=10, kde=False, color='skyblue')\n",
    "plt.title(\"F1 Score Distribution\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='exact_match', data=df, palette=['salmon', 'lightgreen'])\n",
    "plt.title(\"Exact Match Count\")\n",
    "plt.xticks([0, 1], ['Miss', 'Hit'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98548458",
   "metadata": {},
   "source": [
    "## 6. Failure Analysis\n",
    "Examine top failures (Low F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0db1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "failures = df[df['f1'] < 0.5].head(5)\n",
    "\n",
    "for _, row in failures.iterrows():\n",
    "    print(f\"Q: {row['question']}\")\n",
    "    print(f\"GT: {row['ground_truth']}\")\n",
    "    print(f\"Pred: {row['prediction']}\")\n",
    "    print(f\"F1: {row['f1']:.2f}\")\n",
    "    # Display image?\n",
    "    # sample = val_dataset[row['image_idx']]\n",
    "    # T.ToPILImage()(sample['image']).show() \n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
