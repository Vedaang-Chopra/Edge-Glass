{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixmo Vision-Text Alignment with 4096-dim Embeddings\n",
    "\n",
    "This notebook demonstrates the improved alignment training with:\n",
    "1. **Pixmo Parquet Dataset** with embedded image bytes\n",
    "2. **4096-dim embeddings** with MRL dimensions [2048, 1024, 512, 256, 128]\n",
    "3. **Learnable attention pooling** instead of CLS/mean pooling\n",
    "4. **Updated loss weights**: MRL=1.0, CLIP=0.25\n",
    "5. **Text dropout** for better image reliance\n",
    "6. **Improved training** with warmup+cosine LR, checkpointing, crash recovery\n",
    "7. **Comprehensive logging** and visualization\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- 1-2 H200 GPUs\n",
    "- ~40-50GB GPU memory per GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our modules\n",
    "from config import load_config\n",
    "from models import MultimodalAlignmentModel\n",
    "from data.dataset_builder import build_image_datasets_from_parquet\n",
    "from data.transforms import get_image_transforms\n",
    "from training.improved_trainer import ImprovedMultimodalTrainer\n",
    "from utils.visualization import TrainingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configuration\n",
    "config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "config = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Experiment: {config.name}\")\n",
    "print(f\"\\nDataset Configuration:\")\n",
    "print(f\"  Train Parquet: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val Parquet: {config.dataset.val_parquet}\")\n",
    "print(f\"  Batch Size: {config.dataset.batch_size}\")\n",
    "print(f\"  Text Dropout: {config.dataset.text_dropout_prob}\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  Projection Dim: {config.vision_encoder.projection_dim}\")\n",
    "print(f\"  MRL Dimensions: {config.vision_encoder.mrl_dimensions}\")\n",
    "print(f\"  Attention Pooling: {config.vision_encoder.use_attention_pooling}\")\n",
    "print(f\"  Pooling Type: {config.vision_encoder.pooling_type}\")\n",
    "\n",
    "print(f\"\\nLoss Configuration:\")\n",
    "print(f\"  CLIP Weight: {config.losses.contrastive}\")\n",
    "print(f\"  MRL Weight: {config.losses.mrl}\")\n",
    "print(f\"  Sample Single MRL Dim: {config.losses.sample_single_mrl_dim}\")\n",
    "\n",
    "print(f\"\\nOptimization Configuration:\")\n",
    "print(f\"  Learning Rate: {config.optimization.lr}\")\n",
    "print(f\"  Weight Decay: {config.optimization.weight_decay}\")\n",
    "print(f\"  Max Grad Norm: {config.optimization.max_grad_norm}\")\n",
    "print(f\"  Warmup Ratio: {config.optimization.warmup_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating multimodal alignment model with 4096-dim embeddings...\")\n",
    "model = MultimodalAlignmentModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "model.print_parameter_counts()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pixmo Parquet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image transforms\n",
    "train_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Build datasets from parquet files\n",
    "print(\"Loading Pixmo datasets from parquet files...\")\n",
    "datasets = build_image_datasets_from_parquet(\n",
    "    cfg=config,\n",
    "    train_parquet_path=config.dataset.train_parquet,\n",
    "    val_parquet_path=config.dataset.val_parquet,\n",
    "    test_parquet_path=config.dataset.test_parquet,\n",
    "    train_transforms=train_transforms,\n",
    "    val_transforms=val_transforms,\n",
    "    max_text_length=config.dataset.max_text_length,\n",
    "    text_dropout_prob=config.dataset.text_dropout_prob,\n",
    ")\n",
    "\n",
    "train_dataset = datasets['train']\n",
    "val_dataset = datasets['val']\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "if 'test' in datasets:\n",
    "    print(f\"Test samples: {len(datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    "    persistent_workers=config.dataset.persistent_workers if config.dataset.num_workers > 0 else False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "print(f\"Image tensor shape: {sample_batch['image'].shape}\")\n",
    "print(f\"Number of captions: {len(sample_batch['text'])}\")\n",
    "\n",
    "# Visualize first 4 images with captions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(min(4, len(sample_batch['image']))):\n",
    "    # Denormalize image\n",
    "    img = sample_batch['image'][idx].cpu()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    caption_text = sample_batch['text'][idx] if sample_batch['text'][idx] else \"[DROPPED TEXT]\"\n",
    "    axes[idx].set_title(caption_text[:60] + \"...\", fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Some captions may show '[DROPPED TEXT]' due to text dropout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with improved features\n",
    "use_wandb = False  # Set to True if you want to use WandB\n",
    "\n",
    "trainer = ImprovedMultimodalTrainer(\n",
    "    cfg=config,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    use_wandb=use_wandb,\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer initialized with:\")\n",
    "print(f\"  Checkpoint directory: {trainer.ckpt_dir}\")\n",
    "print(f\"  Effective batch size: {trainer.effective_batch_size}\")\n",
    "print(f\"  World size (GPUs): {trainer.world_size}\")\n",
    "print(f\"  Starting epoch: {trainer.state.epoch}\")\n",
    "print(f\"  Starting step: {trainer.state.global_step}\")\n",
    "print(f\"  Best val loss: {trainer.state.best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {trainer.state.best_val_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "output_dir = Path(config.trainer.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "visualizer = TrainingVisualizer(save_dir=output_dir)\n",
    "\n",
    "# Plot training curves\n",
    "visualizer.plot_training_curves(history)\n",
    "print(f\"Training curves saved to {output_dir / 'training_curves.png'}\")\n",
    "\n",
    "# Plot loss components\n",
    "visualizer.plot_loss_components(history)\n",
    "print(f\"Loss components saved to {output_dir / 'loss_components.png'}\")\n",
    "\n",
    "# Plot LR schedule\n",
    "if history['lr']:\n",
    "    visualizer.plot_lr_schedule(history['lr'])\n",
    "    print(f\"LR schedule saved to {output_dir / 'lr_schedule.png'}\")\n",
    "\n",
    "# Display training curves\n",
    "from IPython.display import Image as IPImage, display\n",
    "display(IPImage(filename=str(output_dir / 'training_curves.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate and Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_checkpoint_path = trainer.ckpt_dir / \"checkpoint_best.pt\"\n",
    "if best_checkpoint_path.exists():\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "    model_to_load = model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model\n",
    "    model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from {best_checkpoint_path}\")\n",
    "    print(f\"Best epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Best val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"No best checkpoint found, using current model state.\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect embeddings from validation set\n",
    "print(\"Collecting embeddings for visualization...\")\n",
    "vision_embs = []\n",
    "text_embs = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(list(val_loader)[:10]):  # First 10 batches\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        \n",
    "        vision_embs.append(outputs.vision_emb.cpu().numpy())\n",
    "        text_embs.append(outputs.text_emb.cpu().numpy())\n",
    "        captions.extend(texts)\n",
    "\n",
    "vision_embs = np.vstack(vision_embs)\n",
    "text_embs = np.vstack(text_embs)\n",
    "\n",
    "print(f\"Vision embeddings: {vision_embs.shape}\")\n",
    "print(f\"Text embeddings: {text_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding space\n",
    "visualizer.plot_embedding_space(\n",
    "    vision_embs=vision_embs,\n",
    "    text_embs=text_embs,\n",
    "    method=\"pca\",\n",
    "    n_samples=500,\n",
    ")\n",
    "print(f\"Embedding space saved to {output_dir / 'embedding_space.png'}\")\n",
    "\n",
    "display(IPImage(filename=str(output_dir / 'embedding_space.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity matrix\n",
    "visualizer.plot_similarity_matrix(\n",
    "    vision_embs=vision_embs,\n",
    "    text_embs=text_embs,\n",
    "    n_samples=50,\n",
    ")\n",
    "print(f\"Similarity matrix saved to {output_dir / 'similarity_matrix.png'}\")\n",
    "\n",
    "display(IPImage(filename=str(output_dir / 'similarity_matrix.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image-to-text retrieval\n",
    "def retrieve_top_k(query_emb, database_embs, k=5):\n",
    "    \"\"\"Retrieve top-k nearest neighbors.\"\"\"\n",
    "    similarities = np.dot(database_embs, query_emb)\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return top_indices, similarities[top_indices]\n",
    "\n",
    "# Test on a few examples\n",
    "n_examples = 3\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Ground Truth Caption: {captions[i]}\")\n",
    "    \n",
    "    # Image-to-text retrieval\n",
    "    top_indices, scores = retrieve_top_k(vision_embs[i], text_embs, k=5)\n",
    "    \n",
    "    print(\"\\nTop 5 Retrieved Captions:\")\n",
    "    for rank, (idx, score) in enumerate(zip(top_indices, scores), 1):\n",
    "        match = \"✓ MATCH\" if idx == i else \"\"\n",
    "        print(f\"  {rank}. [{score:.3f}] {captions[idx][:80]}... {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {config.name}\")\n",
    "print(f\"  Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  Embedding Dimension: {config.vision_encoder.projection_dim}\")\n",
    "print(f\"  MRL Dimensions: {config.vision_encoder.mrl_dimensions}\")\n",
    "print(f\"  Attention Pooling: {config.vision_encoder.use_attention_pooling}\")\n",
    "print(f\"  Text Dropout: {config.dataset.text_dropout_prob}\")\n",
    "\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"  CLIP Loss: {config.losses.contrastive}\")\n",
    "print(f\"  MRL Loss: {config.losses.mrl}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {config.trainer.epochs}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Best validation loss: {trainer.state.best_val_loss:.4f}\")\n",
    "\n",
    "if history['val_i2t_r1']:\n",
    "    best_r1 = max(history['val_i2t_r1']) * 100\n",
    "    best_r5 = max(history['val_i2t_r5']) * 100\n",
    "    best_r10 = max(history['val_i2t_r10']) * 100\n",
    "    print(f\"  Best R@1: {best_r1:.2f}%\")\n",
    "    print(f\"  Best R@5: {best_r5:.2f}%\")\n",
    "    print(f\"  Best R@10: {best_r10:.2f}%\")\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Trainable: {model.num_trainable_parameters:,}\")\n",
    "print(f\"  Total: {model.num_total_parameters:,}\")\n",
    "print(f\"  Trainable %: {100*model.num_trainable_parameters/model.num_total_parameters:.2f}%\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Best model: {trainer.ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "print(f\"  Latest checkpoint: {trainer.ckpt_dir / 'checkpoint_latest.pt'}\")\n",
    "print(f\"  Training history: {trainer.ckpt_dir / 'training_history.json'}\")\n",
    "print(f\"  Visualizations: {output_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Test MRL performance at different dimensions\")\n",
    "print(\"2. Evaluate on test set\")\n",
    "print(\"3. Try different attention pooling strategies\")\n",
    "print(\"4. Fine-tune with decoder for instruction following\")\n",
    "print(\"5. Export model for deployment\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics table\n",
    "if history['val_i2t_r1']:\n",
    "    final_metrics = {\n",
    "        'best_val_loss': trainer.state.best_val_loss,\n",
    "        'best_r1': max(history['val_i2t_r1']),\n",
    "        'best_r5': max(history['val_i2t_r5']),\n",
    "        'best_r10': max(history['val_i2t_r10']),\n",
    "        'final_train_loss': history['train_loss'][-1] if history['train_loss'] else 0,\n",
    "        'final_val_loss': history['val_loss'][-1] if history['val_loss'] else 0,\n",
    "        'embedding_dim': config.vision_encoder.projection_dim,\n",
    "        'mrl_dims': str(config.vision_encoder.mrl_dimensions),\n",
    "        'clip_weight': config.losses.contrastive,\n",
    "        'mrl_weight': config.losses.mrl,\n",
    "    }\n",
    "    \n",
    "    visualizer.save_metrics_table(final_metrics)\n",
    "    print(f\"Final metrics saved to {output_dir / 'metrics.csv'}\")\n",
    "\n",
    "print(\"\\n✓ Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
