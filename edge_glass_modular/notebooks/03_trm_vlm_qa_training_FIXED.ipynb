{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset - FIXED VERSION\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Pretrained aligned vision encoder (CLIP + projections + MRL) - FROZEN\n",
    "2. **Pretrained Qwen2.5-7B-Instruct decoder with LoRA** - TRAINABLE\n",
    "3. PixMo QA dataset with question-answer pairs\n",
    "4. Optional TRM latent recursion on top of Qwen\n",
    "\n",
    "## Key Fixes from Original:\n",
    "\n",
    "\u2705 **Pretrained Decoder**: Uses Qwen2.5-7B instead of random 34M TRM\n",
    "\n",
    "\u2705 **Proper Training**: Standard autoregressive loss with prefix_embeds\n",
    "\n",
    "\u2705 **Baseline Mode**: Can disable TRM recursion for comparison\n",
    "\n",
    "\u2705 **Debug Tools**: Text-only sanity check, parameter audit, first-batch logging\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  \u2193\n",
    "Aligned Vision Encoder [FROZEN]\n",
    "  \u2193 (B, 577, 4096)\n",
    "Vision Projection (4096 \u2192 d_qwen) [TRAINABLE]\n",
    "  \u2193 (B, 577, d_qwen)\n",
    "Qwen2.5-7B Decoder + LoRA [TRAINABLE]\n",
    "  \u2193\n",
    "Token Layout: [IMG_TOKENS] [QUESTION] [ANSWER]\n",
    "Loss: Only on answer tokens (vision/question masked with -100)\n",
    "```\n",
    "\n",
    "Optional: TRM latent recursion wrapper on top of Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "print(f\"Added to path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modular imports from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from decoders.trm import TRMConfig, TRMLayer, RMSNorm\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "from data.transforms import get_image_transforms\n",
    "from models.alignment import MultimodalAlignmentModel\n",
    "\n",
    "# Set matplotlib style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: trm_vlm_qa\n",
      "\n",
      "Dataset:\n",
      "  Train: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_train.parquet\n",
      "  Val: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_val.parquet\n",
      "  Image size: 336\n",
      "  Batch size: 16\n",
      "\n",
      "Decoder:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Use LoRA: True\n",
      "  Load in 8bit: False\n",
      "Checkpoint root: /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/notebooks/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Load experiment config\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val: {config.dataset.val_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Load in 8bit: {config.decoder.load_in_8bit}\")\n",
    "\n",
    "# Resolve checkpoint root (works from repo root or notebooks directory)\n",
    "CKPT_ROOT_CANDIDATES = [\n",
    "    Path.cwd() / 'checkpoints',\n",
    "    Path.cwd().parent / 'checkpoints',\n",
    "    Path.cwd() / 'edge_glass_modular/notebooks/checkpoints',\n",
    "    Path.cwd().parent / 'edge_glass_modular/notebooks/checkpoints',\n",
    "]\n",
    "CKPT_ROOT = next((p for p in CKPT_ROOT_CANDIDATES if p.exists()), None)\n",
    "if CKPT_ROOT is None:\n",
    "    raise FileNotFoundError('No checkpoint directory found; expected one of: ' + ', '.join(str(p) for p in CKPT_ROOT_CANDIDATES))\n",
    "print(f\"Checkpoint root: {CKPT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Aligned Vision Encoder (FROZEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aligned vision encoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8b164e6ff741b19be01e3ed4fdc96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "\u2713 Loaded aligned model from /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/notebooks/checkpoints/pixmo_alignment/checkpoint_best.pt\n",
      "  Checkpoint epoch: 0\n",
      "  Val loss: 0.0000\n",
      "  Vision output: (B, num_tokens, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Load alignment config\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "\n",
    "# Build aligned model\n",
    "print(\"Loading aligned vision encoder...\")\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = CKPT_ROOT / 'pixmo_alignment/checkpoint_best.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "aligned_model.eval()\n",
    "\n",
    "# FREEZE all parameters\n",
    "for param in aligned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"\u2713 Loaded aligned model from {checkpoint_path}\")\n",
    "print(f\"  Checkpoint epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Val loss: {checkpoint.get('best_val_loss', 0.0):.4f}\")\n",
    "\n",
    "# Get vision encoder output dimension\n",
    "vision_token_dim = alignment_config.vision_encoder.projection_dim\n",
    "print(f\"  Vision output: (B, num_tokens, {vision_token_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vision Encoding Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision tokens shape: torch.Size([2, 577, 4096])\n",
      "Expected: (2, num_tokens, 4096)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Encode images to vision tokens using frozen aligned encoder.\n",
    "    \n",
    "    Args:\n",
    "        images: (B, 3, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        vision_tokens: (B, num_tokens, vision_token_dim)\n",
    "    \"\"\"\n",
    "    vision_output = aligned_model.vision_encoder(images, return_sequence=True)\n",
    "    if vision_output.sequence is None:\n",
    "        raise ValueError(\"Vision encoder did not return sequence embeddings\")\n",
    "    return vision_output.sequence\n",
    "\n",
    "# Test\n",
    "test_img = torch.randn(2, 3, 336, 336).to(device)\n",
    "test_vision_tokens = encode_images(test_img)\n",
    "print(f\"Vision tokens shape: {test_vision_tokens.shape}\")\n",
    "print(f\"Expected: (2, num_tokens, {vision_token_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QwenVLM Model Class\n",
    "\n",
    "**Main VLM wrapper with optional TRM recursion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 QwenVLM class defined\n"
     ]
    }
   ],
   "source": [
    "class QwenVLM(nn.Module):\n",
    "    \"\"\"Qwen-based VLM with optional TRM latent recursion.\n",
    "    \n",
    "    Features:\n",
    "    - Pretrained Qwen2.5 decoder with LoRA\n",
    "    - Vision token projection to Qwen hidden dim\n",
    "    - Proper prefix_embeds integration\n",
    "    - Standard autoregressive loss with -100 masking\n",
    "    - Optional TRM latent recursion on top\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qwen_decoder: QwenDecoder,\n",
    "        vision_token_dim: int = 4096,\n",
    "        use_trm_recursion: bool = False,\n",
    "        num_trm_layers: int = 2,\n",
    "        num_recursion_steps: int = 4,\n",
    "        confidence_threshold: float = 0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qwen = qwen_decoder\n",
    "        self.hidden_dim = qwen_decoder.hidden_dim\n",
    "        self.use_trm_recursion = use_trm_recursion\n",
    "        self.num_recursion_steps = num_recursion_steps\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Vision projection: 4096 \u2192 Qwen hidden dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, self.hidden_dim)\n",
    "        \n",
    "        # Optional TRM recursion components\n",
    "        if use_trm_recursion:\n",
    "            trm_config = TRMConfig(\n",
    "                vocab_size=qwen_decoder.vocab_size,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_layers=num_trm_layers,\n",
    "                num_heads=8,\n",
    "                max_seq_len=2048,\n",
    "            )\n",
    "            \n",
    "            self.trm_layers = nn.ModuleList([\n",
    "                TRMLayer(trm_config) for _ in range(num_trm_layers)\n",
    "            ])\n",
    "            self.trm_norm = RMSNorm(self.hidden_dim)\n",
    "            self.z_init = nn.Parameter(torch.randn(1, 1, self.hidden_dim) * 0.02)\n",
    "            \n",
    "            print(f\"  \u2713 TRM recursion enabled ({num_trm_layers} layers, {num_recursion_steps} steps)\")\n",
    "        else:\n",
    "            print(f\"  \u2713 Baseline mode (no TRM recursion)\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, vision_dim)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "        answer_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass with proper autoregressive training.\n",
    "        \n",
    "        Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        Labels: [-100 for img/question] [answer_ids]\n",
    "        \"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Project vision tokens\n",
    "        device = vision_tokens.device\n",
    "        qwen_dtype = self.qwen.model.dtype\n",
    "\n",
    "        # Align aux modules and inputs with Qwen sharded dtype/device\n",
    "        if self.vision_proj.weight.device != device or self.vision_proj.weight.dtype != qwen_dtype:\n",
    "            self.vision_proj = self.vision_proj.to(device=device, dtype=qwen_dtype)\n",
    "        if self.use_trm_recursion:\n",
    "            self.trm_layers.to(device=device, dtype=qwen_dtype)\n",
    "            self.trm_norm.to(device=device, dtype=qwen_dtype)\n",
    "            if self.z_init.device != device or self.z_init.dtype != qwen_dtype:\n",
    "                self.z_init.data = self.z_init.data.to(device=device, dtype=qwen_dtype)\n",
    "\n",
    "        vision_tokens = vision_tokens.to(device=device, dtype=qwen_dtype)\n",
    "\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d_qwen)\n",
    "        \n",
    "        # Prepare input_ids and labels\n",
    "        input_ids = torch.cat([question_ids, answer_ids], dim=1)  # (B, L_q + L_a)\n",
    "        \n",
    "        # Labels: -100 for question, real IDs for answer\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        labels = torch.cat([question_labels, answer_ids], dim=1)  # (B, L_q + L_a)\n",
    "        \n",
    "        if not self.use_trm_recursion:\n",
    "            # ===== BASELINE: Standard Qwen forward =====\n",
    "            outputs = self.qwen(\n",
    "                input_ids=input_ids,\n",
    "                prefix_embeds=vision_emb,  # Vision tokens as prefix\n",
    "                labels=labels,\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'loss': outputs.loss,\n",
    "                'logits': outputs.logits,\n",
    "                'confidence': None,\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # ===== TRM MODE: Latent recursion on answer embeddings =====\n",
    "            # Get text embeddings\n",
    "            text_emb = self.qwen.model.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # Split into question and answer\n",
    "            L_q = question_ids.shape[1]\n",
    "            L_a = answer_ids.shape[1]\n",
    "            question_emb = text_emb[:, :L_q, :]\n",
    "            answer_emb = text_emb[:, L_q:, :]\n",
    "            \n",
    "            # Context: [vision, question]\n",
    "            x = torch.cat([vision_emb, question_emb], dim=1)  # (B, K_img + L_q, d)\n",
    "            \n",
    "            # Answer to be refined\n",
    "            y = answer_emb  # (B, L_a, d)\n",
    "            \n",
    "            # Initialize latent state\n",
    "            z = self.z_init.expand(batch_size, L_a, -1)  # (B, L_a, d)\n",
    "            \n",
    "            # Latent recursion\n",
    "            for _ in range(self.num_recursion_steps):\n",
    "                x, y, z = self.latent_recursion(x, y, z)\n",
    "            \n",
    "            # Final logits from refined answer\n",
    "            y = self.trm_norm(y)\n",
    "            lm_head = self.qwen.model.lm_head\n",
    "            logits_answer = lm_head(y)  # (B, L_a, vocab)\n",
    "            \n",
    "            # Compute loss on answer only\n",
    "            shift_logits = logits_answer[:, :-1, :].contiguous()\n",
    "            shift_labels = answer_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Align labels to logits device in sharded setups\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # Compute confidence\n",
    "            probs = torch.softmax(logits_answer, dim=-1)\n",
    "            max_probs = torch.max(probs, dim=-1)[0]\n",
    "            confidence = torch.mean(max_probs, dim=-1)\n",
    "            \n",
    "            return {\n",
    "                'loss': loss,\n",
    "                'logits': logits_answer,\n",
    "                'confidence': confidence,\n",
    "            }\n",
    "    \n",
    "    def latent_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,  # Context: (B, L_ctx, d)\n",
    "        y: torch.Tensor,  # Answer: (B, L_ans, d)\n",
    "        z: torch.Tensor,  # Latent: (B, L_ans, d)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Single TRM latent recursion step.\"\"\"\n",
    "        # Concatenate [x, y, z]\n",
    "        concat = torch.cat([x, y, z], dim=1)\n",
    "        \n",
    "        # Pass through TRM layers\n",
    "        hidden = concat\n",
    "        for layer in self.trm_layers:\n",
    "            hidden = layer(hidden)\n",
    "        \n",
    "        # Split back\n",
    "        L_ctx = x.shape[1]\n",
    "        L_ans = y.shape[1]\n",
    "        \n",
    "        x_out = hidden[:, :L_ctx, :]\n",
    "        y_out = hidden[:, L_ctx:L_ctx+L_ans, :]\n",
    "        z_out = hidden[:, L_ctx+L_ans:, :]\n",
    "        \n",
    "        return x_out, y_out, z_out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.0,\n",
    "        use_confidence: bool = False,\n",
    "        return_stats: bool = False,\n",
    "    ):\n",
    "        \"\"\"Generate answers with optional TRM recursion.\"\"\"\n",
    "        # Project vision\n",
    "        device = vision_tokens.device\n",
    "        target_dtype = self.qwen.model.dtype\n",
    "\n",
    "        # Align vision projection with Qwen sharded dtype/device\n",
    "        if self.vision_proj.weight.device != device or self.vision_proj.weight.dtype != target_dtype:\n",
    "            self.vision_proj = self.vision_proj.to(device=device, dtype=target_dtype)\n",
    "\n",
    "        vision_tokens = vision_tokens.to(\n",
    "            device=self.vision_proj.weight.device,\n",
    "            dtype=self.vision_proj.weight.dtype,\n",
    "        )\n",
    "\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        \n",
    "        if not self.use_trm_recursion:\n",
    "            # Baseline: standard Qwen generation\n",
    "            outputs = self.qwen.generate(\n",
    "                input_ids=question_ids,\n",
    "                prefix_embeds=vision_emb,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=max(temperature, 0.01) if temperature > 0 else 1.0,\n",
    "                do_sample=temperature > 0,\n",
    "            )\n",
    "            \n",
    "            # Extract only generated part\n",
    "            prompt_len = question_ids.shape[1] + vision_emb.shape[1]\n",
    "            generated_ids = outputs[:, prompt_len:]\n",
    "            \n",
    "            if return_stats:\n",
    "                return {\n",
    "                    'predictions': generated_ids,\n",
    "                    'confidences': torch.ones_like(generated_ids, dtype=torch.float),\n",
    "                    'recursion_steps': torch.ones_like(generated_ids, dtype=torch.float),\n",
    "                }\n",
    "            return generated_ids\n",
    "        \n",
    "        else:\n",
    "            # TRM mode: autoregressive with recursion\n",
    "            batch_size = vision_tokens.shape[0]\n",
    "            generated_ids = []\n",
    "            confidences = []\n",
    "            recursion_steps_used = []\n",
    "            \n",
    "            for step in range(max_new_tokens):\n",
    "                # Current answer sequence\n",
    "                if len(generated_ids) == 0:\n",
    "                    answer_ids = torch.tensor(\n",
    "                        [[self.qwen.tokenizer.pad_token_id]],\n",
    "                        device=vision_tokens.device\n",
    "                    ).expand(batch_size, 1)\n",
    "                else:\n",
    "                    answer_ids = torch.stack(generated_ids, dim=1)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = self.forward(vision_tokens, question_ids, answer_ids)\n",
    "                logits = outputs['logits'][:, -1, :]  # Last position\n",
    "                \n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                generated_ids.append(next_token)\n",
    "                \n",
    "                # Track confidence\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                conf = torch.max(probs, dim=-1)[0]\n",
    "                confidences.append(conf)\n",
    "                \n",
    "                recursion_steps_used.append(\n",
    "                    self.num_recursion_steps if self.use_trm_recursion else 1\n",
    "                )\n",
    "            \n",
    "            generated_ids = torch.stack(generated_ids, dim=1)\n",
    "            \n",
    "            if return_stats:\n",
    "                return {\n",
    "                    'predictions': generated_ids,\n",
    "                    'confidences': torch.stack(confidences, dim=1),\n",
    "                    'recursion_steps': torch.tensor(\n",
    "                        recursion_steps_used\n",
    "                    ).unsqueeze(0).expand(batch_size, -1).float(),\n",
    "                }\n",
    "            return generated_ids\n",
    "\n",
    "print(\"\u2713 QwenVLM class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Pretrained Qwen Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING QWEN DECODER\n",
      "============================================================\n",
      "\n",
      "Loading: Qwen/Qwen2.5-7B-Instruct\n",
      "  LoRA: True\n",
      "  8-bit: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4383491703b47b09b23aa0c13abda7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "\n",
      "\u2713 Qwen decoder loaded\n",
      "  Hidden dim: 3584\n",
      "  Vocab size: 152064\n",
      "\n",
      "Creating QwenVLM wrapper\n",
      "  Vision token dim: 4096\n",
      "  Use TRM recursion: False\n",
      "  \u2713 Baseline mode (no TRM recursion)\n",
      "\n",
      "\u2713 QwenVLM model created\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "USE_TRM_RECURSION = False  # Start with baseline, then try True\n",
    "NUM_TRM_LAYERS = 4         # Only used if TRM recursion enabled\n",
    "NUM_RECURSION_STEPS = 4\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "# ====================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING QWEN DECODER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Qwen decoder\n",
    "print(f\"\\nLoading: {config.decoder.model_name}\")\n",
    "print(f\"  LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  8-bit: {config.decoder.load_in_8bit}\")\n",
    "\n",
    "qwen_decoder = QwenDecoder(\n",
    "    model_name=config.decoder.model_name,\n",
    "    load_in_8bit=config.decoder.load_in_8bit,\n",
    "    load_in_4bit=False,\n",
    "    use_lora=config.decoder.use_lora,\n",
    "    lora_r=config.decoder.get('lora_r', 32),\n",
    "    lora_alpha=config.decoder.get('lora_alpha', 64),\n",
    "    lora_dropout=config.decoder.get('lora_dropout', 0.1),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Qwen decoder loaded\")\n",
    "print(f\"  Hidden dim: {qwen_decoder.hidden_dim}\")\n",
    "print(f\"  Vocab size: {qwen_decoder.vocab_size}\")\n",
    "\n",
    "# Create QwenVLM wrapper\n",
    "print(f\"\\nCreating QwenVLM wrapper\")\n",
    "print(f\"  Vision token dim: {vision_token_dim}\")\n",
    "print(f\"  Use TRM recursion: {USE_TRM_RECURSION}\")\n",
    "\n",
    "model = QwenVLM(\n",
    "    qwen_decoder=qwen_decoder,\n",
    "    vision_token_dim=vision_token_dim,\n",
    "    use_trm_recursion=USE_TRM_RECURSION,\n",
    "    num_trm_layers=NUM_TRM_LAYERS,\n",
    "    num_recursion_steps=NUM_RECURSION_STEPS,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 QwenVLM model created\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text-Only Sanity Check (BEFORE Training)\n",
    "\n",
    "**Critical test**: Verify pretrained Qwen can generate coherent English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEXT-ONLY SANITY CHECK (Before Training)\n",
      "============================================================\n",
      "\n",
      "Prompt: Question: What is 2 + 2? Answer:\n",
      "Generated:  2 + 2 is equal to 4. This is a basic arithmetic operation in mathematics where\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: Question: What color is the sky? Answer:\n",
      "Generated:  The sky is typically blue during the day, except at sunrise, sunset, and on overcast days\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: The capital of France is\n",
      "Generated:  Paris, and the capital of Germany is Berlin. Which of the following statements is true?\n",
      "A.\n",
      "------------------------------------------------------------\n",
      "\n",
      "\u2713 If coherent English \u2192 Decoder works!\n",
      "\u2717 If garbage \u2192 Decoder loading issue\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def text_only_sanity_check(decoder, prompts, max_tokens=20):\n",
    "    \"\"\"Test decoder on text-only prompts without vision.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT-ONLY SANITY CHECK (Before Training)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Encode\n",
    "        inputs = decoder.tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = decoder.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        generated = decoder.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n\u2713 If coherent English \u2192 Decoder works!\")\n",
    "    print(\"\u2717 If garbage \u2192 Decoder loading issue\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Question: What is 2 + 2? Answer:\",\n",
    "    \"Question: What color is the sky? Answer:\",\n",
    "    \"The capital of France is\",\n",
    "]\n",
    "\n",
    "text_only_sanity_check(qwen_decoder, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Audit\n",
    "\n",
    "Verify which parameters are frozen vs trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARAMETER AUDIT\n",
      "============================================================\n",
      "\n",
      "\ud83d\udfe2 TRAINABLE (226 groups):\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: 32,768\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: 32,768\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: 32,768\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: 32,768\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: 229,376\n",
      "  qwen.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: 32,768\n",
      "  ... and 206 more\n",
      "\n",
      "\ud83d\udd34 FROZEN (1507 groups):\n",
      "  qwen.model.base_model.model.model.embed_tokens.weight: 544,997,376\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: 12,845,056\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias: 3,584\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: 1,835,008\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias: 512\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: 1,835,008\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias: 512\n",
      "  qwen.model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: 12,845,056\n",
      "  qwen.model.base_model.model.model.layers.0.mlp.gate_proj.weight: 67,895,296\n",
      "  qwen.model.base_model.model.model.layers.0.mlp.up_proj.weight: 67,895,296\n",
      "  ... and 1497 more\n",
      "\n",
      "\ud83d\udcca SUMMARY:\n",
      "  Total: 16,686,605,185\n",
      "  Trainable: 55,053,824 (0.33%)\n",
      "  Frozen: 16,631,551,361 (99.67%)\n",
      "\n",
      "\u2713 VERIFICATION:\n",
      "  Aligned model frozen: True\n",
      "  Vision proj trainable: True\n",
      "  Qwen LoRA trainable: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PARAMETER AUDIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainable_params = []\n",
    "frozen_params = []\n",
    "\n",
    "# Audit QwenVLM model\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((name, param.numel()))\n",
    "\n",
    "# Audit aligned model\n",
    "for name, param in aligned_model.named_parameters():\n",
    "    full_name = f\"aligned_model.{name}\"\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((full_name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((full_name, param.numel()))\n",
    "\n",
    "print(f\"\\n\ud83d\udfe2 TRAINABLE ({len(trainable_params)} groups):\")\n",
    "total_trainable = 0\n",
    "for name, count in trainable_params[:20]:\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_trainable += count\n",
    "\n",
    "if len(trainable_params) > 20:\n",
    "    print(f\"  ... and {len(trainable_params) - 20} more\")\n",
    "    for _, count in trainable_params[20:]:\n",
    "        total_trainable += count\n",
    "\n",
    "print(f\"\\n\ud83d\udd34 FROZEN ({len(frozen_params)} groups):\")\n",
    "total_frozen = 0\n",
    "for name, count in frozen_params[:10]:\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_frozen += count\n",
    "\n",
    "for _, count in frozen_params[10:]:\n",
    "    total_frozen += count\n",
    "\n",
    "if len(frozen_params) > 10:\n",
    "    print(f\"  ... and {len(frozen_params) - 10} more\")\n",
    "\n",
    "total = total_trainable + total_frozen\n",
    "print(f\"\\n\ud83d\udcca SUMMARY:\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable: {total_trainable:,} ({100*total_trainable/total:.2f}%)\")\n",
    "print(f\"  Frozen: {total_frozen:,} ({100*total_frozen/total:.2f}%)\")\n",
    "\n",
    "print(f\"\\n\u2713 VERIFICATION:\")\n",
    "print(f\"  Aligned model frozen: {all(not p.requires_grad for p in aligned_model.parameters())}\")\n",
    "print(f\"  Vision proj trainable: {'vision_proj' in str([n for n, _ in trainable_params])}\")\n",
    "\n",
    "if USE_TRM_RECURSION:\n",
    "    print(f\"  TRM layers trainable: {'trm_layers' in str(trainable_params)}\")\n",
    "    print(f\"  z_init trainable: {'z_init' in str([n for n, _ in trainable_params])}\")\n",
    "else:\n",
    "    print(f\"  Qwen LoRA trainable: {'lora' in str(trainable_params).lower()}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8400 samples from /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_train.parquet\n",
      "Loaded 1800 samples from /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_val.parquet\n",
      "Dataset sizes:\n",
      "  Train: 8,400\n",
      "  Val: 1,800\n",
      "\n",
      "DataLoader:\n",
      "  Batch size: 16\n",
      "  Train batches: 525\n",
      "  Val batches: 113\n"
     ]
    }
   ],
   "source": [
    "def collate_qa_batch(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    questions = [item[\"question_ids\"] for item in batch]\n",
    "    answers = [item[\"answer_ids\"] for item in batch]\n",
    "    \n",
    "    # Process images\n",
    "    # Stack if already tensors (from dataset transform)\n",
    "    if isinstance(images[0], torch.Tensor):\n",
    "        images = torch.stack(images)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # We need to pad questions (left) and answers (right)\n",
    "    # Get max lengths\n",
    "    max_q_len = max([q.size(0) for q in questions])\n",
    "    max_a_len = max([a.size(0) for a in answers])\n",
    "    \n",
    "    # Create padded tensors\n",
    "    bs = len(batch)\n",
    "    padded_questions = torch.full((bs, max_q_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    padded_answers = torch.full((bs, max_a_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    \n",
    "    # Create answer mask (1 for valid, 0 for pad)\n",
    "    answer_mask = torch.zeros((bs, max_a_len), dtype=torch.long)\n",
    "    \n",
    "    for i in range(bs):\n",
    "        # Left pad question? Or right? Usually left for generation, but here we are training.\n",
    "        # Right padding is standard for training with attention masks.\n",
    "        q_len = questions[i].size(0)\n",
    "        padded_questions[i, :q_len] = questions[i]\n",
    "        \n",
    "        a_len = answers[i].size(0)\n",
    "        padded_answers[i, :a_len] = answers[i]\n",
    "        \n",
    "        # Set mask for valid answer tokens\n",
    "        answer_mask[i, :a_len] = 1\n",
    "        \n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"question_ids\": padded_questions,\n",
    "        \"answer_ids\": padded_answers,\n",
    "        \"answer_mask\": answer_mask # Return mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Evaluation metrics defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for evaluation.\"\"\"\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    s = s.lower().strip()\n",
    "    s = ' '.join([w for w in s.split() if w not in {'a', 'an', 'the'}])\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return float(normalize_answer(pred) == normalize_answer(target))\n",
    "\n",
    "def compute_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute token-level F1.\"\"\"\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    target_tokens = normalize_answer(target).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
    "        return float(pred_tokens == target_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(target_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(target_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def evaluate_qa(predictions: List[str], targets: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate QA predictions.\"\"\"\n",
    "    em_scores = [compute_exact_match(p, t) for p, t in zip(predictions, targets)]\n",
    "    f1_scores = [compute_f1(p, t) for p, t in zip(predictions, targets)]\n",
    "    \n",
    "    return {\n",
    "        'em': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "    }\n",
    "\n",
    "print(\"\u2713 Evaluation metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Debug Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Debug helpers defined\n"
     ]
    }
   ],
   "source": [
    "def debug_first_batch(batch, vision_tokens, outputs, tokenizer):\n",
    "    \"\"\"Debug first training batch.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST BATCH DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udce6 Shapes:\")\n",
    "    print(f\"  Images: {batch['images'].shape}\")\n",
    "    print(f\"  Vision tokens: {vision_tokens.shape}\")\n",
    "    print(f\"  Question IDs: {batch['question_ids'].shape}\")\n",
    "    print(f\"  Answer IDs: {batch['answer_ids'].shape}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcdd First example:\")\n",
    "    print(f\"  Q: {batch['questions'][0][:100]}...\")\n",
    "    print(f\"  A: {batch['answers'][0][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd24 Decoded tokens:\")\n",
    "    q_dec = tokenizer.decode(batch['question_ids'][0], skip_special_tokens=False)\n",
    "    a_dec = tokenizer.decode(batch['answer_ids'][0], skip_special_tokens=False)\n",
    "    print(f\"  Question: {q_dec[:150]}...\")\n",
    "    print(f\"  Answer: {a_dec[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Outputs:\")\n",
    "    print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "    if outputs.get('logits') is not None:\n",
    "        print(f\"  Logits: {outputs['logits'].shape}\")\n",
    "    if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "        print(f\"  Confidence: {outputs['confidence'].mean().item():.3f}\")\n",
    "    \n",
    "    # Token counts\n",
    "    num_q = (batch['question_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "    num_a = (batch['answer_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "    num_img = vision_tokens.shape[1]\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccf Token counts:\")\n",
    "    print(f\"  Vision: {num_img}\")\n",
    "    print(f\"  Question (non-pad): {num_q}\")\n",
    "    print(f\"  Answer (non-pad): {num_a}\")\n",
    "    print(f\"  Supervised: {num_a} ({100*num_a/(num_img+num_q+num_a):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n\u2713 Expected:\")\n",
    "    print(f\"  - Loss < 10 (pretrained range)\")\n",
    "    print(f\"  - Supervised = answer only\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\u2713 Debug helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config:\n",
      "  Epochs: 10\n",
      "  LR: 0.0001\n",
      "  Total steps: 5250\n",
      "  Warmup steps: 262\n"
     ]
    }
   ],
   "source": [
    "# Training config\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LOG_EVERY = 20\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  LR: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/notebooks/wandb/run-20251205_121219-hfwpy9qp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm/runs/hfwpy9qp' target=\"_blank\">qwen_vlm_baseline</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm/runs/hfwpy9qp' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_qwen_vlm/runs/hfwpy9qp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    run_name = f\"qwen_vlm_{'trm' if USE_TRM_RECURSION else 'baseline'}\"\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_qwen_vlm\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            'use_trm_recursion': USE_TRM_RECURSION,\n",
    "            'num_trm_layers': NUM_TRM_LAYERS if USE_TRM_RECURSION else 0,\n",
    "            'num_recursion_steps': NUM_RECURSION_STEPS if USE_TRM_RECURSION else 0,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': batch_size,\n",
    "            'decoder': config.decoder.model_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "first_batch_debugged = False\n",
    "\n",
    "# Checkpoint dir\n",
    "ckpt_dir = CKPT_ROOT / f\"qwen_vlm_qa_{'trm' if USE_TRM_RECURSION else 'baseline'}\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Loop (Uncommented and Fixed)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "steps = 0\n",
    "max_steps = 100 # Short run for verification\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(range(max_steps))\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch[\"images\"].to(device)\n",
    "    question_ids = batch[\"question_ids\"].to(device)\n",
    "    answer_ids = batch[\"answer_ids\"].to(device)\n",
    "    answer_mask = batch[\"answer_mask\"].to(device) # Get answer mask\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        images=images, \n",
    "        question_ids=question_ids, \n",
    "        answer_ids=answer_ids,\n",
    "        answer_mask=answer_mask # Pass answer mask\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    steps += 1\n",
    "    if steps >= max_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QwenVLM(\n",
       "  (qwen): QwenDecoder(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): Qwen2ForCausalLM(\n",
       "          (model): Qwen2Model(\n",
       "            (embed_tokens): Embedding(152064, 3584)\n",
       "            (layers): ModuleList(\n",
       "              (0-27): 28 x Qwen2DecoderLayer(\n",
       "                (self_attn): Qwen2Attention(\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (mlp): Qwen2MLP(\n",
       "                  (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                  (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                  (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "                  (act_fn): SiLUActivation()\n",
       "                )\n",
       "                (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "                (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "              )\n",
       "            )\n",
       "            (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision_proj): Linear(in_features=4096, out_features=3584, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(ckpt_dir / 'checkpoint_best.pt', map_location=device, weights_only=False)\n",
    "model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/hice1/vchopra37/scratch/projects/edge_glass/complex_curve.png')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complex VLM question probe on the curve image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "probe_image_path = Path(\"/home/hice1/vchopra37/scratch/projects/edge_glass/complex_curve.png\")\n",
    "probe_question = (\n",
    "    \"Provide a short narrative interpretation of the plotted curve: describe its overall trend, where the slope changes,\"    \" and what that implies about acceleration, saturation, or decay in the underlying relationship.\"\n",
    ")\n",
    "display(probe_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Provide a short narrative interpretation of the plotted curve: describe its overall trend, where the slope changes, and what that implies about acceleration, saturation, or decay in the underlying relationship.\n",
      "Answer: \n",
      "Confidence trace: []\n",
      "Recursion steps per token: []\n"
     ]
    }
   ],
   "source": [
    "probe_image = Image.open(probe_image_path).convert(\"RGB\")\n",
    "probe_tensor = val_transforms(T.ToTensor()(probe_image)).unsqueeze(0).to(device)\n",
    "probe_tokens = encode_images(probe_tensor)\n",
    "\n",
    "probe_question_ids = qwen_decoder.tokenizer(\n",
    "    probe_question, return_tensors='pt', add_special_tokens=True\n",
    ").input_ids.to(device)\n",
    "\n",
    "probe_gen = model.generate(\n",
    "    vision_tokens=probe_tokens,\n",
    "    question_ids=probe_question_ids,\n",
    "    max_new_tokens=96,\n",
    "    temperature=0.7,\n",
    "    use_confidence=True,\n",
    "    return_stats=True,\n",
    ")\n",
    "\n",
    "if isinstance(probe_gen, torch.Tensor):\n",
    "    gen_ids = probe_gen[0].detach().cpu().tolist()\n",
    "    confidence_trace = None\n",
    "    recursion_steps = None\n",
    "else:\n",
    "    gen_ids = probe_gen[\"predictions\"][0].detach().cpu().tolist()\n",
    "    confidence_trace = probe_gen.get(\"confidences\")\n",
    "    if confidence_trace is not None:\n",
    "        confidence_trace = probe_gen[\"confidences\"][0].detach().cpu().tolist()\n",
    "    recursion_steps = probe_gen.get(\"recursion_steps\")\n",
    "    if recursion_steps is not None:\n",
    "        recursion_steps = probe_gen[\"recursion_steps\"][0].detach().cpu().tolist()\n",
    "\n",
    "probe_answer = qwen_decoder.tokenizer.decode(\n",
    "    gen_ids,\n",
    "    skip_special_tokens=True,\n",
    ").strip()\n",
    "\n",
    "print(\"Question:\", probe_question)\n",
    "print(\"Answer:\", probe_answer)\n",
    "if confidence_trace is not None:\n",
    "    print(\"Confidence trace:\", [round(float(c), 3) for c in confidence_trace])\n",
    "if recursion_steps is not None:\n",
    "    print(\"Recursion steps per token:\", recursion_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FULL EVALUATION\n",
      "============================================================\n",
      "Mode: Baseline\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33d79d3cb4b4c5198e7d26c42935113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TRM' if USE_TRM_RECURSION else 'Baseline'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidences = []\n",
    "all_recursion_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        gen_outputs = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.0,\n",
    "            return_stats=True,\n",
    "        )\n",
    "        \n",
    "        if isinstance(gen_outputs, dict):\n",
    "            generated_ids = gen_outputs['predictions']\n",
    "            if gen_outputs.get('confidences') is not None:\n",
    "                all_confidences.extend(gen_outputs['confidences'].mean(dim=1).cpu().tolist())\n",
    "            if gen_outputs.get('recursion_steps') is not None:\n",
    "                all_recursion_steps.extend(gen_outputs['recursion_steps'].mean(dim=1).cpu().tolist())\n",
    "        else:\n",
    "            generated_ids = gen_outputs\n",
    "        \n",
    "        predictions = qwen_decoder.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "\n",
    "# Metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"EM: {metrics['em']:.2f}%\")\n",
    "print(f\"F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "if all_confidences:\n",
    "    print(f\"\\nConfidence: {np.mean(all_confidences):.3f} \u00b1 {np.std(all_confidences):.3f}\")\n",
    "\n",
    "if all_recursion_steps:\n",
    "    print(f\"\\nRecursion: {np.mean(all_recursion_steps):.2f} steps/seq\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({'val/em_final': metrics['em'], 'val/f1_final': metrics['f1']})\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"  Q: {val_dataset.df.iloc[i]['question'][:80]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:80]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:80]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Config:\")\n",
    "print(f\"  Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  TRM Recursion: {USE_TRM_RECURSION}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Training:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Eval:\")\n",
    "print(f\"  EM: {metrics['em']:.2f}%\")\n",
    "print(f\"  F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Dataset:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Output:\")\n",
    "print(f\"  {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY IMPROVEMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\u2705 Pretrained Qwen decoder (vs random 34M TRM)\")\n",
    "print(\"\u2705 Proper autoregressive training (prefix_embeds + -100 masking)\")\n",
    "print(\"\u2705 Baseline mode for comparison\")\n",
    "print(\"\u2705 Debug instrumentation (sanity check, param audit, first-batch)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. If EM/F1 still low:\")\n",
    "print(\"   - Check text-only generation\")\n",
    "print(\"   - Check first-batch debug\")\n",
    "print(\"   - Increase epochs/LR\")\n",
    "print(\"\\n2. If baseline works:\")\n",
    "print(\"   - Set USE_TRM_RECURSION = True\")\n",
    "print(\"   - Compare vs baseline\")\n",
    "print(\"\\n3. Expected baseline: EM > 15%, F1 > 25%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\u2713 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}