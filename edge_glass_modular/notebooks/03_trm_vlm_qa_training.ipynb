{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Modular vision encoder (CLIP + Perceiver + MRL) from `edge_glass_modular/src/encoders`\n",
    "2. Qwen decoder with LoRA from `edge_glass_modular/src/decoders`\n",
    "3. PixMo QA dataset with question-answer pairs\n",
    "4. Proper modular design following the edge_glass_modular architecture\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  \u2193\n",
    "Vision Encoder (frozen aligned model)\n",
    "  \u2193 (B, num_latents, hidden_dim)\n",
    "Projection to Qwen hidden dim\n",
    "  \u2193 (B, num_latents, qwen_dim)\n",
    "Qwen Decoder with LoRA (trainable)\n",
    "  \u2193\n",
    "Token Layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "  \u2193\n",
    "Loss on answer tokens only\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- Modular design using imports from `edge_glass_modular/src`\n",
    "- Frozen aligned vision encoder\n",
    "- Qwen2.5 decoder with LoRA fine-tuning\n",
    "- Real QA dataset (not synthetic)\n",
    "- Proper configuration management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Import modular components from edge_glass_modular\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mencoders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionEncoder\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdecoders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqwen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QwenDecoder\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PixmoParquetImageTextDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src/encoders/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Encoder modules for vision, audio, and text.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionEncoder, VisionEncoderOutput\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioEncoder, AudioEncoderOutput\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextEncoder, TextEncoderOutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src/encoders/vision.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPVisionModel, CLIPImageProcessor\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/__init__.py:958\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    956\u001b[39m _import_structure = {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure.items()}\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m import_structure = \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})].update(_import_structure)\n\u001b[32m    961\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\n\u001b[32m    962\u001b[39m     \u001b[34m__name__\u001b[39m,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    966\u001b[39m     extra_objects={\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m: __version__},\n\u001b[32m    967\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/import_utils.py:2867\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2843\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2846\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2847\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2865\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2866\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/import_utils.py:2580\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2580\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2582\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2583\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/import_utils.py:2579\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2576\u001b[39m adjacent_modules = []\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m-> \u001b[39m\u001b[32m2579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2580\u001b[39m         import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n\u001b[32m   2582\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:42\u001b[39m, in \u001b[36misdir\u001b[39m\u001b[34m(s)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "from collections import Counter\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# Import modular components from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.dataset_builder import PixmoParquetImageTextDataset\n",
    "from data.transforms import get_image_transforms\n",
    "from models.alignment import MultimodalAlignmentModel\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "Load the experiment configuration from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train parquet: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val parquet: {config.dataset.val_parquet}\")\n",
    "print(f\"  Test parquet: {config.dataset.test_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Max question length: {config.dataset.max_question_length}\")\n",
    "print(f\"  Max answer length: {config.dataset.max_answer_length}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Type: {config.decoder.type}\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Load in 8bit: {config.decoder.load_in_8bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Aligned Vision Encoder\n",
    "\n",
    "Load the pretrained Perceiver+MRL alignment model and freeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment config\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "\n",
    "# Load aligned model\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"checkpoints/pixmo_alignment/checkpoint_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "aligned_model.eval()\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in aligned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Loaded aligned model from {checkpoint_path}\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"  Vision encoder output: (B, 64, 4096)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Vision Encoder Method\n",
    "\n",
    "Create a clean interface to get vision embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Encode images to vision tokens.\n",
    "    \n",
    "    Args:\n",
    "        images: (B, 3, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        vision_tokens: (B, num_latents, 4096)\n",
    "    \"\"\"\n",
    "    vision_output = aligned_model.vision_encoder(images, return_sequence=True)\n",
    "    if vision_output.sequence is None:\n",
    "        raise ValueError(\"Vision encoder did not return sequence embeddings\")\n",
    "    # Get sequence output (B, num_latents, dim)\n",
    "    return vision_output.sequence\n",
    "\n",
    "# Test\n",
    "test_img = torch.randn(2, 3, 336, 336).to(device)\n",
    "test_vision_tokens = encode_images(test_img)\n",
    "print(f\"Vision tokens shape: {test_vision_tokens.shape}\")\n",
    "if getattr(aligned_model.vision_encoder, \"use_perceiver\", False):\n",
    "    expected_tokens = aligned_model.config.vision_encoder.perceiver_num_latents\n",
    "else:\n",
    "    expected_tokens = test_vision_tokens.shape[1]\n",
    "print(f\"Expected: ({test_img.shape[0]}, {expected_tokens}, {aligned_model.config.vision_encoder.projection_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Plain Tiny Decoder Baseline\n",
    "\n",
    "First, implement a simple baseline decoder without TRM recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoders.trm import TRMConfig, TRMDecoder\n",
    "\n",
    "class TinyVLMDecoder(nn.Module):\n",
    "    \"\"\"Plain tiny decoder baseline for VLM.\n",
    "    \n",
    "    Architecture:\n",
    "        - Projects vision tokens from 4096 -> d_dec\n",
    "        - Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        - Causal masking\n",
    "        - Loss only on answer tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        vision_token_dim: int = 4096,\n",
    "        max_seq_len: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vision_token_dim = vision_token_dim\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # TRM decoder\n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.decoder = TRMDecoder(trm_config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "    ):\n",
    "        \"\"\"Forward pass with proper token layout and loss masking.\n",
    "        \n",
    "        Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        Loss only on answer tokens.\n",
    "        \"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        num_img_tokens = vision_tokens.shape[1]\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d_dec)\n",
    "        \n",
    "        # Embed question and answer tokens\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)  # (B, L_q, d_dec)\n",
    "        answer_emb = self.decoder.embed_tokens(answer_ids)      # (B, L_a, d_dec)\n",
    "        \n",
    "        # Concatenate: [vision | question | answer]\n",
    "        full_sequence = torch.cat([vision_emb, question_emb, answer_emb], dim=1)\n",
    "        \n",
    "        # Create labels: -100 for image and question tokens, actual IDs for answer\n",
    "        img_labels = torch.full(\n",
    "            (batch_size, num_img_tokens),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=vision_tokens.device\n",
    "        )\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        answer_labels = answer_ids\n",
    "        \n",
    "        # Concatenate labels\n",
    "        full_labels = torch.cat([img_labels, question_labels, answer_labels], dim=1)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        hidden_states = full_sequence\n",
    "        for layer in self.decoder.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        \n",
    "        hidden_states = self.decoder.norm(hidden_states)\n",
    "        logits = self.decoder.lm_head(hidden_states)\n",
    "        \n",
    "        # Compute loss (shift for next-token prediction)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = full_labels[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"Generate answer tokens autoregressively.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Project vision\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)\n",
    "        \n",
    "        # Start with image + question\n",
    "        current_emb = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        generated_ids = []\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            hidden = current_emb\n",
    "            for layer in self.decoder.layers:\n",
    "                hidden = layer(hidden)\n",
    "            hidden = self.decoder.norm(hidden)\n",
    "            logits = self.decoder.lm_head(hidden)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            # Embed and append\n",
    "            next_emb = self.decoder.embed_tokens(next_token.unsqueeze(1))\n",
    "            current_emb = torch.cat([current_emb, next_emb], dim=1)\n",
    "        \n",
    "        return torch.stack(generated_ids, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement TRM-Style Recursive Decoder\n",
    "\n",
    "Now implement the TRM version with latent recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVLMWithConfidence(nn.Module):\n",
    "    \"\"\"TRM VLM decoder with confidence-based recursive refinement.\n",
    "    \n",
    "    Key features:\n",
    "    - Uses aligned vision encoder (frozen)\n",
    "    - Projects vision tokens to TRM hidden dim\n",
    "    - Implements latent recursion for reasoning\n",
    "    - Confidence-based early stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        vision_token_dim: int = 4096,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        num_heads: int = 8,\n",
    "        num_inner_steps: int = 4,\n",
    "        confidence_threshold: float = 0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Tiny transformer for recursion (use TRM components)\n",
    "        from decoders.trm import TRMConfig, TRMLayer\n",
    "        \n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=1024,\n",
    "        )\n",
    "        \n",
    "        self.tiny_transformer = nn.ModuleList([\n",
    "            TRMLayer(trm_config) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        from decoders.trm import RMSNorm\n",
    "        self.norm = RMSNorm(hidden_dim)\n",
    "        \n",
    "        # LM head\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed_tokens.weight  # Tie weights\n",
    "        \n",
    "        # Learned initial z state\n",
    "        self.z_init = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)\n",
    "    \n",
    "    def latent_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,  # Context: (B, L_ctx, d)\n",
    "        y: torch.Tensor,  # Answer: (B, L_ans, d)\n",
    "        z: torch.Tensor,  # Latent: (B, L_ans, d)\n",
    "    ):\n",
    "        \"\"\"Single step of latent recursion.\"\"\"\n",
    "        # Concatenate along sequence: [x, y, z]\n",
    "        concat = torch.cat([x, y, z], dim=1)  # (B, L_ctx + 2*L_ans, d)\n",
    "        \n",
    "        # Pass through tiny transformer\n",
    "        hidden = concat\n",
    "        for layer in self.tiny_transformer:\n",
    "            hidden = layer(hidden)\n",
    "        \n",
    "        # Split back\n",
    "        L_ctx = x.shape[1]\n",
    "        L_ans = y.shape[1]\n",
    "        \n",
    "        x_out = hidden[:, :L_ctx, :]\n",
    "        y_out = hidden[:, L_ctx:L_ctx+L_ans, :]\n",
    "        z_out = hidden[:, L_ctx+L_ans:, :]\n",
    "        \n",
    "        return x_out, y_out, z_out\n",
    "    \n",
    "    def compute_confidence(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute confidence score from logits.\n",
    "        \n",
    "        Args:\n",
    "            logits: (B, L, vocab_size)\n",
    "        \n",
    "        Returns:\n",
    "            confidence: (B,) - mean max softmax probability\n",
    "        \"\"\"\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        max_probs = torch.max(probs, dim=-1)[0]  # (B, L)\n",
    "        confidence = torch.mean(max_probs, dim=-1)  # (B,)\n",
    "        return confidence\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "        num_recursion_steps: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Forward pass with TRM recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        L_ans = answer_ids.shape[0]\n",
    "        \n",
    "        # Use default or override recursion steps\n",
    "        n_steps = num_recursion_steps if num_recursion_steps is not None else self.num_inner_steps\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d)\n",
    "        \n",
    "        # Embed question\n",
    "        question_emb = self.embed_tokens(question_ids)  # (B, L_q, d)\n",
    "        \n",
    "        # Context x = [vision | question]\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)  # (B, L_ctx, d)\n",
    "        \n",
    "        # Teacher-forced answer embeddings\n",
    "        y = self.embed_tokens(answer_ids)  # (B, L_ans, d)\n",
    "        \n",
    "        # Initialize latent z\n",
    "        z = self.z_init.expand(batch_size, answer_ids.shape[1], -1)  # (B, L_ans, d)\n",
    "        \n",
    "        # Inner recursion (n steps)\n",
    "        for _ in range(n_steps):\n",
    "            x, y, z = self.latent_recursion(x, y, z)\n",
    "        \n",
    "        # Final answer from y\n",
    "        y = self.norm(y)\n",
    "        logits = self.lm_head(y)  # (B, L_ans, vocab_size)\n",
    "        \n",
    "        # Compute loss (standard next-token prediction on answer)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = answer_ids[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        # Compute confidence\n",
    "        confidence = self.compute_confidence(logits)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'confidence': confidence,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "        use_confidence: bool = True,\n",
    "    ):\n",
    "        \"\"\"Generate answer with confidence-based recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Context\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.embed_tokens(question_ids)\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        \n",
    "        # Start with empty answer\n",
    "        generated_ids = []\n",
    "        confidence_scores = []\n",
    "        recursion_steps_used = []\n",
    "        \n",
    "        # Generate autoregressively\n",
    "        for step in range(max_new_tokens):\n",
    "            # Current y from generated so far\n",
    "            if len(generated_ids) == 0:\n",
    "                # First token: use learned blank\n",
    "                y = torch.zeros(batch_size, 1, self.hidden_dim, device=x.device)\n",
    "                z = self.z_init.expand(batch_size, 1, -1)\n",
    "            else:\n",
    "                y = self.embed_tokens(torch.stack(generated_ids, dim=1))\n",
    "                z = self.z_init.expand(batch_size, len(generated_ids), -1)\n",
    "            \n",
    "            # Adaptive recursion based on confidence\n",
    "            if use_confidence and len(generated_ids) > 0:\n",
    "                # Run once to check confidence\n",
    "                x_temp, y_temp, z_temp = self.latent_recursion(x, y, z)\n",
    "                y_normed = self.norm(y_temp)\n",
    "                logits_temp = self.lm_head(y_normed[:, -1, :])\n",
    "                conf = self.compute_confidence(logits_temp.unsqueeze(1))\n",
    "                \n",
    "                if conf.mean() >= self.confidence_threshold:\n",
    "                    # High confidence - use result\n",
    "                    y = y_temp\n",
    "                    num_steps = 1\n",
    "                else:\n",
    "                    # Low confidence - run more recursion\n",
    "                    y = y_temp\n",
    "                    z = z_temp\n",
    "                    for _ in range(self.num_inner_steps - 1):\n",
    "                        x_temp, y, z = self.latent_recursion(x, y, z)\n",
    "                    num_steps = self.num_inner_steps\n",
    "                \n",
    "                recursion_steps_used.append(num_steps)\n",
    "            else:\n",
    "                # Fixed recursion\n",
    "                for _ in range(self.num_inner_steps):\n",
    "                    x_temp, y, z = self.latent_recursion(x, y, z)\n",
    "                recursion_steps_used.append(self.num_inner_steps)\n",
    "            \n",
    "            # Get logits for last position\n",
    "            y = self.norm(y)\n",
    "            logits = self.lm_head(y[:, -1, :]) / temperature\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Track confidence\n",
    "            conf = self.compute_confidence(logits.unsqueeze(1))\n",
    "            confidence_scores.append(conf.mean().item())\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            # Stop at EOS (optional)\n",
    "            # if (next_token == tokenizer.eos_token_id).all():\n",
    "            #     break\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.stack(generated_ids, dim=1),\n",
    "            'confidence': confidence_scores,\n",
    "            'recursion_steps': recursion_steps_used,\n",
    "        }\n",
    "\n",
    "print(\"TRM VLM with Confidence-Based Recursion defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics (EM and F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer text for evaluation.\"\"\"\n",
    "    # Remove punctuation\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    # Lowercase and strip\n",
    "    s = s.lower().strip()\n",
    "    # Remove articles\n",
    "    s = ' '.join([w for w in s.split() if w not in {'a', 'an', 'the'}])\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return float(normalize_answer(pred) == normalize_answer(target))\n",
    "\n",
    "def compute_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score.\"\"\"\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    target_tokens = normalize_answer(target).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
    "        return float(pred_tokens == target_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(target_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(target_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def evaluate_qa(\n",
    "    predictions: List[str],\n",
    "    targets: List[str],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate QA predictions.\"\"\"\n",
    "    em_scores = [compute_exact_match(p, t) for p, t in zip(predictions, targets)]\n",
    "    f1_scores = [compute_f1(p, t) for p, t in zip(predictions, targets)]\n",
    "    \n",
    "    return {\n",
    "        'em': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (use GPT2 tokenizer)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "# Get transforms\n",
    "train_transforms = get_image_transforms(image_size=336, is_training=True)\n",
    "val_transforms = get_image_transforms(image_size=336, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import QA dataset\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "\n",
    "# Define collate function for QA batches\n",
    "def collate_qa_batch(batch):\n",
    "    \"\"\"Collate QA batch with padding.\"\"\"\n",
    "    # Stack images\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    \n",
    "    # Pad question_ids\n",
    "    question_ids = [item['question_ids'] for item in batch]\n",
    "    max_q_len = max(q.shape[0] for q in question_ids)\n",
    "    question_ids_padded = torch.stack([\n",
    "        torch.cat([q, torch.full((max_q_len - q.shape[0],), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "        for q in question_ids\n",
    "    ])\n",
    "    \n",
    "    # Pad answer_ids\n",
    "    answer_ids = [item['answer_ids'] for item in batch]\n",
    "    max_a_len = max(a.shape[0] for a in answer_ids)\n",
    "    answer_ids_padded = torch.stack([\n",
    "        torch.cat([a, torch.full((max_a_len - a.shape[0],), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "        for a in answer_ids\n",
    "    ])\n",
    "    \n",
    "    # Get raw text\n",
    "    questions = [item['question'] for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'question_ids': question_ids_padded,\n",
    "        'answer_ids': answer_ids_padded,\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.train_parquet,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=train_transforms,\n",
    "    max_question_length=128,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "val_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.val_parquet,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=val_transforms,\n",
    "    max_question_length=128,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val: {len(val_dataset):,}\")\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  Images: {sample_batch['images'].shape}\")\n",
    "print(f\"  Question IDs: {sample_batch['question_ids'].shape}\")\n",
    "print(f\"  Answer IDs: {sample_batch['answer_ids'].shape}\")\n",
    "\n",
    "print(\"\\nSample QA pairs:\")\n",
    "for i in range(min(3, len(sample_batch['questions']))):\n",
    "    print(f\"\\n  [{i+1}]\")\n",
    "    print(f\"    Q: {sample_batch['questions'][i]}\")\n",
    "    print(f\"    A: {sample_batch['answers'][i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Decoder Model\n",
    "\n",
    "Choose between TinyVLMDecoder (baseline) or TRMVLMDecoder (recursive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_TRM = True  # Always use TRM for this notebook\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2  # Small for TRM\n",
    "NUM_HEADS = 8\n",
    "NUM_INNER_STEPS = 4\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "\n",
    "# Get vision token dimension from aligned model\n",
    "vision_token_dim = aligned_model.vision_encoder.projector.out_features  # Should be 4096\n",
    "\n",
    "# Initialize TRM VLM model\n",
    "model = TRMVLMWithConfidence(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    vision_token_dim=vision_token_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_inner_steps=NUM_INNER_STEPS,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Initialized TRM VLM Decoder with Confidence-Based Recursion\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")\n",
    "print(f\"  Inner recursion steps: {NUM_INNER_STEPS}\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PARAMETER AUDIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainable_params = []\n",
    "frozen_params = []\n",
    "\n",
    "# Audit all model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((name, param.numel()))\n",
    "\n",
    "# Audit aligned model (should all be frozen)\n",
    "for name, param in aligned_model.named_parameters():\n",
    "    full_name = f\"aligned_model.{name}\"\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((full_name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((full_name, param.numel()))\n",
    "\n",
    "print(f\"\\n\ud83d\udfe2 TRAINABLE PARAMETERS ({len(trainable_params)} groups):\")\n",
    "total_trainable = 0\n",
    "for name, count in trainable_params[:20]:  # Show first 20\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_trainable += count\n",
    "\n",
    "if len(trainable_params) > 20:\n",
    "    print(f\"  ... and {len(trainable_params) - 20} more\")\n",
    "    for _, count in trainable_params[20:]:\n",
    "        total_trainable += count\n",
    "\n",
    "print(f\"\\n\ud83d\udd34 FROZEN PARAMETERS ({len(frozen_params)} groups):\")\n",
    "total_frozen = 0\n",
    "sample_frozen = frozen_params[:10]\n",
    "for name, count in sample_frozen:\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_frozen += count\n",
    "\n",
    "for _, count in frozen_params[10:]:\n",
    "    total_frozen += count\n",
    "\n",
    "if len(frozen_params) > 10:\n",
    "    print(f\"  ... and {len(frozen_params) - 10} more\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca SUMMARY:\")\n",
    "total = total_trainable + total_frozen\n",
    "print(f\"  Total parameters: {total:,}\")\n",
    "print(f\"  Trainable: {total_trainable:,} ({100*total_trainable/total:.2f}%)\")\n",
    "print(f\"  Frozen: {total_frozen:,} ({100*total_frozen/total:.2f}%)\")\n",
    "\n",
    "print(f\"\\n\u2713 VERIFICATION:\")\n",
    "print(f\"  \u2713 Aligned model frozen: {all(not p.requires_grad for p in aligned_model.parameters())}\")\n",
    "print(f\"  \u2713 Vision projection trainable: {'vision_proj' in [n for n, _ in trainable_params]}\")\n",
    "\n",
    "if USE_TRM_RECURSION:\n",
    "    print(f\"  \u2713 TRM layers trainable: {'trm_layers' in str(trainable_params)}\")\n",
    "    print(f\"  \u2713 z_init trainable: {'z_init' in [n for n, _ in trainable_params]}\")\n",
    "else:\n",
    "    print(f\"  \u2713 Qwen LoRA trainable: {'lora' in str(trainable_params).lower()}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Parameter Audit\n",
    "\n",
    "Verify which parameters are frozen vs trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text_only_sanity_check(decoder, prompts, max_tokens=20):\n",
    "    \"\"\"Test decoder on text-only prompts without vision.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT-ONLY SANITY CHECK (Before Training)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Encode prompt\n",
    "        inputs = decoder.tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = decoder.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = decoder.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n\u2713 If generation is coherent English \u2192 Decoder works!\")\n",
    "    print(\"\u2717 If generation is garbage \u2192 Decoder loading issue\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Question: What is 2 + 2? Answer:\",\n",
    "    \"Question: What color is the sky? Answer:\",\n",
    "    \"The capital of France is\",\n",
    "]\n",
    "\n",
    "text_only_sanity_check(qwen_decoder, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_first_batch(batch, vision_tokens, outputs, qwen_tokenizer):\n",
    "    \"\"\"Debug first training batch to verify labels, masks, and loss.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST BATCH DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udce6 Batch shapes:\")\n",
    "    print(f\"  Images: {batch['images'].shape}\")\n",
    "    print(f\"  Vision tokens: {vision_tokens.shape}\")\n",
    "    print(f\"  Question IDs: {batch['question_ids'].shape}\")\n",
    "    print(f\"  Answer IDs: {batch['answer_ids'].shape}\")\n",
    "    \n",
    "    # Analyze first example\n",
    "    print(f\"\\n\ud83d\udcdd First example:\")\n",
    "    print(f\"  Question: {batch['questions'][0][:100]}...\")\n",
    "    print(f\"  Answer: {batch['answers'][0][:100]}...\")\n",
    "    \n",
    "    # Decode tokens\n",
    "    print(f\"\\n\ud83d\udd24 Decoded tokens (first example):\")\n",
    "    q_decoded = qwen_tokenizer.decode(batch['question_ids'][0], skip_special_tokens=False)\n",
    "    a_decoded = qwen_tokenizer.decode(batch['answer_ids'][0], skip_special_tokens=False)\n",
    "    print(f\"  Question tokens: {q_decoded[:150]}...\")\n",
    "    print(f\"  Answer tokens: {a_decoded[:100]}...\")\n",
    "    \n",
    "    # Check loss and logits\n",
    "    print(f\"\\n\ud83d\udcca Training outputs:\")\n",
    "    print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "    if outputs.get('logits') is not None:\n",
    "        print(f\"  Logits shape: {outputs['logits'].shape}\")\n",
    "    if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "        print(f\"  Confidence: {outputs['confidence'].mean().item():.3f}\")\n",
    "    \n",
    "    # Token counts\n",
    "    num_q_tokens = (batch['question_ids'][0] != qwen_tokenizer.pad_token_id).sum().item()\n",
    "    num_a_tokens = (batch['answer_ids'][0] != qwen_tokenizer.pad_token_id).sum().item()\n",
    "    num_img_tokens = vision_tokens.shape[1]\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccf Token counts (first example):\")\n",
    "    print(f\"  Vision tokens: {num_img_tokens}\")\n",
    "    print(f\"  Question tokens (non-pad): {num_q_tokens}\")\n",
    "    print(f\"  Answer tokens (non-pad): {num_a_tokens}\")\n",
    "    print(f\"  Total context: {num_img_tokens + num_q_tokens}\")\n",
    "    print(f\"  Supervised tokens (answer): {num_a_tokens}\")\n",
    "    \n",
    "    total_seq_len = num_img_tokens + num_q_tokens + num_a_tokens\n",
    "    print(f\"\\n  Total sequence length: {total_seq_len}\")\n",
    "    print(f\"  Supervised %: {100 * num_a_tokens / total_seq_len:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\u2713 Expected behavior:\")\n",
    "    print(\"  - Loss should be < 10 (pretrained LM range)\")\n",
    "    print(\"  - Supervised tokens should be answer only (~10-30 tokens)\")\n",
    "    print(\"  - Vision + question tokens should NOT contribute to loss\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\u2713 Debug helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    run_name = f\"qwen_vlm_{'trm' if USE_TRM_RECURSION else 'baseline'}\"\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_qwen_vlm\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            'use_trm_recursion': USE_TRM_RECURSION,\n",
    "            'num_trm_layers': NUM_TRM_LAYERS if USE_TRM_RECURSION else 0,\n",
    "            'num_recursion_steps': NUM_RECURSION_STEPS if USE_TRM_RECURSION else 0,\n",
    "            'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': batch_size,\n",
    "            'decoder': config.decoder.model_name,\n",
    "            'use_lora': config.decoder.use_lora,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_em': [], 'val_f1': []}\n",
    "first_batch_debugged = False  # Flag for first-batch debug\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = Path(f\"checkpoints/qwen_vlm_qa_{'trm' if USE_TRM_RECURSION else 'baseline'}\")\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TRM Recursion' if USE_TRM_RECURSION else 'Baseline (No Recursion)'}\")\n",
    "print(f\"Checkpoint dir: {ckpt_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answer_ids = batch['answer_ids'].to(device)\n",
    "        \n",
    "        # Encode images (frozen)\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # \ud83d\udc1b DEBUG FIRST BATCH\n",
    "        if epoch == 0 and batch_idx == 0 and not first_batch_debugged:\n",
    "            debug_first_batch(batch, vision_tokens, outputs, qwen_decoder.tokenizer)\n",
    "            first_batch_debugged = True\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        epoch_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % LOG_EVERY == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-LOG_EVERY:])\n",
    "            pbar_dict = {\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            }\n",
    "            \n",
    "            if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "                avg_conf = outputs['confidence'].mean().item()\n",
    "                pbar_dict['conf'] = f'{avg_conf:.3f}'\n",
    "            \n",
    "            pbar.set_postfix(pbar_dict)\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                log_dict = {\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/lr': scheduler.get_last_lr()[0],\n",
    "                    'step': global_step,\n",
    "                }\n",
    "                if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "                    log_dict['train/confidence'] = avg_conf\n",
    "                wandb.log(log_dict)\n",
    "    \n",
    "    # Epoch-end evaluation\n",
    "    print(f\"\\n  Epoch {epoch+1} average loss: {np.mean(epoch_losses):.4f}\")\n",
    "    history['train_loss'].append(np.mean(epoch_losses))\n",
    "    \n",
    "    # Validation (simple loss for now, full eval is expensive)\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['images'].to(device)\n",
    "            question_ids = batch['question_ids'].to(device)\n",
    "            answer_ids = batch['answer_ids'].to(device)\n",
    "            \n",
    "            vision_tokens = encode_images(images)\n",
    "            outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "            val_losses.append(outputs['loss'].item())\n",
    "            \n",
    "            if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "                val_confidences.append(outputs['confidence'].mean().item())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "    if val_confidences:\n",
    "        val_conf = np.mean(val_confidences)\n",
    "        print(f\"  Validation confidence: {val_conf:.3f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        log_dict = {\n",
    "            'val/loss': val_loss,\n",
    "            'epoch': epoch + 1,\n",
    "        }\n",
    "        if val_confidences:\n",
    "            log_dict['val/confidence'] = val_conf\n",
    "        wandb.log(log_dict)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': {\n",
    "                'use_trm_recursion': USE_TRM_RECURSION,\n",
    "                'num_trm_layers': NUM_TRM_LAYERS,\n",
    "                'num_recursion_steps': NUM_RECURSION_STEPS,\n",
    "                'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            },\n",
    "        }, ckpt_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"  \u2713 Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Text-Only Sanity Check (BEFORE Training)\n",
    "\n",
    "**CRITICAL TEST**: Verify that the pretrained Qwen decoder can generate coherent English BEFORE multimodal training.\n",
    "\n",
    "If this fails \u2192 decoder loading issue\n",
    "If this passes \u2192 decoder is working, ready for multimodal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(ckpt_dir / \"checkpoint_best.pt\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION ON VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TRM Recursion' if USE_TRM_RECURSION else 'Baseline (No Recursion)'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidence_scores = []\n",
    "all_recursion_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating answers\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        # Encode images\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Generate answers\n",
    "        gen_outputs = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.0,  # Greedy for evaluation\n",
    "            use_confidence=USE_TRM_RECURSION,  # Only for TRM mode\n",
    "            return_stats=True,\n",
    "        )\n",
    "        \n",
    "        # Handle different output formats\n",
    "        if isinstance(gen_outputs, dict):\n",
    "            generated_ids = gen_outputs['predictions']\n",
    "            if gen_outputs.get('confidences') is not None:\n",
    "                all_confidence_scores.extend(gen_outputs['confidences'].mean(dim=1).cpu().tolist())\n",
    "            if gen_outputs.get('recursion_steps') is not None:\n",
    "                all_recursion_steps.extend(gen_outputs['recursion_steps'].mean(dim=1).cpu().tolist())\n",
    "        else:\n",
    "            generated_ids = gen_outputs\n",
    "        \n",
    "        # Decode\n",
    "        predictions = qwen_decoder.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"Token F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "if all_confidence_scores:\n",
    "    print(f\"\\n\ud83d\udcca Confidence Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(all_confidence_scores):.3f}\")\n",
    "    print(f\"  Std: {np.std(all_confidence_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(all_confidence_scores):.3f}\")\n",
    "    print(f\"  Max: {np.max(all_confidence_scores):.3f}\")\n",
    "\n",
    "if all_recursion_steps:\n",
    "    print(f\"\\n\ud83d\udd01 Recursion Statistics:\")\n",
    "    print(f\"  Avg steps per sequence: {np.mean(all_recursion_steps):.2f}\")\n",
    "    print(f\"  Std: {np.std(all_recursion_steps):.2f}\")\n",
    "    if USE_TRM_RECURSION:\n",
    "        recursion_triggered = sum(1 for s in all_recursion_steps if s > 1) / len(all_recursion_steps) * 100\n",
    "        print(f\"  Recursion triggered: {recursion_triggered:.1f}% of sequences\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        'val/em_final': metrics['em'],\n",
    "        'val/f1_final': metrics['f1'],\n",
    "    })\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"  Q: {val_dataset.df.iloc[i]['question'][:80]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:80]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:80]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Initialize Pretrained Qwen Decoder\n",
    "\n",
    "Load Qwen2.5-7B-Instruct with LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenVLM(nn.Module):\n",
    "    \"\"\"Qwen-based VLM with optional TRM recursion.\n",
    "    \n",
    "    This is the FIXED implementation that uses pretrained Qwen instead of random init.\n",
    "    \n",
    "    Features:\n",
    "    - Pretrained Qwen2.5-7B-Instruct with LoRA\n",
    "    - Vision token projection to Qwen's hidden dim\n",
    "    - Proper prefix_embeds integration\n",
    "    - Standard autoregressive loss with -100 masking\n",
    "    - Optional TRM latent recursion on top (for comparison)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qwen_decoder: QwenDecoder,\n",
    "        vision_token_dim: int = 4096,\n",
    "        use_trm_recursion: bool = False,\n",
    "        num_trm_layers: int = 2,\n",
    "        num_recursion_steps: int = 4,\n",
    "        confidence_threshold: float = 0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qwen = qwen_decoder\n",
    "        self.hidden_dim = qwen_decoder.hidden_dim\n",
    "        self.use_trm_recursion = use_trm_recursion\n",
    "        self.num_recursion_steps = num_recursion_steps\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Project vision tokens from 4096 -> Qwen hidden dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, self.hidden_dim)\n",
    "        \n",
    "        # Optional: TRM recursion components\n",
    "        if use_trm_recursion:\n",
    "            from decoders.trm import TRMConfig, TRMLayer, RMSNorm\n",
    "            \n",
    "            trm_config = TRMConfig(\n",
    "                vocab_size=qwen_decoder.vocab_size,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_layers=num_trm_layers,\n",
    "                num_heads=8,\n",
    "                max_seq_len=2048,\n",
    "            )\n",
    "            \n",
    "            self.trm_layers = nn.ModuleList([\n",
    "                TRMLayer(trm_config) for _ in range(num_trm_layers)\n",
    "            ])\n",
    "            self.trm_norm = RMSNorm(self.hidden_dim)\n",
    "            self.z_init = nn.Parameter(torch.randn(1, 1, self.hidden_dim) * 0.02)\n",
    "            \n",
    "            print(f\"  \u2713 TRM recursion enabled ({num_trm_layers} layers, {num_recursion_steps} steps)\")\n",
    "        else:\n",
    "            print(f\"  \u2713 Baseline mode (no TRM recursion)\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "    ):\n",
    "        \"\"\"Forward pass with proper autoregressive training.\n",
    "        \n",
    "        Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        Loss only on answer tokens via -100 masking.\n",
    "        \"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        num_img_tokens = vision_tokens.shape[1]\n",
    "        \n",
    "        # Project vision tokens to Qwen hidden dim\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d_qwen)\n",
    "        \n",
    "        # Prepare input_ids and labels for standard autoregressive training\n",
    "        # Input: [question_ids, answer_ids]\n",
    "        # Labels: [-100 for question, answer_ids]\n",
    "        \n",
    "        input_ids = torch.cat([question_ids, answer_ids], dim=1)  # (B, L_q + L_a)\n",
    "        \n",
    "        # Create labels: -100 for question, real IDs for answer\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        labels = torch.cat([question_labels, answer_ids], dim=1)  # (B, L_q + L_a)\n",
    "        \n",
    "        if not self.use_trm_recursion:\n",
    "            # ===== BASELINE MODE: Standard Qwen forward =====\n",
    "            # Use prefix_embeds for vision tokens\n",
    "            outputs = self.qwen(\n",
    "                input_ids=input_ids,\n",
    "                prefix_embeds=vision_emb,\n",
    "                labels=labels,\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'loss': outputs.loss,\n",
    "                'logits': outputs.logits,\n",
    "                'confidence': None,\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # ===== TRM MODE: Latent recursion on top of Qwen =====\n",
    "            # Get embeddings\n",
    "            text_emb = self.qwen.model.get_input_embeddings()(input_ids)  # (B, L_q+L_a, d)\n",
    "            \n",
    "            # Split into question and answer\n",
    "            L_q = question_ids.shape[1]\n",
    "            L_a = answer_ids.shape[1]\n",
    "            question_emb = text_emb[:, :L_q, :]\n",
    "            answer_emb = text_emb[:, L_q:, :]\n",
    "            \n",
    "            # Context: [vision, question]\n",
    "            x = torch.cat([vision_emb, question_emb], dim=1)  # (B, K_img + L_q, d)\n",
    "            \n",
    "            # Answer representation (to be refined by TRM)\n",
    "            y = answer_emb  # (B, L_a, d)\n",
    "            \n",
    "            # Initialize latent state\n",
    "            z = self.z_init.expand(batch_size, L_a, -1)  # (B, L_a, d)\n",
    "            \n",
    "            # Latent recursion\n",
    "            for _ in range(self.num_recursion_steps):\n",
    "                x, y, z = self.latent_recursion(x, y, z)\n",
    "            \n",
    "            # Final logits from refined y\n",
    "            y = self.trm_norm(y)\n",
    "            # Use Qwen's LM head\n",
    "            lm_head = self.qwen.model.lm_head\n",
    "            logits_answer = lm_head(y)  # (B, L_a, vocab)\n",
    "            \n",
    "            # Compute loss only on answer tokens\n",
    "            shift_logits = logits_answer[:, :-1, :].contiguous()\n",
    "            shift_labels = answer_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            # Compute confidence\n",
    "            probs = torch.softmax(logits_answer, dim=-1)\n",
    "            max_probs = torch.max(probs, dim=-1)[0]\n",
    "            confidence = torch.mean(max_probs, dim=-1)\n",
    "            \n",
    "            return {\n",
    "                'loss': loss,\n",
    "                'logits': logits_answer,\n",
    "                'confidence': confidence,\n",
    "            }\n",
    "    \n",
    "    def latent_recursion(self, x, y, z):\n",
    "        \"\"\"TRM latent recursion step.\"\"\"\n",
    "        # Concatenate [x, y, z]\n",
    "        concat = torch.cat([x, y, z], dim=1)\n",
    "        \n",
    "        # Pass through TRM layers\n",
    "        hidden = concat\n",
    "        for layer in self.trm_layers:\n",
    "            hidden = layer(hidden)\n",
    "        \n",
    "        # Split back\n",
    "        L_ctx = x.shape[1]\n",
    "        L_ans = y.shape[1]\n",
    "        \n",
    "        x_out = hidden[:, :L_ctx, :]\n",
    "        y_out = hidden[:, L_ctx:L_ctx+L_ans, :]\n",
    "        z_out = hidden[:, L_ctx+L_ans:, :]\n",
    "        \n",
    "        return x_out, y_out, z_out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "        use_confidence: bool = True,\n",
    "        return_stats: bool = False,\n",
    "    ):\n",
    "        \"\"\"Generate answers with optional TRM recursion.\"\"\"\n",
    "        # Project vision\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        \n",
    "        if not self.use_trm_recursion:\n",
    "            # Baseline: standard Qwen generation\n",
    "            outputs = self.qwen.generate(\n",
    "                input_ids=question_ids,\n",
    "                prefix_embeds=vision_emb,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=False,  # Use greedy for eval\n",
    "            )\n",
    "            \n",
    "            # Extract only the generated part (remove input)\n",
    "            prompt_len = question_ids.shape[1] + vision_emb.shape[1]\n",
    "            generated_ids = outputs[:, prompt_len:]\n",
    "            \n",
    "            if return_stats:\n",
    "                return {\n",
    "                    'predictions': generated_ids,\n",
    "                    'confidences': torch.ones(generated_ids.shape),  # Dummy\n",
    "                    'recursion_steps': torch.ones(generated_ids.shape),  # No recursion\n",
    "                }\n",
    "            return generated_ids\n",
    "        \n",
    "        else:\n",
    "            # TRM mode: autoregressive generation with recursion\n",
    "            batch_size = vision_tokens.shape[0]\n",
    "            generated_ids = []\n",
    "            confidences = []\n",
    "            recursion_steps_used = []\n",
    "            \n",
    "            for step in range(max_new_tokens):\n",
    "                # Current answer so far\n",
    "                if len(generated_ids) == 0:\n",
    "                    answer_ids = torch.tensor(\n",
    "                        [[self.qwen.tokenizer.pad_token_id]],\n",
    "                        device=vision_tokens.device\n",
    "                    ).expand(batch_size, 1)\n",
    "                else:\n",
    "                    answer_ids = torch.stack(generated_ids, dim=1)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.forward(vision_tokens, question_ids, answer_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                # Sample next token\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "                generated_ids.append(next_token)\n",
    "                \n",
    "                # Track confidence\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                conf = torch.max(probs, dim=-1)[0]\n",
    "                confidences.append(conf.cpu())\n",
    "                \n",
    "                recursion_steps_used.append(\n",
    "                    self.num_recursion_steps if self.use_trm_recursion else 1\n",
    "                )\n",
    "            \n",
    "            generated_ids = torch.stack(generated_ids, dim=1)\n",
    "            \n",
    "            if return_stats:\n",
    "                return {\n",
    "                    'predictions': generated_ids,\n",
    "                    'confidences': torch.stack(confidences, dim=1),\n",
    "                    'recursion_steps': torch.tensor(recursion_steps_used).unsqueeze(0).expand(batch_size, -1),\n",
    "                }\n",
    "            return generated_ids\n",
    "\n",
    "print(\"\u2713 QwenVLM class defined (with baseline and TRM modes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 NEW: Qwen-Based VLM Decoder (Pretrained)\n",
    "\n",
    "**CRITICAL FIX**: The original TRM decoder was randomly initialized, attempting to learn language from scratch. \n",
    "\n",
    "This new implementation uses **pretrained Qwen2.5-7B** with LoRA, giving us:\n",
    "- \u2705 Pretrained language knowledge (no need to learn grammar/vocabulary)\n",
    "- \u2705 Efficient fine-tuning with LoRA (only ~40M trainable params)\n",
    "- \u2705 Proper multimodal prefix support\n",
    "- \u2705 Standard autoregressive training with -100 label masking\n",
    "\n",
    "We'll implement:\n",
    "1. **Baseline mode**: Standard Qwen decoder without TRM recursion\n",
    "2. **TRM mode**: Qwen + TRM latent recursion on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Model Configuration:\")\n",
    "print(f\"  Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Use TRM Recursion: {USE_TRM_RECURSION}\")\n",
    "\n",
    "if USE_TRM_RECURSION:\n",
    "    print(f\"  TRM layers: {NUM_TRM_LAYERS}\")\n",
    "    print(f\"  Recursion steps: {NUM_RECURSION_STEPS}\")\n",
    "    print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Training:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Evaluation Results:\")\n",
    "print(f\"  Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"  Token F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "if all_recursion_steps and USE_TRM_RECURSION:\n",
    "    print(f\"  Avg recursion steps: {np.mean(all_recursion_steps):.2f}\")\n",
    "    print(f\"  Recursion triggered: {recursion_triggered:.1f}% of sequences\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Dataset:\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Output Files:\")\n",
    "print(f\"  Best checkpoint: {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY IMPROVEMENTS FROM ORIGINAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\u2705 1. Pretrained Qwen Decoder:\")\n",
    "print(\"   - Replaced random TRM (34M params) with Qwen-7B + LoRA\")\n",
    "print(\"   - Model now has language prior (no need to learn from scratch)\")\n",
    "print(\"   - Text-only sanity check validates decoder works\")\n",
    "print()\n",
    "print(\"\u2705 2. Proper Autoregressive Training:\")\n",
    "print(\"   - Uses prefix_embeds for vision tokens\")\n",
    "print(\"   - Standard -100 label masking for vision/question tokens\")\n",
    "print(\"   - Loss only computed on answer tokens\")\n",
    "print()\n",
    "print(\"\u2705 3. Baseline Mode:\")\n",
    "print(\"   - Can disable TRM recursion for comparison\")\n",
    "print(\"   - Isolates decoder issues from recursion issues\")\n",
    "print(\"   - Establishes performance floor\")\n",
    "print()\n",
    "print(\"\u2705 4. Debug Instrumentation:\")\n",
    "print(\"   - Text-only generation test before training\")\n",
    "print(\"   - Parameter audit (frozen vs trainable)\")\n",
    "print(\"   - First-batch debug logging\")\n",
    "print(\"   - Detailed recursion statistics\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83d\udccb NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. If EM/F1 are still low with baseline:\")\n",
    "print(\"   - Check first-batch debug output\")\n",
    "print(\"   - Verify text-only generation works\")\n",
    "print(\"   - Increase training epochs or learning rate\")\n",
    "print()\n",
    "print(\"2. If baseline works well:\")\n",
    "print(\"   - Set USE_TRM_RECURSION = True\")\n",
    "print(\"   - Compare TRM vs baseline performance\")\n",
    "print(\"   - Tune confidence threshold and recursion steps\")\n",
    "print()\n",
    "print(\"3. Expected performance:\")\n",
    "print(\"   - Baseline: EM > 15%, F1 > 25% (if Qwen works)\")\n",
    "print(\"   - With TRM: EM >= baseline (should match or exceed)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\u2713 Notebook complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_trm_vlm\",\n",
    "        name=f\"trm_vlm_d{HIDDEN_DIM}_l{NUM_LAYERS}_n{NUM_INNER_STEPS}_conf{CONFIDENCE_THRESHOLD}\",\n",
    "        config={\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'num_heads': NUM_HEADS,\n",
    "            'num_inner_steps': NUM_INNER_STEPS,\n",
    "            'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': batch_size,\n",
    "            'total_params': total_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_em': [], 'val_f1': []}\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = Path(f\"checkpoints/trm_vlm_qa\")\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answer_ids = batch['answer_ids'].to(device)\n",
    "        \n",
    "        # Encode images (frozen)\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        epoch_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % LOG_EVERY == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-LOG_EVERY:])\n",
    "            avg_conf = outputs['confidence'].mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'conf': f'{avg_conf:.3f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/confidence': avg_conf,\n",
    "                    'train/lr': scheduler.get_last_lr()[0],\n",
    "                    'step': global_step,\n",
    "                })\n",
    "    \n",
    "    # Epoch-end evaluation\n",
    "    print(f\"\\n  Epoch {epoch+1} average loss: {np.mean(epoch_losses):.4f}\")\n",
    "    history['train_loss'].append(np.mean(epoch_losses))\n",
    "    \n",
    "    # Validation (simple loss for now, full eval is expensive)\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['images'].to(device)\n",
    "            question_ids = batch['question_ids'].to(device)\n",
    "            answer_ids = batch['answer_ids'].to(device)\n",
    "            \n",
    "            vision_tokens = encode_images(images)\n",
    "            outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "            val_losses.append(outputs['loss'].item())\n",
    "            val_confidences.append(outputs['confidence'].mean().item())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_conf = np.mean(val_confidences)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"  Validation confidence: {val_conf:.3f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'val/loss': val_loss,\n",
    "            'val/confidence': val_conf,\n",
    "            'epoch': epoch + 1,\n",
    "        })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': {\n",
    "                'hidden_dim': HIDDEN_DIM,\n",
    "                'num_layers': NUM_LAYERS,\n",
    "                'num_heads': NUM_HEADS,\n",
    "                'num_inner_steps': NUM_INNER_STEPS,\n",
    "                'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            },\n",
    "        }, ckpt_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"  \u2713 Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Evaluation with EM and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(ckpt_dir / \"checkpoint_best.pt\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"\\\\nRunning full evaluation on validation set...\")\n",
    "print(\"Using confidence-based adaptive recursion during generation\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidence_scores = []\n",
    "all_recursion_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating answers\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        # Encode images\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Generate answers with confidence-based recursion\n",
    "        gen_outputs = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            use_confidence=True,  # Enable adaptive recursion\n",
    "        )\n",
    "        \n",
    "        generated_ids = gen_outputs['ids']\n",
    "        \n",
    "        # Decode\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "        all_confidence_scores.extend(gen_outputs['confidence'])\n",
    "        all_recursion_steps.append(gen_outputs['recursion_steps'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "# Analyze recursion statistics\n",
    "flat_recursion_steps = [step for batch_steps in all_recursion_steps for step in batch_steps]\n",
    "avg_recursion_steps = np.mean(flat_recursion_steps)\n",
    "recursion_triggered = sum(1 for s in flat_recursion_steps if s > 1) / len(flat_recursion_steps) * 100\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"Token F1: {metrics['f1']:.2f}%\")\n",
    "print(f\"\\\\nRecursion Statistics:\")\n",
    "print(f\"  Average steps per token: {avg_recursion_steps:.2f}\")\n",
    "print(f\"  Recursion triggered: {recursion_triggered:.1f}% of tokens\")\n",
    "print(f\"  (Threshold: {CONFIDENCE_THRESHOLD})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        'val/em': metrics['em'],\n",
    "        'val/f1': metrics['f1'],\n",
    "        'val/avg_recursion_steps': avg_recursion_steps,\n",
    "        'val/recursion_triggered_pct': recursion_triggered,\n",
    "    })\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\\\nSample predictions with recursion analysis:\")\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\\\n[{i+1}]\")\n",
    "    print(f\"  Question: {val_dataset.df.iloc[i]['question'][:60]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:60]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:60]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")\n",
    "    \n",
    "    # Show first few recursion steps for this sample\n",
    "    if i < len(all_recursion_steps):\n",
    "        sample_steps = all_recursion_steps[i][:5]  # First 5 tokens\n",
    "        print(f\"  Recursion steps (first 5 tokens): {sample_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics (if available)\n",
    "if history.get('val_em'):\n",
    "    axes[1].plot(history['val_em'], label='Exact Match', linewidth=2, marker='o')\n",
    "if history.get('val_f1'):\n",
    "    axes[1].plot(history['val_f1'], label='Token F1', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score (%)')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / \"training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {ckpt_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Complex VLM Qualitative Check\n",
    "\n",
    "Run a manual question on the complex curve plot to sanity check end-to-end VLM reasoning before the final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick VLM spot check on a complex curve image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "model.eval()\n",
    "complex_curve_path = Path(\"/home/hice1/vchopra37/scratch/projects/edge_glass/complex_curve.png\")\n",
    "raw_image = Image.open(complex_curve_path).convert(\"RGB\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.imshow(raw_image)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Complex curve prompt image\")\n",
    "\n",
    "image_tensor = val_transforms(T.ToTensor()(raw_image)).unsqueeze(0).to(device)\n",
    "vision_tokens = encode_images(image_tensor)\n",
    "\n",
    "question = \"Interpret the curve on this image. What does its shape suggest about the underlying relationship?\"\n",
    "question_ids = tokenizer(\n",
    "    question,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(device)\n",
    "\n",
    "generation = model.generate(\n",
    "    vision_tokens=vision_tokens,\n",
    "    question_ids=question_ids,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    use_confidence=True,\n",
    ")\n",
    "\n",
    "decoded_answer = tokenizer.decode(\n",
    "    generation['ids'][0].detach().cpu().tolist(),\n",
    "    skip_special_tokens=True,\n",
    ").strip()\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Predicted answer:\", decoded_answer)\n",
    "print(\"Confidence trace:\", [round(c, 3) for c in generation[\"confidence\"]])\n",
    "print(\"Recursion steps per token:\", generation[\"recursion_steps\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Complex VLM Question Probe\n",
    "\n",
    "Push a harder, multi-part interpretation query on the curve plot before the final summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex VLM question probe on the curve image\n",
    "probe_image_path = Path(\"/home/hice1/vchopra37/scratch/projects/edge_glass/complex_curve.png\")\n",
    "probe_question = (\n",
    "    \"Provide a short narrative interpretation of the plotted curve: describe its overall trend, where the slope changes,\"\n",
    "    \" and what that implies about acceleration, saturation, or decay in the underlying relationship.\"\n",
    ")\n",
    "\n",
    "probe_image = Image.open(probe_image_path).convert(\"RGB\")\n",
    "probe_tensor = val_transforms(T.ToTensor()(probe_image)).unsqueeze(0).to(device)\n",
    "probe_tokens = encode_images(probe_tensor)\n",
    "\n",
    "probe_question_ids = tokenizer(\n",
    "    probe_question, return_tensors='pt', add_special_tokens=True\n",
    ").input_ids.to(device)\n",
    "\n",
    "probe_gen = model.generate(\n",
    "    vision_tokens=probe_tokens,\n",
    "    question_ids=probe_question_ids,\n",
    "    max_new_tokens=96,\n",
    "    temperature=0.7,\n",
    "    use_confidence=True,\n",
    ")\n",
    "\n",
    "probe_answer = tokenizer.decode(\n",
    "    probe_gen['ids'][0].detach().cpu().tolist(),\n",
    "    skip_special_tokens=True,\n",
    ").strip()\n",
    "\n",
    "print(\"Question:\", probe_question)\n",
    "print(\"Answer:\", probe_answer)\n",
    "print(\"Confidence trace:\", [round(c, 3) for c in probe_gen['confidence']])\n",
    "print(\"Recursion steps per token:\", probe_gen['recursion_steps'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\\\nModel Configuration:\")\n",
    "print(f\"  Type: TRM VLM with Confidence-Based Recursive Refinement\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")\n",
    "print(f\"  Inner recursion steps: {NUM_INNER_STEPS}\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\\\nTraining:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\\\nEvaluation Results:\")\n",
    "print(f\"  Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"  Token F1: {metrics['f1']:.2f}%\")\n",
    "print(f\"  Avg recursion steps: {avg_recursion_steps:.2f}\")\n",
    "print(f\"  Recursion triggered: {recursion_triggered:.1f}% of tokens\")\n",
    "\n",
    "print(f\"\\\\nDataset:\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\\\nOutput Files:\")\n",
    "print(f\"  Best checkpoint: {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "print(f\"  Training curves: {ckpt_dir / 'training_curves.png'} (if saved)\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. TRM Recursion vs Baseline:\")\n",
    "print(\"   - TRM uses latent recursion (n inner steps) for reasoning\")\n",
    "print(\"   - Confidence-based early stopping saves computation\")\n",
    "print(\"   - Higher confidence \u2192 skip extra recursion (efficient)\")\n",
    "print(\"   - Lower confidence \u2192 trigger full recursion (quality)\")\n",
    "print()\n",
    "print(\"2. Confidence Threshold Analysis:\")\n",
    "print(f\"   - Set to {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"   - Triggered on {recursion_triggered:.1f}% of tokens\")\n",
    "print(\"   - Can tune this for speed/quality tradeoff\")\n",
    "print()\n",
    "print(\"3. Next Experiments:\")\n",
    "print(\"   - Ablation: Run with use_confidence=False (fixed recursion)\")\n",
    "print(\"   - Sweep confidence thresholds: {0.5, 0.6, 0.7, 0.8, 0.9}\")\n",
    "print(\"   - Try different inner steps: {2, 4, 6, 8}\")\n",
    "print(\"   - Add outer deep recursion (T > 1)\")\n",
    "print(\"   - Compare to baseline Tiny VLM (no TRM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\n\u2713 Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}