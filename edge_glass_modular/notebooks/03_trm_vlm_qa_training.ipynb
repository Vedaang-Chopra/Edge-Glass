{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Modular vision encoder (CLIP + Perceiver + MRL) from `edge_glass_modular/src/encoders`\n",
    "2. Qwen decoder with LoRA from `edge_glass_modular/src/decoders`\n",
    "3. PixMo QA dataset with question-answer pairs\n",
    "4. Proper modular design following the edge_glass_modular architecture\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  ‚Üì\n",
    "Vision Encoder (frozen aligned model)\n",
    "  ‚Üì (B, num_latents, hidden_dim)\n",
    "Projection to Qwen hidden dim\n",
    "  ‚Üì (B, num_latents, qwen_dim)\n",
    "Qwen Decoder with LoRA (trainable)\n",
    "  ‚Üì\n",
    "Token Layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "  ‚Üì\n",
    "Loss on answer tokens only\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- Modular design using imports from `edge_glass_modular/src`\n",
    "- Frozen aligned vision encoder\n",
    "- Qwen2.5 decoder with LoRA fine-tuning\n",
    "- Real QA dataset (not synthetic)\n",
    "- Proper configuration management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "from collections import Counter\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# Import modular components from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.dataset_builder import PixmoParquetImageTextDataset\n",
    "from data.transforms import get_image_transforms\n",
    "from models.alignment import MultimodalAlignmentModel\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "Load the experiment configuration from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: trm_vlm_qa\n",
      "\n",
      "Dataset:\n",
      "  Train parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_train.parquet\n",
      "  Val parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_val.parquet\n",
      "  Test parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_test.parquet\n",
      "  Image size: 336\n",
      "  Max question length: 128\n",
      "  Max answer length: 256\n",
      "  Batch size: 16\n",
      "\n",
      "Decoder:\n",
      "  Type: qwen\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Use LoRA: True\n",
      "  Load in 8bit: False\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train parquet: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val parquet: {config.dataset.val_parquet}\")\n",
    "print(f\"  Test parquet: {config.dataset.test_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Max question length: {config.dataset.max_question_length}\")\n",
    "print(f\"  Max answer length: {config.dataset.max_answer_length}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Type: {config.decoder.type}\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Load in 8bit: {config.decoder.load_in_8bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Aligned Vision Encoder\n",
    "\n",
    "Load the pretrained Perceiver+MRL alignment model and freeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681cc06de9f94813a467a008218ae2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "Loaded aligned model from checkpoints/pixmo_alignment/checkpoint_best.pt\n",
      "  Epoch: 0\n",
      "  Val loss: 0.0000\n",
      "  Vision encoder output: (B, 64, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Load alignment config\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "\n",
    "# Load aligned model\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"checkpoints/pixmo_alignment/checkpoint_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "aligned_model.eval()\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in aligned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Loaded aligned model from {checkpoint_path}\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"  Vision encoder output: (B, 64, 4096)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Vision Encoder Method\n",
    "\n",
    "Create a clean interface to get vision embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision tokens shape: torch.Size([2, 577, 4096])\n",
      "Expected: (2, 577, 4096)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Encode images to vision tokens.\n",
    "    \n",
    "    Args:\n",
    "        images: (B, 3, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        vision_tokens: (B, num_latents, 4096)\n",
    "    \"\"\"\n",
    "    vision_output = aligned_model.vision_encoder(images, return_sequence=True)\n",
    "    if vision_output.sequence is None:\n",
    "        raise ValueError(\"Vision encoder did not return sequence embeddings\")\n",
    "    # Get sequence output (B, num_latents, dim)\n",
    "    return vision_output.sequence\n",
    "\n",
    "# Test\n",
    "test_img = torch.randn(2, 3, 336, 336).to(device)\n",
    "test_vision_tokens = encode_images(test_img)\n",
    "print(f\"Vision tokens shape: {test_vision_tokens.shape}\")\n",
    "if getattr(aligned_model.vision_encoder, \"use_perceiver\", False):\n",
    "    expected_tokens = aligned_model.config.vision_encoder.perceiver_num_latents\n",
    "else:\n",
    "    expected_tokens = test_vision_tokens.shape[1]\n",
    "print(f\"Expected: ({test_img.shape[0]}, {expected_tokens}, {aligned_model.config.vision_encoder.projection_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Plain Tiny Decoder Baseline\n",
    "\n",
    "First, implement a simple baseline decoder without TRM recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoders.trm import TRMConfig, TRMDecoder\n",
    "\n",
    "class TinyVLMDecoder(nn.Module):\n",
    "    \"\"\"Plain tiny decoder baseline for VLM.\n",
    "    \n",
    "    Architecture:\n",
    "        - Projects vision tokens from 4096 -> d_dec\n",
    "        - Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        - Causal masking\n",
    "        - Loss only on answer tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        vision_token_dim: int = 4096,\n",
    "        max_seq_len: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vision_token_dim = vision_token_dim\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # TRM decoder\n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.decoder = TRMDecoder(trm_config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "    ):\n",
    "        \"\"\"Forward pass with proper token layout and loss masking.\n",
    "        \n",
    "        Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        Loss only on answer tokens.\n",
    "        \"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        num_img_tokens = vision_tokens.shape[1]\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d_dec)\n",
    "        \n",
    "        # Embed question and answer tokens\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)  # (B, L_q, d_dec)\n",
    "        answer_emb = self.decoder.embed_tokens(answer_ids)      # (B, L_a, d_dec)\n",
    "        \n",
    "        # Concatenate: [vision | question | answer]\n",
    "        full_sequence = torch.cat([vision_emb, question_emb, answer_emb], dim=1)\n",
    "        \n",
    "        # Create labels: -100 for image and question tokens, actual IDs for answer\n",
    "        img_labels = torch.full(\n",
    "            (batch_size, num_img_tokens),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=vision_tokens.device\n",
    "        )\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        answer_labels = answer_ids\n",
    "        \n",
    "        # Concatenate labels\n",
    "        full_labels = torch.cat([img_labels, question_labels, answer_labels], dim=1)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        hidden_states = full_sequence\n",
    "        for layer in self.decoder.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        \n",
    "        hidden_states = self.decoder.norm(hidden_states)\n",
    "        logits = self.decoder.lm_head(hidden_states)\n",
    "        \n",
    "        # Compute loss (shift for next-token prediction)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = full_labels[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"Generate answer tokens autoregressively.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Project vision\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)\n",
    "        \n",
    "        # Start with image + question\n",
    "        current_emb = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        generated_ids = []\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            hidden = current_emb\n",
    "            for layer in self.decoder.layers:\n",
    "                hidden = layer(hidden)\n",
    "            hidden = self.decoder.norm(hidden)\n",
    "            logits = self.decoder.lm_head(hidden)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            # Embed and append\n",
    "            next_emb = self.decoder.embed_tokens(next_token.unsqueeze(1))\n",
    "            current_emb = torch.cat([current_emb, next_emb], dim=1)\n",
    "        \n",
    "        return torch.stack(generated_ids, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement TRM-Style Recursive Decoder\n",
    "\n",
    "Now implement the TRM version with latent recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRM VLM with Confidence-Based Recursion defined.\n"
     ]
    }
   ],
   "source": [
    "class TRMVLMWithConfidence(nn.Module):\n",
    "    \"\"\"TRM VLM decoder with confidence-based recursive refinement.\n",
    "    \n",
    "    Key features:\n",
    "    - Uses aligned vision encoder (frozen)\n",
    "    - Projects vision tokens to TRM hidden dim\n",
    "    - Implements latent recursion for reasoning\n",
    "    - Confidence-based early stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        vision_token_dim: int = 4096,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        num_heads: int = 8,\n",
    "        num_inner_steps: int = 4,\n",
    "        confidence_threshold: float = 0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Tiny transformer for recursion (use TRM components)\n",
    "        from decoders.trm import TRMConfig, TRMLayer\n",
    "        \n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=1024,\n",
    "        )\n",
    "        \n",
    "        self.tiny_transformer = nn.ModuleList([\n",
    "            TRMLayer(trm_config) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        from decoders.trm import RMSNorm\n",
    "        self.norm = RMSNorm(hidden_dim)\n",
    "        \n",
    "        # LM head\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed_tokens.weight  # Tie weights\n",
    "        \n",
    "        # Learned initial z state\n",
    "        self.z_init = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)\n",
    "    \n",
    "    def latent_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,  # Context: (B, L_ctx, d)\n",
    "        y: torch.Tensor,  # Answer: (B, L_ans, d)\n",
    "        z: torch.Tensor,  # Latent: (B, L_ans, d)\n",
    "    ):\n",
    "        \"\"\"Single step of latent recursion.\"\"\"\n",
    "        # Concatenate along sequence: [x, y, z]\n",
    "        concat = torch.cat([x, y, z], dim=1)  # (B, L_ctx + 2*L_ans, d)\n",
    "        \n",
    "        # Pass through tiny transformer\n",
    "        hidden = concat\n",
    "        for layer in self.tiny_transformer:\n",
    "            hidden = layer(hidden)\n",
    "        \n",
    "        # Split back\n",
    "        L_ctx = x.shape[1]\n",
    "        L_ans = y.shape[1]\n",
    "        \n",
    "        x_out = hidden[:, :L_ctx, :]\n",
    "        y_out = hidden[:, L_ctx:L_ctx+L_ans, :]\n",
    "        z_out = hidden[:, L_ctx+L_ans:, :]\n",
    "        \n",
    "        return x_out, y_out, z_out\n",
    "    \n",
    "    def compute_confidence(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute confidence score from logits.\n",
    "        \n",
    "        Args:\n",
    "            logits: (B, L, vocab_size)\n",
    "        \n",
    "        Returns:\n",
    "            confidence: (B,) - mean max softmax probability\n",
    "        \"\"\"\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        max_probs = torch.max(probs, dim=-1)[0]  # (B, L)\n",
    "        confidence = torch.mean(max_probs, dim=-1)  # (B,)\n",
    "        return confidence\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "        num_recursion_steps: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Forward pass with TRM recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        L_ans = answer_ids.shape[0]\n",
    "        \n",
    "        # Use default or override recursion steps\n",
    "        n_steps = num_recursion_steps if num_recursion_steps is not None else self.num_inner_steps\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d)\n",
    "        \n",
    "        # Embed question\n",
    "        question_emb = self.embed_tokens(question_ids)  # (B, L_q, d)\n",
    "        \n",
    "        # Context x = [vision | question]\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)  # (B, L_ctx, d)\n",
    "        \n",
    "        # Teacher-forced answer embeddings\n",
    "        y = self.embed_tokens(answer_ids)  # (B, L_ans, d)\n",
    "        \n",
    "        # Initialize latent z\n",
    "        z = self.z_init.expand(batch_size, answer_ids.shape[1], -1)  # (B, L_ans, d)\n",
    "        \n",
    "        # Inner recursion (n steps)\n",
    "        for _ in range(n_steps):\n",
    "            x, y, z = self.latent_recursion(x, y, z)\n",
    "        \n",
    "        # Final answer from y\n",
    "        y = self.norm(y)\n",
    "        logits = self.lm_head(y)  # (B, L_ans, vocab_size)\n",
    "        \n",
    "        # Compute loss (standard next-token prediction on answer)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = answer_ids[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        # Compute confidence\n",
    "        confidence = self.compute_confidence(logits)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'confidence': confidence,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "        use_confidence: bool = True,\n",
    "    ):\n",
    "        \"\"\"Generate answer with confidence-based recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Context\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.embed_tokens(question_ids)\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        \n",
    "        # Start with empty answer\n",
    "        generated_ids = []\n",
    "        confidence_scores = []\n",
    "        recursion_steps_used = []\n",
    "        \n",
    "        # Generate autoregressively\n",
    "        for step in range(max_new_tokens):\n",
    "            # Current y from generated so far\n",
    "            if len(generated_ids) == 0:\n",
    "                # First token: use learned blank\n",
    "                y = torch.zeros(batch_size, 1, self.hidden_dim, device=x.device)\n",
    "                z = self.z_init.expand(batch_size, 1, -1)\n",
    "            else:\n",
    "                y = self.embed_tokens(torch.stack(generated_ids, dim=1))\n",
    "                z = self.z_init.expand(batch_size, len(generated_ids), -1)\n",
    "            \n",
    "            # Adaptive recursion based on confidence\n",
    "            if use_confidence and len(generated_ids) > 0:\n",
    "                # Run once to check confidence\n",
    "                x_temp, y_temp, z_temp = self.latent_recursion(x, y, z)\n",
    "                y_normed = self.norm(y_temp)\n",
    "                logits_temp = self.lm_head(y_normed[:, -1, :])\n",
    "                conf = self.compute_confidence(logits_temp.unsqueeze(1))\n",
    "                \n",
    "                if conf.mean() >= self.confidence_threshold:\n",
    "                    # High confidence - use result\n",
    "                    y = y_temp\n",
    "                    num_steps = 1\n",
    "                else:\n",
    "                    # Low confidence - run more recursion\n",
    "                    y = y_temp\n",
    "                    z = z_temp\n",
    "                    for _ in range(self.num_inner_steps - 1):\n",
    "                        x_temp, y, z = self.latent_recursion(x, y, z)\n",
    "                    num_steps = self.num_inner_steps\n",
    "                \n",
    "                recursion_steps_used.append(num_steps)\n",
    "            else:\n",
    "                # Fixed recursion\n",
    "                for _ in range(self.num_inner_steps):\n",
    "                    x_temp, y, z = self.latent_recursion(x, y, z)\n",
    "                recursion_steps_used.append(self.num_inner_steps)\n",
    "            \n",
    "            # Get logits for last position\n",
    "            y = self.norm(y)\n",
    "            logits = self.lm_head(y[:, -1, :]) / temperature\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Track confidence\n",
    "            conf = self.compute_confidence(logits.unsqueeze(1))\n",
    "            confidence_scores.append(conf.mean().item())\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            # Stop at EOS (optional)\n",
    "            # if (next_token == tokenizer.eos_token_id).all():\n",
    "            #     break\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.stack(generated_ids, dim=1),\n",
    "            'confidence': confidence_scores,\n",
    "            'recursion_steps': recursion_steps_used,\n",
    "        }\n",
    "\n",
    "print(\"TRM VLM with Confidence-Based Recursion defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics (EM and F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer text for evaluation.\"\"\"\n",
    "    # Remove punctuation\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    # Lowercase and strip\n",
    "    s = s.lower().strip()\n",
    "    # Remove articles\n",
    "    s = ' '.join([w for w in s.split() if w not in {'a', 'an', 'the'}])\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return float(normalize_answer(pred) == normalize_answer(target))\n",
    "\n",
    "def compute_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score.\"\"\"\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    target_tokens = normalize_answer(target).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
    "        return float(pred_tokens == target_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(target_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(target_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def evaluate_qa(\n",
    "    predictions: List[str],\n",
    "    targets: List[str],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate QA predictions.\"\"\"\n",
    "    em_scores = [compute_exact_match(p, t) for p, t in zip(predictions, targets)]\n",
    "    f1_scores = [compute_f1(p, t) for p, t in zip(predictions, targets)]\n",
    "    \n",
    "    return {\n",
    "        'em': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50257\n",
      "Pad token: <|endoftext|>\n",
      "EOS token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer (use GPT2 tokenizer)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "# Get transforms\n",
    "train_transforms = get_image_transforms(image_size=336, is_training=True)\n",
    "val_transforms = get_image_transforms(image_size=336, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8400 samples from /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_train.parquet\n",
      "Loaded 1800 samples from /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo_alignment/pixmo_qa_mixed_val.parquet\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 8,400\n",
      "  Val: 1,800\n",
      "\n",
      "DataLoader info:\n",
      "  Batch size: 32\n",
      "  Train batches: 263\n",
      "  Val batches: 57\n"
     ]
    }
   ],
   "source": [
    "# Import QA dataset\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "\n",
    "# Define collate function for QA batches\n",
    "def collate_qa_batch(batch):\n",
    "    \"\"\"Collate QA batch with padding.\"\"\"\n",
    "    # Stack images\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    \n",
    "    # Pad question_ids\n",
    "    question_ids = [item['question_ids'] for item in batch]\n",
    "    max_q_len = max(q.shape[0] for q in question_ids)\n",
    "    question_ids_padded = torch.stack([\n",
    "        torch.cat([q, torch.full((max_q_len - q.shape[0],), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "        for q in question_ids\n",
    "    ])\n",
    "    \n",
    "    # Pad answer_ids\n",
    "    answer_ids = [item['answer_ids'] for item in batch]\n",
    "    max_a_len = max(a.shape[0] for a in answer_ids)\n",
    "    answer_ids_padded = torch.stack([\n",
    "        torch.cat([a, torch.full((max_a_len - a.shape[0],), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "        for a in answer_ids\n",
    "    ])\n",
    "    \n",
    "    # Get raw text\n",
    "    questions = [item['question'] for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'question_ids': question_ids_padded,\n",
    "        'answer_ids': answer_ids_padded,\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.train_parquet,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=train_transforms,\n",
    "    max_question_length=128,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "val_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.val_parquet,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=val_transforms,\n",
    "    max_question_length=128,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val: {len(val_dataset):,}\")\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch contents:\n",
      "  Images: torch.Size([32, 3, 336, 336])\n",
      "  Question IDs: torch.Size([32, 128])\n",
      "  Answer IDs: torch.Size([32, 32])\n",
      "\n",
      "Sample QA pairs:\n",
      "\n",
      "  [1]\n",
      "    Q: [USER]What's the main object or surface shown in this image?\n",
      "[ASSISTANT]Based on the image, the main object or surface appears to be a textured, gray material with raised bumps covering its surface. It's difficult to determine the exact nature of this object, but it seems to be a vertical rectangular surface made of either fabric or plastic. The texture is characterized by numerous raised bumps of varying sizes, creating a consistent pattern across the visible area. The material has different shades of gray, with the raised bumps appearing slightly lighter than the base surface, providing contrast. The overall appearance suggests it could be some kind of upholstery, wall covering, or specialized textured surface, though its exact purpose is not clear from this image alone.\n",
      "[USER]Can you describe any unusual features or anomalies you notice on that surface?\n",
      "[ASSISTANT]\n",
      "    A: Upon closer inspection of the textured surface, there is indeed an unusual featu...\n",
      "\n",
      "  [2]\n",
      "    Q: [USER]What's the dominant color in this natural scene? Provide your answer as a single emoji that best represents the color.\n",
      "[ASSISTANT]\n",
      "    A: üî¥...\n",
      "\n",
      "  [3]\n",
      "    Q: [USER]What type of business is advertised in the upper left corner of the image? Provide your answer as a single emoji that best represents the business.\n",
      "[ASSISTANT]\n",
      "    A: üçΩÔ∏è...\n"
     ]
    }
   ],
   "source": [
    "# Get sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  Images: {sample_batch['images'].shape}\")\n",
    "print(f\"  Question IDs: {sample_batch['question_ids'].shape}\")\n",
    "print(f\"  Answer IDs: {sample_batch['answer_ids'].shape}\")\n",
    "\n",
    "print(\"\\nSample QA pairs:\")\n",
    "for i in range(min(3, len(sample_batch['questions']))):\n",
    "    print(f\"\\n  [{i+1}]\")\n",
    "    print(f\"    Q: {sample_batch['questions'][i]}\")\n",
    "    print(f\"    A: {sample_batch['answers'][i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Decoder Model\n",
    "\n",
    "Choose between TinyVLMDecoder (baseline) or TRMVLMDecoder (recursive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TRM VLM Decoder with Confidence-Based Recursion\n",
      "\n",
      "Model parameters:\n",
      "  Total: 34,128,896\n",
      "  Trainable: 34,128,896\n",
      "  Hidden dim: 512\n",
      "  Num layers: 2\n",
      "  Num heads: 8\n",
      "  Inner recursion steps: 4\n",
      "  Confidence threshold: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "USE_TRM = True  # Always use TRM for this notebook\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2  # Small for TRM\n",
    "NUM_HEADS = 8\n",
    "NUM_INNER_STEPS = 4\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "\n",
    "# Get vision token dimension from aligned model\n",
    "vision_token_dim = aligned_model.vision_encoder.projector.out_features  # Should be 4096\n",
    "\n",
    "# Initialize TRM VLM model\n",
    "model = TRMVLMWithConfidence(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    vision_token_dim=vision_token_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_inner_steps=NUM_INNER_STEPS,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Initialized TRM VLM Decoder with Confidence-Based Recursion\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")\n",
    "print(f\"  Inner recursion steps: {NUM_INNER_STEPS}\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.0001\n",
      "  Total steps: 2630\n",
      "  Warmup steps: 131\n",
      "  Eval every: 100 steps\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EVAL_EVERY = 100\n",
    "LOG_EVERY = 20\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Eval every: {EVAL_EVERY} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/notebooks/wandb/run-20251204_192352-5bo4xcay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/5bo4xcay' target=\"_blank\">trm_vlm_d512_l2_n4_conf0.75</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/5bo4xcay' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/5bo4xcay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6908969c30d449d941d7c293d56269f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 1 average loss: 50.8349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0708ba082a497ab122f79e4bb27e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 22.7429\n",
      "  Validation confidence: 0.328\n",
      "  ‚úì Saved best checkpoint (val_loss: 22.7429)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c635f787dcb4ee696643be9ec0adae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 2 average loss: 19.5548\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b176f191e5334064a45971612da6ab50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 16.3893\n",
      "  Validation confidence: 0.323\n",
      "  ‚úì Saved best checkpoint (val_loss: 16.3893)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9555629ea44830be49efa444b7a2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 3 average loss: 14.3024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60034a08f0574b858b1f8223b61aec92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 12.4388\n",
      "  Validation confidence: 0.350\n",
      "  ‚úì Saved best checkpoint (val_loss: 12.4388)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5b3a378176436faa7ed31987e6c4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 4 average loss: 11.1532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e78d64c8eb4f2fb0c0ec9016fc0b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 10.1746\n",
      "  Validation confidence: 0.329\n",
      "  ‚úì Saved best checkpoint (val_loss: 10.1746)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2d18d9b9504eb49d946cc0bd45bac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 5 average loss: 9.3911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731aa863bfe94bc1b69a3f537e94dd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 8.9236\n",
      "  Validation confidence: 0.339\n",
      "  ‚úì Saved best checkpoint (val_loss: 8.9236)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5fa146a71437a8c3c47462968b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 6 average loss: 8.3973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9752c8c18b474d60be0cfffe6f2a1899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 8.1839\n",
      "  Validation confidence: 0.330\n",
      "  ‚úì Saved best checkpoint (val_loss: 8.1839)\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ad5544ad374a3a997ab17df9d8f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 7 average loss: 7.8164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e7692c9369463584bd0742d393c483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 7.7766\n",
      "  Validation confidence: 0.343\n",
      "  ‚úì Saved best checkpoint (val_loss: 7.7766)\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1100824bf930477f94e12b2e388e3d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 8 average loss: 7.4674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628781d67ef946b9b0286e99155cf05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 7.5206\n",
      "  Validation confidence: 0.337\n",
      "  ‚úì Saved best checkpoint (val_loss: 7.5206)\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff3a09283a485eb758a14db24f70f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 9 average loss: 7.2587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd276245f434a6daa8f886878e81cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 7.3709\n",
      "  Validation confidence: 0.334\n",
      "  ‚úì Saved best checkpoint (val_loss: 7.3709)\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddc35c64ec84d9793699bf1b4b4d1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe85a60180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 10 average loss: 7.1287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38de39bebd44bb0bc51275b7309539f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 7.2763\n",
      "  Validation confidence: 0.335\n",
      "  ‚úì Saved best checkpoint (val_loss: 7.2763)\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best validation loss: 7.2763\n"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_trm_vlm\",\n",
    "        name=f\"trm_vlm_d{HIDDEN_DIM}_l{NUM_LAYERS}_n{NUM_INNER_STEPS}_conf{CONFIDENCE_THRESHOLD}\",\n",
    "        config={\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'num_heads': NUM_HEADS,\n",
    "            'num_inner_steps': NUM_INNER_STEPS,\n",
    "            'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': batch_size,\n",
    "            'total_params': total_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_em': [], 'val_f1': []}\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = Path(f\"checkpoints/trm_vlm_qa\")\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answer_ids = batch['answer_ids'].to(device)\n",
    "        \n",
    "        # Encode images (frozen)\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        epoch_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % LOG_EVERY == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-LOG_EVERY:])\n",
    "            avg_conf = outputs['confidence'].mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'conf': f'{avg_conf:.3f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/confidence': avg_conf,\n",
    "                    'train/lr': scheduler.get_last_lr()[0],\n",
    "                    'step': global_step,\n",
    "                })\n",
    "    \n",
    "    # Epoch-end evaluation\n",
    "    print(f\"\\n  Epoch {epoch+1} average loss: {np.mean(epoch_losses):.4f}\")\n",
    "    history['train_loss'].append(np.mean(epoch_losses))\n",
    "    \n",
    "    # Validation (simple loss for now, full eval is expensive)\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['images'].to(device)\n",
    "            question_ids = batch['question_ids'].to(device)\n",
    "            answer_ids = batch['answer_ids'].to(device)\n",
    "            \n",
    "            vision_tokens = encode_images(images)\n",
    "            outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "            val_losses.append(outputs['loss'].item())\n",
    "            val_confidences.append(outputs['confidence'].mean().item())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_conf = np.mean(val_confidences)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"  Validation confidence: {val_conf:.3f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'val/loss': val_loss,\n",
    "            'val/confidence': val_conf,\n",
    "            'epoch': epoch + 1,\n",
    "        })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': {\n",
    "                'hidden_dim': HIDDEN_DIM,\n",
    "                'num_layers': NUM_LAYERS,\n",
    "                'num_heads': NUM_HEADS,\n",
    "                'num_inner_steps': NUM_INNER_STEPS,\n",
    "                'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            },\n",
    "        }, ckpt_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"  ‚úì Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Evaluation with EM and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning full evaluation on validation set...\n",
      "Using confidence-based adaptive recursion during generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057895bb432d48248f10a9803d213ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating answers:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Exact Match (EM): 0.00%\n",
      "Token F1: 2.11%\n",
      "\\nRecursion Statistics:\n",
      "  Average steps per token: 1.10\n",
      "  Recursion triggered: 3.2% of tokens\n",
      "  (Threshold: 0.75)\n",
      "============================================================\n",
      "\\nSample predictions with recursion analysis:\n",
      "\\n[1]\n",
      "  Question: [USER]What breed of dog is shown in the image, and how does ...\n",
      "  Target: Breed: Beagle\n",
      "Emotion: Guilty or anxious...\n",
      "  Predicted: 'sijing of these these these these these these these these t...\n",
      "  EM: 0.0\n",
      "  F1: 0.000\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[2]\n",
      "  Question: [USER]What's the main focus of this image?\n",
      "[ASSISTANT]...\n",
      "  Target: The main focus of this image appears to be metal rails. They...\n",
      "  Predicted:  the Ratt of these these these these these these these these...\n",
      "  EM: 0.0\n",
      "  F1: 0.038\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[3]\n",
      "  Question: Where is this image located?...\n",
      "  Target: This image is located in Castel del Giudice, a small town in...\n",
      "  Predicted:  are Saud of these these these these these these these these...\n",
      "  EM: 0.0\n",
      "  F1: 0.020\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[4]\n",
      "  Question: [USER]Estimate the number of ice cubes in the ice chest. Fir...\n",
      "  Target: To estimate the number of ice cubes:\n",
      "\n",
      "1. Estimate ice chest ...\n",
      "  Predicted: ijing of these these these these these these these these the...\n",
      "  EM: 0.0\n",
      "  F1: 0.018\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[5]\n",
      "  Question: [USER]Is there a corgi in the image?\n",
      "[ASSISTANT]...\n",
      "  Target: Yes. The image clearly shows a corgi dog, which is the main ...\n",
      "  Predicted:  the Ratt of these these these these these these these these...\n",
      "  EM: 0.0\n",
      "  F1: 0.047\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[6]\n",
      "  Question: [USER]What do you see in this image?\n",
      "[ASSISTANT]...\n",
      "  Target: This image appears to be a four-panel meme format based on a...\n",
      "  Predicted: 'srior of these these these these these these these these th...\n",
      "  EM: 0.0\n",
      "  F1: 0.017\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[7]\n",
      "  Question: [USER]Can you describe what's in this image using bullet poi...\n",
      "  Target: Side handle, black hue\n",
      "Adjusts clipper depth with care\n",
      "Fine-...\n",
      "  Predicted:  the Ratt of these these these these these these these these...\n",
      "  EM: 0.0\n",
      "  F1: 0.000\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[8]\n",
      "  Question: [USER]What species of bird is shown in this image? Please fo...\n",
      "  Target: I've carefully considered the bird's features as described, ...\n",
      "  Predicted:  with some some some some some some some some some some some...\n",
      "  EM: 0.0\n",
      "  F1: 0.022\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[9]\n",
      "  Question: [USER]What do you see in this image?\n",
      "[ASSISTANT]I see two im...\n",
      "  Target: In the top image, it's difficult to say for certain due to t...\n",
      "  Predicted: ! COR that this duct! COR that this duct! COR that this duct...\n",
      "  EM: 0.0\n",
      "  F1: 0.066\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n",
      "\\n[10]\n",
      "  Question: [USER]Describe the two figures depicted in the circular port...\n",
      "  Target: OBSERVE: Woman pointing at ear, three-bun hairstyle, facing ...\n",
      "  Predicted:  theMany. criticize'sijing of these these these these these ...\n",
      "  EM: 0.0\n",
      "  F1: 0.000\n",
      "  Recursion steps (first 5 tokens): [4, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(ckpt_dir / \"checkpoint_best.pt\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"\\\\nRunning full evaluation on validation set...\")\n",
    "print(\"Using confidence-based adaptive recursion during generation\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidence_scores = []\n",
    "all_recursion_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating answers\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        # Encode images\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Generate answers with confidence-based recursion\n",
    "        gen_outputs = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            use_confidence=True,  # Enable adaptive recursion\n",
    "        )\n",
    "        \n",
    "        generated_ids = gen_outputs['ids']\n",
    "        \n",
    "        # Decode\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "        all_confidence_scores.extend(gen_outputs['confidence'])\n",
    "        all_recursion_steps.append(gen_outputs['recursion_steps'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "# Analyze recursion statistics\n",
    "flat_recursion_steps = [step for batch_steps in all_recursion_steps for step in batch_steps]\n",
    "avg_recursion_steps = np.mean(flat_recursion_steps)\n",
    "recursion_triggered = sum(1 for s in flat_recursion_steps if s > 1) / len(flat_recursion_steps) * 100\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"Token F1: {metrics['f1']:.2f}%\")\n",
    "print(f\"\\\\nRecursion Statistics:\")\n",
    "print(f\"  Average steps per token: {avg_recursion_steps:.2f}\")\n",
    "print(f\"  Recursion triggered: {recursion_triggered:.1f}% of tokens\")\n",
    "print(f\"  (Threshold: {CONFIDENCE_THRESHOLD})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        'val/em': metrics['em'],\n",
    "        'val/f1': metrics['f1'],\n",
    "        'val/avg_recursion_steps': avg_recursion_steps,\n",
    "        'val/recursion_triggered_pct': recursion_triggered,\n",
    "    })\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\\\nSample predictions with recursion analysis:\")\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\\\n[{i+1}]\")\n",
    "    print(f\"  Question: {val_dataset.df.iloc[i]['question'][:60]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:60]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:60]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")\n",
    "    \n",
    "    # Show first few recursion steps for this sample\n",
    "    if i < len(all_recursion_steps):\n",
    "        sample_steps = all_recursion_steps[i][:5]  # First 5 tokens\n",
    "        print(f\"  Recursion steps (first 5 tokens): {sample_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         nonzero_params += (p.data != \u001b[32m0\u001b[39m).sum().item()\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total float params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, non-zero: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnonzero_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m debug_print_param_stats(\u001b[43mdecoder\u001b[49m, prefix=\u001b[33m\"\u001b[39m\u001b[33mDecoder\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m debug_print_param_stats(lm_head, prefix=\u001b[33m\"\u001b[39m\u001b[33mLM head\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/models/tmp/ipykernel_1042666/2045897119.py:20: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axes[1].legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training curves saved to checkpoints/trm_vlm_qa/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics (if available)\n",
    "if history.get('val_em'):\n",
    "    axes[1].plot(history['val_em'], label='Exact Match', linewidth=2, marker='o')\n",
    "if history.get('val_f1'):\n",
    "    axes[1].plot(history['val_f1'], label='Token F1', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score (%)')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / \"training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {ckpt_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\\\nModel Configuration:\")\n",
    "print(f\"  Type: TRM VLM with Confidence-Based Recursive Refinement\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")\n",
    "print(f\"  Inner recursion steps: {NUM_INNER_STEPS}\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\\\nTraining:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\\\nEvaluation Results:\")\n",
    "print(f\"  Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"  Token F1: {metrics['f1']:.2f}%\")\n",
    "print(f\"  Avg recursion steps: {avg_recursion_steps:.2f}\")\n",
    "print(f\"  Recursion triggered: {recursion_triggered:.1f}% of tokens\")\n",
    "\n",
    "print(f\"\\\\nDataset:\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\\\nOutput Files:\")\n",
    "print(f\"  Best checkpoint: {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "print(f\"  Training curves: {ckpt_dir / 'training_curves.png'} (if saved)\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. TRM Recursion vs Baseline:\")\n",
    "print(\"   - TRM uses latent recursion (n inner steps) for reasoning\")\n",
    "print(\"   - Confidence-based early stopping saves computation\")\n",
    "print(\"   - Higher confidence ‚Üí skip extra recursion (efficient)\")\n",
    "print(\"   - Lower confidence ‚Üí trigger full recursion (quality)\")\n",
    "print()\n",
    "print(\"2. Confidence Threshold Analysis:\")\n",
    "print(f\"   - Set to {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"   - Triggered on {recursion_triggered:.1f}% of tokens\")\n",
    "print(\"   - Can tune this for speed/quality tradeoff\")\n",
    "print()\n",
    "print(\"3. Next Experiments:\")\n",
    "print(\"   - Ablation: Run with use_confidence=False (fixed recursion)\")\n",
    "print(\"   - Sweep confidence thresholds: {0.5, 0.6, 0.7, 0.8, 0.9}\")\n",
    "print(\"   - Try different inner steps: {2, 4, 6, 8}\")\n",
    "print(\"   - Add outer deep recursion (T > 1)\")\n",
    "print(\"   - Compare to baseline Tiny VLM (no TRM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\n‚úì Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
