{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Modular vision encoder (CLIP + Perceiver + MRL) from `edge_glass_modular/src/encoders`\n",
    "2. Qwen decoder with LoRA from `edge_glass_modular/src/decoders`\n",
    "3. PixMo QA dataset with question-answer pairs\n",
    "4. Proper modular design following the edge_glass_modular architecture\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  ↓\n",
    "Vision Encoder (frozen aligned model)\n",
    "  ↓ (B, num_latents, hidden_dim)\n",
    "Projection to Qwen hidden dim\n",
    "  ↓ (B, num_latents, qwen_dim)\n",
    "Qwen Decoder with LoRA (trainable)\n",
    "  ↓\n",
    "Token Layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "  ↓\n",
    "Loss on answer tokens only\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- Modular design using imports from `edge_glass_modular/src`\n",
    "- Frozen aligned vision encoder\n",
    "- Qwen2.5 decoder with LoRA fine-tuning\n",
    "- Real QA dataset (not synthetic)\n",
    "- Proper configuration management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "import warnings\n",
    "\n",
    "# Import modular components from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "from data.transforms import get_image_transforms\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "Load the experiment configuration from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train parquet: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val parquet: {config.dataset.val_parquet}\")\n",
    "print(f\"  Test parquet: {config.dataset.test_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Max question length: {config.dataset.max_question_length}\")\n",
    "print(f\"  Max answer length: {config.dataset.max_answer_length}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Type: {config.decoder.type}\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Load in 8bit: {config.decoder.load_in_8bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Aligned Vision Encoder\n",
    "\n",
    "Load the pretrained Perceiver+MRL alignment model and freeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment config\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "\n",
    "# Load aligned model\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"checkpoints/pixmo_alignment/checkpoint_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "aligned_model.eval()\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in aligned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Loaded aligned model from {checkpoint_path}\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"  Vision encoder output: (B, 64, 4096)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Vision Encoder Method\n",
    "\n",
    "Create a clean interface to get vision embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Encode images to vision tokens.\n",
    "    \n",
    "    Args:\n",
    "        images: (B, 3, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        vision_tokens: (B, num_latents, 4096)\n",
    "    \"\"\"\n",
    "    vision_output = aligned_model.vision_encoder(images)\n",
    "    # Get sequence output (B, num_latents, dim)\n",
    "    return vision_output.sequence\n",
    "\n",
    "# Test\n",
    "test_img = torch.randn(2, 3, 336, 336).to(device)\n",
    "test_vision_tokens = encode_images(test_img)\n",
    "print(f\"Vision tokens shape: {test_vision_tokens.shape}\")\n",
    "print(f\"Expected: (2, 64, 4096)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Plain Tiny Decoder Baseline\n",
    "\n",
    "First, implement a simple baseline decoder without TRM recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoders.trm import TRMConfig, TRMDecoder\n",
    "\n",
    "class TinyVLMDecoder(nn.Module):\n",
    "    \"\"\"Plain tiny decoder baseline for VLM.\n",
    "    \n",
    "    Architecture:\n",
    "        - Projects vision tokens from 4096 -> d_dec\n",
    "        - Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        - Causal masking\n",
    "        - Loss only on answer tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        vision_token_dim: int = 4096,\n",
    "        max_seq_len: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vision_token_dim = vision_token_dim\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # TRM decoder\n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.decoder = TRMDecoder(trm_config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "    ):\n",
    "        \"\"\"Forward pass with proper token layout and loss masking.\n",
    "        \n",
    "        Token layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "        Loss only on answer tokens.\n",
    "        \"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        num_img_tokens = vision_tokens.shape[1]\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d_dec)\n",
    "        \n",
    "        # Embed question and answer tokens\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)  # (B, L_q, d_dec)\n",
    "        answer_emb = self.decoder.embed_tokens(answer_ids)      # (B, L_a, d_dec)\n",
    "        \n",
    "        # Concatenate: [vision | question | answer]\n",
    "        full_sequence = torch.cat([vision_emb, question_emb, answer_emb], dim=1)\n",
    "        \n",
    "        # Create labels: -100 for image and question tokens, actual IDs for answer\n",
    "        img_labels = torch.full(\n",
    "            (batch_size, num_img_tokens),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=vision_tokens.device\n",
    "        )\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        answer_labels = answer_ids\n",
    "        \n",
    "        # Concatenate labels\n",
    "        full_labels = torch.cat([img_labels, question_labels, answer_labels], dim=1)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        hidden_states = full_sequence\n",
    "        for layer in self.decoder.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        \n",
    "        hidden_states = self.decoder.norm(hidden_states)\n",
    "        logits = self.decoder.lm_head(hidden_states)\n",
    "        \n",
    "        # Compute loss (shift for next-token prediction)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = full_labels[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"Generate answer tokens autoregressively.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Project vision\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.decoder.embed_tokens(question_ids)\n",
    "        \n",
    "        # Start with image + question\n",
    "        current_emb = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        generated_ids = []\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            hidden = current_emb\n",
    "            for layer in self.decoder.layers:\n",
    "                hidden = layer(hidden)\n",
    "            hidden = self.decoder.norm(hidden)\n",
    "            logits = self.decoder.lm_head(hidden)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            # Embed and append\n",
    "            next_emb = self.decoder.embed_tokens(next_token.unsqueeze(1))\n",
    "            current_emb = torch.cat([current_emb, next_emb], dim=1)\n",
    "        \n",
    "        return torch.stack(generated_ids, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement TRM-Style Recursive Decoder\n",
    "\n",
    "Now implement the TRM version with latent recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMVLMDecoder(nn.Module):\n",
    "    \"\"\"TRM-style recursive decoder for VLM.\n",
    "    \n",
    "    Uses latent recursion for reasoning:\n",
    "        x = context ([IMG_TOKENS] + [QUESTION_TOKENS])\n",
    "        y = answer embeddings (learned or teacher-forced)\n",
    "        z = latent reasoning state\n",
    "    \n",
    "    Inner recursion: Repeat n times\n",
    "        concat = [x, y, z]\n",
    "        concat' = TinyTransformer(concat)\n",
    "        x', y', z' = split(concat')\n",
    "        y, z = y', z'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 512,\n",
    "        num_layers: int = 2,  # Small for tiny network\n",
    "        num_heads: int = 8,\n",
    "        vision_token_dim: int = 4096,\n",
    "        max_seq_len: int = 256,\n",
    "        num_inner_steps: int = 4,  # n = number of inner recursion steps\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        \n",
    "        # Project vision tokens to decoder dim\n",
    "        self.vision_proj = nn.Linear(vision_token_dim, hidden_dim)\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Tiny transformer for recursion\n",
    "        trm_config = TRMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.tiny_transformer = nn.ModuleList([\n",
    "            TRMDecoder(trm_config).layers[i] for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm = TRMDecoder(trm_config).norm\n",
    "        \n",
    "        # LM head\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed_tokens.weight  # Tie weights\n",
    "        \n",
    "        # Learned initial z state\n",
    "        self.z_init = nn.Parameter(torch.randn(1, 1, hidden_dim) * 0.02)\n",
    "    \n",
    "    def latent_recursion(\n",
    "        self,\n",
    "        x: torch.Tensor,  # Context: (B, L_ctx, d)\n",
    "        y: torch.Tensor,  # Answer: (B, L_ans, d)\n",
    "        z: torch.Tensor,  # Latent: (B, L_ans, d)\n",
    "    ):\n",
    "        \"\"\"Single step of latent recursion.\"\"\"\n",
    "        # Concatenate along sequence: [x, y, z]\n",
    "        concat = torch.cat([x, y, z], dim=1)  # (B, L_ctx + 2*L_ans, d)\n",
    "        \n",
    "        # Pass through tiny transformer\n",
    "        hidden = concat\n",
    "        for layer in self.tiny_transformer:\n",
    "            hidden = layer(hidden)\n",
    "        \n",
    "        # Split back\n",
    "        L_ctx = x.shape[1]\n",
    "        L_ans = y.shape[1]\n",
    "        \n",
    "        x_out = hidden[:, :L_ctx, :]\n",
    "        y_out = hidden[:, L_ctx:L_ctx+L_ans, :]\n",
    "        z_out = hidden[:, L_ctx+L_ans:, :]\n",
    "        \n",
    "        return x_out, y_out, z_out\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,  # (B, K_img, 4096)\n",
    "        question_ids: torch.Tensor,   # (B, L_q)\n",
    "        answer_ids: torch.Tensor,     # (B, L_a)\n",
    "    ):\n",
    "        \"\"\"Forward pass with TRM recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        L_ans = answer_ids.shape[1]\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_emb = self.vision_proj(vision_tokens)  # (B, K_img, d)\n",
    "        \n",
    "        # Embed question\n",
    "        question_emb = self.embed_tokens(question_ids)  # (B, L_q, d)\n",
    "        \n",
    "        # Context x = [vision | question]\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)  # (B, L_ctx, d)\n",
    "        \n",
    "        # Teacher-forced answer embeddings\n",
    "        y = self.embed_tokens(answer_ids)  # (B, L_ans, d)\n",
    "        \n",
    "        # Initialize latent z\n",
    "        z = self.z_init.expand(batch_size, L_ans, -1)  # (B, L_ans, d)\n",
    "        \n",
    "        # Inner recursion (n steps)\n",
    "        for _ in range(self.num_inner_steps):\n",
    "            x, y, z = self.latent_recursion(x, y, z)\n",
    "        \n",
    "        # Final answer from y\n",
    "        y = self.norm(y)\n",
    "        logits = self.lm_head(y)  # (B, L_ans, vocab_size)\n",
    "        \n",
    "        # Compute loss (standard next-token prediction on answer)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = answer_ids[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_tokens: torch.Tensor,\n",
    "        question_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 32,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"Generate answer with TRM recursion.\"\"\"\n",
    "        batch_size = vision_tokens.shape[0]\n",
    "        \n",
    "        # Context\n",
    "        vision_emb = self.vision_proj(vision_tokens)\n",
    "        question_emb = self.embed_tokens(question_ids)\n",
    "        x = torch.cat([vision_emb, question_emb], dim=1)\n",
    "        \n",
    "        # Start with empty answer (or learned start token)\n",
    "        generated_ids = []\n",
    "        \n",
    "        # Generate autoregressively\n",
    "        for step in range(max_new_tokens):\n",
    "            # Current y from generated so far\n",
    "            if len(generated_ids) == 0:\n",
    "                # First token: use learned blank\n",
    "                y = torch.zeros(batch_size, 1, self.hidden_dim, device=x.device)\n",
    "                z = self.z_init.expand(batch_size, 1, -1)\n",
    "            else:\n",
    "                y = self.embed_tokens(torch.stack(generated_ids, dim=1))\n",
    "                z = self.z_init.expand(batch_size, len(generated_ids), -1)\n",
    "            \n",
    "            # Run recursion\n",
    "            for _ in range(self.num_inner_steps):\n",
    "                x_temp, y, z = self.latent_recursion(x, y, z)\n",
    "            \n",
    "            # Get logits for last position\n",
    "            y = self.norm(y)\n",
    "            logits = self.lm_head(y[:, -1, :]) / temperature\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "        \n",
    "        return torch.stack(generated_ids, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics (EM and F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer text for evaluation.\"\"\"\n",
    "    # Remove punctuation\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    # Lowercase and strip\n",
    "    s = s.lower().strip()\n",
    "    # Remove articles\n",
    "    s = ' '.join([w for w in s.split() if w not in {'a', 'an', 'the'}])\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return float(normalize_answer(pred) == normalize_answer(target))\n",
    "\n",
    "def compute_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score.\"\"\"\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    target_tokens = normalize_answer(target).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
    "        return float(pred_tokens == target_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(target_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(target_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def evaluate_qa(\n",
    "    predictions: List[str],\n",
    "    targets: List[str],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate QA predictions.\"\"\"\n",
    "    em_scores = [compute_exact_match(p, t) for p, t in zip(predictions, targets)]\n",
    "    f1_scores = [compute_f1(p, t) for p, t in zip(predictions, targets)]\n",
    "    \n",
    "    return {\n",
    "        'em': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (use GPT2 tokenizer for simplicity)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get transforms\n",
    "train_transforms = get_image_transforms(image_size=336, is_training=True)\n",
    "val_transforms = get_image_transforms(image_size=336, is_training=False)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PixMoQADataset(\n",
    "    parquet_path=\"/home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_train.parquet\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=train_transforms,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "val_dataset = PixMoQADataset(\n",
    "    parquet_path=\"/home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_val.parquet\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=val_transforms,\n",
    "    max_answer_length=32,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val: {len(val_dataset):,}\")\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  Images: {sample_batch['images'].shape}\")\n",
    "print(f\"  Question IDs: {sample_batch['question_ids'].shape}\")\n",
    "print(f\"  Answer IDs: {sample_batch['answer_ids'].shape}\")\n",
    "\n",
    "print(\"\\nSample QA pairs:\")\n",
    "for i in range(min(3, len(sample_batch['questions']))):\n",
    "    print(f\"\\n  [{i+1}]\")\n",
    "    print(f\"    Q: {sample_batch['questions'][i]}\")\n",
    "    print(f\"    A: {sample_batch['answers'][i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Decoder Model\n",
    "\n",
    "Choose between TinyVLMDecoder (baseline) or TRMVLMDecoder (recursive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_TRM = True  # Set to False for baseline\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 4 if not USE_TRM else 2  # TRM uses smaller network\n",
    "NUM_HEADS = 8\n",
    "NUM_INNER_STEPS = 4  # Only for TRM\n",
    "\n",
    "# Initialize model\n",
    "if USE_TRM:\n",
    "    model = TRMVLMDecoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_inner_steps=NUM_INNER_STEPS,\n",
    "    ).to(device)\n",
    "    print(f\"Initialized TRM VLM Decoder (n={NUM_INNER_STEPS} inner steps)\")\n",
    "else:\n",
    "    model = TinyVLMDecoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "    ).to(device)\n",
    "    print(\"Initialized Plain Tiny Decoder (baseline)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EVAL_EVERY = 100\n",
    "LOG_EVERY = 20\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Eval every: {EVAL_EVERY} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_trm_vlm\",\n",
    "        name=f\"{'trm' if USE_TRM else 'baseline'}_decoder_d{HIDDEN_DIM}_l{NUM_LAYERS}\",\n",
    "        config={\n",
    "            'use_trm': USE_TRM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'num_heads': NUM_HEADS,\n",
    "            'num_inner_steps': NUM_INNER_STEPS if USE_TRM else None,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': batch_size,\n",
    "            'total_params': total_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_em': [], 'val_f1': []}\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = Path(f\"checkpoints/trm_vlm_{'trm' if USE_TRM else 'baseline'}\")\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answer_ids = batch['answer_ids'].to(device)\n",
    "        \n",
    "        # Encode images (frozen)\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        epoch_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % LOG_EVERY == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-LOG_EVERY:])\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "            \n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/lr': scheduler.get_last_lr()[0],\n",
    "                    'step': global_step,\n",
    "                })\n",
    "    \n",
    "    # Epoch-end evaluation\n",
    "    print(f\"\\n  Epoch {epoch+1} average loss: {np.mean(epoch_losses):.4f}\")\n",
    "    history['train_loss'].append(np.mean(epoch_losses))\n",
    "    \n",
    "    # Validation (simple loss for now, full eval is expensive)\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['images'].to(device)\n",
    "            question_ids = batch['question_ids'].to(device)\n",
    "            answer_ids = batch['answer_ids'].to(device)\n",
    "            \n",
    "            vision_tokens = encode_images(images)\n",
    "            outputs = model(vision_tokens, question_ids, answer_ids)\n",
    "            val_losses.append(outputs['loss'].item())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'val/loss': val_loss,\n",
    "            'epoch': epoch + 1,\n",
    "        })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': {\n",
    "                'use_trm': USE_TRM,\n",
    "                'hidden_dim': HIDDEN_DIM,\n",
    "                'num_layers': NUM_LAYERS,\n",
    "                'num_heads': NUM_HEADS,\n",
    "                'num_inner_steps': NUM_INNER_STEPS if USE_TRM else None,\n",
    "            },\n",
    "        }, ckpt_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"  ✓ Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Evaluation with EM and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(ckpt_dir / \"checkpoint_best.pt\", map_location=device)\n",
    "model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nRunning full evaluation on validation set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating answers\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        # Encode images\n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        # Generate answers\n",
    "        generated_ids = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"Token F1: {metrics['f1']:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        'val/em': metrics['em'],\n",
    "        'val/f1': metrics['f1'],\n",
    "    })\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"  Target: {all_targets[i][:80]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:80]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics (if available)\n",
    "if history.get('val_em'):\n",
    "    axes[1].plot(history['val_em'], label='Exact Match', linewidth=2, marker='o')\n",
    "if history.get('val_f1'):\n",
    "    axes[1].plot(history['val_f1'], label='Token F1', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score (%)')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / \"training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {ckpt_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Type: {'TRM Recursive Decoder' if USE_TRM else 'Plain Tiny Decoder (Baseline)'}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num heads: {NUM_HEADS}\")\n",
    "if USE_TRM:\n",
    "    print(f\"  Inner recursion steps: {NUM_INNER_STEPS}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"  Exact Match (EM): {metrics['em']:.2f}%\")\n",
    "print(f\"  Token F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Checkpoint: {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "print(f\"  Training curves: {ckpt_dir / 'training_curves.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run ablation: Switch USE_TRM flag and compare baseline vs TRM\")\n",
    "print(\"2. Try different recursion depths (num_inner_steps = {2, 4, 6, 8})\")\n",
    "print(\"3. Experiment with hidden_dim = {256, 512, 1024}\")\n",
    "print(\"4. Add outer deep recursion (T > 1)\")\n",
    "print(\"5. Evaluate on text-only baseline (no vision tokens)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
