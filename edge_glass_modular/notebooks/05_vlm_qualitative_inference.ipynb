{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825af6db",
   "metadata": {},
   "source": [
    "# VLM Qualitative Inference\n",
    "\n",
    "Interactive probing of the VLM with custom images and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path(os.path.abspath('..'))\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from config import load_config\n",
    "from models.alignment import MultimodalAlignmentModel\n",
    "from models.trm_qwen_vlm import QwenVLM\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.transforms import get_image_transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "config_path = \"../configs/trm_vlm_qa_qwen1.5.yaml\"\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "# Update checkpoint path!\n",
    "checkpoint_path = \"../checkpoints/vlm_run/checkpoint-epoch-9\" \n",
    "alignment_checkpoint = \"../notebooks/checkpoints/pixmo_alignment/checkpoint_best.pt\"\n",
    "use_trm = True\n",
    "\n",
    "# --- Load Models ---\n",
    "print(\"Loading Models...\")\n",
    "# 1. Vision\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "alignment_config.decoder = None; alignment_config.text_encoder = None\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config)\n",
    "if os.path.exists(alignment_checkpoint):\n",
    "    ckpt = torch.load(alignment_checkpoint, map_location='cpu', weights_only=False)\n",
    "    aligned_model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "aligned_model.eval().to(device)\n",
    "\n",
    "# 2. VLM\n",
    "config = load_config(config_path)\n",
    "qwen_decoder = QwenDecoder(config.decoder.model_name, load_in_4bit=True, use_lora=True, device_map=\"auto\")\n",
    "model = QwenVLM(\n",
    "    qwen_decoder, alignment_config.vision_encoder.projection_dim,\n",
    "    use_trm_recursion=use_trm, num_trm_layers=4, num_recursion_steps=4\n",
    ").to(device)\n",
    "\n",
    "# Checkpoint\n",
    "if os.path.isdir(checkpoint_path):\n",
    "    bin_path = Path(checkpoint_path) / \"pytorch_model.bin\"\n",
    "    if bin_path.exists(): model.load_state_dict(torch.load(bin_path, map_location='cpu'), strict=False)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu')['model_state_dict'], strict=False)\n",
    "model.eval()\n",
    "print(\"Ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318685b",
   "metadata": {},
   "source": [
    "## Interactive Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference(image_path, question):\n",
    "    # Load and Plot Image\n",
    "    try:\n",
    "        if isinstance(image_path, str):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        else:\n",
    "            image = image_path # Allow passing PIL object\n",
    "            \n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Q: {question}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Transform\n",
    "        transform = get_image_transforms(config.dataset.image_size, is_training=False)\n",
    "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Encode\n",
    "        with torch.no_grad():\n",
    "            vision_tokens = aligned_model.vision_encoder(img_tensor, return_sequence=True).sequence\n",
    "            \n",
    "        # Generate\n",
    "        inputs = qwen_decoder.tokenizer([question], return_tensors='pt', padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                vision_tokens=vision_tokens,\n",
    "                question_ids=inputs.input_ids,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.2\n",
    "            )\n",
    "        \n",
    "        answer = qwen_decoder.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        print(f\"ðŸ¤– Answer: {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "# run_inference(\"/path/to/image.jpg\", \"What is in this image?\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
