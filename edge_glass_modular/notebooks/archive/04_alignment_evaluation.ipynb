{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment Model Evaluation and Explainability\n",
    "\n",
    "This notebook provides comprehensive evaluation and explainability analysis for aligned vision-text models.\n",
    "\n",
    "## Features:\n",
    "\n",
    "### 1. Retrieval Evaluation\n",
    "- Image-to-text and text-to-image retrieval\n",
    "- Comprehensive metrics: R@K, mAP, NDCG, rank statistics\n",
    "- Comparison between MLP and Perceiver architectures\n",
    "\n",
    "### 2. Matryoshka Representation Learning (MRL)\n",
    "- Performance across different embedding dimensions\n",
    "- Efficiency vs. accuracy trade-offs\n",
    "- MRL curves and analysis\n",
    "\n",
    "### 3. Explainability Analysis\n",
    "- Embedding space visualization (PCA, t-SNE)\n",
    "- Dimension importance analysis\n",
    "- Modality separation metrics\n",
    "- Similarity distribution analysis\n",
    "\n",
    "### 4. Benchmarking\n",
    "- Standardized evaluation protocols\n",
    "- Comparable metrics across models\n",
    "- Performance reports\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- 1 GPU (H200 recommended)\n",
    "- ~20-30GB GPU memory\n",
    "- ~32GB RAM for large-scale visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import our modular evaluation components\n",
    "from config import load_config\n",
    "from models import MultimodalAlignmentModel\n",
    "from data.dataset_builder import build_image_datasets_from_parquet\n",
    "from data.transforms import get_image_transforms\n",
    "\n",
    "# Evaluation modules\n",
    "from evaluation import (\n",
    "    RetrievalMetrics,\n",
    "    compute_retrieval_metrics,\n",
    "    compute_mrl_performance,\n",
    "    AlignmentBenchmark,\n",
    "    ExplainabilityAnalyzer,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "from utils.visualization import TrainingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Experiment: pixmo_vision_text_alignment\n",
      "\n",
      "Dataset:\n",
      "  Test parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pixmo/pixmo_test.parquet\n",
      "  Batch size: 128\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetConfig' object has no attribute 'max_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Test parquet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_config.dataset.test_parquet\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_config.dataset.batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Max samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43meval_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMetrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Recall@K: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_config.metrics.recall_at_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatasetConfig' object has no attribute 'max_samples'"
     ]
    }
   ],
   "source": [
    "# Load evaluation configuration\n",
    "eval_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "eval_config = load_config(eval_config_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nExperiment: {eval_config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Test parquet: {eval_config.dataset.test_parquet}\")\n",
    "print(f\"  Batch size: {eval_config.dataset.batch_size}\")\n",
    "print(f\"  Max samples: {eval_config.dataset.max_samples}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Recall@K: {eval_config.metrics.recall_at_k}\")\n",
    "print(f\"  MRL enabled: {eval_config.mrl.enabled}\")\n",
    "print(f\"  MRL dimensions: {eval_config.mrl.dimensions}\")\n",
    "print(f\"\\nExplainability:\")\n",
    "print(f\"  Enabled: {eval_config.explainability.enabled}\")\n",
    "print(f\"  Embedding viz method: {eval_config.explainability.embedding_viz.method}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Model to Evaluate\n",
    "\n",
    "You can evaluate either the MLP-based or Perceiver-based alignment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which model to evaluate\n",
    "MODEL_TYPE = \"perceiver_mrl\"  # Options: \"pixmo_mlp\" or \"perceiver_mrl\"\n",
    "\n",
    "if hasattr(eval_config, 'checkpoints') and MODEL_TYPE in eval_config.checkpoints:\n",
    "    checkpoint_info = eval_config.checkpoints[MODEL_TYPE]\n",
    "    model_config_path = checkpoint_info.config\n",
    "    checkpoint_path = checkpoint_info.path\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  No checkpoint info for {MODEL_TYPE} in config. Using defaults/discovery.\")\n",
    "    model_config_path = eval_config_path\n",
    "    \n",
    "    # Try different possible checkpoint locations\n",
    "    possible_dirs = [\n",
    "        Path(\"checkpoints\") / \"pixmo_alignment\",\n",
    "        Path(\"checkpoints\") / \"perceiver_mrl_alignment\",\n",
    "        Path(\"outputs\") / \"pixmo_alignment\",\n",
    "    ]\n",
    "    # Add explicit output dir from config if present\n",
    "    if hasattr(eval_config, 'trainer') and eval_config.trainer.output_dir:\n",
    "         possible_dirs.insert(0, Path(eval_config.trainer.output_dir))\n",
    "    if hasattr(eval_config, 'trainer') and eval_config.trainer.ckpt_dir:\n",
    "         possible_dirs.insert(0, Path(eval_config.trainer.ckpt_dir))\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    for ckpt_dir in possible_dirs:\n",
    "        if ckpt_dir.exists():\n",
    "            pt_files = list(ckpt_dir.glob(\"*.pt\"))\n",
    "            if pt_files:\n",
    "                # Prefer 'best' in name, or recent ones\n",
    "                best = next((f for f in pt_files if 'best' in f.name), pt_files[0])\n",
    "                checkpoint_path = str(best)\n",
    "                print(f\"  Found checkpoint in {ckpt_dir}\")\n",
    "                break\n",
    "    \n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = \"checkpoints/pixmo_alignment/best_model.pt\"\n",
    "        print(f\"  Could not find checkpoint file, defaulting to: {checkpoint_path}\")\n",
    "\n",
    "print(f\"Evaluating: {MODEL_TYPE}\")\n",
    "print(f\"Config: {model_config_path}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "model_config = load_config(model_config_path)\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Vision Encoder: {model_config.vision_encoder.model_name}\")\n",
    "print(f\"  Projection Dim: {model_config.vision_encoder.projection_dim}\")\n",
    "print(f\"  MRL Dimensions: {model_config.vision_encoder.mrl_dimensions}\")\n",
    "\n",
    "if hasattr(model_config.vision_encoder, 'perceiver_num_latents'):\n",
    "    print(f\"  Perceiver Latents: {model_config.vision_encoder.perceiver_num_latents}\")\n",
    "    print(f\"  Perceiver Layers: {model_config.vision_encoder.perceiver_num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load model\n",
    "print(\"\\nLoading model...\")\n",
    "model = MultimodalAlignmentModel(model_config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "if Path(checkpoint_path).exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    print(f\"\u2713 Loaded checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    if 'best_val_loss' in checkpoint:\n",
    "        print(f\"  Best val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Checkpoint not found: {checkpoint_path}\")\n",
    "    print(\"   Using randomly initialized model\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Print parameter counts\n",
    "print(f\"\\nModel Architecture:\")\n",
    "model.print_parameter_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation configuration\n",
    "eval_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "eval_config = load_config(eval_config_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nExperiment: {eval_config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Test parquet: {eval_config.dataset.test_parquet}\")\n",
    "print(f\"  Batch size: {eval_config.dataset.batch_size}\")\n",
    "# Safe access\n",
    "max_samples = getattr(eval_config.dataset, 'max_samples', None)\n",
    "print(f\"  Max samples: {max_samples}\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "# Default metrics\n",
    "metrics_cfg = getattr(eval_config, 'metrics', None)\n",
    "recall_at_k = getattr(metrics_cfg, 'recall_at_k', [1, 5, 10]) if metrics_cfg else [1, 5, 10]\n",
    "print(f\"  Recall@K: {recall_at_k}\")\n",
    "\n",
    "# MRL\n",
    "use_mrl = False\n",
    "mrl_dims = []\n",
    "if hasattr(eval_config, 'mrl'):\n",
    "    use_mrl = eval_config.mrl.enabled\n",
    "    mrl_dims = eval_config.mrl.dimensions\n",
    "elif hasattr(eval_config, 'vision_encoder') and hasattr(eval_config.vision_encoder, 'use_mrl'):\n",
    "    use_mrl = eval_config.vision_encoder.use_mrl\n",
    "    mrl_dims = getattr(eval_config.vision_encoder, 'mrl_dimensions', [])\n",
    "\n",
    "print(f\"  MRL enabled: {use_mrl}\")\n",
    "print(f\"  MRL dimensions: {mrl_dims}\")\n",
    "\n",
    "print(f\"\\nExplainability:\")\n",
    "explain_cfg = getattr(eval_config, 'explainability', None)\n",
    "explain_enabled = getattr(explain_cfg, 'enabled', False) if explain_cfg else False\n",
    "viz_method = \"pca\"\n",
    "if explain_enabled and hasattr(explain_cfg, 'embedding_viz'):\n",
    "    viz_method = getattr(explain_cfg.embedding_viz, 'method', 'pca')\n",
    "\n",
    "print(f\"  Enabled: {explain_enabled}\")\n",
    "print(f\"  Embedding viz method: {viz_method}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=eval_config.dataset.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=eval_config.dataset.num_workers,\n",
    "    pin_memory=eval_config.dataset.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Evaluation Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark\n",
    "benchmark = AlignmentBenchmark(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    mrl_dims=eval_config.mrl.dimensions if eval_config.mrl.enabled else None,\n",
    ")\n",
    "\n",
    "# Initialize explainability analyzer\n",
    "explainability = ExplainabilityAnalyzer(\n",
    "    model=model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize visualizer\n",
    "output_dir = Path(eval_config.visualization.output_dir) / MODEL_TYPE\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "visualizer = TrainingVisualizer(\n",
    "    save_dir=output_dir,\n",
    "    style=eval_config.visualization.style,\n",
    ")\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Weights & Biases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_config.logging.use_wandb:\n",
    "    wandb.init(\n",
    "        project=eval_config.logging.wandb_project,\n",
    "        entity=eval_config.logging.wandb_entity,\n",
    "        name=f\"{eval_config.logging.run_name}_{MODEL_TYPE}\",\n",
    "        config={\n",
    "            \"model_type\": MODEL_TYPE,\n",
    "            \"checkpoint\": checkpoint_path,\n",
    "            \"dataset\": eval_config.dataset.test_parquet,\n",
    "            \"batch_size\": eval_config.dataset.batch_size,\n",
    "        },\n",
    "    )\n",
    "    print(\"\u2713 Weights & Biases initialized\")\n",
    "else:\n",
    "    print(\"\u2298 Weights & Biases logging disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = benchmark.run_full_evaluation(\n",
    "    dataloader=test_loader,\n",
    "    max_batches=None,  # Use full dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Retrieval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "i2t_metrics = results['retrieval']['i2t']\n",
    "t2i_metrics = results['retrieval']['t2i']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RETRIEVAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nImage-to-Text Retrieval:\")\n",
    "print(f\"  R@1:   {i2t_metrics.r_at_1:.2f}%\")\n",
    "print(f\"  R@5:   {i2t_metrics.r_at_5:.2f}%\")\n",
    "print(f\"  R@10:  {i2t_metrics.r_at_10:.2f}%\")\n",
    "print(f\"  R@50:  {i2t_metrics.r_at_50:.2f}%\")\n",
    "print(f\"  Mean Rank: {i2t_metrics.mean_rank:.2f}\")\n",
    "print(f\"  Median Rank: {i2t_metrics.median_rank:.0f}\")\n",
    "print(f\"  mAP@10: {i2t_metrics.map_at_10:.4f}\")\n",
    "print(f\"  NDCG@10: {i2t_metrics.ndcg_at_10:.4f}\")\n",
    "\n",
    "print(\"\\nText-to-Image Retrieval:\")\n",
    "print(f\"  R@1:   {t2i_metrics.r_at_1:.2f}%\")\n",
    "print(f\"  R@5:   {t2i_metrics.r_at_5:.2f}%\")\n",
    "print(f\"  R@10:  {t2i_metrics.r_at_10:.2f}%\")\n",
    "print(f\"  R@50:  {t2i_metrics.r_at_50:.2f}%\")\n",
    "print(f\"  Mean Rank: {t2i_metrics.mean_rank:.2f}\")\n",
    "print(f\"  Median Rank: {t2i_metrics.median_rank:.0f}\")\n",
    "print(f\"  mAP@10: {t2i_metrics.map_at_10:.4f}\")\n",
    "print(f\"  NDCG@10: {t2i_metrics.ndcg_at_10:.4f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to wandb\n",
    "if eval_config.logging.use_wandb:\n",
    "    wandb.log({\n",
    "        \"i2t/R@1\": i2t_metrics.r_at_1,\n",
    "        \"i2t/R@5\": i2t_metrics.r_at_5,\n",
    "        \"i2t/R@10\": i2t_metrics.r_at_10,\n",
    "        \"i2t/mean_rank\": i2t_metrics.mean_rank,\n",
    "        \"t2i/R@1\": t2i_metrics.r_at_1,\n",
    "        \"t2i/R@5\": t2i_metrics.r_at_5,\n",
    "        \"t2i/R@10\": t2i_metrics.r_at_10,\n",
    "        \"t2i/mean_rank\": t2i_metrics.mean_rank,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MRL Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results['mrl']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MRL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPerformance vs. Dimension (Image\u2192Text):\")\n",
    "    print(f\"{'Dim':>6} | {'R@1':>7} | {'R@5':>7} | {'R@10':>7} | {'Mean Rank':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for dim in sorted(results['mrl'].keys()):\n",
    "        metrics = results['mrl'][dim]['i2t']\n",
    "        print(f\"{dim:6d} | {metrics.r_at_1:6.2f}% | {metrics.r_at_5:6.2f}% | {metrics.r_at_10:6.2f}% | {metrics.mean_rank:10.2f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"MRL evaluation not enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization: Rank Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rank histograms\n",
    "visualizer.plot_rank_histogram(\n",
    "    ranks=i2t_metrics.ranks,\n",
    "    title=\"Image\u2192Text Retrieval Ranks\",\n",
    "    save_name=\"i2t_rank_histogram.png\",\n",
    ")\n",
    "\n",
    "visualizer.plot_rank_histogram(\n",
    "    ranks=t2i_metrics.ranks,\n",
    "    title=\"Text\u2192Image Retrieval Ranks\",\n",
    "    save_name=\"t2i_rank_histogram.png\",\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Rank histograms saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rank CDFs\n",
    "visualizer.plot_rank_cdf(\n",
    "    ranks=i2t_metrics.ranks,\n",
    "    title=\"Image\u2192Text Rank CDF\",\n",
    "    save_name=\"i2t_rank_cdf.png\",\n",
    ")\n",
    "\n",
    "visualizer.plot_rank_cdf(\n",
    "    ranks=t2i_metrics.ranks,\n",
    "    title=\"Text\u2192Image Rank CDF\",\n",
    "    save_name=\"t2i_rank_cdf.png\",\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Rank CDFs saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one of the plots\n",
    "from IPython.display import Image as IPImage, display\n",
    "display(IPImage(filename=str(output_dir / 'i2t_rank_histogram.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization: Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "vision_embs = results['embeddings']['vision']\n",
    "text_embs = results['embeddings']['text']\n",
    "\n",
    "# Compute similarity distributions\n",
    "vision_norm = F.normalize(vision_embs, p=2, dim=-1)\n",
    "text_norm = F.normalize(text_embs, p=2, dim=-1)\n",
    "\n",
    "sims = torch.matmul(vision_norm, text_norm.t())\n",
    "N = sims.size(0)\n",
    "\n",
    "# Positive pairs (diagonal)\n",
    "pos_sims = sims.diag().cpu().numpy()\n",
    "\n",
    "# Negative pairs (off-diagonal, sample subset)\n",
    "mask = torch.eye(N, dtype=torch.bool, device=sims.device)\n",
    "neg_sims_all = sims[~mask].cpu().numpy()\n",
    "neg_sims = np.random.choice(neg_sims_all, size=min(10000, len(neg_sims_all)), replace=False)\n",
    "\n",
    "# Plot\n",
    "visualizer.plot_similarity_distributions(\n",
    "    positive_sims=pos_sims,\n",
    "    negative_sims=neg_sims,\n",
    "    title=\"Positive vs. Negative Pair Similarities\",\n",
    "    save_name=\"similarity_distributions.png\",\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Similarity distributions saved to {output_dir}\")\n",
    "\n",
    "# Display\n",
    "display(IPImage(filename=str(output_dir / 'similarity_distributions.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity matrix (subset)\n",
    "visualizer.plot_similarity_matrix(\n",
    "    vision_embs=vision_embs.numpy(),\n",
    "    text_embs=text_embs.numpy(),\n",
    "    n_samples=50,\n",
    "    save_name=\"similarity_matrix.png\",\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Similarity matrix saved to {output_dir}\")\n",
    "display(IPImage(filename=str(output_dir / 'similarity_matrix.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualization: MRL Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results['mrl']:\n",
    "    # Plot MRL performance curves for different metrics\n",
    "    for metric in ['r_at_1', 'r_at_5', 'r_at_10']:\n",
    "        visualizer.plot_mrl_curves(\n",
    "            mrl_results=results['mrl'],\n",
    "            metric=metric,\n",
    "            title=f\"MRL Performance: {metric.upper()}\",\n",
    "            save_name=f\"mrl_{metric}.png\",\n",
    "        )\n",
    "    \n",
    "    print(f\"\u2713 MRL curves saved to {output_dir}\")\n",
    "    \n",
    "    # Display R@1 curve\n",
    "    display(IPImage(filename=str(output_dir / 'mrl_r_at_1.png')))\n",
    "else:\n",
    "    print(\"MRL evaluation not enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualization: Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding space\n",
    "visualizer.plot_embedding_space(\n",
    "    vision_embs=vision_embs.numpy(),\n",
    "    text_embs=text_embs.numpy(),\n",
    "    method=eval_config.explainability.embedding_viz.method,\n",
    "    n_samples=eval_config.explainability.embedding_viz.n_samples,\n",
    "    save_name=\"embedding_space.png\",\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Embedding space visualization saved to {output_dir}\")\n",
    "display(IPImage(filename=str(output_dir / 'embedding_space.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_config.explainability.enabled:\n",
    "    # Generate comprehensive explainability report\n",
    "    explainability_report = explainability.generate_explainability_report(\n",
    "        vision_embs=vision_embs,\n",
    "        text_embs=text_embs,\n",
    "        save_dir=output_dir,\n",
    "    )\n",
    "else:\n",
    "    print(\"Explainability analysis not enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Retrieval Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some retrieval examples\n",
    "num_examples = 5\n",
    "\n",
    "# Get a batch for visualization\n",
    "sample_batch = next(iter(test_loader))\n",
    "images_sample = sample_batch['image'][:num_examples]\n",
    "texts_sample = sample_batch['text'][:num_examples]\n",
    "\n",
    "# Get embeddings for this batch\n",
    "with torch.no_grad():\n",
    "    outputs = model(images=images_sample.to(device), texts=texts_sample, return_embeddings=True)\n",
    "    query_embs = outputs.vision_emb\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMAGE\u2192TEXT RETRIEVAL EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(num_examples, len(texts_sample))):\n",
    "    if not texts_sample[i]:  # Skip dropped texts\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Ground Truth: {texts_sample[i][:100]}...\")\n",
    "    \n",
    "    # Retrieve top-5 matches\n",
    "    indices, scores = explainability.compute_retrieval_attention(\n",
    "        query_emb=query_embs[i],\n",
    "        key_embs=text_embs,\n",
    "        top_k=5,\n",
    "    )\n",
    "    \n",
    "    print(\"Top-5 Retrieved:\")\n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "        # Get text from dataset (note: might need to handle indexing)\n",
    "        print(f\"  {rank}. [{score:.3f}] Sample {idx}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "results_dir = Path(eval_config.output.save_dir) / MODEL_TYPE\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "benchmark.save_results(results, results_dir)\n",
    "\n",
    "print(f\"\\n\u2713 Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'model_type': MODEL_TYPE,\n",
    "    'checkpoint': str(checkpoint_path),\n",
    "    'dataset': str(eval_config.dataset.test_parquet),\n",
    "    'num_samples': len(test_dataset),\n",
    "    'embedding_dim': vision_embs.shape[1],\n",
    "    'retrieval': {\n",
    "        'i2t': i2t_metrics.to_dict(),\n",
    "        't2i': t2i_metrics.to_dict(),\n",
    "    },\n",
    "    'similarity': results['similarity'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Add MRL summary if available\n",
    "if results['mrl']:\n",
    "    summary['mrl'] = {\n",
    "        dim: {\n",
    "            'i2t': metrics['i2t'].to_dict(),\n",
    "            't2i': metrics['t2i'].to_dict(),\n",
    "        }\n",
    "        for dim, metrics in results['mrl'].items()\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "with open(results_dir / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Summary report saved to {results_dir / 'summary.json'}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {MODEL_TYPE}\")\n",
    "print(f\"Samples: {len(test_dataset)}\")\n",
    "print(f\"Embedding dim: {vision_embs.shape[1]}\")\n",
    "print(f\"\\nBest I2T R@1: {i2t_metrics.r_at_1:.2f}%\")\n",
    "print(f\"Best T2I R@1: {t2i_metrics.r_at_1:.2f}%\")\n",
    "print(f\"\\nResults saved to: {results_dir}\")\n",
    "print(f\"Visualizations saved to: {output_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Compare Models (Optional)\n",
    "\n",
    "If you've evaluated both models, you can compare them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from both models for comparison\n",
    "# This cell is optional and requires running the evaluation for both models\n",
    "\n",
    "def load_model_results(model_type):\n",
    "    \"\"\"Load saved results for a model.\"\"\"\n",
    "    results_path = Path(eval_config.output.save_dir) / model_type / 'summary.json'\n",
    "    if results_path.exists():\n",
    "        with open(results_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# Try to load both model results\n",
    "mlp_results = load_model_results('pixmo_mlp')\n",
    "perceiver_results = load_model_results('perceiver_mrl')\n",
    "\n",
    "if mlp_results and perceiver_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Metric':<20} | {'MLP':>10} | {'Perceiver':>10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    metrics_to_compare = ['R@1', 'R@5', 'R@10', 'mean_rank']\n",
    "    \n",
    "    for metric in metrics_to_compare:\n",
    "        mlp_val = mlp_results['retrieval']['i2t'][metric]\n",
    "        perc_val = perceiver_results['retrieval']['i2t'][metric]\n",
    "        print(f\"{metric:<20} | {mlp_val:10.2f} | {perc_val:10.2f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Run evaluation for both models to enable comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "if eval_config.logging.use_wandb:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n\u2713 Evaluation complete!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Review visualizations in {output_dir}\")\n",
    "print(f\"2. Check detailed results in {results_dir}\")\n",
    "print(f\"3. Compare with other models or baselines\")\n",
    "print(f\"4. Use insights to improve model architecture or training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}