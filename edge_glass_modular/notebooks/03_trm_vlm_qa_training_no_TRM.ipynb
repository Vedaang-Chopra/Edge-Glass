{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset - FIXED VERSION\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Pretrained aligned vision encoder (CLIP + projections + MRL) - FROZEN\n",
    "2. **Pretrained Qwen2.5-7B-Instruct decoder with LoRA** - TRAINABLE\n",
    "3. PixMo QA dataset with question-answer pairs\n",
    "4. Optional TRM latent recursion on top of Qwen\n",
    "\n",
    "## Key Fixes from Original:\n",
    "\n",
    "‚úÖ **Pretrained Decoder**: Uses Qwen2.5-7B instead of random 34M TRM\n",
    "\n",
    "‚úÖ **Proper Training**: Standard autoregressive loss with prefix_embeds\n",
    "\n",
    "‚úÖ **Baseline Mode**: Can disable TRM recursion for comparison\n",
    "\n",
    "‚úÖ **Debug Tools**: Text-only sanity check, parameter audit, first-batch logging\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  ‚Üì\n",
    "Aligned Vision Encoder [FROZEN]\n",
    "  ‚Üì (B, 577, 4096)\n",
    "Vision Projection (4096 ‚Üí d_qwen) [TRAINABLE]\n",
    "  ‚Üì (B, 577, d_qwen)\n",
    "Qwen2.5-7B Decoder + LoRA [TRAINABLE]\n",
    "  ‚Üì\n",
    "Token Layout: [IMG_TOKENS] [QUESTION] [ANSWER]\n",
    "Loss: Only on answer tokens (vision/question masked with -100)\n",
    "```\n",
    "\n",
    "Optional: TRM latent recursion wrapper on top of Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "sys.path.insert(0, str(src_path))\n",
    "print(f\"Added to path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modular imports from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from decoders.trm import TRMConfig, TRMLayer, RMSNorm\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "from data.transforms import get_image_transforms\n",
    "from models.alignment import MultimodalAlignmentModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment config\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train: {config.dataset.train_parquet}\")\n",
    "print(f\"  Val: {config.dataset.val_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Load in 8bit: {config.decoder.load_in_8bit}\")\n",
    "\n",
    "# Resolve checkpoint root (works from repo root or notebooks directory)\n",
    "CKPT_ROOT_CANDIDATES = [\n",
    "    Path.cwd() / 'checkpoints',\n",
    "    Path.cwd().parent / 'checkpoints',\n",
    "    Path.cwd() / 'edge_glass_modular/notebooks/checkpoints',\n",
    "    Path.cwd().parent / 'edge_glass_modular/notebooks/checkpoints',\n",
    "]\n",
    "CKPT_ROOT = next((p for p in CKPT_ROOT_CANDIDATES if p.exists()), None)\n",
    "if CKPT_ROOT is None:\n",
    "    raise FileNotFoundError('No checkpoint directory found; expected one of: ' + ', '.join(str(p) for p in CKPT_ROOT_CANDIDATES))\n",
    "print(f\"Checkpoint root: {CKPT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Aligned Vision Encoder (FROZEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment config\n",
    "alignment_config_path = \"../configs/pixmo_alignment.yaml\"\n",
    "alignment_config = load_config(alignment_config_path)\n",
    "\n",
    "# Build aligned model\n",
    "print(\"Loading aligned vision encoder...\")\n",
    "aligned_model = MultimodalAlignmentModel(alignment_config).to('cuda:1')\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = CKPT_ROOT / 'pixmo_alignment/checkpoint_best.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "aligned_model.eval()\n",
    "\n",
    "# FREEZE all parameters\n",
    "for param in aligned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"‚úì Loaded aligned model from {checkpoint_path}\")\n",
    "print(f\"  Checkpoint epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Val loss: {checkpoint.get('best_val_loss', 0.0):.4f}\")\n",
    "\n",
    "# Get vision encoder output dimension\n",
    "vision_token_dim = alignment_config.vision_encoder.projection_dim\n",
    "print(f\"  Vision output: (B, num_tokens, {vision_token_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vision Encoding Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Encode images to vision tokens using frozen aligned encoder.\n",
    "    \n",
    "    Args:\n",
    "        images: (B, 3, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        vision_tokens: (B, num_tokens, vision_token_dim)\n",
    "    \"\"\"\n",
    "    # Ensure images are on same device as model\n",
    "    device = next(aligned_model.parameters()).device\n",
    "    images = images.to(device)\n",
    "    vision_output = aligned_model.vision_encoder(images, return_sequence=True)\n",
    "    if vision_output.sequence is None:\n",
    "        raise ValueError(\"Vision encoder did not return sequence embeddings\")\n",
    "    return vision_output.sequence\n",
    "\n",
    "# Test\n",
    "test_img = torch.randn(2, 3, 336, 336).to(device)\n",
    "test_vision_tokens = encode_images(test_img)\n",
    "print(f\"Vision tokens shape: {test_vision_tokens.shape}\")\n",
    "print(f\"Expected: (2, num_tokens, {vision_token_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QwenVLM Model Class\n",
    "\n",
    "**Main VLM wrapper with optional TRM recursion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced inline class with imported refactored class\n",
    "from models.trm_qwen_vlm import QwenVLM\n",
    "\n",
    "print(\"‚úì QwenVLM class imported from src.models.trm_qwen_vlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Pretrained Qwen Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "USE_TRM_RECURSION = False  # Start with baseline, then try True\n",
    "NUM_TRM_LAYERS = 4         # Only used if TRM recursion enabled\n",
    "NUM_RECURSION_STEPS = 4\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "# ====================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING QWEN DECODER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Qwen decoder\n",
    "print(f\"\\nLoading: {config.decoder.model_name}\")\n",
    "print(f\"  LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  8-bit: {config.decoder.load_in_8bit}\")\n",
    "\n",
    "qwen_decoder = QwenDecoder(\n",
    "    model_name=config.decoder.model_name,\n",
    "    load_in_8bit=config.decoder.load_in_8bit,\n",
    "    load_in_4bit=False,\n",
    "    use_lora=config.decoder.use_lora,\n",
    "    lora_r=config.decoder.get('lora_r', 32),\n",
    "    lora_alpha=config.decoder.get('lora_alpha', 64),\n",
    "    lora_dropout=config.decoder.get('lora_dropout', 0.1),\n",
    "    device_map=\"balanced\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Qwen decoder loaded\")\n",
    "print(f\"  Hidden dim: {qwen_decoder.hidden_dim}\")\n",
    "print(f\"  Vocab size: {qwen_decoder.vocab_size}\")\n",
    "\n",
    "# Create QwenVLM wrapper\n",
    "print(f\"\\nCreating QwenVLM wrapper\")\n",
    "print(f\"  Vision token dim: {vision_token_dim}\")\n",
    "print(f\"  Use TRM recursion: {USE_TRM_RECURSION}\")\n",
    "\n",
    "model = QwenVLM(\n",
    "    qwen_decoder=qwen_decoder,\n",
    "    vision_token_dim=vision_token_dim,\n",
    "    use_trm_recursion=USE_TRM_RECURSION,\n",
    "    num_trm_layers=NUM_TRM_LAYERS,\n",
    "    num_recursion_steps=NUM_RECURSION_STEPS,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì QwenVLM model created\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset and Data Loader\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets\n",
    "# Define transforms globally\n",
    "train_transforms = get_image_transforms(config.dataset.image_size, is_training=True)\n",
    "val_transforms = get_image_transforms(config.dataset.image_size, is_training=False)\n",
    "\n",
    "train_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.train_parquet,\n",
    "    tokenizer=qwen_decoder.tokenizer,\n",
    "    image_transforms=get_image_transforms(config.dataset.image_size, is_training=True),\n",
    "    max_question_length=128,\n",
    "    max_answer_length=256,\n",
    ")\n",
    "\n",
    "val_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.val_parquet,\n",
    "    tokenizer=qwen_decoder.tokenizer,\n",
    "    image_transforms=get_image_transforms(config.dataset.image_size, is_training=False),\n",
    "    max_question_length=128,\n",
    "    max_answer_length=256,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    pad_idx = qwen_decoder.tokenizer.pad_token_id\n",
    "    \n",
    "    images = torch.stack([b['image'] for b in batch])\n",
    "    \n",
    "    q_padded = pad_sequence([b['question_ids'] for b in batch], batch_first=True, padding_value=pad_idx)\n",
    "    a_padded = pad_sequence([b['answer_ids'] for b in batch], batch_first=True, padding_value=pad_idx)\n",
    "    q_mask = pad_sequence([b['question_mask'] for b in batch], batch_first=True, padding_value=0)\n",
    "    a_mask = pad_sequence([b['answer_mask'] for b in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'question_ids': q_padded,\n",
    "        'answer_ids': a_padded,\n",
    "        'question_mask': q_mask,\n",
    "        'answer_mask': a_mask,\n",
    "        'answers': [b['answer'] for b in batch],\n",
    "    }\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text-Only Sanity Check (BEFORE Training)\n",
    "\n",
    "**Critical test**: Verify pretrained Qwen can generate coherent English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text_only_sanity_check(decoder, prompts, max_tokens=20):\n",
    "    \"\"\"Test decoder on text-only prompts without vision.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT-ONLY SANITY CHECK (Before Training)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Encode\n",
    "        inputs = decoder.tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = decoder.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        generated = decoder.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n‚úì If coherent English ‚Üí Decoder works!\")\n",
    "    print(\"‚úó If garbage ‚Üí Decoder loading issue\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Question: What is 2 + 2? Answer:\",\n",
    "    \"Question: What color is the sky? Answer:\",\n",
    "    \"The capital of France is\",\n",
    "]\n",
    "\n",
    "text_only_sanity_check(qwen_decoder, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Audit\n",
    "\n",
    "Verify which parameters are frozen vs trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PARAMETER AUDIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainable_params = []\n",
    "frozen_params = []\n",
    "\n",
    "# Audit QwenVLM model\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((name, param.numel()))\n",
    "\n",
    "# Audit aligned model\n",
    "for name, param in aligned_model.named_parameters():\n",
    "    full_name = f\"aligned_model.{name}\"\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append((full_name, param.numel()))\n",
    "    else:\n",
    "        frozen_params.append((full_name, param.numel()))\n",
    "\n",
    "print(f\"\\nüü¢ TRAINABLE ({len(trainable_params)} groups):\")\n",
    "total_trainable = 0\n",
    "for name, count in trainable_params[:20]:\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_trainable += count\n",
    "\n",
    "if len(trainable_params) > 20:\n",
    "    print(f\"  ... and {len(trainable_params) - 20} more\")\n",
    "    for _, count in trainable_params[20:]:\n",
    "        total_trainable += count\n",
    "\n",
    "print(f\"\\nüî¥ FROZEN ({len(frozen_params)} groups):\")\n",
    "total_frozen = 0\n",
    "for name, count in frozen_params[:10]:\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "    total_frozen += count\n",
    "\n",
    "for _, count in frozen_params[10:]:\n",
    "    total_frozen += count\n",
    "\n",
    "if len(frozen_params) > 10:\n",
    "    print(f\"  ... and {len(frozen_params) - 10} more\")\n",
    "\n",
    "total = total_trainable + total_frozen\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable: {total_trainable:,} ({100*total_trainable/total:.2f}%)\")\n",
    "print(f\"  Frozen: {total_frozen:,} ({100*total_frozen/total:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì VERIFICATION:\")\n",
    "print(f\"  Aligned model frozen: {all(not p.requires_grad for p in aligned_model.parameters())}\")\n",
    "print(f\"  Vision proj trainable: {'vision_proj' in str([n for n, _ in trainable_params])}\")\n",
    "\n",
    "if USE_TRM_RECURSION:\n",
    "    print(f\"  TRM layers trainable: {'trm_layers' in str(trainable_params)}\")\n",
    "    print(f\"  z_init trainable: {'z_init' in str([n for n, _ in trainable_params])}\")\n",
    "else:\n",
    "    print(f\"  Qwen LoRA trainable: {'lora' in str(trainable_params).lower()}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_qa_batch(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    questions = [item[\"question_ids\"] for item in batch]\n",
    "    answers = [item[\"answer_ids\"] for item in batch]\n",
    "    \n",
    "    # Process images\n",
    "    # Stack if already tensors (from dataset transform)\n",
    "    if isinstance(images[0], torch.Tensor):\n",
    "        images = torch.stack(images)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # We need to pad questions (left) and answers (right)\n",
    "    # Get max lengths\n",
    "    max_q_len = max([q.size(0) for q in questions])\n",
    "    max_a_len = max([a.size(0) for a in answers])\n",
    "    \n",
    "    # Create padded tensors\n",
    "    bs = len(batch)\n",
    "    padded_questions = torch.full((bs, max_q_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    padded_answers = torch.full((bs, max_a_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    \n",
    "    # Create answer mask (1 for valid, 0 for pad)\n",
    "    answer_mask = torch.zeros((bs, max_a_len), dtype=torch.long)\n",
    "    \n",
    "    for i in range(bs):\n",
    "        # Left pad question? Or right? Usually left for generation, but here we are training.\n",
    "        # Right padding is standard for training with attention masks.\n",
    "        q_len = questions[i].size(0)\n",
    "        padded_questions[i, :q_len] = questions[i]\n",
    "        \n",
    "        a_len = answers[i].size(0)\n",
    "        padded_answers[i, :a_len] = answers[i]\n",
    "        \n",
    "        # Set mask for valid answer tokens\n",
    "        answer_mask[i, :a_len] = 1\n",
    "        \n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"question_ids\": padded_questions,\n",
    "        \"answer_ids\": padded_answers,\n",
    "        \"answer_mask\": answer_mask # Return mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for evaluation.\"\"\"\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    s = s.lower().strip()\n",
    "    s = ' '.join([w for w in s.split() if w not in {'a', 'an', 'the'}])\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return float(normalize_answer(pred) == normalize_answer(target))\n",
    "\n",
    "def compute_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"Compute token-level F1.\"\"\"\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    target_tokens = normalize_answer(target).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
    "        return float(pred_tokens == target_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(target_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(target_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def evaluate_qa(predictions: List[str], targets: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate QA predictions.\"\"\"\n",
    "    em_scores = [compute_exact_match(p, t) for p, t in zip(predictions, targets)]\n",
    "    f1_scores = [compute_f1(p, t) for p, t in zip(predictions, targets)]\n",
    "    \n",
    "    return {'em': np.mean(em_scores) * 100, 'f1': np.mean(f1_scores) * 100 ,}\n",
    "\n",
    "print(\"‚úì Evaluation metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Debug Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_first_batch(batch, vision_tokens, outputs, tokenizer):\n",
    "    \"\"\"Debug first training batch.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST BATCH DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüì¶ Shapes:\")\n",
    "    print(f\"  Images: {batch['images'].shape}\")\n",
    "    print(f\"  Vision tokens: {vision_tokens.shape}\")\n",
    "    print(f\"  Question IDs: {batch['question_ids'].shape}\")\n",
    "    print(f\"  Answer IDs: {batch['answer_ids'].shape}\")\n",
    "    \n",
    "    print(f\"\\nüìù First example:\")\n",
    "    print(f\"  Q: {batch['questions'][0][:100]}...\")\n",
    "    print(f\"  A: {batch['answers'][0][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nüî§ Decoded tokens:\")\n",
    "    q_dec = tokenizer.decode(batch['question_ids'][0], skip_special_tokens=False)\n",
    "    a_dec = tokenizer.decode(batch['answer_ids'][0], skip_special_tokens=False)\n",
    "    print(f\"  Question: {q_dec[:150]}...\")\n",
    "    print(f\"  Answer: {a_dec[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nüìä Outputs:\")\n",
    "    print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "    if outputs.get('logits') is not None:\n",
    "        print(f\"  Logits: {outputs['logits'].shape}\")\n",
    "    if outputs.get('confidence') is not None and outputs['confidence'] is not None:\n",
    "        print(f\"  Confidence: {outputs['confidence'].mean().item():.3f}\")\n",
    "    \n",
    "    # Token counts\n",
    "    num_q = (batch['question_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "    num_a = (batch['answer_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "    num_img = vision_tokens.shape[1]\n",
    "    \n",
    "    print(f\"\\nüìè Token counts:\")\n",
    "    print(f\"  Vision: {num_img}\")\n",
    "    print(f\"  Question (non-pad): {num_q}\")\n",
    "    print(f\"  Answer (non-pad): {num_a}\")\n",
    "    print(f\"  Supervised: {num_a} ({100*num_a/(num_img+num_q+num_a):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Expected:\")\n",
    "    print(f\"  - Loss < 10 (pretrained range)\")\n",
    "    print(f\"  - Supervised = answer only\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"‚úì Debug helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LOG_EVERY = 20\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  LR: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    run_name = f\"qwen_vlm_{'trm' if USE_TRM_RECURSION else 'baseline'}\"\n",
    "    wandb.init(\n",
    "        project=\"edge_glass_qwen_vlm\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            'use_trm_recursion': USE_TRM_RECURSION,\n",
    "            'num_trm_layers': NUM_TRM_LAYERS if USE_TRM_RECURSION else 0,\n",
    "            'num_recursion_steps': NUM_RECURSION_STEPS if USE_TRM_RECURSION else 0,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': config.dataset.batch_size,\n",
    "            'decoder': config.decoder.model_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "first_batch_debugged = False\n",
    "\n",
    "# Checkpoint dir\n",
    "ckpt_dir = CKPT_ROOT / f\"qwen_vlm_qa_{'trm' if USE_TRM_RECURSION else 'baseline'}\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Loop (Uncommented and Fixed)\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "steps = 0\n",
    "max_steps = 100 # Short run for verification\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(range(max_steps))\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"Training started for {max_steps} steps...\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch[\"images\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        vision_tokens = encode_images(images)\n",
    "    question_ids = batch[\"question_ids\"].to(device)\n",
    "    answer_ids = batch[\"answer_ids\"].to(device)\n",
    "    answer_mask = batch[\"answer_mask\"].to(device) # Get answer mask\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        vision_tokens=vision_tokens, \n",
    "        question_ids=question_ids, \n",
    "        answer_ids=answer_ids,\n",
    "        answer_mask=answer_mask # Pass answer mask\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Checkpointing Logic (Save Best and Last)\n",
    "    current_loss = loss.item()\n",
    "    \n",
    "    # Save best\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        torch.save({\n",
    "            'step': steps,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, ckpt_dir / 'checkpoint_best.pt')\n",
    "        \n",
    "    # Save last periodically (every 50 steps)\n",
    "    if steps > 0 and steps % 50 == 0:\n",
    "         torch.save({\n",
    "            'step': steps,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': current_loss,\n",
    "        }, ckpt_dir / 'checkpoint_last.pt')\n",
    "    \n",
    "    progress_bar.set_description(f\"Loss: {current_loss:.4f} | Best: {best_loss:.4f}\")\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    steps += 1\n",
    "    if steps >= max_steps:\n",
    "        break\n",
    "        \n",
    "# Final save\n",
    "torch.save({\n",
    "    'step': steps,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': current_loss,\n",
    "}, ckpt_dir / 'checkpoint_last.pt')\n",
    "print(f\"Training complete. Best loss: {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "try:\n",
    "    ckpt_path = ckpt_dir / 'checkpoint_best.pt'\n",
    "    if ckpt_path.exists():\n",
    "        print(f\"Loading best checkpoint from {ckpt_path}\")\n",
    "        best_ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "        print(\"‚úì Loaded best checkpoint\")\n",
    "    else:\n",
    "        print(f\"‚ö† Checkpoint not found at {ckpt_path}. Using current model state.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ö† Error loading checkpoint: {e}\")\n",
    "    print(\"This is likely due to a size mismatch from an old checkpoint.\")\n",
    "    print(\"Continuing with current model state...\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex VLM question probe on the curve image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "probe_image_path = Path(\"/home/hice1/vchopra37/scratch/projects/edge_glass/complex_curve.png\")\n",
    "probe_question = (\n",
    "    \"Provide a short narrative interpretation of the plotted curve: describe its overall trend, where the slope changes,\"    \" and what that implies about acceleration, saturation, or decay in the underlying relationship.\"\n",
    ")\n",
    "display(probe_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_image = Image.open(probe_image_path).convert(\"RGB\")\n",
    "probe_tensor = val_transforms(T.ToTensor()(probe_image)).unsqueeze(0).to(device)\n",
    "probe_tokens = encode_images(probe_tensor)\n",
    "\n",
    "probe_question_ids = qwen_decoder.tokenizer(\n",
    "    probe_question, return_tensors='pt', add_special_tokens=True\n",
    ").input_ids.to(device)\n",
    "\n",
    "probe_gen = model.generate(\n",
    "    vision_tokens=probe_tokens,\n",
    "    question_ids=probe_question_ids,\n",
    "    max_new_tokens=96,\n",
    "    temperature=0.7,\n",
    "    use_confidence=True,\n",
    "    return_stats=True,\n",
    ")\n",
    "\n",
    "if isinstance(probe_gen, torch.Tensor):\n",
    "    gen_ids = probe_gen[0].detach().cpu().tolist()\n",
    "    confidence_trace = None\n",
    "    recursion_steps = None\n",
    "else:\n",
    "    gen_ids = probe_gen[\"predictions\"][0].detach().cpu().tolist()\n",
    "    confidence_trace = probe_gen.get(\"confidences\")\n",
    "    if confidence_trace is not None:\n",
    "        confidence_trace = probe_gen[\"confidences\"][0].detach().cpu().tolist()\n",
    "    recursion_steps = probe_gen.get(\"recursion_steps\")\n",
    "    if recursion_steps is not None:\n",
    "        recursion_steps = probe_gen[\"recursion_steps\"][0].detach().cpu().tolist()\n",
    "\n",
    "probe_answer = qwen_decoder.tokenizer.decode(\n",
    "    gen_ids,\n",
    "    skip_special_tokens=True,\n",
    ").strip()\n",
    "\n",
    "print(\"Question:\", probe_question)\n",
    "print(\"Answer:\", probe_answer)\n",
    "if confidence_trace is not None:\n",
    "    print(\"Confidence trace:\", [round(float(c), 3) for c in confidence_trace])\n",
    "if recursion_steps is not None:\n",
    "    print(\"Recursion steps per token:\", recursion_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TRM' if USE_TRM_RECURSION else 'Baseline'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidences = []\n",
    "all_recursion_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating\"):\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        answers = batch['answers']\n",
    "        \n",
    "        vision_tokens = encode_images(images)\n",
    "        \n",
    "        gen_outputs = model.generate(\n",
    "            vision_tokens,\n",
    "            question_ids,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.0,\n",
    "            return_stats=True,\n",
    "        )\n",
    "        \n",
    "        if isinstance(gen_outputs, dict):\n",
    "            generated_ids = gen_outputs['predictions']\n",
    "            if gen_outputs.get('confidences') is not None:\n",
    "                all_confidences.extend(gen_outputs['confidences'].mean(dim=1).cpu().tolist())\n",
    "            if gen_outputs.get('recursion_steps') is not None:\n",
    "                all_recursion_steps.extend(gen_outputs['recursion_steps'].mean(dim=1).cpu().tolist())\n",
    "        else:\n",
    "            generated_ids = gen_outputs\n",
    "        \n",
    "        predictions = qwen_decoder.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(answers)\n",
    "\n",
    "# Metrics\n",
    "metrics = evaluate_qa(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"EM: {metrics['em']:.2f}%\")\n",
    "print(f\"F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "if all_confidences:\n",
    "    print(f\"\\nConfidence: {np.mean(all_confidences):.3f} ¬± {np.std(all_confidences):.3f}\")\n",
    "\n",
    "if all_recursion_steps:\n",
    "    print(f\"\\nRecursion: {np.mean(all_recursion_steps):.2f} steps/seq\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({'val/em_final': metrics['em'], 'val/f1_final': metrics['f1']})\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(10, len(all_predictions))):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"  Q: {val_dataset.df.iloc[i]['question'][:80]}...\")\n",
    "    print(f\"  Target: {all_targets[i][:80]}...\")\n",
    "    print(f\"  Predicted: {all_predictions[i][:80]}...\")\n",
    "    print(f\"  EM: {compute_exact_match(all_predictions[i], all_targets[i])}\")\n",
    "    print(f\"  F1: {compute_f1(all_predictions[i], all_targets[i]):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîß Config:\")\n",
    "print(f\"  Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  TRM Recursion: {USE_TRM_RECURSION}\")\n",
    "\n",
    "print(f\"\\nüìä Training:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Eval:\")\n",
    "print(f\"  EM: {metrics['em']:.2f}%\")\n",
    "print(f\"  F1: {metrics['f1']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüì¶ Dataset:\")\n",
    "print(f\"  Train: {len(train_dataset):,}\")\n",
    "print(f\"  Val: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"\\nüíæ Output:\")\n",
    "print(f\"  {ckpt_dir / 'checkpoint_best.pt'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY IMPROVEMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Pretrained Qwen decoder (vs random 34M TRM)\")\n",
    "print(\"‚úÖ Proper autoregressive training (prefix_embeds + -100 masking)\")\n",
    "print(\"‚úÖ Baseline mode for comparison\")\n",
    "print(\"‚úÖ Debug instrumentation (sanity check, param audit, first-batch)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. If EM/F1 still low:\")\n",
    "print(\"   - Check text-only generation\")\n",
    "print(\"   - Check first-batch debug\")\n",
    "print(\"   - Increase epochs/LR\")\n",
    "print(\"\\n2. If baseline works:\")\n",
    "print(\"   - Set USE_TRM_RECURSION = True\")\n",
    "print(\"   - Compare vs baseline\")\n",
    "print(\"\\n3. Expected baseline: EM > 15%, F1 > 25%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úì Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
