{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Vision-Text Alignment with Qwen Decoder\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and configuring the Vision-Text alignment model\n",
    "2. Training with multimodal alignment losses (contrastive + MRL)\n",
    "3. Instruction tuning with Qwen-7B decoder\n",
    "4. Evaluation and caption generation\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- 1-2 H200 GPUs\n",
    "- ~30-40GB GPU memory per GPU\n",
    "\n",
    "**Dataset:**\n",
    "- PixMo-Cap: 20K image-caption pairs\n",
    "- Open-Orca: 50K instruction samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import our modules\n",
    "from config import load_config\n",
    "from models import MultimodalAlignmentModel\n",
    "from data import ImageTextDataset, get_image_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n"
     ]
    }
   ],
   "source": [
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: vision_text_qwen\n",
      "Description: Vision-Text alignment with Qwen-7B decoder for instruction tuning\n",
      "\n",
      "Configuration:\n",
      "  Vision Encoder: openai/clip-vit-large-patch14\n",
      "  Text Encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Decoder: Qwen/Qwen2.5-7B-Instruct\n",
      "  Use Perceiver: False\n",
      "  Use MRL: True\n",
      "  MRL Dimensions: [512, 256, 128]\n",
      "  Batch Size: 32\n",
      "  Learning Rate: 0.0002\n",
      "  Epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# Load experiment configuration\n",
    "config_path = \"../configs/vision_text_qwen.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Experiment: {config.name}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  Text Encoder: {config.text_encoder.model_name}\")\n",
    "print(f\"  Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  Use Perceiver: {config.vision_encoder.use_perceiver}\")\n",
    "print(f\"  Use MRL: {config.vision_encoder.use_mrl}\")\n",
    "print(f\"  MRL Dimensions: {config.vision_encoder.mrl_dimensions}\")\n",
    "print(f\"  Batch Size: {config.dataset.batch_size}\")\n",
    "print(f\"  Learning Rate: {config.optimization.learning_rate}\")\n",
    "print(f\"  Epochs: {config.training.num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Creating multimodal alignment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b08cf8032e14e4296ecf89d4fcccb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
      "\n",
      "============================================================\n",
      "Component                            Trainable           Total\n",
      "------------------------------------------------------------\n",
      "Vision Encoder                       1,053,697     304,233,473\n",
      "Text Encoder                           394,240      23,107,456\n",
      "Decoder                             20,185,088   7,635,801,600\n",
      "------------------------------------------------------------\n",
      "TOTAL                              873,136,129   8,814,645,633\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating multimodal alignment model...\")\n",
    "model = MultimodalAlignmentModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "model.print_parameter_counts()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/data/pixmo/metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_dataset = \u001b[43mImageTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/pixmo/metadata.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_transforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_text_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_text_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create validation split (last 10% of data)\u001b[39;00m\n\u001b[32m     21\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.9\u001b[39m * \u001b[38;5;28mlen\u001b[39m(train_dataset))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src/data/dataset_builder.py:32\u001b[39m, in \u001b[36mImageTextDataset.__init__\u001b[39m\u001b[34m(self, metadata, metadata_path, transform, image_transforms, max_text_length)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     metadata = json.loads(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Allow either transform or image_transforms for compatibility\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m image_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/pathlib.py:1027\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[33;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1026\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.12.5-5sase6atfv2x5tf7dy5x5sqfzyguhsia/lib/python3.12/pathlib.py:1013\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1012\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/data/pixmo/metadata.json'"
     ]
    }
   ],
   "source": [
    "# Get image transforms\n",
    "train_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = ImageTextDataset(\n",
    "    metadata_path=f\"{str(DATA_DIR)}/pixmo/metadata.json\",\n",
    "    image_transforms=train_transforms,\n",
    "    max_text_length=config.dataset.max_text_length,\n",
    ")\n",
    "\n",
    "# Create validation split (last 10% of data)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    "    persistent_workers=config.dataset.persistent_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "print(f\"Image tensor shape: {sample_batch['image'].shape}\")\n",
    "print(f\"Number of captions: {len(sample_batch['text'])}\")\n",
    "\n",
    "# Visualize first 4 images with captions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(min(4, len(sample_batch['image']))):\n",
    "    # Denormalize image\n",
    "    img = sample_batch['image'][idx].cpu()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(sample_batch['text'][idx][:60] + \"...\", fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.optimization.learning_rate,\n",
    "    weight_decay=config.optimization.weight_decay,\n",
    "    betas=config.optimization.betas,\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "num_training_steps = len(train_loader) * config.training.num_epochs\n",
    "num_warmup_steps = config.optimization.warmup_steps\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if config.optimization.mixed_precision != \"no\" else None\n",
    "\n",
    "# Initialize WandB (optional)\n",
    "use_wandb = False  # Set to True if you want to use WandB\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=config.training.wandb_project,\n",
    "        name=config.training.wandb_run_name,\n",
    "        config=config.to_dict(),\n",
    "    )\n",
    "\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {config.optimization.learning_rate}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Mixed precision: {config.optimization.mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, epoch, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_contrastive_loss = 0\n",
    "    total_mrl_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        # Forward pass\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                outputs = model(images=images, texts=texts)\n",
    "                loss = outputs.loss\n",
    "        else:\n",
    "            outputs = model(images=images, texts=texts)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.optimization.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.optimization.max_grad_norm)\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        if outputs.losses is not None:\n",
    "            if 'contrastive_loss' in outputs.losses:\n",
    "                total_contrastive_loss += outputs.losses['contrastive_loss'].item()\n",
    "            if 'mrl_loss_512' in outputs.losses:\n",
    "                total_mrl_loss += outputs.losses['mrl_loss_512'].item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Log to WandB\n",
    "        if use_wandb and batch_idx % config.training.logging_steps == 0:\n",
    "            wandb.log({\n",
    "                'train/loss': loss.item(),\n",
    "                'train/lr': scheduler.get_last_lr()[0],\n",
    "                'train/epoch': epoch,\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_contrastive = total_contrastive_loss / len(dataloader)\n",
    "    avg_mrl = total_mrl_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'contrastive_loss': avg_contrastive,\n",
    "        'mrl_loss': avg_mrl,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Collect embeddings for retrieval metrics\n",
    "    vision_embeddings = []\n",
    "    text_embeddings = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        \n",
    "        if outputs.loss is not None:\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        # Collect embeddings\n",
    "        if outputs.vision_emb is not None:\n",
    "            vision_embeddings.append(outputs.vision_emb.cpu())\n",
    "        if outputs.text_emb is not None:\n",
    "            text_embeddings.append(outputs.text_emb.cpu())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    metrics = {'loss': avg_loss}\n",
    "    \n",
    "    if vision_embeddings and text_embeddings:\n",
    "        vision_embs = torch.cat(vision_embeddings, dim=0)\n",
    "        text_embs = torch.cat(text_embeddings, dim=0)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.matmul(vision_embs, text_embs.t())\n",
    "        \n",
    "        # Image-to-text retrieval\n",
    "        ranks = torch.argsort(similarity, dim=1, descending=True)\n",
    "        correct_indices = torch.arange(len(vision_embs)).unsqueeze(1)\n",
    "        \n",
    "        # R@1, R@5, R@10\n",
    "        r1 = (ranks[:, :1] == correct_indices).any(dim=1).float().mean().item()\n",
    "        r5 = (ranks[:, :5] == correct_indices).any(dim=1).float().mean().item()\n",
    "        r10 = (ranks[:, :10] == correct_indices).any(dim=1).float().mean().item()\n",
    "        \n",
    "        metrics.update({\n",
    "            'i2t_r1': r1,\n",
    "            'i2t_r5': r5,\n",
    "            'i2t_r10': r10,\n",
    "        })\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_i2t_r1': [],\n",
    "    'val_i2t_r5': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "output_dir = Path(config.training.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.training.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.training.num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, scaler, epoch, device)\n",
    "    print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
    "    print(f\"  - Contrastive: {train_metrics['contrastive_loss']:.4f}\")\n",
    "    print(f\"  - MRL: {train_metrics['mrl_loss']:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    print(f\"\\nValidation Loss: {val_metrics['loss']:.4f}\")\n",
    "    if 'i2t_r1' in val_metrics:\n",
    "        print(f\"  - Image→Text R@1:  {val_metrics['i2t_r1']*100:.2f}%\")\n",
    "        print(f\"  - Image→Text R@5:  {val_metrics['i2t_r5']*100:.2f}%\")\n",
    "        print(f\"  - Image→Text R@10: {val_metrics['i2t_r10']*100:.2f}%\")\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    if 'i2t_r1' in val_metrics:\n",
    "        history['val_i2t_r1'].append(val_metrics['i2t_r1'])\n",
    "        history['val_i2t_r5'].append(val_metrics['i2t_r5'])\n",
    "    \n",
    "    # Log to WandB\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'val/loss': val_metrics['loss'],\n",
    "            'val/i2t_r1': val_metrics.get('i2t_r1', 0),\n",
    "            'val/i2t_r5': val_metrics.get('i2t_r5', 0),\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        checkpoint_path = output_dir / \"best_model.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"\\n✓ Saved best model to {checkpoint_path}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % (config.training.save_steps // len(train_loader)) == 0:\n",
    "        checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✓ Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Retrieval metrics\n",
    "if history['val_i2t_r1']:\n",
    "    axes[1].plot([r*100 for r in history['val_i2t_r1']], label='R@1', marker='o')\n",
    "    axes[1].plot([r*100 for r in history['val_i2t_r5']], label='R@5', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Recall (%)')\n",
    "    axes[1].set_title('Image→Text Retrieval')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {output_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Generation on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(output_dir / \"best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model for inference\")\n",
    "print(f\"Best epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Best val loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images\n",
    "sample_batch = next(iter(val_loader))\n",
    "images = sample_batch['image'][:4].to(device)\n",
    "ground_truth = sample_batch['text'][:4]\n",
    "\n",
    "# Generate captions\n",
    "print(\"Generating captions...\\n\")\n",
    "with torch.no_grad():\n",
    "    generated_captions = model.generate(\n",
    "        images=images,\n",
    "        prompt=\"Describe this image in detail:\",\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(4):\n",
    "    # Denormalize image\n",
    "    img = images[idx].cpu()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Add captions\n",
    "    title = f\"Ground Truth: {ground_truth[idx][:80]}...\\n\\n\"\n",
    "    title += f\"Generated: {generated_captions[idx][:80]}...\"\n",
    "    axes[idx].set_title(title, fontsize=9, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'generated_captions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print full captions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Full Generated Captions:\")\n",
    "print(\"=\"*60)\n",
    "for idx, (gt, gen) in enumerate(zip(ground_truth, generated_captions)):\n",
    "    print(f\"\\nImage {idx+1}:\")\n",
    "    print(f\"  Ground Truth: {gt}\")\n",
    "    print(f\"  Generated:    {gen}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Embedding Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect embeddings from validation set\n",
    "print(\"Collecting embeddings for visualization...\")\n",
    "vision_embs = []\n",
    "text_embs = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(list(val_loader)[:10]):  # First 10 batches\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        \n",
    "        vision_embs.append(outputs.vision_emb.cpu())\n",
    "        text_embs.append(outputs.text_emb.cpu())\n",
    "        captions.extend(texts)\n",
    "\n",
    "vision_embs = torch.cat(vision_embs, dim=0).numpy()\n",
    "text_embs = torch.cat(text_embs, dim=0).numpy()\n",
    "\n",
    "print(f\"Vision embeddings: {vision_embs.shape}\")\n",
    "print(f\"Text embeddings: {text_embs.shape}\")\n",
    "\n",
    "# Apply PCA\n",
    "all_embs = np.vstack([vision_embs, text_embs])\n",
    "pca = PCA(n_components=2)\n",
    "all_embs_2d = pca.fit_transform(all_embs)\n",
    "\n",
    "vision_embs_2d = all_embs_2d[:len(vision_embs)]\n",
    "text_embs_2d = all_embs_2d[len(vision_embs):]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(vision_embs_2d[:, 0], vision_embs_2d[:, 1], \n",
    "           c='blue', alpha=0.6, s=50, label='Vision')\n",
    "plt.scatter(text_embs_2d[:, 0], text_embs_2d[:, 1], \n",
    "           c='red', alpha=0.6, s=50, label='Text')\n",
    "\n",
    "# Draw lines connecting matching pairs\n",
    "for i in range(min(50, len(vision_embs_2d))):  # First 50 pairs\n",
    "    plt.plot([vision_embs_2d[i, 0], text_embs_2d[i, 0]], \n",
    "            [vision_embs_2d[i, 1], text_embs_2d[i, 1]], \n",
    "            'gray', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Vision-Text Embedding Space (PCA Projection)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(output_dir / 'embedding_space.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEmbedding space visualization saved to {output_dir / 'embedding_space.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Model: {config.name}\")\n",
    "print(f\"  - Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  - Text Encoder: {config.text_encoder.model_name}\")\n",
    "print(f\"  - Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  - Use MRL: {config.vision_encoder.use_mrl}\")\n",
    "print(f\"  - Use Perceiver: {config.vision_encoder.use_perceiver}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {config.training.num_epochs}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "if history['val_i2t_r1']:\n",
    "    print(f\"  - Best R@1: {max(history['val_i2t_r1'])*100:.2f}%\")\n",
    "    print(f\"  - Best R@5: {max(history['val_i2t_r5'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  - Trainable: {model.num_trainable_parameters:,}\")\n",
    "print(f\"  - Total: {model.num_total_parameters:,}\")\n",
    "print(f\"  - Trainable %: {100*model.num_trainable_parameters/model.num_total_parameters:.2f}%\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - Best model: {output_dir / 'best_model.pt'}\")\n",
    "print(f\"  - Training curves: {output_dir / 'training_curves.png'}\")\n",
    "print(f\"  - Generated captions: {output_dir / 'generated_captions.png'}\")\n",
    "print(f\"  - Embedding space: {output_dir / 'embedding_space.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Experiment with different MRL dimensions\")\n",
    "print(\"2. Try enabling Perceiver resampler\")\n",
    "print(\"3. Test on your own images\")\n",
    "print(\"4. Run tri-modal experiment (Notebook 2)\")\n",
    "print(\"5. Compare with TRM decoder (Notebook 3)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save training history\n",
    "history_path = output_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# Close WandB\n",
    "if use_wandb:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n✓ Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
