{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Vision-Text Alignment with Qwen Decoder\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and configuring the Vision-Text alignment model\n",
    "2. Training with multimodal alignment losses (contrastive + MRL)\n",
    "3. Instruction tuning with Qwen-7B decoder\n",
    "4. Evaluation and caption generation\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- 1-2 H200 GPUs\n",
    "- ~30-40GB GPU memory per GPU\n",
    "\n",
    "**Dataset:**\n",
    "- PixMo-Cap: 20K image-caption pairs\n",
    "- Open-Orca: 50K instruction samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "Path.cwd().parent / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import our modules\n",
    "from config import load_config\n",
    "from models import MultimodalAlignmentModel\n",
    "from data import ImageTextDataset, get_image_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: vision_text_qwen\n",
      "Description: Vision-Text alignment with Qwen-7B decoder for instruction tuning\n",
      "\n",
      "Configuration:\n",
      "  Vision Encoder: openai/clip-vit-large-patch14\n",
      "  Text Encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Decoder: Qwen/Qwen2.5-7B-Instruct\n",
      "  Use Perceiver: False\n",
      "  Use MRL: True\n",
      "  MRL Dimensions: [512, 256, 128]\n",
      "  Batch Size: 32\n",
      "  Learning Rate: 0.0002\n",
      "  Epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# Load experiment configuration\n",
    "config_path = \"../configs/vision_text_qwen.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Experiment: {config.name}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  Text Encoder: {config.text_encoder.model_name}\")\n",
    "print(f\"  Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  Use Perceiver: {config.vision_encoder.use_perceiver}\")\n",
    "print(f\"  Use MRL: {config.vision_encoder.use_mrl}\")\n",
    "print(f\"  MRL Dimensions: {config.vision_encoder.mrl_dimensions}\")\n",
    "print(f\"  Batch Size: {config.dataset.batch_size}\")\n",
    "print(f\"  Learning Rate: {config.optimization.learning_rate}\")\n",
    "print(f\"  Epochs: {config.training.num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Creating multimodal alignment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0214b9c4f04a48b9bc75e48bedbaeff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
      "\n",
      "============================================================\n",
      "Component                            Trainable           Total\n",
      "------------------------------------------------------------\n",
      "Vision Encoder                       1,049,600     304,229,376\n",
      "Text Encoder                           394,240      23,107,456\n",
      "Decoder                             20,185,088   7,635,801,600\n",
      "------------------------------------------------------------\n",
      "TOTAL                              873,132,032   8,814,641,536\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating multimodal alignment model...\")\n",
    "model = MultimodalAlignmentModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "model.print_parameter_counts()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/data')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train samples: 15898\n",
      "Validation samples: 1767\n",
      "\n",
      "Train batches: 497\n",
      "Validation batches: 56\n"
     ]
    }
   ],
   "source": [
    "# Get image transforms\n",
    "train_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = ImageTextDataset(\n",
    "    metadata_path=f\"{str(DATA_DIR)}/pixmo/metadata.json\",\n",
    "    image_transforms=train_transforms,\n",
    "    max_text_length=config.dataset.max_text_length,\n",
    ")\n",
    "\n",
    "# Create validation split (last 10% of data)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    "    persistent_workers=config.dataset.persistent_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 416, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src/data/dataset_builder.py\", line 44, in __getitem__\n    image = read_image(item[\"image_path\"]).float() / 255.0\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchvision/io/image.py\", line 336, in read_image\n    data = read_file(path)\n           ^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchvision/io/image.py\", line 64, in read_file\n    data = torch.ops.image.read_file(str(path))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/_ops.py\", line 1255, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: [Errno 2] No such file or directory: 'data/pixmo/pixmo_0008667.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get a sample batch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sample_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 416, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src/data/dataset_builder.py\", line 44, in __getitem__\n    image = read_image(item[\"image_path\"]).float() / 255.0\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchvision/io/image.py\", line 336, in read_image\n    data = read_file(path)\n           ^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torchvision/io/image.py\", line 64, in read_file\n    data = torch.ops.image.read_file(str(path))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/_ops.py\", line 1255, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: [Errno 2] No such file or directory: 'data/pixmo/pixmo_0008667.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "print(f\"Image tensor shape: {sample_batch['image'].shape}\")\n",
    "print(f\"Number of captions: {len(sample_batch['text'])}\")\n",
    "\n",
    "# Visualize first 4 images with captions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(min(4, len(sample_batch['image']))):\n",
    "    # Denormalize image\n",
    "    img = sample_batch['image'][idx].cpu()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(sample_batch['text'][idx][:60] + \"...\", fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.optimization.learning_rate,\n",
    "    weight_decay=config.optimization.weight_decay,\n",
    "    betas=config.optimization.betas,\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "num_training_steps = len(train_loader) * config.training.num_epochs\n",
    "num_warmup_steps = config.optimization.warmup_steps\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if config.optimization.mixed_precision != \"no\" else None\n",
    "\n",
    "# Initialize WandB (optional)\n",
    "use_wandb = False  # Set to True if you want to use WandB\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=config.training.wandb_project,\n",
    "        name=config.training.wandb_run_name,\n",
    "        config=config.to_dict(),\n",
    "    )\n",
    "\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {config.optimization.learning_rate}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Mixed precision: {config.optimization.mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, epoch, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_contrastive_loss = 0\n",
    "    total_mrl_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        # Forward pass\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                outputs = model(images=images, texts=texts)\n",
    "                loss = outputs.loss\n",
    "        else:\n",
    "            outputs = model(images=images, texts=texts)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.optimization.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.optimization.max_grad_norm)\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        if outputs.losses is not None:\n",
    "            if 'contrastive_loss' in outputs.losses:\n",
    "                total_contrastive_loss += outputs.losses['contrastive_loss'].item()\n",
    "            if 'mrl_loss_512' in outputs.losses:\n",
    "                total_mrl_loss += outputs.losses['mrl_loss_512'].item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Log to WandB\n",
    "        if use_wandb and batch_idx % config.training.logging_steps == 0:\n",
    "            wandb.log({\n",
    "                'train/loss': loss.item(),\n",
    "                'train/lr': scheduler.get_last_lr()[0],\n",
    "                'train/epoch': epoch,\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_contrastive = total_contrastive_loss / len(dataloader)\n",
    "    avg_mrl = total_mrl_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'contrastive_loss': avg_contrastive,\n",
    "        'mrl_loss': avg_mrl,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Collect embeddings for retrieval metrics\n",
    "    vision_embeddings = []\n",
    "    text_embeddings = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        \n",
    "        if outputs.loss is not None:\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        # Collect embeddings\n",
    "        if outputs.vision_emb is not None:\n",
    "            vision_embeddings.append(outputs.vision_emb.cpu())\n",
    "        if outputs.text_emb is not None:\n",
    "            text_embeddings.append(outputs.text_emb.cpu())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    metrics = {'loss': avg_loss}\n",
    "    \n",
    "    if vision_embeddings and text_embeddings:\n",
    "        vision_embs = torch.cat(vision_embeddings, dim=0)\n",
    "        text_embs = torch.cat(text_embeddings, dim=0)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.matmul(vision_embs, text_embs.t())\n",
    "        \n",
    "        # Image-to-text retrieval\n",
    "        ranks = torch.argsort(similarity, dim=1, descending=True)\n",
    "        correct_indices = torch.arange(len(vision_embs)).unsqueeze(1)\n",
    "        \n",
    "        # R@1, R@5, R@10\n",
    "        r1 = (ranks[:, :1] == correct_indices).any(dim=1).float().mean().item()\n",
    "        r5 = (ranks[:, :5] == correct_indices).any(dim=1).float().mean().item()\n",
    "        r10 = (ranks[:, :10] == correct_indices).any(dim=1).float().mean().item()\n",
    "        \n",
    "        metrics.update({\n",
    "            'i2t_r1': r1,\n",
    "            'i2t_r5': r5,\n",
    "            'i2t_r10': r10,\n",
    "        })\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_i2t_r1': [],\n",
    "    'val_i2t_r5': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "output_dir = Path(config.training.output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.training.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.training.num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, scaler, epoch, device)\n",
    "    print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
    "    print(f\"  - Contrastive: {train_metrics['contrastive_loss']:.4f}\")\n",
    "    print(f\"  - MRL: {train_metrics['mrl_loss']:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    print(f\"\\nValidation Loss: {val_metrics['loss']:.4f}\")\n",
    "    if 'i2t_r1' in val_metrics:\n",
    "        print(f\"  - Image→Text R@1:  {val_metrics['i2t_r1']*100:.2f}%\")\n",
    "        print(f\"  - Image→Text R@5:  {val_metrics['i2t_r5']*100:.2f}%\")\n",
    "        print(f\"  - Image→Text R@10: {val_metrics['i2t_r10']*100:.2f}%\")\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    if 'i2t_r1' in val_metrics:\n",
    "        history['val_i2t_r1'].append(val_metrics['i2t_r1'])\n",
    "        history['val_i2t_r5'].append(val_metrics['i2t_r5'])\n",
    "    \n",
    "    # Log to WandB\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'val/loss': val_metrics['loss'],\n",
    "            'val/i2t_r1': val_metrics.get('i2t_r1', 0),\n",
    "            'val/i2t_r5': val_metrics.get('i2t_r5', 0),\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        checkpoint_path = output_dir / \"best_model.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"\\n✓ Saved best model to {checkpoint_path}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % (config.training.save_steps // len(train_loader)) == 0:\n",
    "        checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'config': config,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✓ Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Retrieval metrics\n",
    "if history['val_i2t_r1']:\n",
    "    axes[1].plot([r*100 for r in history['val_i2t_r1']], label='R@1', marker='o')\n",
    "    axes[1].plot([r*100 for r in history['val_i2t_r5']], label='R@5', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Recall (%)')\n",
    "    axes[1].set_title('Image→Text Retrieval')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {output_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Generation on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(output_dir / \"best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model for inference\")\n",
    "print(f\"Best epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Best val loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images\n",
    "sample_batch = next(iter(val_loader))\n",
    "images = sample_batch['image'][:4].to(device)\n",
    "ground_truth = sample_batch['text'][:4]\n",
    "\n",
    "# Generate captions\n",
    "print(\"Generating captions...\\n\")\n",
    "with torch.no_grad():\n",
    "    generated_captions = model.generate(\n",
    "        images=images,\n",
    "        prompt=\"Describe this image in detail:\",\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(4):\n",
    "    # Denormalize image\n",
    "    img = images[idx].cpu()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Add captions\n",
    "    title = f\"Ground Truth: {ground_truth[idx][:80]}...\\n\\n\"\n",
    "    title += f\"Generated: {generated_captions[idx][:80]}...\"\n",
    "    axes[idx].set_title(title, fontsize=9, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'generated_captions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print full captions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Full Generated Captions:\")\n",
    "print(\"=\"*60)\n",
    "for idx, (gt, gen) in enumerate(zip(ground_truth, generated_captions)):\n",
    "    print(f\"\\nImage {idx+1}:\")\n",
    "    print(f\"  Ground Truth: {gt}\")\n",
    "    print(f\"  Generated:    {gen}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Embedding Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect embeddings from validation set\n",
    "print(\"Collecting embeddings for visualization...\")\n",
    "vision_embs = []\n",
    "text_embs = []\n",
    "captions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(list(val_loader)[:10]):  # First 10 batches\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        \n",
    "        vision_embs.append(outputs.vision_emb.cpu())\n",
    "        text_embs.append(outputs.text_emb.cpu())\n",
    "        captions.extend(texts)\n",
    "\n",
    "vision_embs = torch.cat(vision_embs, dim=0).numpy()\n",
    "text_embs = torch.cat(text_embs, dim=0).numpy()\n",
    "\n",
    "print(f\"Vision embeddings: {vision_embs.shape}\")\n",
    "print(f\"Text embeddings: {text_embs.shape}\")\n",
    "\n",
    "# Apply PCA\n",
    "all_embs = np.vstack([vision_embs, text_embs])\n",
    "pca = PCA(n_components=2)\n",
    "all_embs_2d = pca.fit_transform(all_embs)\n",
    "\n",
    "vision_embs_2d = all_embs_2d[:len(vision_embs)]\n",
    "text_embs_2d = all_embs_2d[len(vision_embs):]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(vision_embs_2d[:, 0], vision_embs_2d[:, 1], \n",
    "           c='blue', alpha=0.6, s=50, label='Vision')\n",
    "plt.scatter(text_embs_2d[:, 0], text_embs_2d[:, 1], \n",
    "           c='red', alpha=0.6, s=50, label='Text')\n",
    "\n",
    "# Draw lines connecting matching pairs\n",
    "for i in range(min(50, len(vision_embs_2d))):  # First 50 pairs\n",
    "    plt.plot([vision_embs_2d[i, 0], text_embs_2d[i, 0]], \n",
    "            [vision_embs_2d[i, 1], text_embs_2d[i, 1]], \n",
    "            'gray', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Vision-Text Embedding Space (PCA Projection)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(output_dir / 'embedding_space.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEmbedding space visualization saved to {output_dir / 'embedding_space.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Model: {config.name}\")\n",
    "print(f\"  - Vision Encoder: {config.vision_encoder.model_name}\")\n",
    "print(f\"  - Text Encoder: {config.text_encoder.model_name}\")\n",
    "print(f\"  - Decoder: {config.decoder.model_name}\")\n",
    "print(f\"  - Use MRL: {config.vision_encoder.use_mrl}\")\n",
    "print(f\"  - Use Perceiver: {config.vision_encoder.use_perceiver}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {config.training.num_epochs}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "if history['val_i2t_r1']:\n",
    "    print(f\"  - Best R@1: {max(history['val_i2t_r1'])*100:.2f}%\")\n",
    "    print(f\"  - Best R@5: {max(history['val_i2t_r5'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  - Trainable: {model.num_trainable_parameters:,}\")\n",
    "print(f\"  - Total: {model.num_total_parameters:,}\")\n",
    "print(f\"  - Trainable %: {100*model.num_trainable_parameters/model.num_total_parameters:.2f}%\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - Best model: {output_dir / 'best_model.pt'}\")\n",
    "print(f\"  - Training curves: {output_dir / 'training_curves.png'}\")\n",
    "print(f\"  - Generated captions: {output_dir / 'generated_captions.png'}\")\n",
    "print(f\"  - Embedding space: {output_dir / 'embedding_space.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Experiment with different MRL dimensions\")\n",
    "print(\"2. Try enabling Perceiver resampler\")\n",
    "print(\"3. Test on your own images\")\n",
    "print(\"4. Run tri-modal experiment (Notebook 2)\")\n",
    "print(\"5. Compare with TRM decoder (Notebook 3)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save training history\n",
    "history_path = output_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# Close WandB\n",
    "if use_wandb:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n✓ Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
