{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Training with PixMo QA Dataset (Modular Version)\n",
    "\n",
    "This notebook implements a **Vision-Language Model for Question Answering** using:\n",
    "1. Modular vision encoder from `edge_glass_modular/src/encoders`\n",
    "2. Qwen decoder with LoRA from `edge_glass_modular/src/decoders`\n",
    "3. PixMo QA dataset with real question-answer pairs\n",
    "4. Configuration-driven architecture\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "```\n",
    "Image (B, 3, 336, 336)\n",
    "  ↓\n",
    "Vision Encoder (frozen or trainable)\n",
    "  ↓ (B, num_latents, vision_dim)\n",
    "Projection to Qwen hidden dim\n",
    "  ↓ (B, num_latents, qwen_dim)\n",
    "Qwen Decoder with LoRA (trainable)\n",
    "  ↓\n",
    "Token Layout: [IMG_TOKENS] [QUESTION_TOKENS] [ANSWER_TOKENS]\n",
    "  ↓\n",
    "Loss on answer tokens only\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "print(f\"Added to path: {Path.cwd().parent / 'src'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 150.11 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from typing import Optional, Dict\n",
    "import warnings\n",
    "\n",
    "# Import modular components from edge_glass_modular\n",
    "from config import load_config\n",
    "from encoders.vision import VisionEncoder\n",
    "from decoders.qwen import QwenDecoder\n",
    "from data.dataset_builder import PixmoQADataset\n",
    "from data.transforms import get_image_transforms\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: trm_vlm_qa\n",
      "\n",
      "Dataset:\n",
      "  Train parquet: /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pimo-alignment/pixmo_qa_mixed_with_bytes.parquet\n",
      "  Image size: 336\n",
      "  Max question length: 128\n",
      "  Max answer length: 256\n",
      "  Batch size: 16\n",
      "\n",
      "Decoder:\n",
      "  Type: qwen\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Use LoRA: True\n",
      "  LoRA rank: 64\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/trm_vlm_qa.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(f\"Loaded config: {config.name}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train parquet: {config.dataset.train_parquet}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Max question length: {config.dataset.max_question_length}\")\n",
    "print(f\"  Max answer length: {config.dataset.max_answer_length}\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  Type: {config.decoder.type}\")\n",
    "print(f\"  Model: {config.decoder.model_name}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  LoRA rank: {config.decoder.lora_r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Vision Encoder\n",
    "\n",
    "Load the pretrained vision encoder. We can either:\n",
    "1. Load a pretrained aligned model checkpoint, or  \n",
    "2. Create a fresh vision encoder\n",
    "\n",
    "For this notebook, we'll load the aligned model if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e2970e4f2c4b9eb4a58799ca471ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "✓ Using vision encoder from aligned model\n",
      "\n",
      "Vision Encoder:\n",
      "  Model: openai/clip-vit-large-patch14-336\n",
      "  Projection dim: 4096\n",
      "  Use Perceiver: True\n",
      "  Use MRL: True\n",
      "  Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Load from aligned model checkpoint (recommended)\n",
    "try:\n",
    "    from models.alignment import MultimodalAlignmentModel\n",
    "    \n",
    "    alignment_config_path = \"../configs/perceiver_mrl_alignment.yaml\"\n",
    "    alignment_config = load_config(alignment_config_path)\n",
    "    \n",
    "    aligned_model = MultimodalAlignmentModel(alignment_config).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = \"checkpoints/perceiver_mrl_alignment/checkpoint_best.pt\"\n",
    "    if Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        aligned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✓ Loaded aligned model from {checkpoint_path}\")\n",
    "        print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    \n",
    "    # Extract vision encoder\n",
    "    vision_encoder = aligned_model.vision_encoder\n",
    "    print(f\"✓ Using vision encoder from aligned model\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load aligned model: {e}\")\n",
    "    print(\"Creating fresh vision encoder...\")\n",
    "    \n",
    "    # Option 2: Create fresh vision encoder\n",
    "    vision_encoder = VisionEncoder(\n",
    "        model_name=\"openai/clip-vit-large-patch14\",\n",
    "        projection_dim=4096,\n",
    "        freeze=True,\n",
    "        use_perceiver=True,\n",
    "        perceiver_num_latents=64,\n",
    "        perceiver_latent_dim=512,\n",
    "        perceiver_num_layers=3,\n",
    "        use_mrl=True,\n",
    "    ).to(device)\n",
    "    print(f\"✓ Created fresh vision encoder\")\n",
    "\n",
    "# Freeze vision encoder (optional)\n",
    "vision_encoder.eval()\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"\\nVision Encoder:\")\n",
    "print(f\"  Model: {vision_encoder.model_name}\")\n",
    "print(f\"  Projection dim: {vision_encoder.projection_dim}\")\n",
    "print(f\"  Use Perceiver: {vision_encoder.use_perceiver}\")\n",
    "print(f\"  Use MRL: {vision_encoder.use_mrl}\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in vision_encoder.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Qwen Decoder\n",
    "\n",
    "Initialize the Qwen decoder with LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955d76bdc0a147aaa9c49ca5e8cb9fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "\n",
      "Qwen Decoder:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Hidden dim: 3584\n",
      "  Vocab size: 152064\n",
      "  Use LoRA: True\n",
      "  Trainable params: 40,370,176\n",
      "  Total params: 7,655,986,688\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qwen decoder\n",
    "decoder = QwenDecoder(\n",
    "    model_name=config.decoder.model_name,\n",
    "    load_in_8bit=config.decoder.load_in_8bit,\n",
    "    load_in_4bit=config.decoder.load_in_4bit,\n",
    "    use_lora=config.decoder.use_lora,\n",
    "    lora_r=config.decoder.lora_r,\n",
    "    lora_alpha=config.decoder.lora_alpha,\n",
    "    lora_dropout=config.decoder.lora_dropout,\n",
    "    lora_target_modules=config.decoder.lora_target_modules,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = decoder.tokenizer\n",
    "\n",
    "print(f\"\\nQwen Decoder:\")\n",
    "print(f\"  Model: {decoder.model_name}\")\n",
    "print(f\"  Hidden dim: {decoder.hidden_dim}\")\n",
    "print(f\"  Vocab size: {decoder.vocab_size}\")\n",
    "print(f\"  Use LoRA: {config.decoder.use_lora}\")\n",
    "print(f\"  Trainable params: {decoder.num_parameters:,}\")\n",
    "print(f\"  Total params: {decoder.num_total_parameters:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Vision Projection Layer\n",
    "\n",
    "Project vision tokens to Qwen's hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Projection:\n",
      "  Input dim: 4096\n",
      "  Output dim: 3584\n",
      "  Trainable params: 14,683,648\n"
     ]
    }
   ],
   "source": [
    "# Create projection layer: vision_dim -> qwen_dim\n",
    "vision_proj = nn.Linear(vision_encoder.projection_dim, decoder.hidden_dim).to(device)\n",
    "\n",
    "print(f\"Vision Projection:\")\n",
    "print(f\"  Input dim: {vision_encoder.projection_dim}\")\n",
    "print(f\"  Output dim: {decoder.hidden_dim}\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in vision_proj.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12000 samples from /home/hice1/vchopra37/scratch/projects/edge_glass/dataset/final_dataset/pimo-alignment/pixmo_qa_mixed_with_bytes.parquet\n",
      "\n",
      "Dataset:\n",
      "  Train samples: 12,000\n",
      "  Image size: 336\n",
      "  Max question tokens: 128\n",
      "  Max answer tokens: 256\n"
     ]
    }
   ],
   "source": [
    "# Get image transforms\n",
    "train_transforms = get_image_transforms(\n",
    "    image_size=config.dataset.image_size, \n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = PixmoQADataset(\n",
    "    parquet_path=config.dataset.train_parquet,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transforms=train_transforms,\n",
    "    max_question_length=config.dataset.max_question_length,\n",
    "    max_answer_length=config.dataset.max_answer_length,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"  Max question tokens: {config.dataset.max_question_length}\")\n",
    "print(f\"  Max answer tokens: {config.dataset.max_answer_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from dataset:\n",
      "  Image shape: torch.Size([3, 336, 336])\n",
      "  Question: [USER]Why is John Travolta wearing such an obviously fake mustache in this photo? Is this from a mov...\n",
      "  Answer: This image is likely from a movie or promotional event. The exaggerated fake mustache suggests it's ...\n",
      "  Question IDs shape: torch.Size([34])\n",
      "  Answer IDs shape: torch.Size([87])\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample from dataset:\")\n",
    "print(f\"  Image shape: {sample['image'].shape}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Answer: {sample['answer'][:100]}...\")\n",
    "print(f\"  Question IDs shape: {sample['question_ids'].shape}\")\n",
    "print(f\"  Answer IDs shape: {sample['answer_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoader:\n",
      "  Batch size: 16\n",
      "  Num batches: 750\n",
      "  Num workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Create custom collate function\n",
    "def collate_qa_batch(batch):\n",
    "    \"\"\"Collate function with proper padding.\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    questions = [item['question'] for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    question_ids = nn.utils.rnn.pad_sequence(\n",
    "        [item['question_ids'] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    question_mask = nn.utils.rnn.pad_sequence(\n",
    "        [item['question_mask'] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    answer_ids = nn.utils.rnn.pad_sequence(\n",
    "        [item['answer_ids'] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    answer_mask = nn.utils.rnn.pad_sequence(\n",
    "        [item['answer_mask'] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'question_ids': question_ids,\n",
    "        'question_mask': question_mask,\n",
    "        'answer_ids': answer_ids,\n",
    "        'answer_mask': answer_mask,\n",
    "    }\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.dataset.num_workers,\n",
    "    collate_fn=collate_qa_batch,\n",
    "    pin_memory=config.dataset.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader:\")\n",
    "print(f\"  Batch size: {config.dataset.batch_size}\")\n",
    "print(f\"  Num batches: {len(train_loader)}\")\n",
    "print(f\"  Num workers: {config.dataset.num_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Setup:\n",
      "  Num epochs: 10\n",
      "  Learning rate: 0.0001\n",
      "  Total steps: 7500\n",
      "  Warmup steps: 375\n",
      "  Trainable parameters: 55,053,824\n"
     ]
    }
   ],
   "source": [
    "# Optimizer - only train projection and decoder LoRA\n",
    "trainable_params = list(vision_proj.parameters()) + list(decoder.parameters())\n",
    "trainable_params = [p for p in trainable_params if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=config.optimization.learning_rate,\n",
    "    betas=config.optimization.betas,\n",
    "    weight_decay=config.optimization.weight_decay,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.training.num_epochs\n",
    "warmup_steps = int(total_steps * config.optimization.warmup_ratio)\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Training Setup:\")\n",
    "print(f\"  Num epochs: {config.training.num_epochs}\")\n",
    "print(f\"  Learning rate: {config.optimization.learning_rate}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvedaangchopra\u001b[0m (\u001b[33mvedaangchopra_gatech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/notebooks/wandb/run-20251204_134000-ker8wp99</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/ker8wp99' target=\"_blank\">trm_vlm_qa_Qwen2.5-7B-Instruct</a></strong> to <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/ker8wp99' target=\"_blank\">https://wandb.ai/vedaangchopra_gatech/edge_glass_trm_vlm/runs/ker8wp99</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint directory: checkpoints/trm_vlm_qa\n"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "if config.trainer.use_wandb:\n",
    "    wandb.init(\n",
    "        project=config.trainer.wandb_project,\n",
    "        name=f\"{config.name}_{config.decoder.model_name.split('/')[-1]}\",\n",
    "        config=config.to_dict(),\n",
    "        tags=config.tags if hasattr(config, \"tags\") else [\"vlm\", \"qa\"],\n",
    "    )\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = Path(config.trainer.ckpt_dir)\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {ckpt_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b5f4d6f3d44fba98b597e416d014b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 average loss: 0.7420\n",
      "✓ Saved best checkpoint (loss: 0.7420)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0b655dd6834eadb259f9eb8c6c1a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 average loss: 0.4978\n",
      "✓ Saved best checkpoint (loss: 0.4978)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520af337703b488cb6a605d1c5216952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 average loss: 0.4412\n",
      "✓ Saved best checkpoint (loss: 0.4412)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfd582275a84df3acadb1670f4dd529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 average loss: 0.3706\n",
      "✓ Saved best checkpoint (loss: 0.3706)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba266149dc9412eab3eb367a9819c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 average loss: 0.3011\n",
      "✓ Saved best checkpoint (loss: 0.3011)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e83a962f6984d7c91a757ce8f01efd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 average loss: 0.2393\n",
      "✓ Saved best checkpoint (loss: 0.2393)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc94bbe995a41c58f59c8953ec47828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 average loss: 0.1880\n",
      "✓ Saved best checkpoint (loss: 0.1880)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b47fb62d8647d494469a661475b9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 average loss: 0.1492\n",
      "✓ Saved best checkpoint (loss: 0.1492)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfd1bce82a0413eb7d4e251db6ff603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (123500000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (133340000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "vision_proj.train()\n",
    "decoder.model.train()\n",
    "\n",
    "for epoch in range(config.training.num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.training.num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_losses = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['images'].to(device)\n",
    "        question_ids = batch['question_ids'].to(device)\n",
    "        question_mask = batch['question_mask'].to(device)\n",
    "        answer_ids = batch['answer_ids'].to(device)\n",
    "        answer_mask = batch['answer_mask'].to(device)\n",
    "        \n",
    "        # Encode images (frozen)\n",
    "        with torch.no_grad():\n",
    "            vision_output = vision_encoder(images, return_sequence=True)\n",
    "            vision_tokens = vision_output.sequence  # (B, num_latents, vision_dim)\n",
    "        \n",
    "        # Project vision tokens\n",
    "        vision_prefix = vision_proj(vision_tokens)  # (B, num_latents, qwen_dim)\n",
    "        \n",
    "        # Concatenate question and answer\n",
    "        input_ids = torch.cat([question_ids, answer_ids], dim=1)\n",
    "        attention_mask = torch.cat([question_mask, answer_mask], dim=1)\n",
    "        \n",
    "        # Create labels: -100 for question, answer_ids for answer\n",
    "        question_labels = torch.full_like(question_ids, fill_value=-100)\n",
    "        labels = torch.cat([question_labels, answer_ids], dim=1)\n",
    "        \n",
    "        # Forward pass through decoder\n",
    "        outputs = decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            prefix_embeds=vision_prefix,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, config.optimization.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        epoch_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % config.training.logging_steps == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-config.training.logging_steps:])\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            if config.trainer.use_wandb:\n",
    "                wandb.log({\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/lr': scheduler.get_last_lr()[0],\n",
    "                    'step': global_step,\n",
    "                })\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    print(f\"\\nEpoch {epoch+1} average loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'vision_proj_state_dict': vision_proj.state_dict(),\n",
    "            'decoder_state_dict': decoder.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'config': config.to_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, ckpt_dir / \"checkpoint_best.pt\")\n",
    "        print(f\"✓ Saved best checkpoint (loss: {best_loss:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(ckpt_dir / \"checkpoint_best.pt\", map_location=device)\n",
    "vision_proj.load_state_dict(checkpoint['vision_proj_state_dict'])\n",
    "decoder.model.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "vision_proj.eval()\n",
    "decoder.model.eval()\n",
    "\n",
    "print(\"Loaded best checkpoint for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer for a sample\n",
    "sample_idx = 0\n",
    "sample = train_dataset[sample_idx]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"  Question: {sample['question']}\")\n",
    "print(f\"  Ground truth: {sample['answer'][:200]}...\\n\")\n",
    "\n",
    "# Prepare inputs\n",
    "image = sample['image'].unsqueeze(0).to(device)\n",
    "question_ids = sample['question_ids'].unsqueeze(0).to(device)\n",
    "question_mask = sample['question_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image\n",
    "with torch.no_grad():\n",
    "    vision_output = vision_encoder(image, return_sequence=True)\n",
    "    vision_tokens = vision_output.sequence\n",
    "    vision_prefix = vision_proj(vision_tokens)\n",
    "    \n",
    "    # Generate\n",
    "    generated_ids = decoder.generate(\n",
    "        input_ids=question_ids,\n",
    "        prefix_embeds=vision_prefix,\n",
    "        attention_mask=question_mask,\n",
    "        max_new_tokens=config.dataset.max_answer_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    # Skip the question tokens\n",
    "    answer_start = question_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(generated_ids[0][answer_start:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Generated answer: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. ✓ Modular architecture using components from `edge_glass_modular/src`\n",
    "2. ✓ Configuration-driven design with YAML configs\n",
    "3. ✓ Vision encoder (CLIP + Perceiver + MRL) - frozen or trainable\n",
    "4. ✓ Qwen decoder with LoRA for efficient fine-tuning\n",
    "5. ✓ Real PixMo QA dataset with question-answer pairs\n",
    "6. ✓ Proper token layout with vision prefix\n",
    "7. ✓ Training loop with checkpointing\n",
    "8. ✓ Inference example\n",
    "\n",
    "### Next Steps:\n",
    "- Add validation dataset and evaluation metrics\n",
    "- Implement generation with different sampling strategies\n",
    "- Add evaluation metrics (BLEU, ROUGE, etc.)\n",
    "- Experiment with different vision encoders\n",
    "- Try different decoder models (Llama, Mistral, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
