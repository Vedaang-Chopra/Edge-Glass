{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1a1497",
   "metadata": {},
   "source": [
    "# Comprehensive Alignment Evaluation\n",
    "\n",
    "This notebook evaluates the trained vision-language alignment model against benchmarks from:\n",
    "- **Freeze-Align**: Zero-shot classification, I2T/T2I retrieval\n",
    "- **Matryoshka Multimodal Models**: MRL dimension analysis\n",
    "\n",
    "**Output**: A comparison table summarizing all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c77610",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d32608",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf0f2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Add src to path\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir).endswith('notebooks'):\n",
    "    root_dir = current_dir.parent\n",
    "    os.chdir(root_dir)\n",
    "    sys.path.insert(0, str(root_dir))\n",
    "else:\n",
    "    root_dir = current_dir\n",
    "\n",
    "print(f'Working directory: {Path.cwd()}')\n",
    "\n",
    "from src.config import load_config\n",
    "from src.models.alignment import MultimodalAlignmentModel\n",
    "from src.data.dataset_builder import build_image_datasets_from_parquet\n",
    "from src.data.transforms import get_image_transforms\n",
    "from src.evaluation.benchmark import AlignmentBenchmark\n",
    "from src.evaluation.zero_shot import ZeroShotClassifier\n",
    "from src.evaluation.templates import OPENAI_IMAGENET_TEMPLATES\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe1f43",
   "metadata": {},
   "source": [
    "## 2. Load Configuration & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc758b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/configs/pixmo_alignment_1.yaml...\n",
      "Initializing model...\n",
      "DEBUG: Initial config num_key_value_heads: 4\n",
      "DEBUG: Final config num_key_value_heads: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd10df5765884783a137268d911dd24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = root_dir / 'configs/pixmo_alignment_1.yaml'\n",
    "print(f'Loading config from {config_path}...')\n",
    "config = load_config(str(config_path))\n",
    "\n",
    "# Initialize Model\n",
    "print('Initializing model...')\n",
    "model = MultimodalAlignmentModel(config)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('Model initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4dac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: pixmo_notebooks -> /storage/ice1/1/0/vchopra37/projects/edge_glass/edge_glass_modular/checkpoints/pixmo_alignment/checkpoint_best.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultimodalAlignmentModel:\n\tMissing key(s) in state_dict: \"vision_encoder.encoder.vision_model.embeddings.class_embedding\", \"vision_encoder.encoder.vision_model.embeddings.patch_embedding.weight\", \"vision_encoder.encoder.vision_model.embeddings.position_embedding.weight\", \"vision_encoder.encoder.vision_model.pre_layrnorm.weight\", \"vision_encoder.encoder.vision_model.pre_layrnorm.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.post_layernorm.weight\", \"vision_encoder.encoder.vision_model.post_layernorm.bias\", \"text_encoder.encoder.0.auto_model.embeddings.word_embeddings.weight\", \"text_encoder.encoder.0.auto_model.embeddings.position_embeddings.weight\", \"text_encoder.encoder.0.auto_model.embeddings.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.embeddings.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.relative_attention_bias.weight\", \"text_encoder.encoder.0.auto_model.pooler.dense.weight\", \"text_encoder.encoder.0.auto_model.pooler.dense.bias\", \"decoder.model.base_model.model.model.embed_tokens.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.0.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.0.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.1.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.1.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.2.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.2.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.3.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.3.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.4.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.4.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.5.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.5.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.6.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.6.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.7.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.7.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.8.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.8.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.9.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.9.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.10.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.10.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.11.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.11.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.12.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.12.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.13.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.13.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.14.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.14.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.15.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.15.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.16.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.16.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.17.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.17.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.18.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.18.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.19.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.19.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.20.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.20.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.21.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.21.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.22.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.22.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.23.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.23.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.24.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.24.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.25.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.25.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.26.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.26.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.27.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.27.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.norm.weight\", \"decoder.model.base_model.model.lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUsing checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m checkpoint = torch.load(ckpt_path, map_location=device, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m model.eval()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCheckpoint loaded.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/scratch/projects/edge_glass/edge_glass_env/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for MultimodalAlignmentModel:\n\tMissing key(s) in state_dict: \"vision_encoder.encoder.vision_model.embeddings.class_embedding\", \"vision_encoder.encoder.vision_model.embeddings.patch_embedding.weight\", \"vision_encoder.encoder.vision_model.embeddings.position_embedding.weight\", \"vision_encoder.encoder.vision_model.pre_layrnorm.weight\", \"vision_encoder.encoder.vision_model.pre_layrnorm.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.0.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.1.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.2.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.3.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.4.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.5.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.6.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.7.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.8.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.9.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.10.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.11.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.12.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.13.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.14.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.15.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.16.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.17.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.18.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.19.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.20.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.21.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.22.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.k_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.k_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.v_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.v_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.q_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.q_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.out_proj.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.self_attn.out_proj.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc1.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc1.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.mlp.fc2.bias\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm2.weight\", \"vision_encoder.encoder.vision_model.encoder.layers.23.layer_norm2.bias\", \"vision_encoder.encoder.vision_model.post_layernorm.weight\", \"vision_encoder.encoder.vision_model.post_layernorm.bias\", \"text_encoder.encoder.0.auto_model.embeddings.word_embeddings.weight\", \"text_encoder.encoder.0.auto_model.embeddings.position_embeddings.weight\", \"text_encoder.encoder.0.auto_model.embeddings.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.embeddings.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.0.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.1.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.2.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.3.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.4.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.5.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.6.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.7.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.8.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.9.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.10.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.q.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.q.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.k.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.k.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.v.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.v.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.o.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.attn.o.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.attention.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.intermediate.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.intermediate.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.dense.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.dense.bias\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.LayerNorm.weight\", \"text_encoder.encoder.0.auto_model.encoder.layer.11.output.LayerNorm.bias\", \"text_encoder.encoder.0.auto_model.encoder.relative_attention_bias.weight\", \"text_encoder.encoder.0.auto_model.pooler.dense.weight\", \"text_encoder.encoder.0.auto_model.pooler.dense.bias\", \"decoder.model.base_model.model.model.embed_tokens.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.0.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.0.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.0.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.1.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.1.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.1.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.2.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.2.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.2.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.3.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.3.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.3.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.4.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.4.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.4.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.5.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.5.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.5.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.6.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.6.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.6.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.7.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.7.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.7.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.8.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.8.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.8.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.9.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.9.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.9.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.10.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.10.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.10.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.11.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.11.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.11.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.12.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.12.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.12.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.13.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.13.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.13.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.14.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.14.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.14.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.15.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.15.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.15.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.16.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.16.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.16.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.17.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.17.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.17.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.18.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.18.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.18.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.19.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.19.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.19.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.20.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.20.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.20.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.21.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.21.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.21.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.22.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.22.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.22.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.23.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.23.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.23.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.24.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.24.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.24.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.25.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.25.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.25.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.26.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.26.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.26.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.k_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias\", \"decoder.model.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.gate_proj.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.up_proj.weight\", \"decoder.model.base_model.model.model.layers.27.mlp.down_proj.weight\", \"decoder.model.base_model.model.model.layers.27.input_layernorm.weight\", \"decoder.model.base_model.model.model.layers.27.post_attention_layernorm.weight\", \"decoder.model.base_model.model.model.norm.weight\", \"decoder.model.base_model.model.lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "# Load Best Checkpoint\n",
    "checkpoint_paths = {\n",
    "    'pixmo_notebooks': root_dir / 'checkpoints/pixmo_alignment/checkpoint_best.pt',\n",
    "    'pixmo_outputs': root_dir / 'outputs/pixmo_alignment/checkpoint_best.pt',\n",
    "    'perceiver_mrl': root_dir / 'notebooks/ddsp_scripts/checkpoints/perceiver_mrl_alignment/checkpoint_best.pt',\n",
    "}\n",
    "\n",
    "checkpoint_paths = {k: v for k, v in checkpoint_paths.items() if v.exists()}\n",
    "if not checkpoint_paths:\n",
    "    raise FileNotFoundError('No checkpoints found!')\n",
    "\n",
    "# Use first available\n",
    "ckpt_name = list(checkpoint_paths.keys())[0]\n",
    "ckpt_path = checkpoint_paths[ckpt_name]\n",
    "print(f'Using checkpoint: {ckpt_name} -> {ckpt_path}')\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "model.eval()\n",
    "print('Checkpoint loaded with strict=False.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5ba5c",
   "metadata": {},
   "source": [
    "## 3. Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = config.dataset.image_size\n",
    "val_transforms = get_image_transforms(image_size, is_training=False)\n",
    "\n",
    "datasets = build_image_datasets_from_parquet(\n",
    "    cfg=config,\n",
    "    train_parquet_path=config.dataset.train_parquet,\n",
    "    val_parquet_path=config.dataset.val_parquet,\n",
    "    test_parquet_path=config.dataset.test_parquet,\n",
    "    train_transforms=val_transforms,\n",
    "    val_transforms=val_transforms,\n",
    ")\n",
    "\n",
    "val_dataset = datasets['val']\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Validation batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c175f6a",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(model, loader, device):\n",
    "    vision_embs, text_embs, captions = [], [], []\n",
    "    for batch in tqdm(loader, desc='Extracting embeddings'):\n",
    "        images = batch['image'].to(device)\n",
    "        texts = batch['text']\n",
    "        \n",
    "        # Filter dropped texts\n",
    "        valid_idx = [i for i, t in enumerate(texts) if t]\n",
    "        if not valid_idx:\n",
    "            continue\n",
    "        images = images[valid_idx]\n",
    "        texts = [texts[i] for i in valid_idx]\n",
    "        \n",
    "        outputs = model(images=images, texts=texts, return_embeddings=True)\n",
    "        v_emb = F.normalize(outputs.vision_emb, dim=-1)\n",
    "        t_emb = F.normalize(outputs.text_emb, dim=-1)\n",
    "        \n",
    "        vision_embs.append(v_emb.cpu())\n",
    "        text_embs.append(t_emb.cpu())\n",
    "        captions.extend(texts)\n",
    "    \n",
    "    return torch.cat(vision_embs), torch.cat(text_embs), captions\n",
    "\n",
    "vision_embs, text_embs, captions = extract_embeddings(model, val_loader, device)\n",
    "print(f'Extracted {len(vision_embs)} embeddings of dim {vision_embs.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42f22b",
   "metadata": {},
   "source": [
    "## 5. Retrieval Evaluation (Freeze-Align Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(query_embs, gallery_embs, ks=[1, 5, 10]):\n",
    "    sim = query_embs @ gallery_embs.T\n",
    "    n = sim.shape[0]\n",
    "    max_k = max(ks)\n",
    "    _, topk = sim.topk(max_k, dim=1)\n",
    "    gt = torch.arange(n).unsqueeze(1).expand(-1, max_k)\n",
    "    matches = (topk == gt)\n",
    "    \n",
    "    results = {}\n",
    "    for k in ks:\n",
    "        recall = matches[:, :k].any(dim=1).float().mean().item() * 100\n",
    "        results[f'R@{k}'] = recall\n",
    "    return results\n",
    "\n",
    "print('Computing Retrieval Metrics...')\n",
    "i2t_metrics = compute_recall_at_k(vision_embs, text_embs)\n",
    "t2i_metrics = compute_recall_at_k(text_embs, vision_embs)\n",
    "\n",
    "print(f\"Image-to-Text: {i2t_metrics}\")\n",
    "print(f\"Text-to-Image: {t2i_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cece8",
   "metadata": {},
   "source": [
    "## 6. Zero-Shot Classification (Freeze-Align Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_datasets = ['CIFAR10', 'CIFAR100']\n",
    "datasets_root = './data'\n",
    "\n",
    "zs_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "zs_results = {}\n",
    "benchmark = AlignmentBenchmark(model, device=str(device))\n",
    "\n",
    "for ds_name in zs_datasets:\n",
    "    try:\n",
    "        print(f'\\nLoading {ds_name}...')\n",
    "        dataset = ZeroShotClassifier.load_dataset(ds_name.lower(), root_dir=datasets_root, split='test', transform=zs_transform)\n",
    "        loader = DataLoader(dataset, batch_size=64, num_workers=4, shuffle=False)\n",
    "        \n",
    "        metrics = benchmark.evaluate_zero_shot(\n",
    "            dataset_name=ds_name,\n",
    "            class_names=dataset.classes,\n",
    "            templates=OPENAI_IMAGENET_TEMPLATES,\n",
    "            dataloader=loader\n",
    "        )\n",
    "        zs_results[ds_name] = metrics\n",
    "    except Exception as e:\n",
    "        print(f'Skipping {ds_name}: {e}')\n",
    "        zs_results[ds_name] = {'top1': 0.0, 'top5': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af8b98",
   "metadata": {},
   "source": [
    "## 7. MRL Dimension Analysis (Matryoshka Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval at different embedding dimensions\n",
    "mrl_dims = [64, 128, 256, 512, 1024, 2048, 4096]\n",
    "full_dim = vision_embs.shape[1]\n",
    "mrl_dims = [d for d in mrl_dims if d <= full_dim]\n",
    "\n",
    "mrl_results = {}\n",
    "print('Evaluating MRL performance...')\n",
    "for dim in tqdm(mrl_dims, desc='MRL Dims'):\n",
    "    v_trunc = F.normalize(vision_embs[:, :dim], dim=-1)\n",
    "    t_trunc = F.normalize(text_embs[:, :dim], dim=-1)\n",
    "    \n",
    "    i2t = compute_recall_at_k(v_trunc, t_trunc, ks=[1])\n",
    "    mrl_results[dim] = i2t['R@1']\n",
    "\n",
    "print('MRL Results:')\n",
    "for dim, r1 in mrl_results.items():\n",
    "    print(f'  Dim {dim}: R@1 = {r1:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a81700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MRL Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "dims = list(mrl_results.keys())\n",
    "r1s = list(mrl_results.values())\n",
    "plt.plot(dims, r1s, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "plt.ylabel('R@1 (%)', fontsize=12)\n",
    "plt.title('MRL Performance: R@1 vs Embedding Dimension', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log', base=2)\n",
    "plt.xticks(dims, [str(d) for d in dims])\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/mrl_curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e44f8",
   "metadata": {},
   "source": [
    "## 8. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all metrics into a comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [],\n",
    "    'Value': [],\n",
    "    'Category': []\n",
    "}\n",
    "\n",
    "# Retrieval\n",
    "for k, v in i2t_metrics.items():\n",
    "    comparison_data['Metric'].append(f'I2T {k}')\n",
    "    comparison_data['Value'].append(f'{v:.2f}')\n",
    "    comparison_data['Category'].append('Retrieval')\n",
    "\n",
    "for k, v in t2i_metrics.items():\n",
    "    comparison_data['Metric'].append(f'T2I {k}')\n",
    "    comparison_data['Value'].append(f'{v:.2f}')\n",
    "    comparison_data['Category'].append('Retrieval')\n",
    "\n",
    "# Zero-Shot\n",
    "for ds, metrics in zs_results.items():\n",
    "    comparison_data['Metric'].append(f'{ds} Top-1')\n",
    "    comparison_data['Value'].append(f\"{metrics['top1']:.2f}\")\n",
    "    comparison_data['Category'].append('Zero-Shot')\n",
    "\n",
    "# MRL Best\n",
    "if mrl_results:\n",
    "    best_dim = max(mrl_results, key=mrl_results.get)\n",
    "    comparison_data['Metric'].append(f'MRL Best (dim={best_dim})')\n",
    "    comparison_data['Value'].append(f'{mrl_results[best_dim]:.2f}')\n",
    "    comparison_data['Category'].append('MRL')\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('\\n' + '='*60)\n",
    "print('COMPREHENSIVE EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('='*60)\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('outputs/alignment_evaluation_results.csv', index=False)\n",
    "print('\\nResults saved to outputs/alignment_evaluation_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f5580",
   "metadata": {},
   "source": [
    "## 9. Qualitative Retrieval Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_retrieval_example(idx, top_k=5):\n",
    "    query = vision_embs[idx]\n",
    "    scores = text_embs @ query\n",
    "    top_scores, top_indices = scores.topk(top_k)\n",
    "    \n",
    "    print(f'\\n--- Query {idx} ---')\n",
    "    print(f'Ground Truth: {captions[idx][:100]}...')\n",
    "    print('Top Retrieved:')\n",
    "    for i, (score, ridx) in enumerate(zip(top_scores, top_indices)):\n",
    "        marker = '[GT]' if ridx == idx else ''\n",
    "        print(f'  {i+1}. [{score:.4f}] {captions[ridx][:80]}... {marker}')\n",
    "\n",
    "# Show 3 random examples\n",
    "np.random.seed(42)\n",
    "for idx in np.random.choice(len(captions), 3, replace=False):\n",
    "    show_retrieval_example(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e260f08",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated the alignment model with:\n",
    "- **Retrieval**: Image-to-Text and Text-to-Image R@1, R@5, R@10\n",
    "- **Zero-Shot Classification**: CIFAR-10, CIFAR-100 accuracy\n",
    "- **MRL Analysis**: Performance across embedding dimensions\n",
    "\n",
    "Results are saved to `outputs/alignment_evaluation_results.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}