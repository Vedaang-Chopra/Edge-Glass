
import json
from pathlib import Path

notebook_path = Path("/home/hice1/vchopra37/scratch/projects/edge_glass/edge_glass_modular/notebooks/05_vlm_qualitative_inference.ipynb")

with open(notebook_path, "r") as f:
    nb = json.load(f)

# 1. Inject Imports (eval-score cell or config cell)
# Let's add to the first code cell (imports)
first_code_cell = next(c for c in nb["cells"] if c["cell_type"] == "code")
if "simple_metrics" not in "".join(first_code_cell["source"]):
    first_code_cell["source"].append("\n")
    first_code_cell["source"].append("# Added manual metrics\n")
    first_code_cell["source"].append("from evaluation.simple_metrics import compute_bleu, compute_rouge_l, compute_perplexity\n")

# 2. Fix run_inference (Qualitative)
# Locate cell with `def run_inference`
for cell in nb["cells"]:
    if cell["cell_type"] == "code" and "def run_inference" in "".join(cell["source"]):
        # Rewrite the function completely to be robust
        new_source = [
            "def run_inference(image_source, question, ground_truth=None):\n",
            "    \"\"\"\n",
            "    Runs inference on an image path or PIL image.\n",
            "    \"\"\"\n",
            "    # Load and Plot Image\n",
            "    try:\n",
            "        if isinstance(image_source, str) or isinstance(image_source, Path):\n",
            "            image = Image.open(image_source).convert('RGB')\n",
            "        else:\n",
            "            image = image_source # Allow passing PIL object directly\n",
            "            \n",
            "        plt.figure(figsize=(6,6))\n",
            "        plt.imshow(image)\n",
            "        plt.axis('off')\n",
            "        title = f\"Q: {question}\"\n",
            "        if ground_truth:\n",
            "            title += f\"\\nGT: {ground_truth}\"\n",
            "        plt.title(title)\n",
            "        plt.show()\n",
            "        \n",
            "        # Detect model device\n",
            "        if hasattr(model.qwen.model, \"device\"):\n",
            "             model_device = model.qwen.model.device \n",
            "        else:\n",
            "             model_device = next(model.parameters()).device\n",
            "        \n",
            "        # Transform\n",
            "        transform = get_image_transforms(config.dataset.image_size, is_training=False)\n",
            "        # Validate tensor device\n",
            "        img_tensor = transform(image).unsqueeze(0).to(model_device)\n",
            "        \n",
            "        # Encode\n",
            "        with torch.no_grad():\n",
            "            # Ensure aligned encoder inputs are on its device\n",
            "            enc_device = aligned_model.vision_encoder.device if hasattr(aligned_model.vision_encoder, 'device') else next(aligned_model.vision_encoder.parameters()).device\n",
            "            vision_tokens = aligned_model.vision_encoder(img_tensor.to(enc_device), return_sequence=True).sequence\n",
            "            \n",
            "        # Generate\n",
            "        # Ensure inputs are on model device\n",
            "        inputs = qwen_decoder.tokenizer([question], return_tensors='pt', padding=True).to(model_device)\n",
            "        \n",
            "        # Vision tokens need to be on the same device as inputs for the model forward usually, \n",
            "        # but QwenVLM.generate might handle projection. Let's force vision_tokens to model_device just in case.\n",
            "        vision_tokens = vision_tokens.to(model_device)\n",
            "        \n",
            "        with torch.no_grad():\n",
            "            gen_ids = model.generate(\n",
            "                vision_tokens=vision_tokens,\n",
            "                question_ids=inputs.input_ids,\n",
            "                max_new_tokens=128,\n",
            "                temperature=0.2, # Slight temp for variety in qualitative\n",
            "                do_sample=True\n",
            "            )\n",
            "        \n",
            "        answer = qwen_decoder.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
            "        print(f\"ðŸ¤– Model Answer: {answer}\")\n",
            "        if ground_truth:\n",
            "             print(f\"âœ… Ground Truth: {ground_truth}\")\n",
            "             # Metrics\n",
            "             f1 = compute_f1(answer, ground_truth)\n",
            "             em = compute_exact_match(answer, ground_truth)\n",
            "             bleu = compute_bleu(ground_truth, answer)\n",
            "             rouge = compute_rouge_l(ground_truth, answer)\n",
            "             print(f\"F1: {f1:.2f} | EM: {em} | BLEU-4: {bleu:.2f} | ROUGE-L: {rouge:.2f}\")\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Error: {e}\")\n",
            "        import traceback\n",
            "        traceback.print_exc()\n",
            "\n",
            "# Interactive Helper\n",
            "def check_random_sample(dataset):\n",
            "    idx = np.random.randint(0, len(dataset))\n",
            "    item = dataset[idx]\n",
            "\n",
            "    # Better approach for qualitative: Inverse transform the tensor to show it\n",
            "    img_tensor = item['image']\n",
            "    # Un-normalize (approximate standard mean/std for CLIP/SigLIP usually)\n",
            "    # Assuming CLIP mean/std\n",
            "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
            "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
            "    img = img_tensor * std + mean\n",
            "    img = img.clamp(0, 1).permute(1, 2, 0).numpy()\n",
            "    \n",
            "    # Convert to PIL for the function\n",
            "    pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
            "    \n",
            "    q_text = qwen_decoder.tokenizer.decode(item['question_ids'], skip_special_tokens=True)\n",
            "    a_text = item['answer']\n",
            "    \n",
            "    run_inference(pil_img, q_text, ground_truth=a_text)\n",
            "\n",
            "print(\"Running a single random sample check...\")\n",
            "check_random_sample(val_dataset)\n"
        ]
        cell["source"] = new_source
        
# 3. Fix evaluate_dataset (Quantitative) to add metrics and fix device
for cell in nb["cells"]:
    if cell["cell_type"] == "code" and "def evaluate_dataset" in "".join(cell["source"]):
        new_source = [
            "def evaluate_dataset(max_samples=None):\n",
            "    scores = {'params': [], 'f1': [], 'em': [], 'bleu': [], 'rouge': []}\n",
            "    count = 0\n",
            "    \n",
            "    print(f\"Starting evaluation (Max samples: {max_samples if max_samples else 'All'})...\")\n",
            "    \n",
            "    # Detect model device\n",
            "    if hasattr(model.qwen.model, \"device\"):\n",
            "         model_device = model.qwen.model.device \n",
            "    else:\n",
            "         model_device = next(model.parameters()).device\n",
            "    \n",
            "    enc_device = aligned_model.vision_encoder.device if hasattr(aligned_model.vision_encoder, 'device') else next(aligned_model.vision_encoder.parameters()).device\n",
            "\n",
            "    for batch in tqdm(val_loader):\n",
            "        for item in batch:\n",
            "            if item is None: continue\n",
            "            \n",
            "            image = item['image'].unsqueeze(0).to(enc_device)\n",
            "            question_ids = item['question_ids'].unsqueeze(0).to(model_device)\n",
            "            ground_truth = item['answer']\n",
            "            \n",
            "            # Encode Vision\n",
            "            with torch.no_grad():\n",
            "                vision_tokens = aligned_model.vision_encoder(image, return_sequence=True).sequence\n",
            "                vision_tokens = vision_tokens.to(model_device)\n",
            "            \n",
            "            # Generate Answer\n",
            "            with torch.no_grad():\n",
            "                gen_ids = model.generate(\n",
            "                    vision_tokens=vision_tokens,\n",
            "                    question_ids=question_ids,\n",
            "                    max_new_tokens=64,\n",
            "                    temperature=0.0, # Greedy for evaluation\n",
            "                    do_sample=False\n",
            "                )\n",
            "            \n",
            "            # Decode\n",
            "            predicted_answer = qwen_decoder.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
            "            \n",
            "            # Compute Metrics\n",
            "            em = compute_exact_match(predicted_answer, ground_truth)\n",
            "            f1 = compute_f1(predicted_answer, ground_truth)\n",
            "            bleu = compute_bleu(ground_truth, predicted_answer)\n",
            "            rouge = compute_rouge_l(ground_truth, predicted_answer)\n",
            "            \n",
            "            scores['em'].append(em)\n",
            "            scores['f1'].append(f1)\n",
            "            scores['bleu'].append(bleu)\n",
            "            scores['rouge'].append(rouge)\n",
            "            \n",
            "            count += 1\n",
            "            if max_samples and count >= max_samples:\n",
            "                break\n",
            "        \n",
            "        if max_samples and count >= max_samples:\n",
            "            break\n",
            "            \n",
            "    avg_em = sum(scores['em']) / len(scores['em'])\n",
            "    avg_f1 = sum(scores['f1']) / len(scores['f1'])\n",
            "    avg_bleu = sum(scores['bleu']) / len(scores['bleu'])\n",
            "    avg_rouge = sum(scores['rouge']) / len(scores['rouge'])\n",
            "    \n",
            "    print(f\"\\nResults ({count} samples):\")\n",
            "    print(f\"Average EM: {avg_em:.4f}\")\n",
            "    print(f\"Average F1: {avg_f1:.4f}\")\n",
            "    print(f\"Average BLEU-4: {avg_bleu:.4f}\")\n",
            "    print(f\"Average ROUGE-L: {avg_rouge:.4f}\")\n",
            "    return avg_em, avg_f1, avg_bleu, avg_rouge\n",
            "    \n",
            "# Run small eval to test\n",
            "evaluate_dataset(max_samples=20)\n"
        ]
        cell["source"] = new_source

with open(notebook_path, "w") as f:
    json.dump(nb, f, indent=4)
print("Successfully modified notebook with v2 fixes.")
